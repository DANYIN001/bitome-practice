{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying ALE SNPs and Essential Genes with Bitome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "BITOME_KB_PATH = Path('..', 'bitome-kb')\n",
    "sys.path.append(BITOME_KB_PATH.absolute().as_posix())\n",
    "\n",
    "from bitome.core import Bitome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('..', 'data', 'classification')\n",
    "LOCAL_CACHE_PATH = Path('..', 'local_cache')\n",
    "FIG_PATH = Path('..', 'figures', 'figure_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Prepared Bitome Knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitome = Bitome.init_from_file(Path(LOCAL_CACHE_PATH, 'bitome.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALE SNPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Feature Matrix\n",
    "\n",
    "Prepare versions that include/exclude the sequence features, as well as include ONLY the sequence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_snp shape: (3349, 1634)\n"
     ]
    }
   ],
   "source": [
    "# load the locus tags and SNP labels for training\n",
    "snp_train_labels = pd.read_csv(Path(DATA_PATH, 'snp_train.csv'), index_col=0, squeeze=True)\n",
    "\n",
    "snp_y = snp_train_labels.values\n",
    "\n",
    "# extract the sub-matrix for each locus tag; ensure that they are all in the same \"orientation\";\n",
    "# i.e. flip the reverse-strand matrices so that the translation start site is always on the \"left\"\n",
    "snp_genes = []\n",
    "snp_gene_sub_matrices = []\n",
    "for locus_tag in snp_train_labels.index:\n",
    "    \n",
    "    # find the Gene object from the Bitome KB\n",
    "    for g in bitome.genes:\n",
    "        if g.locus_tag == locus_tag:\n",
    "            gene = g\n",
    "            snp_genes.append(gene)\n",
    "            break\n",
    "            \n",
    "    # pull out the relevant portion of the matrix\n",
    "    location = gene.location\n",
    "    gene_range = location.start.position, location.end.position\n",
    "    sub_matrix = bitome.extract(column_range=gene_range).tocsc()\n",
    "    \n",
    "    strand = location.strand\n",
    "    if strand == -1:\n",
    "        sub_matrix = sub_matrix[:, ::-1]\n",
    "    \n",
    "    snp_gene_sub_matrices.append(sub_matrix)\n",
    "    \n",
    "# now we can sum across the positions\n",
    "X_snp_sum = np.array([np.asarray(sub_mat.sum(axis=1)).flatten() for sub_mat in snp_gene_sub_matrices])\n",
    "print(f'X_snp shape: {X_snp_sum.shape}')\n",
    "\n",
    "# also make sure to scale this summed matrix to ensure more equal feature importances\n",
    "scaler = StandardScaler()\n",
    "X_snp_sum = scaler.fit_transform(X_snp_sum)\n",
    "\n",
    "# prepare final X_matrices for each of our desired feature sets\n",
    "X_to_try = {\n",
    "    'bitome': X_snp_sum,\n",
    "    'seq_only': X_snp_sum[:, :8],\n",
    "    'no_seq': X_snp_sum[:, 8:],\n",
    "    'shuffled': np.random.permutation(X_snp_sum)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_try = {\n",
    "    'LR': LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='saga',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'SVM': LinearSVC(\n",
    "        penalty='l1',\n",
    "        class_weight='balanced',\n",
    "        dual=False,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ),\n",
    "    'RF': RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ),\n",
    "    'NN': MLPClassifier(verbose=1),\n",
    "    'XGBoost': XGBClassifier(verbose=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "LR: bitome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   13.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: seq_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: no_seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   11.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "SVM: bitome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   42.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: seq_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: no_seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   41.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "RF: bitome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: seq_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: no_seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "NN: bitome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   24.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: seq_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: no_seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   26.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   29.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "XGBoost: bitome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   31.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: seq_only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: no_seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   34.5s finished\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(\n",
    "    columns=['model', 'x_matrix', 'train_auc', 'test_auc']\n",
    ")\n",
    "\n",
    "N_CV = 5\n",
    "\n",
    "for model_name, model in models_to_try.items():\n",
    "    print(f'{model_name}')\n",
    "    \n",
    "    for X_name, X in X_to_try.items():\n",
    "        print(f'{model_name}: {X_name}')\n",
    "        \n",
    "        cv_result = cross_validate(\n",
    "            model,\n",
    "            X,\n",
    "            y=snp_y,\n",
    "            cv=N_CV,\n",
    "            scoring='roc_auc',\n",
    "            return_train_score=True,\n",
    "            verbose=1,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        sub_result_df = pd.DataFrame(\n",
    "            data={\n",
    "                'model': [model_name] * N_CV,\n",
    "                'x_matrix': [X_name] * N_CV,\n",
    "                'train_auc': cv_result['train_score'],\n",
    "                'test_auc': cv_result['test_score']\n",
    "            }\n",
    "        )\n",
    "\n",
    "        result_df = result_df.append(sub_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result df so we don't have to run this initial screen again later\n",
    "result_df.to_csv(Path(DATA_PATH, 'model_selection_snp.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKIP TO HERE IF RESULT_DF UNCHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(Path(DATA_PATH, 'model_selection_snp.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14ca36590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.swarmplot(x='model', y='train_auc', data=result_df, hue='x_matrix', dodge=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14d331210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.swarmplot(x='model', y='test_auc', data=result_df, hue='x_matrix', dodge=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "We're getting quite nice performance from all of these models; let's optimize all of their hyperparameters and see where we get\n",
    "\n",
    "NOTE: interestingly, the SVM and NN actually perform better out of the box with just the sequence features\n",
    "\n",
    "Let's use the full bitome features for these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameter Distributions to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_models = {\n",
    "    'LR': {\n",
    "        'model': LogisticRegression(\n",
    "            solver='saga',\n",
    "            class_weight='balanced',\n",
    "            verbose=1\n",
    "        ),\n",
    "        'param_dists': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': np.arange(0.01, 5, 0.05)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': LinearSVC(\n",
    "            class_weight='balanced',\n",
    "            dual=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        'param_dists': {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'loss': ['hinge', 'squared_hinge'],\n",
    "            'C': np.arange(0.01, 5, 0.05)\n",
    "        }\n",
    "    },\n",
    "    'RF': {\n",
    "        'model': RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            verbose=1\n",
    "        ),\n",
    "        'param_dists': {\n",
    "            'n_estimators': np.arange(50, 250, 10),\n",
    "            'max_depth': np.arange(3, 15),\n",
    "            'min_samples_split': np.arange(0.001, 0.02, 0.001),\n",
    "            'min_samples_leaf': np.arange(5, 15)\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(verbose=1),\n",
    "        'param_dists': {\n",
    "            'n_estimators': np.arange(50, 250, 10),\n",
    "            'max_depth': np.arange(3, 15),\n",
    "            'learning_rate': np.arange(0.1, 0.9, 0.05)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperoptimization Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/.virtualenvs/bitome/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/.virtualenvs/bitome/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed:   33.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 220 out of 220 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed: 20.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { verbose } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperopt_results = {}\n",
    "\n",
    "for model_name, model_hyperopt_dict in hyperopt_models.items():\n",
    "    print(model_name)\n",
    "\n",
    "    random_search_hyperopt = RandomizedSearchCV(\n",
    "        model_hyperopt_dict['model'],\n",
    "        model_hyperopt_dict['param_dists'],\n",
    "        n_iter=25,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    random_search_hyperopt.fit(X_snp_sum, snp_y)\n",
    "    \n",
    "    hyperopt_results_df = pd.DataFrame(random_search_hyperopt.cv_results_).sort_values(\n",
    "        by='mean_test_score',\n",
    "        ascending=False\n",
    "    )\n",
    "    hyperopt_results[model_name] = hyperopt_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the hyperopt results into a DF, making sure to add a 'model' column to track which model is which\n",
    "dfs_with_models = []\n",
    "for model_name, model_hyperopt_df in hyperopt_results.items():\n",
    "    model_hyperopt_df['model'] = [model_name] * model_hyperopt_df.shape[0]\n",
    "    dfs_with_models.append(model_hyperopt_df)\n",
    "    \n",
    "hyperopt_result_df = pd.concat(dfs_with_models, axis=0, ignore_index=True)\n",
    "hyperopt_result_df.to_csv(Path(DATA_PATH, 'hyperopt_snp_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKIP TO HERE IF ALREADY RAN FIRST ROUND OF HYPEROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_result_df = pd.read_csv(Path(DATA_PATH, 'hyperopt_snp_1.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14dffdfd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.swarmplot(x='model', y='mean_test_score', data=hyperopt_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO LR and the Random Forest are turned in well, and XGBoost has some potential; let's continue with those; which parameters were best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.21000000000000002}</td>\n",
       "      <td>0.783283</td>\n",
       "      <td>0.921459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.41000000000000003}</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>0.928586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.51}</td>\n",
       "      <td>0.775071</td>\n",
       "      <td>0.929970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.6100000000000001}</td>\n",
       "      <td>0.774129</td>\n",
       "      <td>0.930897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'penalty': 'l1', 'C': 1.11}</td>\n",
       "      <td>0.771215</td>\n",
       "      <td>0.932906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        params  mean_test_score  \\\n",
       "0  {'penalty': 'l1', 'C': 0.21000000000000002}         0.783283   \n",
       "1  {'penalty': 'l1', 'C': 0.41000000000000003}         0.776423   \n",
       "2                 {'penalty': 'l1', 'C': 0.51}         0.775071   \n",
       "3   {'penalty': 'l1', 'C': 0.6100000000000001}         0.774129   \n",
       "4                 {'penalty': 'l1', 'C': 1.11}         0.771215   \n",
       "\n",
       "   mean_train_score  \n",
       "0          0.921459  \n",
       "1          0.928586  \n",
       "2          0.929970  \n",
       "3          0.930897  \n",
       "4          0.932906  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_result_df[\n",
    "    hyperopt_result_df['model'] == 'LR'\n",
    "].sort_values(by='mean_test_score', ascending=False)[['params', 'mean_test_score', 'mean_train_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LR, it seems clear that strong L1 regularization helps the model; there's a little bit of overfitting but it's not too too bad. \n",
    "\n",
    "What about the RF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.778335</td>\n",
       "      <td>0.877746</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.777368</td>\n",
       "      <td>0.887718</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.777358</td>\n",
       "      <td>0.842895</td>\n",
       "      <td>190.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.777280</td>\n",
       "      <td>0.858451</td>\n",
       "      <td>180.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.776975</td>\n",
       "      <td>0.839256</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.776802</td>\n",
       "      <td>0.879036</td>\n",
       "      <td>130.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.775828</td>\n",
       "      <td>0.843714</td>\n",
       "      <td>140.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.775810</td>\n",
       "      <td>0.831703</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.775620</td>\n",
       "      <td>0.854091</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.775521</td>\n",
       "      <td>0.832391</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.775183</td>\n",
       "      <td>0.824661</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.774761</td>\n",
       "      <td>0.835937</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.774585</td>\n",
       "      <td>0.822461</td>\n",
       "      <td>140.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.774511</td>\n",
       "      <td>0.833021</td>\n",
       "      <td>200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.774320</td>\n",
       "      <td>0.830001</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.774001</td>\n",
       "      <td>0.818196</td>\n",
       "      <td>210.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.773969</td>\n",
       "      <td>0.816196</td>\n",
       "      <td>90.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.773753</td>\n",
       "      <td>0.813761</td>\n",
       "      <td>230.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.772051</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>90.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.803757</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.807245</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.771626</td>\n",
       "      <td>0.802738</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.770042</td>\n",
       "      <td>0.797634</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.767974</td>\n",
       "      <td>0.801604</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.766349</td>\n",
       "      <td>0.795352</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  mean_train_score  param_n_estimators  param_max_depth  \\\n",
       "50         0.778335          0.877746               220.0             10.0   \n",
       "51         0.777368          0.887718               100.0             13.0   \n",
       "52         0.777358          0.842895               190.0              9.0   \n",
       "53         0.777280          0.858451               180.0             13.0   \n",
       "54         0.776975          0.839256                80.0              9.0   \n",
       "55         0.776802          0.879036               130.0             13.0   \n",
       "56         0.775828          0.843714               140.0              8.0   \n",
       "57         0.775810          0.831703                90.0              7.0   \n",
       "58         0.775620          0.854091                50.0             13.0   \n",
       "59         0.775521          0.832391               120.0              8.0   \n",
       "60         0.775183          0.824661               190.0              7.0   \n",
       "61         0.774761          0.835937                50.0              8.0   \n",
       "62         0.774585          0.822461               140.0              7.0   \n",
       "63         0.774511          0.833021               200.0              8.0   \n",
       "64         0.774320          0.830001               150.0              7.0   \n",
       "65         0.774001          0.818196               210.0              6.0   \n",
       "66         0.773969          0.816196                90.0              6.0   \n",
       "67         0.773753          0.813761               230.0              5.0   \n",
       "68         0.772051          0.805157                90.0              4.0   \n",
       "69         0.771930          0.803757               110.0              4.0   \n",
       "70         0.771700          0.807245                70.0              5.0   \n",
       "71         0.771626          0.802738               190.0              4.0   \n",
       "72         0.770042          0.797634               210.0              3.0   \n",
       "73         0.767974          0.801604                50.0              4.0   \n",
       "74         0.766349          0.795352                70.0              3.0   \n",
       "\n",
       "    param_learning_rate  \n",
       "50                  NaN  \n",
       "51                  NaN  \n",
       "52                  NaN  \n",
       "53                  NaN  \n",
       "54                  NaN  \n",
       "55                  NaN  \n",
       "56                  NaN  \n",
       "57                  NaN  \n",
       "58                  NaN  \n",
       "59                  NaN  \n",
       "60                  NaN  \n",
       "61                  NaN  \n",
       "62                  NaN  \n",
       "63                  NaN  \n",
       "64                  NaN  \n",
       "65                  NaN  \n",
       "66                  NaN  \n",
       "67                  NaN  \n",
       "68                  NaN  \n",
       "69                  NaN  \n",
       "70                  NaN  \n",
       "71                  NaN  \n",
       "72                  NaN  \n",
       "73                  NaN  \n",
       "74                  NaN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_result_df[\n",
    "    hyperopt_result_df['model'] == 'RF'\n",
    "].sort_values(by='mean_test_score', ascending=False)[[\n",
    "    'mean_test_score', 'mean_train_score', 'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "    'param_min_samples_leaf'\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little harder to just eyeball, but we're seeing pretty similar performance overall; however, less overfitting with the lower max depth values; min_samples_ don't seem to matter much\n",
    "\n",
    "And XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.786085</td>\n",
       "      <td>0.995607</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.779597</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>210.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.778118</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.776815</td>\n",
       "      <td>0.975430</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.771745</td>\n",
       "      <td>0.999698</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.765746</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.762215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.762092</td>\n",
       "      <td>0.997953</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.761421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.759373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.757918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.753492</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.753175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.749427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.746550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.743013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>190.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.742969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.738723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.734944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.734492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>190.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.734116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>230.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.730795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>210.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.729797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.729405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.720163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>210.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  mean_train_score  param_n_estimators  param_max_depth  \\\n",
       "75         0.786085          0.995607                90.0              8.0   \n",
       "76         0.779597          0.999955               210.0              7.0   \n",
       "77         0.778118          0.999126               200.0              5.0   \n",
       "78         0.776815          0.975430               120.0              3.0   \n",
       "79         0.771745          0.999698                80.0              6.0   \n",
       "80         0.765746          0.999648               130.0              5.0   \n",
       "81         0.762215          1.000000               150.0              9.0   \n",
       "82         0.762092          0.997953                60.0              5.0   \n",
       "83         0.761421          1.000000               160.0              9.0   \n",
       "84         0.759373          1.000000               140.0              9.0   \n",
       "85         0.757918          1.000000                90.0             13.0   \n",
       "86         0.753492          0.999999               210.0              3.0   \n",
       "87         0.753175          1.000000               110.0              6.0   \n",
       "88         0.749427          1.000000               110.0             10.0   \n",
       "89         0.746550          1.000000               160.0              5.0   \n",
       "90         0.743013          1.000000               190.0              5.0   \n",
       "91         0.742969          1.000000               240.0              4.0   \n",
       "92         0.738723          1.000000               170.0              7.0   \n",
       "93         0.734944          1.000000                50.0             14.0   \n",
       "94         0.734492          1.000000               190.0              8.0   \n",
       "95         0.734116          1.000000               230.0              7.0   \n",
       "96         0.730795          1.000000               210.0              9.0   \n",
       "97         0.729797          1.000000                90.0             14.0   \n",
       "98         0.729405          1.000000               180.0              9.0   \n",
       "99         0.720163          1.000000               210.0              9.0   \n",
       "\n",
       "    param_learning_rate  \n",
       "75                 0.10  \n",
       "76                 0.10  \n",
       "77                 0.15  \n",
       "78                 0.30  \n",
       "79                 0.30  \n",
       "80                 0.25  \n",
       "81                 0.20  \n",
       "82                 0.45  \n",
       "83                 0.20  \n",
       "84                 0.25  \n",
       "85                 0.30  \n",
       "86                 0.55  \n",
       "87                 0.50  \n",
       "88                 0.45  \n",
       "89                 0.55  \n",
       "90                 0.60  \n",
       "91                 0.65  \n",
       "92                 0.75  \n",
       "93                 0.60  \n",
       "94                 0.65  \n",
       "95                 0.65  \n",
       "96                 0.50  \n",
       "97                 0.70  \n",
       "98                 0.70  \n",
       "99                 0.75  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_result_df[\n",
    "    hyperopt_result_df['model'] == 'XGBoost'\n",
    "].sort_values(by='mean_test_score', ascending=False)[[\n",
    "    'mean_test_score', 'mean_train_score', 'param_n_estimators', 'param_max_depth', 'param_learning_rate'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these models appear significantly overfit; let's drop XGBoost and continue with LR and RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperoptimization Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_models_2 = {\n",
    "    'LR': {\n",
    "        'model': LogisticRegression(\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            class_weight='balanced',\n",
    "            verbose=1\n",
    "        ),\n",
    "        'param_dists': {\n",
    "            'C': np.arange(0.0001, 0.05, 0.001)\n",
    "        }\n",
    "    },\n",
    "    'RF': {\n",
    "        'model': RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            verbose=1\n",
    "        ),\n",
    "        'param_dists': {\n",
    "            'n_estimators': np.arange(50, 500, 10),\n",
    "            'max_depth': np.arange(2, 8)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed:  3.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/.virtualenvs/bitome/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=4)]: Done 125 out of 125 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 280 out of 280 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "hyperopt_results_2 = {}\n",
    "\n",
    "for model_name, model_hyperopt_dict in hyperopt_models_2.items():\n",
    "    print(model_name)\n",
    "\n",
    "    random_search_hyperopt = RandomizedSearchCV(\n",
    "        model_hyperopt_dict['model'],\n",
    "        model_hyperopt_dict['param_dists'],\n",
    "        n_iter=25,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    random_search_hyperopt.fit(X_snp_sum, snp_y)\n",
    "    \n",
    "    hyperopt_results_df = pd.DataFrame(random_search_hyperopt.cv_results_).sort_values(\n",
    "        by='mean_test_score',\n",
    "        ascending=False\n",
    "    )\n",
    "    hyperopt_results_2[model_name] = hyperopt_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the hyperopt results into a DF, making sure to add a 'model' column to track which model is which\n",
    "dfs_with_models_2 = []\n",
    "for model_name, model_hyperopt_df in hyperopt_results_2.items():\n",
    "    model_hyperopt_df['model'] = [model_name] * model_hyperopt_df.shape[0]\n",
    "    dfs_with_models_2.append(model_hyperopt_df)\n",
    "    \n",
    "hyperopt_result_df_2 = pd.concat(dfs_with_models_2, axis=0, ignore_index=True)\n",
    "hyperopt_result_df_2.to_csv(Path(DATA_PATH, 'hyperopt_snp_2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_result_df_2 = pd.read_csv(Path(DATA_PATH, 'hyperopt_snp_2.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14da28150>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEECAYAAAAoDUMLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3xW5f3/8dcng43MsIQwZFoVpBEUVNBSQa1WS22tIs5ibW21/bZqq9a2v69ttY7Wukrr10Kr4sLRqoiiDMGVMFzsKcoIYSYhZH1+f1wnO5Ib4c4N5P18PPLIfV9n5JPA436f6zrnXMfcHRERkaREFyAiIgcHBYKIiAAKBBERiSgQREQEUCCIiEgkJdEF7I/27dt7jx49El2GiMghJSsra4u7p1VvP6QDoUePHmRmZia6DBGRQ4qZra2tXUNGIiICKBBERCSiQBAREUCBICIiEQWCiIgACoR6t2JzLh99tqNK247dRXzy+U5KSismGnR3NuzYTVFJaX2XWO92F5awZkselSdaLCgqIXPNVrbmFSawMpGG5ZC+7PRQ4u5c/+RCXlj4OQCD01sz+cqhvPzhBn79wkcUFJXSrW1TJl0+BAeu/lcWKzbnktayMfd+ZxDDe7fj/jdW8HTWeto0b8QNo/sxvHd7Ply/g4dnr2RPUQkXn9id0/p1oLTUeX3xJlZvyeP0/h3o07HlF9a1Pb+QXQXFdGvbDICp89fzx1eWsGN3ERee0I1fn/MVkpOsfP2Fn25n6cadDDuqffk2b6/MYeayzfTt0JJvDupCSnISz2atZ8r76ziiSSrXnt6b49PbMP3jjdzz2jJ2FRRz0dB0fnRab175cAM3PPsBuwqK6ZXWnEcuPYFdBUVc/uj75OQV0ig5idvPP4YLMrrF7x9HRACwQ3n664yMDD9U7kN4a/kWxj3ybpW2G8f04/43VpBXWFLeduYxncgrLGH2suzyti6tmnDdqD7c+OyH5W1NUpN48drhnP/AvPLtkwyevWYYj727jmey1gOQkmT849IMRvbrUKOmP7++jAfeXEFRiXNir7bcds7RnH3fW1TqqHD7+cdw8dDuANwzfSn3vbECgNRkY+L4DLbs2sMvnvmgfP2xg7vyjeM6c/k/3y9va94omad+cBLfvH8uxZV2fu93B/Lb/3zC9vyi8rYzju5I7p5i5q3MKW87okkK798yisYpyXv7E4tIjMwsy90zqrerh1BPPt+xu0bb6uy8KmEAsDYnv8Ywyec7Cpi1NLtKW0FRKf9+e22V7Usdns5az7Pz15e3FZc6E2evokPLJtz+8ies2ZLPmGM6MXZwV/78+vLy9d5ZtZX731hZJQwA5q/dzsVDu5O3p5iHZ68qby8qce6bsZzC4qpDWs8tWI9TdSd5hSVMee/TKmEAMHPp5iphALAyO7dGDTsLisktKKZxCwWCSDzF5RyCmaWa2b/N7F0zm2dm/astvyJqzzSz66K2Vmb2spm9Y2avm1mneNSWKKf370DLxhX5m2Rw4dB0ju58RJX1zjymE6f1r3pH+eD01gzs1rpKW5LBoG5tavyczkc0oXqnr6TUuXLS+8xdkcNn23fzyFureWDmihrbFpeWVhkeAsjo0YbNOwsoKimluNr5jN2FJTRKqfpfKCUpiR7tmtXY94m92tZoG9KzHV/pUvX3//rRnTjnuM5V2ob3bke7Fo1rbC8iB1a8egjjgRx3H2dmpwJ3A2cDmFkz4BagX7TuYjN7DLgOeM3d7zWz8cCtwI/iVF+9a9+iMVOuPpG/z15FfmEJ407szuD0NvzfZSdwz2tLWZmdx6gBHZlwai8KikpISUpi7ootDOhyBLecPYA2zRrxwfodvPLRBpo3TuGG0f04f/CRzFmxhecWfAbAkJ5tufKUnizfnMuLi8K5iiSDM4/txG9e/KRKPWtz8mjZJIVdBcXlbWOO6cSw3u35x5xV7MgvYkTfNP7y+jJ+OfVD+nRowch+HXhjyeby9S85qTsdWzbhB//OKj/6v3x4D648uRfvrd7GWyu2kJxkXHVyT84+rgubdu7h3teXsbuwhG8NPpLvZnTjtH4duGPaEpZtyuX0/mlc97W+JCcZRzRNZdaybPp3asm1p/WJ9z+PiBCncwhm9gTwsLvPMjMD1rv7kdGy5sBHwPFAKbAQGAS8Clzo7mvNrD3whrsft7efcyidQzhQdhUU0TglucqR+ZoteewuKmFA1NsoLinllY82lp9U7tm+OUN/P4PcPRUf/t8bks73hnTjL68vZ2t+Ib3aN+fVjzeRu6eYgV1bMXH8V/n2w2/z6daKoa5hR7Xj3IFdWLJxFyP6pXFadF5iVXYub63YQt+OLTmxV7vy9dfl5NOscTLtKx3dF5WUUlLqNEnV8I9IotT3OYR2QA6Au7uZuZkluXupu+eZ2SvA4mjdGe6+08zKtwF2AK1r7hbMbAIwASA9PT1O5R+8WjZJrdHWo33zKu9TkpM4Z2CXKm13XXActzz/EVtyCxnUrTX5e4q5a/oyzh3YhVEDOjD09zPYE50PWLR+B3dPX1YlDACWbcrlwiE1/+a90lrQK61Fjfb0WoaOUpOTUBaIHJziFQhbgVYAUQ/B3b00ej8C6A/0ABx40czOrLRNLtAWyK65W3D3icBECD2EONV/2BlzTGdGDejItvxCzn9wHgs/3Q7A7GXZ/HRUn/IwKLMmJ5+vdm9D1tpt5W0j+taYLVdEDiPxujFtBjA2ej0amFNpWWNgh7vvcfdCwge/VdvmfMIQkhxAKclJrM3JZ/22qkf+Cz7dTpdWTaq0fa1/B+6/6HjOOrYT6W2b8b0h3fjNuUfXZ7kiUs/i1UOYBEw2s0wgDxhnZjcDWYQP+jPMbB5QAswHXgHeBqaY2ThgC3BRnGpr0Dq3bkqSUeXSzm5tmnHzWQO4Y9pS1m/L58xjOnPVKb1ITjIevPiriStWROqVbkxrgO59bRl/fWM5pQ7d2zXjxJ7tWLc1n1P6tmfCKb1ISdaMJiKHM92YJuV++vW+XDikG5t37uGOaYt5MvNTAN5elcPO3cXcdGb/OvYgIocjHQo2UJ1bNaV7u2bMW7m1SvtLH36eoIpEJNEUCA1Y88YptGlW9TLWrq1rXioqIg2DAqEBS01O4jfnfoUmqeG/QfsWjfjVWQMSXJWIJIrOITRw3xx0JCP7dmDVllyO7nKEZhQVacDUQ2jgcvcU839zV/PQzJU8lbme0upTjYpIg6EeQgN33RMLmBFNWDf9k03k5O7h+lF9E1yViCSCeggN2I78ovIwKFM2c6ocogrz4JMXYc3cRFcSm9JSWPcubPig7nWrc4fsZVCwo+51Dya52VC8J9FV1Eo9hAasSaOkGlNgd2ip5w4csrZ/Co+cAbuiS4f7fwMufAxWzYQ590BJEQy9Gr5yHhTmw/zJsG11WK/nKfVfb8FOmHQObFgY3g84By6YDEkxHKdu/xQeuwCyF0NqMxj9e8i4PL717s3mxbB9HXQfDo1bQFEBvHUPrHsHug2Bk38GxQXw1HhYMweatIIxf4RBB9eEDAqEw9SugiKeW/AZO3cXce7AI2udebRxSjK/OmsAtz7/EcWlTsvGKdwwJtyUVlxSyufbCziyTdMaD82Rg9S7D1eEAcCS/8LHL8DUq6AkegrfurfhiOnw5u9h1ZsV210wKXyYTbsRPn0vfIiNuQNapIWj8CX/hdbpcPQ3IbnmjLtfyvzJFWEAsPg/sPIN2LgIFj4BLTrA6bdC95NgxQxY+Bg0bQvDroVZd4YwACjKh2k3haBrWvOhUftl8xJo2hpaRs/r2vgRvPNg+HDPuBJ6DIfXboO5fw7Lm7WDy16CeffDwn+HttWzQoA1axvCAEKv5j/XQ98xof0goUA4DBUWlzL2oXks25QLwEMzV/LCtcPp3aElhcWlFJWU0jx6etv3hqRzWr8OLNu0i+PTW9OySSrvr9nKtY/PZ9POPRzZuikPjRvMcV1rnY1cDiYF22u2rZ5VEQYAOCx8vCIMymQ+EtqXR3NK7vgU9uTCsB/Dv86H0uhRpx9Nhe89Diteh/cfgZQmMPwn0OX4sPzzhbB7K/Q4pe7gyN1Ys+2jZ2HR4+F1zvLQC/jWRJhyUagdQji16lZ1u+IC2PFZbIFQUgS7NkKrrmDRwc6uTeFv0r4PHPlV2L09/Oz174ElwUnXhq9Hz4Q9O8M2n7wAFz0F8+6r2Hd+Dsy+C5a+XPVnfjwV0k+qVsceyFkBzYbUXXM9USAchuYszy4PAwjPNH783U/p0b4Zf3p1KXl7ivnGcV340wXHsSW3kNtf+oTFG3ZxSp/23HRmf2569gM27QxjnJ9t382tz3/EC9eenKhfR2J1/CXhyNqj52y36QF9R4cP+8ra9Q4fcl5pyvPU5rDitarrrXwDklIqwgBg6Uvw0XPw7BUV2y+fDj/Ogldvho+eiX52T7ji1XD0+8GTsGVZOBruPqxiX8eMhXcegtJoyLJJqxAmlRXugvcmQuXndO/aAEedHj6sy7TtBUW74bOs8IFeZvPiEBQ9hkNq0zB8NnUC5G4Kf4fvPhZ+5r++BcXRLMDDrw/DUGX799LwoZ/StCIMINT98XNV/44AedkhbLYsq2hr1RX6fD0EdJnmadB5IAcTBcJhqLYhnvzCYm578ePy5y2/uOhzjuvaiv8s+pxF68NJudVb8igpLWXVlrwq267YnFt9d3IwSj8xfAgveiIMXQz5fvjQybgSsh4NH1wDzoUhE8K5g/f/EbZLbQan/Cx80FYewun4ldp/zsrXq34IFubCu3+rCAMI+3/34fD94+dC29y/wNhHQjC88f9g7VzoPQqSUsOwzEk/CifEl02r2I8lQdujavZoMq6A9n1h8YvQunsYv39kVFjW81S4+Fl4/bYwvAPQohNc9l944doQBhCOzqfdCFhFGAC8fX/4O1XnxTXbOh0LnQdV/bsdPw6at4cnx4dAa9QSzrwzhFjBztALat0NRv0WUg6uc3YKhMPQyb3bM7BbaxZFD8Fp3SyVozsfQfWJbRes214eBmVmLt3CadWenXz6gI5xr1kOkG4nhK/KvnEPjLwpDJW0OjK0nX03HPNt2LoKen8tjJGf+1d4+jLYujIcPZ/71zDWvXx6RS+h39nQqZYn29b2wbZ9HXz8fNW2dx+G5a/BB1PC+40fQp/R8M37wzj7EV3CcNOaOeGDdNRt0HMErHoj1Aqh1/Pe3wGHM/8UjsSf/0HFz1g9O4TdOw9VtOVuhNl3hqGwyjYvCUfvlZUWhw/vj6dWtDVpHYaMNi8JvSSAIzNg0MVw7AUheLatgaPPgwHfCMv/ZzFs+gQ6DIAm4fG2nH5z+DpIKRAOQynJSTw54URe/XgjO3cXMeaYzgDc/vLiKk9GG9kvjcy1W8uHhwD6dWrJ3RcM5A+vLGbBuu0M6dmWGzX76aGvRYeabd1PCl9lOh8Xhn7yc0IPo2x8/Zp5VU8qlxSGD/p188LyY78Dw34CWZMqTmpbUgicj5+rGMKCcM5h6StV61g+PZz8fvaKaPjIwtHzoIvhP9fBy7+Axi3DB3LPEeFKnbIj+k9egBO+X/N327aaKsNMEI7Ou50In75T0db3jDC2/1mlafT7jIbBl4SQmz85DHud+ovw/XuPhxArKoCuGRV/o9NvqVlD45aQPrRm+0FMz0NoQGYty+bu6UvZll/IdzO6ce3pfZizPJufPbWI7F176NuxBX+7JIOe1Z7RLFKrjR+G4aZ2R4X329eFo/L8rXD8xWHo5pUbQ68AwtDQRU/Cm7eHsf4ybXtBcuOKq4YgHJEPvRpm3VHRZslhaGv2n6rWcdKPwgnu4oLwPqUp/HAePHFR1X1+ZzJ0HQLTb4GNH0CvkfC128JlostfDyeC2/eFr14azjccxr7oeQgKBKGopJSc3EI6VXuMpsgBsXo2ZC8N5wva9gxhMGVc6E00aw8XPApTr656yWxSalh/WbXexPDrKy7xLHPeQ9C+H7z3N8BCkBw5GHI3w9sPwI71cOy3od+Zcf9VDxUKBBE5eJQUh2Gd1t0hpRHMvANm/r5i+eDx4Qqcl/6noq1RS7huATxzZcXVOt1PhnHPQqoOZvaFnpgmIgeP5JRwzX+ZkTeG3sPq2eGehsGXhvMQeVvCVVMtOoUTzM3T4NIXQy/Dga565veBpB6CiEgD80U9BE1uJyIigAJBREQiCgQREQEUCCIiEjngVxmZWSrwKNAHKAGucPcl0bIxwE2VVm8LPBCtPx/YErW/4e6/O9C1iYjIF4vHZafjgRx3H2dmpwJ3A2cDuPs0YBqAmXUAngYeB3oDc9z9mjjUIyIiMYjHkNEooGxWqDnAoC9Y7z7gdnffBfQHTjCzmWb2opn1+YJtREQkTuIRCO2AHAAPNzm4mVX5OWY2CGjj7tOjpm3AXe4+EriX0GuolZlNMLNMM8vMzs6OQ/kiIg1TPAJhK9AKwMyMkAvVniDBz4H7K72f5e5TCCu/CXSJtq3B3Se6e4a7Z6SlpR346kVEGqh4BMIMYGz0ejRh2KicmTUGRgKVZ636m5ldHi0/Dljrh/It1CIih6B4nFSeBEw2s0wgDxhnZjcDWdFJ5eFApnuVxw/9DpgUhUIxUMsE5yIiEk+ay0hEpIHRXEYiIrJXCgQREQEUCCIiElEgiIgIoEAQEZGIAkFERAAFgoiIRBQIIiICKBBERCSiQBAREUCBICIiEQWCiIgACgQREYkoEEREBFAgiIhIRIEgIiKAAkFERCIKBBERARQIIiISUSCIiAigQBARkYgCQUREgH0IBDMbYGZnmVnHeBYkIiKJkRLLSmb2M+A84AjgCTNLcvc/xLUyERGpV7H2EL4LjAC2ufsdwNnxK0lERBIhph4CsCf67tH30r2tbGapwKNAH6AEuMLdl0TLxgA3VVq9LfAAMAV4InqfC4xz940x1iciIvsp1h7CP4E3gd5m9gLwQh3rjwdy3H0o4cP/7rIF7j7N3Ue6+0jgO8A24HHg58Br7n4iMBm4dR9+DxER2U+x9hCeBuYAA4Fl7v5BHeuPAh6OXs8hHPnX5j7gdnffZWajgAuj9pcJAVGDmU0AJgCkp6fHWL6IiNQl1h7CC+6+3N2fiSEMANoBOQDu7oCbWZWfZWaDgDbuPr36NsAOoHVtO3b3ie6e4e4ZaWlpMZYvIiJ1ibWHsN3M/gZklTW4+8S9rL8VaAVgZhZW9+rnHX4O3F/LNrmE8wjZMdYmIiIHQKw9hEXA50DnSl97MwMYG70eTRg2KmdmjYGRwCtfsM35wKsx1iYiIgdATD0Ed/+tmZ0HHAXMd/c369hkEjDZzDKBPGCcmd0MZLn7NGA4kOnuxZW2uQuYYmbjgC3ARfv4u4iIyH6wMMRfx0pmdwI9gXnAqcAH7n5bnGurU0ZGhmdmZia6DBGRQ4qZZbl7RvX2WM8hDHP3k6PX95rZzANWmYiIHBRiPYdgZVcJRSeJm8avJBERSYRYewiTgHlmNg84AXg+fiWJiEgixHpSeaKZLQTSgUnuvii+ZYmISH2LdbbTO4B27n6VmU03s5fc/S9xrk1EROpRrOcQTnH3q6LXZ1ExxYSIiBwm9uWkcrPK7+NRjIiIJE6sJ5XvAbLM7GOgH3Bv/EoSEZFEiPWk8tNmNh3oC2x090/jW5aIiNS3WE8qXwM0JzwY52oz+4+71zo9tYiIHJpiHTK6BDgFeNzd+5nZe3GsSUREEiDWk8oALYHd0euSONQiIiIJFGsgzAIWA4+Z2V2AeggiIoeZWE8q/9LMbnH3EjNb6O7ZZvYtd58a7wJFRKR+xDxk5O4l0feyJ5ldG5eKREQkIfblHEJ1ujlNROQwsj+BUPeTdURE5JCxP4EgIiKHEQ0ZiYgIEGMgmNkN1d7/BM14KiJyWNnrZadmdiVwFTDAzM4rawbauPt98S5ORETqT133ITwDzAB+D/wyanPg83gWJSIi9W+vQ0buvsPd1wD/A3QEmgA3AUfHvzQREalPsZ5UfpQw2+kvgbeAh+NWkYiIJESsgdDI3d8Emrr740Dx3lY2s1Qz+7eZvWtm88ysf7XlY8xsgZm9Z2a/jdp6mNkHZjYz+prwpX4jERH5UmKd/jrFzH4NLDWz44HUOtYfD+S4+zgzOxW4GzgbwMyaEnoYI4D1wLtm9k9gAPCYu9+x77+GiIjsr1h7CFcTTib/CTgV+GEd648Cyia+mwMMqrRsCLDA3ddG8yONBbYA/YFzzGyWmU0xs44x1iYiIgdArIGwCsgBfgHMB5bUsX67aH3c3QE3s7Kf1QUwM3vazOYAVwK5wFrgd+4+ApgGPFDbjs1sgpllmllmdnZ2bauIiMiXEGsgPAgcCZwBdAAeqmP9rUArCJ/8hFwojZbtAroDlwIjgcGE4aTn3X16tM5TVO1VlHP3ie6e4e4ZaWlpMZYvIiJ1iTUQ+rr7zcBud38W6FXH+jMIQ0EAownDRmXmAzuAgmjIaCdQBEwzs9OidU4HMmOsTUREDoBYTyoXmVl7wtBPE+qe6XQSMNnMMoE8YJyZ3Qxkufs0M3sEmG1mJcB77v6qmW0AHjKzYsIQ0tVf6jcSEZEvxcIQfx0rmQ0F/g70IIz1X+/uM+JbWt0yMjI8M1MdCRGRfWFmWe6eUb091h7Cbnc/Luol5BCuCBIRkcNIXZPbDQZOAH5qZvdUWvRTwn0DIiJymKirh9AS6AQ0BjpHbU6Y20hERA4jew0Ed58FzDKzJ9x9WeVlZvZDd38wrtWJiEi9iemy0+phEPn2Aa5FREQSSI/QFBERYP8Coe7rVUVE5JCxP4EgIiKHEQ0ZiYgIEOONaWbWiDBtdbOyNnc/7Yu3EBGRQ02sdyq/QOgRbIjeOzD9i1cXEZFDTayB0EQ9AhGRw1us5xDWmlnPuFYiIiIJFWsgfB1YZmYboq/P41mUiIjUv5iGjNz9yMrvzeyo+JQjIiKJEutVRiOAcUBy1HQy0DdeRYmISP2LdcjoT8DrhNlPlwEPx60iERFJiFgDIdfdnwQ2ufsfgTFxrElERBIg1kAoNbNRQEszOxlIj2NNIiKSALEGwiVAEXAX8HPgN/EqSEREEiPWq4w2mNkgoB/wK2BxXKsSEZF6F+tVRncAXQmB4MBVwM/iWJeIiNSzWIeMTnH3i4Fd7v534IQ41iQiIgkQayAUmVkTwM1M016LiByG9uU+hLeBr0TfH9zbymaWamb/NrN3zWyemfWvtnyMmS0ws/fM7LdRW1czmx1t85yZNf8Sv4+IiHxJsQZCCbAN+BDIJVx1tDfjgRx3HwrcBNxdtsDMmhJubDsPOAk4O5o47w/AvdE2C4Af7sPvISIi+ynW6a/vBCYA+TGuP4qKu5nnAE9UWjYEWODuawHMbCywFRgBXB6t8zLwa0LPRERE6kGsgbDe3d/eh/22A3IA3N3NzM0syd1LgS6AmdnTQCfgTeA2IMXdi6PtdwCta9uxmU0ghBPp6bo/TkTkQIk1EN4ys3nAkrIGd79iL+tvBVpB+OQPq3tptGwX0B0YDuwhPI3tbKDQzJLdvQRoC2TXtmN3nwhMBMjIyPAY6xcRkTrEGgiXEG5Iy4tx/RnAWGAuMJowbFRmPqEHUODupWa2k3AX9FxCMLwInA+8GuPPEhGRAyDWQFjp7lP3Yb+TgMlmlkkIkXFmdjOQ5e7TzOwRYLaZlQDvufurZrYUeCxabyVw6z78PBER2U/mXveoi5m9QgiPLKAUwN1/Fd/S6paRkeGZmZmJLkNE5JBiZlnunlG9PdYewpQDXI+IiBxkYp3cblK8CxERkcSK9cY0ERE5zCkQREQEUCCIiEhEgSAiIoACQUREIgoEEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQUCCIiAigQREQkokAQERFAgSAiIhEFgoiIAAoEERGJKBBERARQIIiISESBICIigAJBREQiCgQREQEUCCIiEkk50Ds0s1TgUaAPUAJc4e5LKi3/MzAYKI2azoi+zwe2RK/fcPffHejaRETkix3wQADGAznuPs7MTgXuBs6utHwgMNLdywIBMzsamOPu18ShHhERiUE8hoxGAVOj13OAQdWWdwNeMrO3zGx81NYfOMHMZprZi2bWJw51iYjIXsSjh9AOyAFwdzczN7Mkdy81s6bAk8DtQCow08wWAduAu9x9ipmdBjwOnFDbzs1sAjABID09PQ7li4g0TPHoIWwFWgGYmRFyoWx4qAC41d3z3X0H8BpwLDDL3acQVn4T6BJtW4O7T3T3DHfPSEtLi0P5IiINUzwCYQYwNno9mjBsVGYYMMOCFGA44WTy38zscgAzOw5Y6+4eh9pEROQLxGPIaBIw2cwygTxgnJndDGS5+zQzex94n9BbeMLdPzGz3wGTolAoBr4fh7pERGQv7FA+EM/IyPDMzMxElyEickgxsyx3z6jerhvTREQEUCCIiEhEgSAiIoACQUREIgoEEREBFAgiIhJRIIiICKBAEBGRiAJBREQABYKIiEQUCCIiAigQREQkokAQERFAgSAiIhEFgoiIAAoEERGJKBBERARQIIiISESBICIigAJBREQiCgQREQEUCCIiElEgiIgIoEAQEZFISjx2amapwKNAH6AEuMLdl1Ra/mdgMFAaNZ0BNAWeANoCucA4d98Yj/qkQnFJKVMXfMbiDTs5tU8ap/XvkOiSRCRB4tVDGA/kuPtQ4Cbg7mrLBwIj3b3sqxD4OfCau58ITAZujVNtUslNUz/khmc+4NG5a7j8n+8z+e01iS5JRBIkXoEwCpgavZ4DDKq2vBvwkpm9ZWbja9nmZeCUONUmkV0FRTy34LMqbZPmrUlMMSKScHEZMgLaATkA7u5m5maW5O6lZtYUeBK4HUgFZprZosrbADuA1rXt2MwmABMA0tPT41R+w5CanERqslFS6uVtzRrF67+EiBzs4tVD2Aq0AjAzI+RC2fmCAuBWd8939x3Aa8CxlbchnEfIrm3H7j7R3TPcPSMtLS1O5TcMTVKTuWZE7/L3KUnGT77WJ4EViUgixetwcAYwFpgLjCYMG5UZBvyvmZ0OJAPDgX9W2uY+4Hzg1TjVJpVcN6oPI/qlsXjDToYd1Y7u7ZonuiQRSZB4BcIkYLKZZQJ5wOsgPKAAAAMmSURBVDgzuxnIcvdpZvY+8D6ht/CEu39iZncBU8xsHLAFuChOtUk1g7q1ZlC3WkfoRKQBMXeve62DVEZGhmdmZia6DBGRQ4qZZbl7RvV23ZgmIiKAAkFERCIKBBERARQIIiISUSCIiAhwiF9lZGbZwNpE13GYaE+43FfkYKT/nwdWd3evcWfvIR0IcuCYWWZtl6GJHAz0/7N+aMhIREQABYKIiEQUCFJmYqILENkL/f+sBzqHICIigHoIIiISUSCIiAgQv+mv5SBmZiOBH7j7hZXa/gkMJjyoCKAN8LS7/2+9FygNlpn1AD4A5kdNjYBS4FLCM1PWRe/LnBE9k10OAAWCVHaDu08DMLMWwAYze9Ddt9axnciB9Im7jyx7Y2a/AX4SvT3D3QsSUVRDoCEj+SKtgZ1AfqILkQavHfBZootoCNRDkMruNLObCI82PR74hY7GJAGONrOZ0euOQGPCc9d/CEw3s7Iho2fc/f4E1HfYUiBIZZWHjPoD/wUeSmxJ0gBVHzKaQnjOOmjIKK40ZCS1cvclQItE1yEC6P9iPVEPoeE6w8wqP5B6ey3rNDWz7u6uGWUlkfKBrya6iIZAdyqLiAigISMREYkoEEREBFAgiIhIRIEgIiKAAkFERCIKBJE4M7MeZvbOXpZfZmZ/rM+aRGqjQBAREUA3ponUycwuAy4kTLt8FPAv4GSgG/ALoCtwWbT6PHf/uZmlA5MJB13rK+3rROAPhGmdlxHm5xE5KKiHIBKbVHc/C/gV8A3gTOD7wI+Bq4AR7j4M6Glm5wN3AHe7+6nAi5X283fgu+4+HNgAXFGPv4PIXikQRGKTFX3PA5Z4uMU/HziD0CsoipbPBXoDA4E5Uds7AGaWBvQAnopm8xwFdK6P4kVioUAQiU3pF7S/DmSYWXL0/lTC076WAMOituHR9xzCE7/OimbzvBN4LS7VinwJOocgsn8ceBl428wKgNnuPsPM1gCPmtlPgYUA7l5qZtcD08zMgNXANUDPxJQuUpUmtxMREUBDRiIiElEgiIgIoEAQEZGIAkFERAAFgoiIRBQIIiICKBBERCTy/wGAxmc4TbK5zwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.swarmplot(x='model', y='mean_test_score', data=hyperopt_result_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we're not eking much more performance out of these models; let's check on the best parameters to see if there's at least any trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.03610000000000001}</td>\n",
       "      <td>0.805211</td>\n",
       "      <td>0.852865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.035100000000000006}</td>\n",
       "      <td>0.805207</td>\n",
       "      <td>0.851011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 0.0311}</td>\n",
       "      <td>0.805135</td>\n",
       "      <td>0.843289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 0.0291}</td>\n",
       "      <td>0.805070</td>\n",
       "      <td>0.839284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 0.0281}</td>\n",
       "      <td>0.805017</td>\n",
       "      <td>0.837229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  mean_train_score\n",
       "0   {'C': 0.03610000000000001}         0.805211          0.852865\n",
       "1  {'C': 0.035100000000000006}         0.805207          0.851011\n",
       "2                {'C': 0.0311}         0.805135          0.843289\n",
       "3                {'C': 0.0291}         0.805070          0.839284\n",
       "4                {'C': 0.0281}         0.805017          0.837229"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperopt_result_df_2[\n",
    "    hyperopt_result_df_2['model'] == 'LR'\n",
    "].sort_values(by='mean_test_score', ascending=False)[['params', 'mean_test_score', 'mean_train_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually a pretty nice model; AUC of 0.805, not particularly overfit; looks like C=0.03-0.04 is a good range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/Keras Neural Net Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial bias setting from: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cutoff = int(X_snp_sum.shape[0]*0.8)\n",
    "\n",
    "X = tf.convert_to_tensor(X_snp_sum[:val_cutoff, :])\n",
    "y = tf.convert_to_tensor(snp_y[:val_cutoff].reshape((-1, 1)))\n",
    "X_val = tf.convert_to_tensor(X_snp_sum[val_cutoff:, :])\n",
    "y_val = tf.convert_to_tensor(snp_y[val_cutoff:].reshape((-1, 1)))\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=[0, 1],\n",
    "    y=snp_y[:val_cutoff]\n",
    ")\n",
    "class_weight_dict = {k: v for k, v in zip([0, 1], class_weights)}\n",
    "\n",
    "neg, pos = np.bincount(snp_y[:val_cutoff])\n",
    "initial_bias = np.log([pos/neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers_big', 2, 4)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i), 100, 500, 10), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('rate', 0, 1)))\n",
    "    model.add(layers.Dense(units=hp.Int('units_small', 25, 200, 5), activation='relu'))\n",
    "    model.add(layers.Dense(\n",
    "        1,\n",
    "        activation='sigmoid',\n",
    "        bias_initializer=tf.keras.initializers.Constant(initial_bias)\n",
    "    ))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=3,\n",
    "    directory=LOCAL_CACHE_PATH,\n",
    "    project_name='ale_snp_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3793 - accuracy: 0.65 - ETA: 0s - loss: 3.4797 - accuracy: 0.57 - ETA: 0s - loss: 2.8339 - accuracy: 0.56 - ETA: 0s - loss: 2.2800 - accuracy: 0.57 - ETA: 0s - loss: 1.8788 - accuracy: 0.59 - ETA: 0s - loss: 1.6971 - accuracy: 0.61 - 1s 6ms/step - loss: 1.6015 - accuracy: 0.6163 - val_loss: 0.6017 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7422 - accuracy: 0.59 - ETA: 0s - loss: 0.7116 - accuracy: 0.67 - ETA: 0s - loss: 0.7294 - accuracy: 0.68 - ETA: 0s - loss: 0.7171 - accuracy: 0.67 - ETA: 0s - loss: 0.6976 - accuracy: 0.68 - ETA: 0s - loss: 0.6995 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6918 - accuracy: 0.6850 - val_loss: 0.6466 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.81 - ETA: 0s - loss: 0.6036 - accuracy: 0.75 - ETA: 0s - loss: 0.6434 - accuracy: 0.72 - ETA: 0s - loss: 0.6503 - accuracy: 0.70 - ETA: 0s - loss: 0.7195 - accuracy: 0.70 - ETA: 0s - loss: 0.7356 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7381 - accuracy: 0.7006 - val_loss: 0.5860 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.75 - ETA: 0s - loss: 0.7064 - accuracy: 0.71 - ETA: 0s - loss: 0.6932 - accuracy: 0.72 - ETA: 0s - loss: 0.6700 - accuracy: 0.72 - ETA: 0s - loss: 0.6660 - accuracy: 0.73 - ETA: 0s - loss: 0.6834 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6842 - accuracy: 0.7316 - val_loss: 0.6400 - val_accuracy: 0.7284\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.84 - ETA: 0s - loss: 0.7634 - accuracy: 0.70 - ETA: 0s - loss: 0.7116 - accuracy: 0.74 - ETA: 0s - loss: 0.7195 - accuracy: 0.73 - ETA: 0s - loss: 0.7176 - accuracy: 0.73 - ETA: 0s - loss: 0.7338 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7346 - accuracy: 0.7324 - val_loss: 0.6536 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8336 - accuracy: 0.68 - ETA: 0s - loss: 0.6879 - accuracy: 0.74 - ETA: 0s - loss: 0.7530 - accuracy: 0.75 - ETA: 0s - loss: 0.9930 - accuracy: 0.74 - ETA: 0s - loss: 0.9749 - accuracy: 0.73 - ETA: 0s - loss: 1.0016 - accuracy: 0.73 - 0s 4ms/step - loss: 1.0040 - accuracy: 0.7309 - val_loss: 0.6993 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.68 - ETA: 0s - loss: 0.9665 - accuracy: 0.73 - ETA: 0s - loss: 0.9437 - accuracy: 0.73 - ETA: 0s - loss: 1.0335 - accuracy: 0.73 - ETA: 0s - loss: 0.9299 - accuracy: 0.74 - ETA: 0s - loss: 0.9509 - accuracy: 0.74 - 0s 4ms/step - loss: 0.9377 - accuracy: 0.7421 - val_loss: 0.6716 - val_accuracy: 0.7194\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5893 - accuracy: 0.75 - ETA: 0s - loss: 0.6828 - accuracy: 0.76 - ETA: 0s - loss: 0.7576 - accuracy: 0.76 - ETA: 0s - loss: 0.7831 - accuracy: 0.76 - ETA: 0s - loss: 0.7920 - accuracy: 0.76 - ETA: 0s - loss: 0.8574 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8852 - accuracy: 0.7499 - val_loss: 0.6435 - val_accuracy: 0.7493\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8060 - accuracy: 0.78 - ETA: 0s - loss: 1.0581 - accuracy: 0.70 - ETA: 0s - loss: 1.0100 - accuracy: 0.73 - ETA: 0s - loss: 1.0550 - accuracy: 0.73 - ETA: 0s - loss: 1.0407 - accuracy: 0.73 - ETA: 0s - loss: 1.0116 - accuracy: 0.73 - 0s 4ms/step - loss: 1.0088 - accuracy: 0.7387 - val_loss: 0.6387 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.90 - ETA: 0s - loss: 0.9485 - accuracy: 0.75 - ETA: 0s - loss: 0.8283 - accuracy: 0.76 - ETA: 0s - loss: 0.8617 - accuracy: 0.75 - ETA: 0s - loss: 0.8150 - accuracy: 0.75 - 0s 3ms/step - loss: 0.8358 - accuracy: 0.7611 - val_loss: 0.6292 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.78 - ETA: 0s - loss: 0.7823 - accuracy: 0.78 - ETA: 0s - loss: 0.7623 - accuracy: 0.77 - ETA: 0s - loss: 0.7396 - accuracy: 0.76 - ETA: 0s - loss: 0.7274 - accuracy: 0.77 - ETA: 0s - loss: 0.7314 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7190 - accuracy: 0.7738 - val_loss: 0.6522 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.90 - ETA: 0s - loss: 1.1169 - accuracy: 0.76 - ETA: 0s - loss: 1.1327 - accuracy: 0.76 - ETA: 0s - loss: 1.0022 - accuracy: 0.76 - ETA: 0s - loss: 0.9303 - accuracy: 0.76 - 0s 4ms/step - loss: 0.8828 - accuracy: 0.7648 - val_loss: 0.6198 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.71 - ETA: 0s - loss: 0.7853 - accuracy: 0.75 - ETA: 0s - loss: 0.7134 - accuracy: 0.75 - ETA: 0s - loss: 0.6682 - accuracy: 0.77 - ETA: 0s - loss: 0.6808 - accuracy: 0.77 - ETA: 0s - loss: 0.6706 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6662 - accuracy: 0.7809 - val_loss: 0.6903 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.81 - ETA: 0s - loss: 0.6609 - accuracy: 0.78 - ETA: 0s - loss: 0.9906 - accuracy: 0.78 - ETA: 0s - loss: 0.9215 - accuracy: 0.78 - ETA: 0s - loss: 0.8754 - accuracy: 0.78 - ETA: 0s - loss: 0.8332 - accuracy: 0.78 - 0s 4ms/step - loss: 0.8297 - accuracy: 0.7872 - val_loss: 0.6372 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.87 - ETA: 0s - loss: 0.7831 - accuracy: 0.80 - ETA: 0s - loss: 0.7237 - accuracy: 0.78 - ETA: 0s - loss: 0.7411 - accuracy: 0.77 - ETA: 0s - loss: 0.7334 - accuracy: 0.77 - 0s 3ms/step - loss: 0.7140 - accuracy: 0.7783 - val_loss: 0.6724 - val_accuracy: 0.7403\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.62 - ETA: 0s - loss: 0.7326 - accuracy: 0.73 - ETA: 0s - loss: 0.7804 - accuracy: 0.74 - ETA: 0s - loss: 0.7509 - accuracy: 0.76 - ETA: 0s - loss: 0.7397 - accuracy: 0.76 - 0s 3ms/step - loss: 0.7206 - accuracy: 0.7648 - val_loss: 0.6594 - val_accuracy: 0.7448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.93 - ETA: 0s - loss: 0.6421 - accuracy: 0.77 - ETA: 0s - loss: 0.6898 - accuracy: 0.77 - ETA: 0s - loss: 0.7098 - accuracy: 0.77 - ETA: 0s - loss: 0.6901 - accuracy: 0.77 - ETA: 0s - loss: 0.7631 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7697 - accuracy: 0.7716 - val_loss: 0.6337 - val_accuracy: 0.7448\n",
      "Epoch 18/50\n",
      "68/84 [=======================>......] - ETA: 0s - loss: 0.5879 - accuracy: 0.81 - ETA: 0s - loss: 0.8959 - accuracy: 0.75 - ETA: 0s - loss: 1.9416 - accuracy: 0.76 - ETA: 0s - loss: 1.7539 - accuracy: 0.75 - ETA: 0s - loss: 1.5587 - accuracy: 0.7422Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.4091 - accuracy: 0.7350 - val_loss: 0.6700 - val_accuracy: 0.6955\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.8994 - accuracy: 0.71 - ETA: 0s - loss: 4.5373 - accuracy: 0.65 - ETA: 0s - loss: 3.7598 - accuracy: 0.59 - ETA: 0s - loss: 2.9279 - accuracy: 0.60 - ETA: 0s - loss: 2.3559 - accuracy: 0.62 - ETA: 0s - loss: 2.0176 - accuracy: 0.63 - 0s 5ms/step - loss: 1.9488 - accuracy: 0.6331 - val_loss: 0.6092 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0809 - accuracy: 0.53 - ETA: 0s - loss: 0.8302 - accuracy: 0.65 - ETA: 0s - loss: 0.7697 - accuracy: 0.69 - ETA: 0s - loss: 0.7585 - accuracy: 0.67 - ETA: 0s - loss: 0.7401 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7255 - accuracy: 0.6909 - val_loss: 0.6018 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8491 - accuracy: 0.59 - ETA: 0s - loss: 0.6602 - accuracy: 0.70 - ETA: 0s - loss: 0.6300 - accuracy: 0.70 - ETA: 0s - loss: 0.6503 - accuracy: 0.72 - ETA: 0s - loss: 0.6846 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6783 - accuracy: 0.7174 - val_loss: 0.6223 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.68 - ETA: 0s - loss: 0.7046 - accuracy: 0.69 - ETA: 0s - loss: 0.6847 - accuracy: 0.71 - ETA: 0s - loss: 0.7763 - accuracy: 0.71 - ETA: 0s - loss: 0.7553 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7428 - accuracy: 0.7264 - val_loss: 0.6452 - val_accuracy: 0.7552\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8418 - accuracy: 0.71 - ETA: 0s - loss: 0.7390 - accuracy: 0.74 - ETA: 0s - loss: 0.7953 - accuracy: 0.70 - ETA: 0s - loss: 0.8173 - accuracy: 0.69 - ETA: 0s - loss: 0.8146 - accuracy: 0.70 - ETA: 0s - loss: 0.7943 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7943 - accuracy: 0.7107 - val_loss: 0.6824 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.78 - ETA: 0s - loss: 0.8990 - accuracy: 0.68 - ETA: 0s - loss: 0.8562 - accuracy: 0.70 - ETA: 0s - loss: 0.8552 - accuracy: 0.70 - ETA: 0s - loss: 0.8410 - accuracy: 0.69 - 0s 3ms/step - loss: 0.8722 - accuracy: 0.6920 - val_loss: 0.7305 - val_accuracy: 0.3030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7455 - accuracy: 0.53 - ETA: 0s - loss: 1.1673 - accuracy: 0.68 - ETA: 0s - loss: 0.9966 - accuracy: 0.69 - ETA: 0s - loss: 0.9947 - accuracy: 0.67 - ETA: 0s - loss: 1.0595 - accuracy: 0.65 - ETA: 0s - loss: 1.0404 - accuracy: 0.64 - 0s 4ms/step - loss: 1.0377 - accuracy: 0.6491 - val_loss: 0.6582 - val_accuracy: 0.4299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.90 - ETA: 0s - loss: 0.7477 - accuracy: 0.63 - ETA: 0s - loss: 0.8354 - accuracy: 0.62 - ETA: 0s - loss: 0.8682 - accuracy: 0.60 - ETA: 0s - loss: 0.8884 - accuracy: 0.56 - ETA: 0s - loss: 0.9056 - accuracy: 0.53 - 0s 4ms/step - loss: 0.8723 - accuracy: 0.5218 - val_loss: 0.6846 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1333 - accuracy: 0.43 - ETA: 0s - loss: 0.8861 - accuracy: 0.64 - ETA: 0s - loss: 0.9467 - accuracy: 0.69 - ETA: 0s - loss: 0.9629 - accuracy: 0.70 - ETA: 0s - loss: 0.9607 - accuracy: 0.71 - ETA: 0s - loss: 0.9916 - accuracy: 0.71 - 0s 4ms/step - loss: 0.9824 - accuracy: 0.7174 - val_loss: 0.6556 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.75 - ETA: 0s - loss: 0.9121 - accuracy: 0.73 - ETA: 0s - loss: 0.9647 - accuracy: 0.74 - ETA: 0s - loss: 0.8989 - accuracy: 0.75 - ETA: 0s - loss: 0.8811 - accuracy: 0.75 - ETA: 0s - loss: 0.8656 - accuracy: 0.75 - ETA: 0s - loss: 0.8402 - accuracy: 0.75 - 0s 5ms/step - loss: 0.8453 - accuracy: 0.7514 - val_loss: 0.6455 - val_accuracy: 0.7567\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.81 - ETA: 0s - loss: 0.9969 - accuracy: 0.70 - ETA: 0s - loss: 0.8844 - accuracy: 0.73 - ETA: 0s - loss: 0.8368 - accuracy: 0.73 - ETA: 0s - loss: 0.8160 - accuracy: 0.73 - ETA: 0s - loss: 0.8126 - accuracy: 0.73 - 0s 4ms/step - loss: 0.8017 - accuracy: 0.7395 - val_loss: 0.6345 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.71 - ETA: 0s - loss: 0.8152 - accuracy: 0.73 - ETA: 0s - loss: 0.7786 - accuracy: 0.74 - ETA: 0s - loss: 0.7407 - accuracy: 0.73 - ETA: 0s - loss: 0.7356 - accuracy: 0.74 - ETA: 0s - loss: 0.7098 - accuracy: 0.75 - ETA: 0s - loss: 0.7605 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7642 - accuracy: 0.7551 - val_loss: 0.6095 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.87 - ETA: 0s - loss: 0.7598 - accuracy: 0.76 - ETA: 0s - loss: 0.9398 - accuracy: 0.75 - ETA: 0s - loss: 1.6867 - accuracy: 0.75 - ETA: 0s - loss: 1.5138 - accuracy: 0.75 - ETA: 0s - loss: 1.3859 - accuracy: 0.73 - 0s 4ms/step - loss: 1.3413 - accuracy: 0.7417 - val_loss: 0.6527 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.75 - ETA: 0s - loss: 0.7703 - accuracy: 0.78 - ETA: 0s - loss: 0.8594 - accuracy: 0.77 - ETA: 0s - loss: 0.8100 - accuracy: 0.76 - ETA: 0s - loss: 0.8123 - accuracy: 0.76 - ETA: 0s - loss: 0.8227 - accuracy: 0.75 - ETA: 0s - loss: 0.8002 - accuracy: 0.75 - 0s 5ms/step - loss: 0.8226 - accuracy: 0.7600 - val_loss: 0.6865 - val_accuracy: 0.7239\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7775 - accuracy: 0.78 - ETA: 0s - loss: 1.0845 - accuracy: 0.75 - ETA: 0s - loss: 0.9898 - accuracy: 0.74 - ETA: 0s - loss: 0.9648 - accuracy: 0.74 - ETA: 0s - loss: 0.9268 - accuracy: 0.74 - ETA: 0s - loss: 1.0021 - accuracy: 0.75 - ETA: 0s - loss: 0.9934 - accuracy: 0.74 - 0s 5ms/step - loss: 0.9654 - accuracy: 0.7443 - val_loss: 0.6410 - val_accuracy: 0.7269\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5253 - accuracy: 0.84 - ETA: 0s - loss: 0.6439 - accuracy: 0.73 - ETA: 0s - loss: 0.6333 - accuracy: 0.75 - ETA: 0s - loss: 0.6269 - accuracy: 0.75 - ETA: 0s - loss: 0.6820 - accuracy: 0.74 - ETA: 0s - loss: 0.6778 - accuracy: 0.75 - ETA: 0s - loss: 0.7148 - accuracy: 0.74 - 0s 5ms/step - loss: 0.7161 - accuracy: 0.7436 - val_loss: 0.6474 - val_accuracy: 0.7239\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6585 - accuracy: 0.71 - ETA: 0s - loss: 0.7112 - accuracy: 0.73 - ETA: 0s - loss: 0.7753 - accuracy: 0.74 - ETA: 0s - loss: 0.7985 - accuracy: 0.75 - ETA: 0s - loss: 0.7808 - accuracy: 0.74 - ETA: 0s - loss: 0.7694 - accuracy: 0.74 - ETA: 0s - loss: 0.7546 - accuracy: 0.74 - 0s 5ms/step - loss: 0.7591 - accuracy: 0.7368 - val_loss: 0.6574 - val_accuracy: 0.7119\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.65 - ETA: 0s - loss: 0.7391 - accuracy: 0.73 - ETA: 0s - loss: 0.7228 - accuracy: 0.73 - ETA: 0s - loss: 0.7119 - accuracy: 0.73 - ETA: 0s - loss: 0.7352 - accuracy: 0.73 - ETA: 0s - loss: 0.7200 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7309 - accuracy: 0.7346 - val_loss: 0.6687 - val_accuracy: 0.6970\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6285 - accuracy: 0.78 - ETA: 0s - loss: 0.6727 - accuracy: 0.72 - ETA: 0s - loss: 0.7402 - accuracy: 0.71 - ETA: 0s - loss: 0.7489 - accuracy: 0.70 - ETA: 0s - loss: 0.7564 - accuracy: 0.69 - ETA: 0s - loss: 0.7531 - accuracy: 0.61 - 0s 4ms/step - loss: 0.7532 - accuracy: 0.5655 - val_loss: 0.6977 - val_accuracy: 0.3045\n",
      "Epoch 20/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.7595 - accuracy: 0.40 - ETA: 0s - loss: 0.6830 - accuracy: 0.28 - ETA: 0s - loss: 0.7814 - accuracy: 0.36 - ETA: 0s - loss: 0.7609 - accuracy: 0.40 - ETA: 0s - loss: 0.8356 - accuracy: 0.39 - ETA: 0s - loss: 0.9302 - accuracy: 0.38 - ETA: 0s - loss: 0.9035 - accuracy: 0.3713Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.8922 - accuracy: 0.3680 - val_loss: 0.7040 - val_accuracy: 0.3045\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.68 - ETA: 0s - loss: 4.6793 - accuracy: 0.59 - ETA: 0s - loss: 3.1831 - accuracy: 0.61 - ETA: 0s - loss: 2.5087 - accuracy: 0.60 - ETA: 0s - loss: 2.1551 - accuracy: 0.60 - ETA: 0s - loss: 1.9043 - accuracy: 0.61 - 0s 5ms/step - loss: 1.8138 - accuracy: 0.6211 - val_loss: 0.5938 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6223 - accuracy: 0.71 - ETA: 0s - loss: 0.7182 - accuracy: 0.68 - ETA: 0s - loss: 0.7167 - accuracy: 0.66 - ETA: 0s - loss: 0.7086 - accuracy: 0.66 - ETA: 0s - loss: 0.7145 - accuracy: 0.68 - ETA: 0s - loss: 0.7127 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7156 - accuracy: 0.6902 - val_loss: 0.6477 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7703 - accuracy: 0.71 - ETA: 0s - loss: 0.8106 - accuracy: 0.64 - ETA: 0s - loss: 0.7258 - accuracy: 0.66 - ETA: 0s - loss: 0.7099 - accuracy: 0.68 - ETA: 0s - loss: 0.7124 - accuracy: 0.68 - ETA: 0s - loss: 0.7307 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7244 - accuracy: 0.6891 - val_loss: 0.6085 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0163 - accuracy: 0.68 - ETA: 0s - loss: 0.6809 - accuracy: 0.68 - ETA: 0s - loss: 0.6952 - accuracy: 0.69 - ETA: 0s - loss: 0.6813 - accuracy: 0.69 - ETA: 0s - loss: 0.6753 - accuracy: 0.70 - ETA: 0s - loss: 0.6590 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6562 - accuracy: 0.7152 - val_loss: 0.5622 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4099 - accuracy: 0.81 - ETA: 0s - loss: 0.5413 - accuracy: 0.75 - ETA: 0s - loss: 0.5666 - accuracy: 0.74 - ETA: 0s - loss: 0.6400 - accuracy: 0.74 - ETA: 0s - loss: 0.6423 - accuracy: 0.74 - ETA: 0s - loss: 0.6468 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6450 - accuracy: 0.7391 - val_loss: 0.5655 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.81 - ETA: 0s - loss: 0.6498 - accuracy: 0.74 - ETA: 0s - loss: 0.5995 - accuracy: 0.76 - ETA: 0s - loss: 0.6287 - accuracy: 0.75 - ETA: 0s - loss: 0.6039 - accuracy: 0.75 - ETA: 0s - loss: 0.5883 - accuracy: 0.76 - ETA: 0s - loss: 0.6152 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6143 - accuracy: 0.7469 - val_loss: 0.6033 - val_accuracy: 0.7299\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.68 - ETA: 0s - loss: 0.6076 - accuracy: 0.77 - ETA: 0s - loss: 0.5752 - accuracy: 0.75 - ETA: 0s - loss: 0.5993 - accuracy: 0.75 - ETA: 0s - loss: 0.6396 - accuracy: 0.74 - ETA: 0s - loss: 0.6842 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6990 - accuracy: 0.7238 - val_loss: 0.6526 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7497 - accuracy: 0.62 - ETA: 0s - loss: 1.0607 - accuracy: 0.73 - ETA: 0s - loss: 1.0814 - accuracy: 0.71 - ETA: 0s - loss: 0.9814 - accuracy: 0.70 - ETA: 0s - loss: 1.0203 - accuracy: 0.69 - ETA: 0s - loss: 0.9808 - accuracy: 0.71 - 0s 4ms/step - loss: 1.0868 - accuracy: 0.7133 - val_loss: 0.6754 - val_accuracy: 0.7134\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7315 - accuracy: 0.81 - ETA: 0s - loss: 1.0636 - accuracy: 0.70 - ETA: 0s - loss: 1.0448 - accuracy: 0.72 - ETA: 0s - loss: 1.0243 - accuracy: 0.71 - ETA: 0s - loss: 0.9961 - accuracy: 0.71 - ETA: 0s - loss: 0.9411 - accuracy: 0.71 - ETA: 0s - loss: 0.9318 - accuracy: 0.72 - 0s 4ms/step - loss: 0.9318 - accuracy: 0.7249 - val_loss: 0.6765 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.68 - ETA: 0s - loss: 0.7329 - accuracy: 0.72 - ETA: 0s - loss: 0.7963 - accuracy: 0.74 - ETA: 0s - loss: 0.7444 - accuracy: 0.75 - ETA: 0s - loss: 0.7282 - accuracy: 0.74 - ETA: 0s - loss: 0.7239 - accuracy: 0.75 - ETA: 0s - loss: 0.7112 - accuracy: 0.76 - 0s 5ms/step - loss: 0.7093 - accuracy: 0.7611 - val_loss: 0.6689 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.75 - ETA: 0s - loss: 0.5846 - accuracy: 0.77 - ETA: 0s - loss: 0.5964 - accuracy: 0.78 - ETA: 0s - loss: 0.5836 - accuracy: 0.78 - ETA: 0s - loss: 0.5870 - accuracy: 0.78 - ETA: 0s - loss: 0.6066 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6043 - accuracy: 0.7775 - val_loss: 0.6472 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.84 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.7481 - accuracy: 0.79 - ETA: 0s - loss: 0.6967 - accuracy: 0.79 - ETA: 0s - loss: 0.6712 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6673 - accuracy: 0.7895 - val_loss: 0.7235 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6622 - accuracy: 0.78 - ETA: 0s - loss: 0.5772 - accuracy: 0.79 - ETA: 0s - loss: 0.5709 - accuracy: 0.80 - ETA: 0s - loss: 0.5713 - accuracy: 0.80 - ETA: 0s - loss: 0.5690 - accuracy: 0.80 - ETA: 0s - loss: 0.5652 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5600 - accuracy: 0.8010 - val_loss: 0.6382 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.84 - ETA: 0s - loss: 0.5819 - accuracy: 0.80 - ETA: 0s - loss: 0.6057 - accuracy: 0.81 - ETA: 0s - loss: 0.5840 - accuracy: 0.81 - ETA: 0s - loss: 0.5770 - accuracy: 0.82 - ETA: 0s - loss: 0.5780 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5754 - accuracy: 0.8145 - val_loss: 0.6392 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.84 - ETA: 0s - loss: 0.5567 - accuracy: 0.79 - ETA: 0s - loss: 0.6147 - accuracy: 0.79 - ETA: 0s - loss: 0.5985 - accuracy: 0.79 - ETA: 0s - loss: 0.5767 - accuracy: 0.80 - ETA: 0s - loss: 0.5641 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5537 - accuracy: 0.8115 - val_loss: 0.7440 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.84 - ETA: 0s - loss: 0.6060 - accuracy: 0.80 - ETA: 0s - loss: 0.5320 - accuracy: 0.82 - ETA: 0s - loss: 0.5228 - accuracy: 0.82 - ETA: 0s - loss: 0.5335 - accuracy: 0.81 - ETA: 0s - loss: 0.6059 - accuracy: 0.82 - ETA: 0s - loss: 0.6416 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6439 - accuracy: 0.8152 - val_loss: 0.6510 - val_accuracy: 0.7313\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.84 - ETA: 0s - loss: 0.7509 - accuracy: 0.80 - ETA: 0s - loss: 0.6590 - accuracy: 0.80 - ETA: 0s - loss: 0.6426 - accuracy: 0.79 - ETA: 0s - loss: 0.6544 - accuracy: 0.79 - ETA: 0s - loss: 0.6305 - accuracy: 0.81 - ETA: 0s - loss: 0.6216 - accuracy: 0.80 - ETA: 0s - loss: 0.6159 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6059 - accuracy: 0.8085 - val_loss: 0.7670 - val_accuracy: 0.7478\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6742 - accuracy: 0.81 - ETA: 0s - loss: 0.5740 - accuracy: 0.82 - ETA: 0s - loss: 0.5863 - accuracy: 0.81 - ETA: 0s - loss: 0.6075 - accuracy: 0.81 - ETA: 0s - loss: 0.5961 - accuracy: 0.81 - ETA: 0s - loss: 0.6131 - accuracy: 0.81 - ETA: 0s - loss: 0.6366 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6495 - accuracy: 0.8040 - val_loss: 0.6439 - val_accuracy: 0.7373\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8930 - accuracy: 0.81 - ETA: 0s - loss: 0.7550 - accuracy: 0.83 - ETA: 0s - loss: 0.7765 - accuracy: 0.82 - ETA: 0s - loss: 0.8432 - accuracy: 0.81 - ETA: 0s - loss: 0.7661 - accuracy: 0.81 - ETA: 0s - loss: 0.7410 - accuracy: 0.81 - ETA: 0s - loss: 0.7617 - accuracy: 0.80 - 0s 5ms/step - loss: 0.7418 - accuracy: 0.8085 - val_loss: 0.9188 - val_accuracy: 0.7328\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.90 - ETA: 0s - loss: 1.1847 - accuracy: 0.75 - ETA: 0s - loss: 0.9816 - accuracy: 0.77 - ETA: 0s - loss: 0.8630 - accuracy: 0.78 - ETA: 0s - loss: 0.8074 - accuracy: 0.77 - ETA: 0s - loss: 0.7622 - accuracy: 0.78 - ETA: 0s - loss: 0.8512 - accuracy: 0.78 - ETA: 0s - loss: 0.8374 - accuracy: 0.78 - 0s 5ms/step - loss: 0.8374 - accuracy: 0.7813 - val_loss: 0.7896 - val_accuracy: 0.7269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.81 - ETA: 0s - loss: 0.5882 - accuracy: 0.78 - ETA: 0s - loss: 0.5881 - accuracy: 0.80 - ETA: 0s - loss: 0.5859 - accuracy: 0.79 - ETA: 0s - loss: 0.7815 - accuracy: 0.79 - ETA: 0s - loss: 0.8443 - accuracy: 0.78 - ETA: 0s - loss: 0.9132 - accuracy: 0.78 - 0s 4ms/step - loss: 0.8980 - accuracy: 0.7884 - val_loss: 0.6507 - val_accuracy: 0.7254\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.78 - ETA: 0s - loss: 1.1748 - accuracy: 0.76 - ETA: 0s - loss: 1.0615 - accuracy: 0.76 - ETA: 0s - loss: 0.9238 - accuracy: 0.77 - ETA: 0s - loss: 0.8823 - accuracy: 0.76 - ETA: 0s - loss: 0.8178 - accuracy: 0.77 - 0s 4ms/step - loss: 0.8003 - accuracy: 0.7783 - val_loss: 0.7433 - val_accuracy: 0.7328\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.81 - ETA: 0s - loss: 0.6190 - accuracy: 0.79 - ETA: 0s - loss: 0.6090 - accuracy: 0.79 - ETA: 0s - loss: 0.6083 - accuracy: 0.78 - ETA: 0s - loss: 0.5906 - accuracy: 0.79 - ETA: 0s - loss: 0.6106 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6133 - accuracy: 0.7906 - val_loss: 0.6616 - val_accuracy: 0.7478\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1485 - accuracy: 0.84 - ETA: 0s - loss: 0.7833 - accuracy: 0.78 - ETA: 0s - loss: 0.6728 - accuracy: 0.79 - ETA: 0s - loss: 0.6757 - accuracy: 0.79 - ETA: 0s - loss: 0.7122 - accuracy: 0.78 - ETA: 0s - loss: 0.7055 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6961 - accuracy: 0.7850 - val_loss: 0.7277 - val_accuracy: 0.7537\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.75 - ETA: 0s - loss: 0.6085 - accuracy: 0.78 - ETA: 0s - loss: 0.6091 - accuracy: 0.79 - ETA: 0s - loss: 0.5872 - accuracy: 0.80 - ETA: 0s - loss: 0.5708 - accuracy: 0.80 - ETA: 0s - loss: 0.5899 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5945 - accuracy: 0.8033 - val_loss: 0.6889 - val_accuracy: 0.7507\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.87 - ETA: 0s - loss: 0.5608 - accuracy: 0.80 - ETA: 0s - loss: 0.5562 - accuracy: 0.81 - ETA: 0s - loss: 0.5606 - accuracy: 0.81 - ETA: 0s - loss: 0.5703 - accuracy: 0.81 - ETA: 0s - loss: 0.9038 - accuracy: 0.80 - 0s 4ms/step - loss: 0.8951 - accuracy: 0.8070 - val_loss: 0.8567 - val_accuracy: 0.7552\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.78 - ETA: 0s - loss: 0.6000 - accuracy: 0.78 - ETA: 0s - loss: 0.6640 - accuracy: 0.78 - ETA: 0s - loss: 0.6689 - accuracy: 0.79 - ETA: 0s - loss: 0.6630 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6654 - accuracy: 0.7913 - val_loss: 0.6971 - val_accuracy: 0.7612\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.75 - ETA: 0s - loss: 0.5689 - accuracy: 0.82 - ETA: 0s - loss: 0.6223 - accuracy: 0.80 - ETA: 0s - loss: 0.6093 - accuracy: 0.80 - ETA: 0s - loss: 0.6146 - accuracy: 0.80 - 0s 3ms/step - loss: 0.7490 - accuracy: 0.8022 - val_loss: 0.6203 - val_accuracy: 0.7597\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.87 - ETA: 0s - loss: 0.9161 - accuracy: 0.80 - ETA: 0s - loss: 0.7719 - accuracy: 0.80 - ETA: 0s - loss: 0.7658 - accuracy: 0.79 - ETA: 0s - loss: 0.7424 - accuracy: 0.78 - 0s 3ms/step - loss: 0.7168 - accuracy: 0.7842 - val_loss: 0.6340 - val_accuracy: 0.7597\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.78 - ETA: 0s - loss: 0.6572 - accuracy: 0.77 - ETA: 0s - loss: 0.6368 - accuracy: 0.78 - ETA: 0s - loss: 0.6502 - accuracy: 0.77 - ETA: 0s - loss: 0.6263 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6175 - accuracy: 0.7872 - val_loss: 0.6649 - val_accuracy: 0.7597\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6682 - accuracy: 0.75 - ETA: 0s - loss: 0.5599 - accuracy: 0.80 - ETA: 0s - loss: 0.5702 - accuracy: 0.80 - ETA: 0s - loss: 0.5773 - accuracy: 0.80 - ETA: 0s - loss: 0.5736 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5861 - accuracy: 0.7958 - val_loss: 0.7093 - val_accuracy: 0.7627\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.71 - ETA: 0s - loss: 0.6072 - accuracy: 0.79 - ETA: 0s - loss: 0.6410 - accuracy: 0.80 - ETA: 0s - loss: 0.6664 - accuracy: 0.79 - ETA: 0s - loss: 0.6438 - accuracy: 0.80 - ETA: 0s - loss: 0.6271 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6280 - accuracy: 0.8007 - val_loss: 0.9649 - val_accuracy: 0.7597\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.90 - ETA: 0s - loss: 1.0613 - accuracy: 0.81 - ETA: 0s - loss: 0.9111 - accuracy: 0.80 - ETA: 0s - loss: 0.8149 - accuracy: 0.79 - ETA: 0s - loss: 0.7678 - accuracy: 0.79 - 0s 3ms/step - loss: 0.7428 - accuracy: 0.7981 - val_loss: 0.7461 - val_accuracy: 0.7493\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.75 - ETA: 0s - loss: 0.8923 - accuracy: 0.78 - ETA: 0s - loss: 0.9637 - accuracy: 0.79 - ETA: 0s - loss: 0.9387 - accuracy: 0.79 - ETA: 0s - loss: 0.9048 - accuracy: 0.79 - 0s 3ms/step - loss: 0.9184 - accuracy: 0.7984 - val_loss: 0.7079 - val_accuracy: 0.7597\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.78 - ETA: 0s - loss: 0.6213 - accuracy: 0.80 - ETA: 0s - loss: 0.6642 - accuracy: 0.79 - ETA: 0s - loss: 0.6493 - accuracy: 0.78 - ETA: 0s - loss: 0.6306 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6172 - accuracy: 0.7992 - val_loss: 0.6282 - val_accuracy: 0.7507\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.93 - ETA: 0s - loss: 0.5476 - accuracy: 0.81 - ETA: 0s - loss: 0.5752 - accuracy: 0.79 - ETA: 0s - loss: 0.6369 - accuracy: 0.79 - ETA: 0s - loss: 0.6252 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6197 - accuracy: 0.7943 - val_loss: 0.7433 - val_accuracy: 0.7567\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6042 - accuracy: 0.75 - ETA: 0s - loss: 0.5885 - accuracy: 0.78 - ETA: 0s - loss: 0.5656 - accuracy: 0.80 - ETA: 0s - loss: 0.5623 - accuracy: 0.80 - ETA: 0s - loss: 0.5646 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5513 - accuracy: 0.8111 - val_loss: 0.7956 - val_accuracy: 0.7328\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3070 - accuracy: 0.71 - ETA: 0s - loss: 0.6105 - accuracy: 0.79 - ETA: 0s - loss: 0.6258 - accuracy: 0.81 - ETA: 0s - loss: 0.6394 - accuracy: 0.80 - ETA: 0s - loss: 0.6320 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6110 - accuracy: 0.8066 - val_loss: 0.6507 - val_accuracy: 0.7507\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.65 - ETA: 0s - loss: 0.5578 - accuracy: 0.79 - ETA: 0s - loss: 0.9492 - accuracy: 0.80 - ETA: 0s - loss: 0.8502 - accuracy: 0.80 - ETA: 0s - loss: 0.7745 - accuracy: 0.81 - 0s 3ms/step - loss: 0.7554 - accuracy: 0.8108 - val_loss: 0.8862 - val_accuracy: 0.7612\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7621 - accuracy: 0.75 - ETA: 0s - loss: 0.8783 - accuracy: 0.78 - ETA: 0s - loss: 0.9836 - accuracy: 0.79 - ETA: 0s - loss: 0.8636 - accuracy: 0.80 - ETA: 0s - loss: 0.8076 - accuracy: 0.80 - 0s 3ms/step - loss: 0.7611 - accuracy: 0.8044 - val_loss: 0.8006 - val_accuracy: 0.7522\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.84 - ETA: 0s - loss: 0.5057 - accuracy: 0.84 - ETA: 0s - loss: 0.5709 - accuracy: 0.81 - ETA: 0s - loss: 0.5924 - accuracy: 0.80 - ETA: 0s - loss: 0.5957 - accuracy: 0.80 - ETA: 0s - loss: 0.5854 - accuracy: 0.8100Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5854 - accuracy: 0.8100 - val_loss: 0.8213 - val_accuracy: 0.7567\n",
      "Epoch 00041: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 6974bf086ac053cb13f2904758799341</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7562189102172852</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7920136342166519</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 310</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 155</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8052 - accuracy: 0.62 - ETA: 0s - loss: 1.0010 - accuracy: 0.65 - ETA: 0s - loss: 0.7836 - accuracy: 0.69 - ETA: 0s - loss: 0.7114 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7033 - accuracy: 0.7118 - val_loss: 0.5521 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3756 - accuracy: 0.90 - ETA: 0s - loss: 0.5323 - accuracy: 0.78 - ETA: 0s - loss: 0.5250 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7678 - val_loss: 0.7267 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.75 - ETA: 0s - loss: 0.5037 - accuracy: 0.79 - ETA: 0s - loss: 0.4982 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7966 - val_loss: 0.8194 - val_accuracy: 0.6776\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.68 - ETA: 0s - loss: 0.4886 - accuracy: 0.81 - ETA: 0s - loss: 0.4676 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8257 - val_loss: 0.5931 - val_accuracy: 0.7045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.87 - ETA: 0s - loss: 0.4153 - accuracy: 0.85 - ETA: 0s - loss: 0.4574 - accuracy: 0.82 - ETA: 0s - loss: 0.4561 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4561 - accuracy: 0.8302 - val_loss: 0.6273 - val_accuracy: 0.7164\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.84 - ETA: 0s - loss: 0.4496 - accuracy: 0.84 - ETA: 0s - loss: 0.4459 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8414 - val_loss: 0.6394 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.87 - ETA: 0s - loss: 0.4970 - accuracy: 0.86 - ETA: 0s - loss: 0.4687 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8518 - val_loss: 0.6513 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.93 - ETA: 0s - loss: 0.4365 - accuracy: 0.85 - ETA: 0s - loss: 0.4001 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8623 - val_loss: 0.7061 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.78 - ETA: 0s - loss: 0.3334 - accuracy: 0.88 - ETA: 0s - loss: 0.3490 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3647 - accuracy: 0.8772 - val_loss: 0.7835 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.87 - ETA: 0s - loss: 0.2965 - accuracy: 0.91 - ETA: 0s - loss: 0.3466 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8910 - val_loss: 0.6961 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.87 - ETA: 0s - loss: 0.3438 - accuracy: 0.88 - ETA: 0s - loss: 0.3225 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8921 - val_loss: 1.0063 - val_accuracy: 0.7194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.93 - ETA: 0s - loss: 0.3143 - accuracy: 0.90 - ETA: 0s - loss: 0.3236 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8996 - val_loss: 0.6117 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.93 - ETA: 0s - loss: 0.3299 - accuracy: 0.90 - ETA: 0s - loss: 0.3401 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8973 - val_loss: 0.8457 - val_accuracy: 0.7015\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.87 - ETA: 0s - loss: 0.3079 - accuracy: 0.91 - ETA: 0s - loss: 0.4479 - accuracy: 0.88 - ETA: 0s - loss: 0.5264 - accuracy: 0.86 - 0s 2ms/step - loss: 0.5207 - accuracy: 0.8649 - val_loss: 0.7925 - val_accuracy: 0.7149\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.90 - ETA: 0s - loss: 0.4239 - accuracy: 0.85 - ETA: 0s - loss: 0.3724 - accuracy: 0.87 - ETA: 0s - loss: 0.3657 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8824 - val_loss: 0.7628 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.90 - ETA: 0s - loss: 0.2960 - accuracy: 0.90 - ETA: 0s - loss: 0.3021 - accuracy: 0.90 - ETA: 0s - loss: 0.3055 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3057 - accuracy: 0.9074 - val_loss: 0.7695 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.90 - ETA: 0s - loss: 0.2486 - accuracy: 0.92 - ETA: 0s - loss: 0.2634 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2710 - accuracy: 0.9179 - val_loss: 1.0772 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.93 - ETA: 0s - loss: 0.2693 - accuracy: 0.92 - ETA: 0s - loss: 0.2676 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2583 - accuracy: 0.9291 - val_loss: 0.8734 - val_accuracy: 0.6985\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.93 - ETA: 0s - loss: 0.2652 - accuracy: 0.92 - ETA: 0s - loss: 0.2709 - accuracy: 0.92 - ETA: 0s - loss: 0.2993 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2999 - accuracy: 0.9134 - val_loss: 1.0059 - val_accuracy: 0.7104\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.93 - ETA: 0s - loss: 0.2513 - accuracy: 0.92 - ETA: 0s - loss: 0.2720 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2819 - accuracy: 0.9239 - val_loss: 0.9012 - val_accuracy: 0.7224\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.87 - ETA: 0s - loss: 0.3510 - accuracy: 0.89 - ETA: 0s - loss: 0.3563 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3476 - accuracy: 0.9003 - val_loss: 0.8482 - val_accuracy: 0.7239\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.90 - ETA: 0s - loss: 0.2753 - accuracy: 0.92 - ETA: 0s - loss: 0.3201 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3194 - accuracy: 0.9093 - val_loss: 1.3204 - val_accuracy: 0.7090\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.87 - ETA: 0s - loss: 0.2601 - accuracy: 0.93 - ETA: 0s - loss: 0.2573 - accuracy: 0.93 - ETA: 0s - loss: 0.2709 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2660 - accuracy: 0.9321 - val_loss: 1.2973 - val_accuracy: 0.7075\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.93 - ETA: 0s - loss: 0.2635 - accuracy: 0.93 - ETA: 0s - loss: 0.2695 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9351 - val_loss: 1.4658 - val_accuracy: 0.7104\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.90 - ETA: 0s - loss: 0.2288 - accuracy: 0.94 - ETA: 0s - loss: 0.2181 - accuracy: 0.94 - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9462 - val_loss: 1.4811 - val_accuracy: 0.7388\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.96 - ETA: 0s - loss: 0.2119 - accuracy: 0.95 - ETA: 0s - loss: 0.2168 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9511 - val_loss: 1.5319 - val_accuracy: 0.7119\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.96 - ETA: 0s - loss: 0.2310 - accuracy: 0.95 - ETA: 0s - loss: 0.2237 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9533 - val_loss: 1.6167 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.96 - ETA: 0s - loss: 0.1900 - accuracy: 0.95 - ETA: 0s - loss: 0.2006 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9496 - val_loss: 1.3915 - val_accuracy: 0.7149\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.96 - ETA: 0s - loss: 0.2031 - accuracy: 0.95 - ETA: 0s - loss: 0.2088 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2013 - accuracy: 0.9586 - val_loss: 2.1340 - val_accuracy: 0.7164\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 1.00 - ETA: 0s - loss: 0.1817 - accuracy: 0.96 - ETA: 0s - loss: 0.1877 - accuracy: 0.96 - ETA: 0s - loss: 0.1969 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9548 - val_loss: 1.4745 - val_accuracy: 0.7194\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.96 - ETA: 0s - loss: 0.2079 - accuracy: 0.95 - ETA: 0s - loss: 0.2002 - accuracy: 0.95 - ETA: 0s - loss: 0.2074 - accuracy: 0.95 - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9522 - val_loss: 1.2434 - val_accuracy: 0.7090\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.93 - ETA: 0s - loss: 0.1946 - accuracy: 0.96 - ETA: 0s - loss: 0.1874 - accuracy: 0.96 - ETA: 0s - loss: 0.1718 - accuracy: 0.96 - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9657 - val_loss: 1.5607 - val_accuracy: 0.7343\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.96 - ETA: 0s - loss: 0.1423 - accuracy: 0.97 - ETA: 0s - loss: 0.1839 - accuracy: 0.96 - ETA: 0s - loss: 0.2932 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2915 - accuracy: 0.9530 - val_loss: 3.6103 - val_accuracy: 0.6119\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.2895 - accuracy: 0.78 - ETA: 0s - loss: 0.6504 - accuracy: 0.92 - ETA: 0s - loss: 0.5002 - accuracy: 0.91 - ETA: 0s - loss: 0.4431 - accuracy: 0.91 - 0s 2ms/step - loss: 0.4259 - accuracy: 0.9179 - val_loss: 1.1163 - val_accuracy: 0.7328\n",
      "Epoch 35/50\n",
      "63/84 [=====================>........] - ETA: 0s - loss: 0.3788 - accuracy: 0.87 - ETA: 0s - loss: 0.2618 - accuracy: 0.93 - ETA: 0s - loss: 0.2852 - accuracy: 0.9271Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.9298 - val_loss: 1.1550 - val_accuracy: 0.7209\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8861 - accuracy: 0.46 - ETA: 0s - loss: 0.9706 - accuracy: 0.63 - ETA: 0s - loss: 0.8048 - accuracy: 0.64 - 0s 3ms/step - loss: 0.7277 - accuracy: 0.6771 - val_loss: 0.5866 - val_accuracy: 0.7045\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5383 - accuracy: 0.75 - ETA: 0s - loss: 0.5589 - accuracy: 0.74 - ETA: 0s - loss: 0.5439 - accuracy: 0.73 - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7454 - val_loss: 0.6072 - val_accuracy: 0.6597\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.81 - ETA: 0s - loss: 0.5054 - accuracy: 0.77 - ETA: 0s - loss: 0.4828 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7876 - val_loss: 0.7194 - val_accuracy: 0.6687\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.81 - ETA: 0s - loss: 0.4074 - accuracy: 0.81 - ETA: 0s - loss: 0.4291 - accuracy: 0.82 - ETA: 0s - loss: 0.4331 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8108 - val_loss: 0.5987 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.78 - ETA: 0s - loss: 0.3785 - accuracy: 0.82 - ETA: 0s - loss: 0.3837 - accuracy: 0.83 - ETA: 0s - loss: 0.3958 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8302 - val_loss: 0.8346 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6145 - accuracy: 0.71 - ETA: 0s - loss: 0.4006 - accuracy: 0.81 - ETA: 0s - loss: 0.3950 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8096 - val_loss: 0.7173 - val_accuracy: 0.6940\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.90 - ETA: 0s - loss: 0.3851 - accuracy: 0.84 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8339 - val_loss: 1.7172 - val_accuracy: 0.6358\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.75 - ETA: 0s - loss: 0.4294 - accuracy: 0.84 - ETA: 0s - loss: 0.3955 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8589 - val_loss: 0.7844 - val_accuracy: 0.6806\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2965 - accuracy: 0.84 - ETA: 0s - loss: 0.3580 - accuracy: 0.86 - ETA: 0s - loss: 0.3527 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8499 - val_loss: 0.6998 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.90 - ETA: 0s - loss: 0.4074 - accuracy: 0.86 - ETA: 0s - loss: 0.3993 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8522 - val_loss: 1.4719 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.84 - ETA: 0s - loss: 0.3954 - accuracy: 0.85 - ETA: 0s - loss: 0.3820 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8552 - val_loss: 1.6734 - val_accuracy: 0.6910\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.87 - ETA: 0s - loss: 0.4141 - accuracy: 0.84 - ETA: 0s - loss: 0.3669 - accuracy: 0.86 - ETA: 0s - loss: 0.3741 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3749 - accuracy: 0.8630 - val_loss: 1.0877 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.96 - ETA: 0s - loss: 0.2233 - accuracy: 0.91 - ETA: 0s - loss: 0.2392 - accuracy: 0.90 - ETA: 0s - loss: 0.3228 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3502 - accuracy: 0.8619 - val_loss: 2.0500 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.87 - ETA: 0s - loss: 0.5087 - accuracy: 0.86 - ETA: 0s - loss: 0.4138 - accuracy: 0.86 - ETA: 0s - loss: 0.3592 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3565 - accuracy: 0.8750 - val_loss: 6.3701 - val_accuracy: 0.6687\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.81 - ETA: 0s - loss: 0.3153 - accuracy: 0.90 - ETA: 0s - loss: 0.5621 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5951 - accuracy: 0.7940 - val_loss: 0.9785 - val_accuracy: 0.6433\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5500 - accuracy: 0.81 - ETA: 0s - loss: 0.4907 - accuracy: 0.82 - ETA: 0s - loss: 0.4939 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4977 - accuracy: 0.8029 - val_loss: 0.5754 - val_accuracy: 0.7134\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6378 - accuracy: 0.75 - ETA: 0s - loss: 0.4484 - accuracy: 0.82 - ETA: 0s - loss: 0.4334 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8384 - val_loss: 0.9532 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7283 - accuracy: 0.65 - ETA: 0s - loss: 0.5424 - accuracy: 0.77 - ETA: 0s - loss: 0.5256 - accuracy: 0.79 - ETA: 0s - loss: 0.4993 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4859 - accuracy: 0.8156 - val_loss: 0.6313 - val_accuracy: 0.7239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 0.90 - ETA: 0s - loss: 0.3408 - accuracy: 0.87 - ETA: 0s - loss: 0.3292 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8891 - val_loss: 1.9700 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 1.00 - ETA: 0s - loss: 0.3545 - accuracy: 0.90 - ETA: 0s - loss: 0.3587 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8888 - val_loss: 0.7425 - val_accuracy: 0.7119\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.93 - ETA: 0s - loss: 0.2688 - accuracy: 0.92 - ETA: 0s - loss: 0.4935 - accuracy: 0.90 - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8985 - val_loss: 0.6874 - val_accuracy: 0.7104\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.90 - ETA: 0s - loss: 0.2875 - accuracy: 0.89 - ETA: 0s - loss: 0.2939 - accuracy: 0.89 - ETA: 0s - loss: 0.2825 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8981 - val_loss: 1.0467 - val_accuracy: 0.7269\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.87 - ETA: 0s - loss: 0.2638 - accuracy: 0.91 - ETA: 0s - loss: 0.2576 - accuracy: 0.91 - ETA: 0s - loss: 0.2635 - accuracy: 0.92 - ETA: 0s - loss: 0.2716 - accuracy: 0.91 - 0s 3ms/step - loss: 0.2716 - accuracy: 0.9179 - val_loss: 1.8346 - val_accuracy: 0.6866\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9676 - accuracy: 0.75 - ETA: 0s - loss: 0.2879 - accuracy: 0.89 - ETA: 0s - loss: 0.2568 - accuracy: 0.90 - ETA: 0s - loss: 0.2510 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2502 - accuracy: 0.9100 - val_loss: 1.3500 - val_accuracy: 0.7239\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.90 - ETA: 0s - loss: 0.1894 - accuracy: 0.93 - ETA: 0s - loss: 0.2227 - accuracy: 0.92 - ETA: 0s - loss: 0.2268 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9212 - val_loss: 1.1642 - val_accuracy: 0.7209\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.96 - ETA: 0s - loss: 0.2313 - accuracy: 0.93 - ETA: 0s - loss: 0.2141 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9343 - val_loss: 1.1863 - val_accuracy: 0.7254\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 1.00 - ETA: 0s - loss: 0.3153 - accuracy: 0.89 - ETA: 0s - loss: 0.3162 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8962 - val_loss: 1.3564 - val_accuracy: 0.7209\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.90 - ETA: 0s - loss: 0.3372 - accuracy: 0.89 - ETA: 0s - loss: 0.3182 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2989 - accuracy: 0.9100 - val_loss: 1.8115 - val_accuracy: 0.7254\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.96 - ETA: 0s - loss: 0.2405 - accuracy: 0.93 - ETA: 0s - loss: 0.2271 - accuracy: 0.93 - ETA: 0s - loss: 0.2132 - accuracy: 0.9384Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.9384 - val_loss: 1.5171 - val_accuracy: 0.7299\n",
      "Epoch 00029: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7741 - accuracy: 0.65 - ETA: 0s - loss: 0.9333 - accuracy: 0.63 - ETA: 0s - loss: 0.7794 - accuracy: 0.67 - 0s 3ms/step - loss: 0.7165 - accuracy: 0.6865 - val_loss: 0.5775 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.75 - ETA: 0s - loss: 0.5210 - accuracy: 0.74 - ETA: 0s - loss: 0.5170 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7559 - val_loss: 0.6855 - val_accuracy: 0.7030\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.75 - ETA: 0s - loss: 0.4342 - accuracy: 0.80 - ETA: 0s - loss: 0.4514 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7988 - val_loss: 0.6162 - val_accuracy: 0.6672\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.84 - ETA: 0s - loss: 0.4222 - accuracy: 0.82 - ETA: 0s - loss: 0.4336 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8040 - val_loss: 0.6940 - val_accuracy: 0.6672\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.84 - ETA: 0s - loss: 0.3910 - accuracy: 0.83 - ETA: 0s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8126 - val_loss: 1.0043 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4487 - accuracy: 0.81 - ETA: 0s - loss: 0.4753 - accuracy: 0.77 - ETA: 0s - loss: 0.4738 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7764 - val_loss: 0.9717 - val_accuracy: 0.5970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.71 - ETA: 0s - loss: 0.4050 - accuracy: 0.80 - ETA: 0s - loss: 0.4105 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8104 - val_loss: 0.9057 - val_accuracy: 0.6821\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.87 - ETA: 0s - loss: 0.3797 - accuracy: 0.84 - ETA: 0s - loss: 0.3629 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8417 - val_loss: 0.7810 - val_accuracy: 0.7104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.93 - ETA: 0s - loss: 0.2684 - accuracy: 0.87 - ETA: 0s - loss: 0.2940 - accuracy: 0.86 - ETA: 0s - loss: 0.2995 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8682 - val_loss: 0.6997 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.84 - ETA: 0s - loss: 0.2702 - accuracy: 0.89 - ETA: 0s - loss: 0.2823 - accuracy: 0.88 - ETA: 0s - loss: 0.2809 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2809 - accuracy: 0.8850 - val_loss: 2.0770 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.84 - ETA: 0s - loss: 0.2571 - accuracy: 0.88 - ETA: 0s - loss: 0.3077 - accuracy: 0.87 - ETA: 0s - loss: 0.3601 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3614 - accuracy: 0.8608 - val_loss: 0.6677 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.93 - ETA: 0s - loss: 0.3119 - accuracy: 0.88 - ETA: 0s - loss: 0.3079 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8742 - val_loss: 0.7727 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.90 - ETA: 0s - loss: 0.2606 - accuracy: 0.88 - ETA: 0s - loss: 0.2550 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8828 - val_loss: 1.6892 - val_accuracy: 0.6851\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.87 - ETA: 0s - loss: 0.2480 - accuracy: 0.90 - ETA: 0s - loss: 0.2421 - accuracy: 0.90 - ETA: 0s - loss: 0.2589 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2590 - accuracy: 0.9029 - val_loss: 1.0073 - val_accuracy: 0.6970\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 0s - loss: 0.2298 - accuracy: 0.90 - ETA: 0s - loss: 0.2170 - accuracy: 0.91 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2346 - accuracy: 0.9100 - val_loss: 0.6843 - val_accuracy: 0.7030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.90 - ETA: 0s - loss: 0.2222 - accuracy: 0.91 - ETA: 0s - loss: 0.2186 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9078 - val_loss: 1.2652 - val_accuracy: 0.7075\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.93 - ETA: 0s - loss: 0.1800 - accuracy: 0.93 - ETA: 0s - loss: 0.2269 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2463 - accuracy: 0.9171 - val_loss: 1.5018 - val_accuracy: 0.6866\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.90 - ETA: 0s - loss: 0.3308 - accuracy: 0.89 - ETA: 0s - loss: 0.3265 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8992 - val_loss: 1.4288 - val_accuracy: 0.6866\n",
      "Epoch 19/50\n",
      "60/84 [====================>.........] - ETA: 0s - loss: 0.1389 - accuracy: 0.93 - ETA: 0s - loss: 0.2326 - accuracy: 0.91 - ETA: 0s - loss: 0.2277 - accuracy: 0.9151Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9197 - val_loss: 1.7224 - val_accuracy: 0.7149\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 776a99ddbce63f74e122aaaef497158a</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7353233695030212</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.14420668516898927</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_small: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9710 - accuracy: 0.56 - ETA: 0s - loss: 1.5108 - accuracy: 0.62 - ETA: 0s - loss: 1.0276 - accuracy: 0.68 - ETA: 0s - loss: 0.8696 - accuracy: 0.69 - ETA: 0s - loss: 0.8037 - accuracy: 0.71 - ETA: 0s - loss: 0.7766 - accuracy: 0.71 - 1s 6ms/step - loss: 0.7605 - accuracy: 0.7126 - val_loss: 0.6248 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.68 - ETA: 0s - loss: 0.5833 - accuracy: 0.75 - ETA: 0s - loss: 0.5577 - accuracy: 0.76 - ETA: 0s - loss: 0.5714 - accuracy: 0.74 - ETA: 0s - loss: 0.5817 - accuracy: 0.74 - ETA: 0s - loss: 0.5814 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5757 - accuracy: 0.7499 - val_loss: 0.6610 - val_accuracy: 0.6821\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5058 - accuracy: 0.75 - ETA: 0s - loss: 0.4797 - accuracy: 0.80 - ETA: 0s - loss: 0.4896 - accuracy: 0.79 - ETA: 0s - loss: 0.4862 - accuracy: 0.79 - ETA: 0s - loss: 0.4855 - accuracy: 0.79 - ETA: 0s - loss: 0.4940 - accuracy: 0.78 - 0s 4ms/step - loss: 0.4957 - accuracy: 0.7813 - val_loss: 0.6692 - val_accuracy: 0.6776\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5266 - accuracy: 0.84 - ETA: 0s - loss: 0.4730 - accuracy: 0.82 - ETA: 0s - loss: 0.4812 - accuracy: 0.81 - ETA: 0s - loss: 0.4553 - accuracy: 0.82 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - ETA: 0s - loss: 0.4758 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4733 - accuracy: 0.8096 - val_loss: 0.7785 - val_accuracy: 0.6985\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.78 - ETA: 0s - loss: 0.4935 - accuracy: 0.80 - ETA: 0s - loss: 0.4788 - accuracy: 0.80 - ETA: 0s - loss: 0.4492 - accuracy: 0.81 - ETA: 0s - loss: 0.4458 - accuracy: 0.82 - ETA: 0s - loss: 0.4486 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8298 - val_loss: 1.0675 - val_accuracy: 0.6687\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5003 - accuracy: 0.78 - ETA: 0s - loss: 0.5635 - accuracy: 0.83 - ETA: 0s - loss: 0.5242 - accuracy: 0.83 - ETA: 0s - loss: 0.5011 - accuracy: 0.83 - ETA: 0s - loss: 0.4769 - accuracy: 0.84 - ETA: 0s - loss: 0.4652 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4613 - accuracy: 0.8436 - val_loss: 0.7089 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.90 - ETA: 0s - loss: 0.3750 - accuracy: 0.84 - ETA: 0s - loss: 0.3353 - accuracy: 0.86 - ETA: 0s - loss: 0.3459 - accuracy: 0.86 - ETA: 0s - loss: 0.3579 - accuracy: 0.86 - ETA: 0s - loss: 0.3741 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3761 - accuracy: 0.8608 - val_loss: 0.7988 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.93 - ETA: 0s - loss: 0.3364 - accuracy: 0.89 - ETA: 0s - loss: 0.3234 - accuracy: 0.89 - ETA: 0s - loss: 0.3488 - accuracy: 0.88 - ETA: 0s - loss: 0.3381 - accuracy: 0.88 - ETA: 0s - loss: 0.3380 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3458 - accuracy: 0.8820 - val_loss: 0.8554 - val_accuracy: 0.6731\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.81 - ETA: 0s - loss: 0.3440 - accuracy: 0.86 - ETA: 0s - loss: 0.3208 - accuracy: 0.88 - ETA: 0s - loss: 0.3245 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3306 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3292 - accuracy: 0.8873 - val_loss: 0.7969 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.96 - ETA: 0s - loss: 0.2809 - accuracy: 0.91 - ETA: 0s - loss: 0.2881 - accuracy: 0.91 - ETA: 0s - loss: 0.2989 - accuracy: 0.90 - ETA: 0s - loss: 0.2980 - accuracy: 0.89 - ETA: 0s - loss: 0.3000 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2959 - accuracy: 0.8940 - val_loss: 0.9803 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.1041 - accuracy: 0.96 - ETA: 0s - loss: 0.2786 - accuracy: 0.89 - ETA: 0s - loss: 0.2756 - accuracy: 0.90 - ETA: 0s - loss: 0.2727 - accuracy: 0.90 - ETA: 0s - loss: 0.2824 - accuracy: 0.89 - ETA: 0s - loss: 0.3009 - accuracy: 0.8924Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 0.8865 - val_loss: 1.2390 - val_accuracy: 0.6597\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.75 - ETA: 0s - loss: 2.1178 - accuracy: 0.59 - ETA: 0s - loss: 1.4979 - accuracy: 0.62 - ETA: 0s - loss: 1.1732 - accuracy: 0.64 - ETA: 0s - loss: 1.0261 - accuracy: 0.66 - ETA: 0s - loss: 0.9291 - accuracy: 0.67 - 0s 5ms/step - loss: 0.8984 - accuracy: 0.6734 - val_loss: 0.5653 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5885 - accuracy: 0.71 - ETA: 0s - loss: 0.5315 - accuracy: 0.75 - ETA: 0s - loss: 0.5128 - accuracy: 0.75 - ETA: 0s - loss: 0.5005 - accuracy: 0.75 - ETA: 0s - loss: 0.5072 - accuracy: 0.75 - ETA: 0s - loss: 0.5150 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5159 - accuracy: 0.7559 - val_loss: 0.5948 - val_accuracy: 0.6881\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4369 - accuracy: 0.75 - ETA: 0s - loss: 0.4776 - accuracy: 0.77 - ETA: 0s - loss: 0.4444 - accuracy: 0.78 - ETA: 0s - loss: 0.4315 - accuracy: 0.79 - ETA: 0s - loss: 0.4418 - accuracy: 0.79 - ETA: 0s - loss: 0.4439 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4339 - accuracy: 0.7962 - val_loss: 0.6861 - val_accuracy: 0.6881\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.84 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3733 - accuracy: 0.83 - ETA: 0s - loss: 0.3852 - accuracy: 0.82 - ETA: 0s - loss: 0.4022 - accuracy: 0.81 - ETA: 0s - loss: 0.4239 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8100 - val_loss: 0.6477 - val_accuracy: 0.6910\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.90 - ETA: 0s - loss: 0.4102 - accuracy: 0.79 - ETA: 0s - loss: 0.3931 - accuracy: 0.81 - ETA: 0s - loss: 0.3988 - accuracy: 0.81 - ETA: 0s - loss: 0.4070 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4232 - accuracy: 0.8040 - val_loss: 0.6180 - val_accuracy: 0.7000\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.84 - ETA: 0s - loss: 0.3527 - accuracy: 0.84 - ETA: 0s - loss: 0.3503 - accuracy: 0.83 - ETA: 0s - loss: 0.3478 - accuracy: 0.84 - ETA: 0s - loss: 0.3891 - accuracy: 0.83 - ETA: 0s - loss: 0.3865 - accuracy: 0.84 - ETA: 0s - loss: 0.3989 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8466 - val_loss: 0.8853 - val_accuracy: 0.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.90 - ETA: 0s - loss: 0.3388 - accuracy: 0.85 - ETA: 0s - loss: 0.4183 - accuracy: 0.84 - ETA: 0s - loss: 0.4460 - accuracy: 0.84 - ETA: 0s - loss: 0.4490 - accuracy: 0.82 - ETA: 0s - loss: 0.4529 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4519 - accuracy: 0.8253 - val_loss: 0.8996 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7293 - accuracy: 0.78 - ETA: 0s - loss: 0.4957 - accuracy: 0.82 - ETA: 0s - loss: 0.4544 - accuracy: 0.83 - ETA: 0s - loss: 0.4521 - accuracy: 0.82 - ETA: 0s - loss: 0.4534 - accuracy: 0.83 - ETA: 0s - loss: 0.4570 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4632 - accuracy: 0.8302 - val_loss: 0.5602 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.93 - ETA: 0s - loss: 0.4777 - accuracy: 0.83 - ETA: 0s - loss: 0.4621 - accuracy: 0.84 - ETA: 0s - loss: 0.4645 - accuracy: 0.84 - ETA: 0s - loss: 0.4742 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4701 - accuracy: 0.8414 - val_loss: 0.6744 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.90 - ETA: 0s - loss: 0.3793 - accuracy: 0.86 - ETA: 0s - loss: 0.4476 - accuracy: 0.84 - ETA: 0s - loss: 0.4268 - accuracy: 0.85 - ETA: 0s - loss: 0.4300 - accuracy: 0.85 - ETA: 0s - loss: 0.4393 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4638 - accuracy: 0.8503 - val_loss: 0.7534 - val_accuracy: 0.7134\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.93 - ETA: 0s - loss: 0.3268 - accuracy: 0.90 - ETA: 0s - loss: 0.3418 - accuracy: 0.88 - ETA: 0s - loss: 0.3550 - accuracy: 0.88 - ETA: 0s - loss: 0.3633 - accuracy: 0.87 - ETA: 0s - loss: 0.3549 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8776 - val_loss: 0.7827 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.93 - ETA: 0s - loss: 0.2924 - accuracy: 0.90 - ETA: 0s - loss: 0.3317 - accuracy: 0.89 - ETA: 0s - loss: 0.3409 - accuracy: 0.89 - ETA: 0s - loss: 0.3512 - accuracy: 0.88 - ETA: 0s - loss: 0.3549 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3480 - accuracy: 0.8906 - val_loss: 1.0164 - val_accuracy: 0.6761\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.87 - ETA: 0s - loss: 0.3281 - accuracy: 0.88 - ETA: 0s - loss: 0.3144 - accuracy: 0.90 - ETA: 0s - loss: 0.3330 - accuracy: 0.89 - ETA: 0s - loss: 0.3299 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3321 - accuracy: 0.8921 - val_loss: 0.9175 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3390 - accuracy: 0.84 - ETA: 0s - loss: 0.3327 - accuracy: 0.89 - ETA: 0s - loss: 0.3580 - accuracy: 0.88 - ETA: 0s - loss: 0.3420 - accuracy: 0.89 - ETA: 0s - loss: 0.3399 - accuracy: 0.89 - ETA: 0s - loss: 0.3479 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3502 - accuracy: 0.8944 - val_loss: 1.0666 - val_accuracy: 0.7104\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.90 - ETA: 0s - loss: 0.3292 - accuracy: 0.90 - ETA: 0s - loss: 0.3208 - accuracy: 0.89 - ETA: 0s - loss: 0.3247 - accuracy: 0.90 - ETA: 0s - loss: 0.3187 - accuracy: 0.90 - ETA: 0s - loss: 0.3233 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3184 - accuracy: 0.9037 - val_loss: 1.3506 - val_accuracy: 0.6985\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.84 - ETA: 0s - loss: 0.3244 - accuracy: 0.89 - ETA: 0s - loss: 0.3028 - accuracy: 0.90 - ETA: 0s - loss: 0.2932 - accuracy: 0.90 - ETA: 0s - loss: 0.2958 - accuracy: 0.90 - ETA: 0s - loss: 0.2976 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3018 - accuracy: 0.9074 - val_loss: 1.0274 - val_accuracy: 0.7149\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3513 - accuracy: 0.87 - ETA: 0s - loss: 0.3076 - accuracy: 0.91 - ETA: 0s - loss: 0.2821 - accuracy: 0.91 - ETA: 0s - loss: 0.2789 - accuracy: 0.91 - ETA: 0s - loss: 0.2771 - accuracy: 0.91 - ETA: 0s - loss: 0.2717 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2672 - accuracy: 0.9216 - val_loss: 1.2261 - val_accuracy: 0.7060\n",
      "Epoch 18/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.96 - ETA: 0s - loss: 0.1769 - accuracy: 0.95 - ETA: 0s - loss: 0.2012 - accuracy: 0.95 - ETA: 0s - loss: 0.2135 - accuracy: 0.94 - ETA: 0s - loss: 0.2494 - accuracy: 0.93 - ETA: 0s - loss: 0.2588 - accuracy: 0.9306Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2579 - accuracy: 0.9298 - val_loss: 1.2979 - val_accuracy: 0.7090\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8082 - accuracy: 0.68 - ETA: 0s - loss: 1.9831 - accuracy: 0.60 - ETA: 0s - loss: 1.2804 - accuracy: 0.65 - ETA: 0s - loss: 1.0278 - accuracy: 0.67 - ETA: 0s - loss: 0.9129 - accuracy: 0.68 - ETA: 0s - loss: 0.8493 - accuracy: 0.69 - 0s 5ms/step - loss: 0.8110 - accuracy: 0.6950 - val_loss: 0.6140 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.65 - ETA: 0s - loss: 0.5408 - accuracy: 0.75 - ETA: 0s - loss: 0.5194 - accuracy: 0.76 - ETA: 0s - loss: 0.5067 - accuracy: 0.77 - ETA: 0s - loss: 0.5041 - accuracy: 0.76 - ETA: 0s - loss: 0.5146 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5192 - accuracy: 0.7615 - val_loss: 0.5694 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.84 - ETA: 0s - loss: 0.4859 - accuracy: 0.79 - ETA: 0s - loss: 0.4529 - accuracy: 0.80 - ETA: 0s - loss: 0.5120 - accuracy: 0.79 - ETA: 0s - loss: 0.5036 - accuracy: 0.79 - ETA: 0s - loss: 0.5042 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5053 - accuracy: 0.7936 - val_loss: 0.6591 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.78 - ETA: 0s - loss: 0.3784 - accuracy: 0.84 - ETA: 0s - loss: 0.4107 - accuracy: 0.82 - ETA: 0s - loss: 0.4245 - accuracy: 0.81 - ETA: 0s - loss: 0.4296 - accuracy: 0.81 - ETA: 0s - loss: 0.4312 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8186 - val_loss: 0.5939 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.84 - ETA: 0s - loss: 0.3362 - accuracy: 0.86 - ETA: 0s - loss: 0.3423 - accuracy: 0.85 - ETA: 0s - loss: 0.3653 - accuracy: 0.84 - ETA: 0s - loss: 0.3617 - accuracy: 0.84 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3871 - accuracy: 0.8287 - val_loss: 0.6316 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.81 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3546 - accuracy: 0.85 - ETA: 0s - loss: 0.3441 - accuracy: 0.85 - ETA: 0s - loss: 0.3523 - accuracy: 0.85 - ETA: 0s - loss: 0.3475 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8477 - val_loss: 0.7600 - val_accuracy: 0.6507\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.81 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3425 - accuracy: 0.86 - ETA: 0s - loss: 0.3616 - accuracy: 0.86 - ETA: 0s - loss: 0.3787 - accuracy: 0.85 - ETA: 0s - loss: 0.3938 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3896 - accuracy: 0.8470 - val_loss: 0.7909 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.87 - ETA: 0s - loss: 0.3132 - accuracy: 0.87 - ETA: 0s - loss: 0.3098 - accuracy: 0.87 - ETA: 0s - loss: 0.3200 - accuracy: 0.87 - ETA: 0s - loss: 0.3246 - accuracy: 0.87 - ETA: 0s - loss: 0.3194 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3228 - accuracy: 0.8712 - val_loss: 1.3287 - val_accuracy: 0.6418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.78 - ETA: 0s - loss: 0.3127 - accuracy: 0.87 - ETA: 0s - loss: 0.3137 - accuracy: 0.86 - ETA: 0s - loss: 0.3069 - accuracy: 0.86 - ETA: 0s - loss: 0.3223 - accuracy: 0.87 - ETA: 0s - loss: 0.3122 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3117 - accuracy: 0.8720 - val_loss: 2.5382 - val_accuracy: 0.6896\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.93 - ETA: 0s - loss: 0.3262 - accuracy: 0.90 - ETA: 0s - loss: 0.3438 - accuracy: 0.88 - ETA: 0s - loss: 0.3400 - accuracy: 0.87 - ETA: 0s - loss: 0.3452 - accuracy: 0.86 - ETA: 0s - loss: 0.3285 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3309 - accuracy: 0.8768 - val_loss: 0.8114 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.93 - ETA: 0s - loss: 0.2693 - accuracy: 0.90 - ETA: 0s - loss: 0.2654 - accuracy: 0.90 - ETA: 0s - loss: 0.2470 - accuracy: 0.90 - ETA: 0s - loss: 0.2610 - accuracy: 0.90 - ETA: 0s - loss: 0.2598 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2713 - accuracy: 0.8981 - val_loss: 1.9520 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.6579 - accuracy: 0.90 - ETA: 0s - loss: 0.2812 - accuracy: 0.90 - ETA: 0s - loss: 0.2569 - accuracy: 0.92 - ETA: 0s - loss: 0.2441 - accuracy: 0.92 - ETA: 0s - loss: 0.2503 - accuracy: 0.91 - ETA: 0s - loss: 0.2628 - accuracy: 0.9067Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2572 - accuracy: 0.9059 - val_loss: 1.8475 - val_accuracy: 0.7030\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a7229dcbca59dc9c7083f8a0ba07af2f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7273631890614828</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.10362882467074597</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_small: 85</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5787 - accuracy: 0.81 - ETA: 0s - loss: 1.4811 - accuracy: 0.65 - ETA: 0s - loss: 1.1526 - accuracy: 0.67 - ETA: 0s - loss: 0.9774 - accuracy: 0.68 - ETA: 0s - loss: 0.8775 - accuracy: 0.68 - ETA: 0s - loss: 0.8321 - accuracy: 0.69 - 0s 6ms/step - loss: 0.7996 - accuracy: 0.6984 - val_loss: 0.5771 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.84 - ETA: 0s - loss: 0.5064 - accuracy: 0.81 - ETA: 0s - loss: 0.5380 - accuracy: 0.79 - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - ETA: 0s - loss: 0.5475 - accuracy: 0.77 - ETA: 0s - loss: 0.5500 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5498 - accuracy: 0.7689 - val_loss: 0.6245 - val_accuracy: 0.7030\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.90 - ETA: 0s - loss: 0.4450 - accuracy: 0.85 - ETA: 0s - loss: 0.4575 - accuracy: 0.83 - ETA: 0s - loss: 0.4599 - accuracy: 0.83 - ETA: 0s - loss: 0.4911 - accuracy: 0.81 - ETA: 0s - loss: 0.4963 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4975 - accuracy: 0.8078 - val_loss: 0.6491 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.75 - ETA: 0s - loss: 0.4438 - accuracy: 0.83 - ETA: 0s - loss: 0.4086 - accuracy: 0.84 - ETA: 0s - loss: 0.4063 - accuracy: 0.85 - ETA: 0s - loss: 0.4504 - accuracy: 0.82 - ETA: 0s - loss: 0.4642 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4637 - accuracy: 0.8234 - val_loss: 0.6222 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.84 - ETA: 0s - loss: 0.4020 - accuracy: 0.85 - ETA: 0s - loss: 0.4101 - accuracy: 0.84 - ETA: 0s - loss: 0.4186 - accuracy: 0.83 - ETA: 0s - loss: 0.4107 - accuracy: 0.83 - ETA: 0s - loss: 0.4372 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4424 - accuracy: 0.8219 - val_loss: 0.6295 - val_accuracy: 0.7149\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.84 - ETA: 0s - loss: 0.5018 - accuracy: 0.82 - ETA: 0s - loss: 0.4793 - accuracy: 0.82 - ETA: 0s - loss: 0.4773 - accuracy: 0.82 - ETA: 0s - loss: 0.4904 - accuracy: 0.82 - ETA: 0s - loss: 0.4840 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4830 - accuracy: 0.8287 - val_loss: 0.5531 - val_accuracy: 0.7567\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7142 - accuracy: 0.84 - ETA: 0s - loss: 0.4321 - accuracy: 0.86 - ETA: 0s - loss: 0.4829 - accuracy: 0.85 - ETA: 0s - loss: 0.4821 - accuracy: 0.85 - ETA: 0s - loss: 0.4875 - accuracy: 0.84 - ETA: 0s - loss: 0.4877 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4841 - accuracy: 0.8376 - val_loss: 0.6665 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.90 - ETA: 0s - loss: 0.3981 - accuracy: 0.87 - ETA: 0s - loss: 0.3970 - accuracy: 0.86 - ETA: 0s - loss: 0.3974 - accuracy: 0.86 - ETA: 0s - loss: 0.4092 - accuracy: 0.86 - ETA: 0s - loss: 0.4139 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8574 - val_loss: 0.6031 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.96 - ETA: 0s - loss: 0.4644 - accuracy: 0.84 - ETA: 0s - loss: 0.4588 - accuracy: 0.84 - ETA: 0s - loss: 0.4500 - accuracy: 0.84 - ETA: 0s - loss: 0.4532 - accuracy: 0.84 - ETA: 0s - loss: 0.4502 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4452 - accuracy: 0.8473 - val_loss: 0.5657 - val_accuracy: 0.7493\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.81 - ETA: 0s - loss: 0.4231 - accuracy: 0.84 - ETA: 0s - loss: 0.4036 - accuracy: 0.86 - ETA: 0s - loss: 0.4019 - accuracy: 0.86 - ETA: 0s - loss: 0.4136 - accuracy: 0.86 - ETA: 0s - loss: 0.4180 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8600 - val_loss: 1.1153 - val_accuracy: 0.6821\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.93 - ETA: 0s - loss: 0.4253 - accuracy: 0.87 - ETA: 0s - loss: 0.4100 - accuracy: 0.87 - ETA: 0s - loss: 0.4184 - accuracy: 0.86 - ETA: 0s - loss: 0.4136 - accuracy: 0.86 - ETA: 0s - loss: 0.4227 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8604 - val_loss: 0.8222 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.78 - ETA: 0s - loss: 0.4223 - accuracy: 0.86 - ETA: 0s - loss: 0.4019 - accuracy: 0.86 - ETA: 0s - loss: 0.5694 - accuracy: 0.85 - ETA: 0s - loss: 0.5227 - accuracy: 0.85 - ETA: 0s - loss: 0.5025 - accuracy: 0.86 - ETA: 0s - loss: 0.4931 - accuracy: 0.86 - ETA: 0s - loss: 0.4836 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8649 - val_loss: 1.1192 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.84 - ETA: 0s - loss: 0.9068 - accuracy: 0.88 - ETA: 0s - loss: 0.7630 - accuracy: 0.83 - ETA: 0s - loss: 0.6422 - accuracy: 0.84 - ETA: 0s - loss: 0.5900 - accuracy: 0.85 - ETA: 0s - loss: 0.5615 - accuracy: 0.85 - ETA: 0s - loss: 0.5540 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5501 - accuracy: 0.8481 - val_loss: 1.2121 - val_accuracy: 0.6940\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.78 - ETA: 0s - loss: 0.4365 - accuracy: 0.86 - ETA: 0s - loss: 0.4269 - accuracy: 0.86 - ETA: 0s - loss: 0.4205 - accuracy: 0.86 - ETA: 0s - loss: 0.4207 - accuracy: 0.86 - ETA: 0s - loss: 0.4268 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8671 - val_loss: 1.0692 - val_accuracy: 0.7269\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.96 - ETA: 0s - loss: 0.3419 - accuracy: 0.89 - ETA: 0s - loss: 0.3674 - accuracy: 0.88 - ETA: 0s - loss: 0.3792 - accuracy: 0.88 - ETA: 0s - loss: 0.3805 - accuracy: 0.87 - ETA: 0s - loss: 0.3936 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8708 - val_loss: 1.4549 - val_accuracy: 0.7060\n",
      "Epoch 16/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.9587 - accuracy: 0.62 - ETA: 0s - loss: 0.5280 - accuracy: 0.82 - ETA: 0s - loss: 0.4965 - accuracy: 0.85 - ETA: 0s - loss: 0.5101 - accuracy: 0.86 - ETA: 0s - loss: 0.4965 - accuracy: 0.86 - ETA: 0s - loss: 0.4935 - accuracy: 0.8609Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4950 - accuracy: 0.8570 - val_loss: 1.2950 - val_accuracy: 0.6925\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.81 - ETA: 0s - loss: 1.2928 - accuracy: 0.66 - ETA: 0s - loss: 0.9964 - accuracy: 0.68 - ETA: 0s - loss: 0.8558 - accuracy: 0.71 - ETA: 0s - loss: 0.7980 - accuracy: 0.71 - ETA: 0s - loss: 0.7621 - accuracy: 0.72 - 0s 6ms/step - loss: 0.7430 - accuracy: 0.7193 - val_loss: 0.6467 - val_accuracy: 0.6836\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.75 - ETA: 0s - loss: 0.5102 - accuracy: 0.76 - ETA: 0s - loss: 0.5331 - accuracy: 0.76 - ETA: 0s - loss: 0.5300 - accuracy: 0.76 - ETA: 0s - loss: 0.5415 - accuracy: 0.76 - ETA: 0s - loss: 0.5445 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5469 - accuracy: 0.7648 - val_loss: 0.5559 - val_accuracy: 0.7552\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.90 - ETA: 0s - loss: 0.5417 - accuracy: 0.79 - ETA: 0s - loss: 0.5414 - accuracy: 0.79 - ETA: 0s - loss: 0.5426 - accuracy: 0.78 - ETA: 0s - loss: 0.5408 - accuracy: 0.77 - ETA: 0s - loss: 0.5235 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5218 - accuracy: 0.7887 - val_loss: 0.5767 - val_accuracy: 0.7060\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.78 - ETA: 0s - loss: 0.5160 - accuracy: 0.80 - ETA: 0s - loss: 0.4761 - accuracy: 0.82 - ETA: 0s - loss: 0.4757 - accuracy: 0.82 - ETA: 0s - loss: 0.4792 - accuracy: 0.82 - ETA: 0s - loss: 0.4869 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4945 - accuracy: 0.8182 - val_loss: 0.6024 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.87 - ETA: 0s - loss: 0.4645 - accuracy: 0.82 - ETA: 0s - loss: 0.4874 - accuracy: 0.81 - ETA: 0s - loss: 0.4759 - accuracy: 0.82 - ETA: 0s - loss: 0.4828 - accuracy: 0.82 - ETA: 0s - loss: 0.4828 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4814 - accuracy: 0.8298 - val_loss: 0.5635 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.87 - ETA: 0s - loss: 0.4818 - accuracy: 0.83 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - ETA: 0s - loss: 0.4792 - accuracy: 0.82 - ETA: 0s - loss: 0.4876 - accuracy: 0.82 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4869 - accuracy: 0.8253 - val_loss: 0.5783 - val_accuracy: 0.7149\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.90 - ETA: 0s - loss: 0.4382 - accuracy: 0.83 - ETA: 0s - loss: 0.4143 - accuracy: 0.85 - ETA: 0s - loss: 0.4100 - accuracy: 0.86 - ETA: 0s - loss: 0.4285 - accuracy: 0.84 - ETA: 0s - loss: 0.4353 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4366 - accuracy: 0.8429 - val_loss: 0.7288 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.93 - ETA: 0s - loss: 0.4570 - accuracy: 0.83 - ETA: 0s - loss: 0.4515 - accuracy: 0.83 - ETA: 0s - loss: 0.4669 - accuracy: 0.83 - ETA: 0s - loss: 0.4702 - accuracy: 0.82 - ETA: 0s - loss: 0.4769 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4751 - accuracy: 0.8231 - val_loss: 0.7286 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.87 - ETA: 0s - loss: 0.4056 - accuracy: 0.85 - ETA: 0s - loss: 0.4122 - accuracy: 0.86 - ETA: 0s - loss: 0.4129 - accuracy: 0.86 - ETA: 0s - loss: 0.4204 - accuracy: 0.86 - ETA: 0s - loss: 0.4238 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8582 - val_loss: 0.6792 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.81 - ETA: 0s - loss: 0.3389 - accuracy: 0.89 - ETA: 0s - loss: 0.4120 - accuracy: 0.87 - ETA: 0s - loss: 0.4198 - accuracy: 0.86 - ETA: 0s - loss: 0.4285 - accuracy: 0.86 - ETA: 0s - loss: 0.4331 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4364 - accuracy: 0.8578 - val_loss: 0.6752 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.85 - ETA: 0s - loss: 0.4010 - accuracy: 0.85 - ETA: 0s - loss: 0.4141 - accuracy: 0.85 - ETA: 0s - loss: 0.4121 - accuracy: 0.85 - ETA: 0s - loss: 0.4105 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4102 - accuracy: 0.8555 - val_loss: 0.7326 - val_accuracy: 0.7119\n",
      "Epoch 12/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5439 - accuracy: 0.84 - ETA: 0s - loss: 0.4046 - accuracy: 0.87 - ETA: 0s - loss: 0.4052 - accuracy: 0.87 - ETA: 0s - loss: 0.4235 - accuracy: 0.87 - ETA: 0s - loss: 0.4092 - accuracy: 0.87 - ETA: 0s - loss: 0.4158 - accuracy: 0.87 - ETA: 0s - loss: 0.4270 - accuracy: 0.8667Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4272 - accuracy: 0.8664 - val_loss: 0.7186 - val_accuracy: 0.7254\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.68 - ETA: 0s - loss: 1.0524 - accuracy: 0.70 - ETA: 0s - loss: 0.8832 - accuracy: 0.68 - ETA: 0s - loss: 0.8068 - accuracy: 0.68 - ETA: 0s - loss: 0.7551 - accuracy: 0.70 - ETA: 0s - loss: 0.7409 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7322 - accuracy: 0.7088 - val_loss: 0.5667 - val_accuracy: 0.7507\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.59 - ETA: 0s - loss: 0.6036 - accuracy: 0.77 - ETA: 0s - loss: 0.6003 - accuracy: 0.77 - ETA: 0s - loss: 0.5977 - accuracy: 0.77 - ETA: 0s - loss: 0.5840 - accuracy: 0.77 - ETA: 0s - loss: 0.5783 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5712 - accuracy: 0.7813 - val_loss: 0.6195 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.75 - ETA: 0s - loss: 0.5433 - accuracy: 0.77 - ETA: 0s - loss: 0.5054 - accuracy: 0.80 - ETA: 0s - loss: 0.5165 - accuracy: 0.80 - ETA: 0s - loss: 0.5340 - accuracy: 0.79 - ETA: 0s - loss: 0.5379 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5391 - accuracy: 0.7969 - val_loss: 0.5841 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.87 - ETA: 0s - loss: 0.5038 - accuracy: 0.81 - ETA: 0s - loss: 0.5392 - accuracy: 0.79 - ETA: 0s - loss: 0.5341 - accuracy: 0.78 - ETA: 0s - loss: 0.5221 - accuracy: 0.79 - ETA: 0s - loss: 0.5272 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7988 - val_loss: 0.5965 - val_accuracy: 0.7463\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3744 - accuracy: 0.93 - ETA: 0s - loss: 0.5154 - accuracy: 0.82 - ETA: 0s - loss: 0.4876 - accuracy: 0.83 - ETA: 0s - loss: 0.4834 - accuracy: 0.83 - ETA: 0s - loss: 0.4932 - accuracy: 0.83 - ETA: 0s - loss: 0.4930 - accuracy: 0.83 - ETA: 0s - loss: 0.5017 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5017 - accuracy: 0.8238 - val_loss: 0.7126 - val_accuracy: 0.6806\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.81 - ETA: 0s - loss: 0.4520 - accuracy: 0.85 - ETA: 0s - loss: 0.4821 - accuracy: 0.84 - ETA: 0s - loss: 0.4798 - accuracy: 0.83 - ETA: 0s - loss: 0.4849 - accuracy: 0.83 - ETA: 0s - loss: 0.4935 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4928 - accuracy: 0.8246 - val_loss: 0.6347 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.78 - ETA: 0s - loss: 0.5015 - accuracy: 0.80 - ETA: 0s - loss: 0.4819 - accuracy: 0.81 - ETA: 0s - loss: 0.4962 - accuracy: 0.82 - ETA: 0s - loss: 0.4929 - accuracy: 0.82 - ETA: 0s - loss: 0.4752 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4789 - accuracy: 0.8320 - val_loss: 0.7385 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.84 - ETA: 0s - loss: 0.4718 - accuracy: 0.84 - ETA: 0s - loss: 0.4482 - accuracy: 0.86 - ETA: 0s - loss: 0.4561 - accuracy: 0.85 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.4603 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4539 - accuracy: 0.8518 - val_loss: 0.7054 - val_accuracy: 0.7209\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.87 - ETA: 0s - loss: 0.4347 - accuracy: 0.86 - ETA: 0s - loss: 0.4491 - accuracy: 0.85 - ETA: 0s - loss: 0.4559 - accuracy: 0.84 - ETA: 0s - loss: 0.4606 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4576 - accuracy: 0.8421 - val_loss: 0.6075 - val_accuracy: 0.7358\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.84 - ETA: 0s - loss: 0.4224 - accuracy: 0.86 - ETA: 0s - loss: 0.4394 - accuracy: 0.85 - ETA: 0s - loss: 0.4453 - accuracy: 0.85 - ETA: 0s - loss: 0.4483 - accuracy: 0.85 - ETA: 0s - loss: 0.4782 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4788 - accuracy: 0.8458 - val_loss: 0.6318 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.4154 - accuracy: 0.87 - ETA: 0s - loss: 0.4756 - accuracy: 0.83 - ETA: 0s - loss: 0.4610 - accuracy: 0.84 - ETA: 0s - loss: 0.4653 - accuracy: 0.84 - ETA: 0s - loss: 0.4520 - accuracy: 0.85 - ETA: 0s - loss: 0.4512 - accuracy: 0.8522Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.8511 - val_loss: 1.3746 - val_accuracy: 0.6761\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 290ce58bc1cf4c34c851f3d11ebe89c1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7542288502057394</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.509319129875516</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_small: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.71 - ETA: 0s - loss: 1.7669 - accuracy: 0.62 - ETA: 0s - loss: 1.4108 - accuracy: 0.63 - ETA: 0s - loss: 1.2060 - accuracy: 0.64 - ETA: 0s - loss: 1.0614 - accuracy: 0.66 - ETA: 0s - loss: 0.9911 - accuracy: 0.67 - ETA: 0s - loss: 0.9237 - accuracy: 0.69 - 1s 6ms/step - loss: 0.8976 - accuracy: 0.7036 - val_loss: 0.5871 - val_accuracy: 0.7448\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.75 - ETA: 0s - loss: 0.5982 - accuracy: 0.76 - ETA: 0s - loss: 0.6122 - accuracy: 0.75 - ETA: 0s - loss: 0.5974 - accuracy: 0.76 - ETA: 0s - loss: 0.6035 - accuracy: 0.76 - ETA: 0s - loss: 0.6057 - accuracy: 0.76 - ETA: 0s - loss: 0.6044 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5972 - accuracy: 0.7693 - val_loss: 0.5736 - val_accuracy: 0.7478\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.81 - ETA: 0s - loss: 0.6056 - accuracy: 0.77 - ETA: 0s - loss: 0.5849 - accuracy: 0.78 - ETA: 0s - loss: 0.5691 - accuracy: 0.80 - ETA: 0s - loss: 0.5664 - accuracy: 0.80 - ETA: 0s - loss: 0.5595 - accuracy: 0.79 - ETA: 0s - loss: 0.5600 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5586 - accuracy: 0.7958 - val_loss: 0.5678 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.78 - ETA: 0s - loss: 0.5549 - accuracy: 0.78 - ETA: 0s - loss: 0.5347 - accuracy: 0.78 - ETA: 0s - loss: 0.5406 - accuracy: 0.78 - ETA: 0s - loss: 0.5363 - accuracy: 0.79 - ETA: 0s - loss: 0.5370 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5410 - accuracy: 0.7943 - val_loss: 0.5948 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.87 - ETA: 0s - loss: 0.5114 - accuracy: 0.80 - ETA: 0s - loss: 0.5055 - accuracy: 0.80 - ETA: 0s - loss: 0.5119 - accuracy: 0.80 - ETA: 0s - loss: 0.5146 - accuracy: 0.81 - ETA: 0s - loss: 0.5111 - accuracy: 0.81 - ETA: 0s - loss: 0.5159 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5190 - accuracy: 0.8122 - val_loss: 0.6547 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.93 - ETA: 0s - loss: 0.4573 - accuracy: 0.85 - ETA: 0s - loss: 0.4799 - accuracy: 0.84 - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.5092 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - ETA: 0s - loss: 0.5011 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5002 - accuracy: 0.8354 - val_loss: 0.6096 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.81 - ETA: 0s - loss: 0.4476 - accuracy: 0.83 - ETA: 0s - loss: 0.4466 - accuracy: 0.84 - ETA: 0s - loss: 0.4643 - accuracy: 0.84 - ETA: 0s - loss: 0.4693 - accuracy: 0.83 - ETA: 0s - loss: 0.4855 - accuracy: 0.83 - ETA: 0s - loss: 0.5043 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5047 - accuracy: 0.8216 - val_loss: 0.5810 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.75 - ETA: 0s - loss: 0.4911 - accuracy: 0.84 - ETA: 0s - loss: 0.4677 - accuracy: 0.85 - ETA: 0s - loss: 0.4771 - accuracy: 0.84 - ETA: 0s - loss: 0.4780 - accuracy: 0.83 - ETA: 0s - loss: 0.4826 - accuracy: 0.83 - ETA: 0s - loss: 0.4847 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4850 - accuracy: 0.8406 - val_loss: 0.6049 - val_accuracy: 0.7507\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.84 - ETA: 0s - loss: 0.4689 - accuracy: 0.84 - ETA: 0s - loss: 0.4505 - accuracy: 0.85 - ETA: 0s - loss: 0.4491 - accuracy: 0.85 - ETA: 0s - loss: 0.4620 - accuracy: 0.85 - ETA: 0s - loss: 0.4762 - accuracy: 0.84 - ETA: 0s - loss: 0.4802 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4777 - accuracy: 0.8455 - val_loss: 0.6219 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.84 - ETA: 0s - loss: 0.4561 - accuracy: 0.83 - ETA: 0s - loss: 0.4349 - accuracy: 0.85 - ETA: 0s - loss: 0.4480 - accuracy: 0.85 - ETA: 0s - loss: 0.4575 - accuracy: 0.84 - ETA: 0s - loss: 0.4596 - accuracy: 0.84 - ETA: 0s - loss: 0.4631 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4642 - accuracy: 0.8462 - val_loss: 0.7141 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.84 - ETA: 0s - loss: 0.4331 - accuracy: 0.86 - ETA: 0s - loss: 0.4603 - accuracy: 0.85 - ETA: 0s - loss: 0.4663 - accuracy: 0.85 - ETA: 0s - loss: 0.4635 - accuracy: 0.85 - ETA: 0s - loss: 0.4712 - accuracy: 0.84 - ETA: 0s - loss: 0.4771 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8466 - val_loss: 0.6004 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.84 - ETA: 0s - loss: 0.4013 - accuracy: 0.88 - ETA: 0s - loss: 0.4104 - accuracy: 0.88 - ETA: 0s - loss: 0.4557 - accuracy: 0.86 - ETA: 0s - loss: 0.4716 - accuracy: 0.85 - ETA: 0s - loss: 0.4685 - accuracy: 0.86 - ETA: 0s - loss: 0.4766 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4761 - accuracy: 0.8552 - val_loss: 0.6240 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7364 - accuracy: 0.71 - ETA: 0s - loss: 0.4809 - accuracy: 0.84 - ETA: 0s - loss: 0.4828 - accuracy: 0.85 - ETA: 0s - loss: 0.4468 - accuracy: 0.86 - ETA: 0s - loss: 0.4526 - accuracy: 0.86 - ETA: 0s - loss: 0.4534 - accuracy: 0.85 - ETA: 0s - loss: 0.4568 - accuracy: 0.85 - ETA: 0s - loss: 0.4628 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4621 - accuracy: 0.8585 - val_loss: 0.6971 - val_accuracy: 0.7090\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.87 - ETA: 0s - loss: 0.4357 - accuracy: 0.86 - ETA: 0s - loss: 0.4478 - accuracy: 0.86 - ETA: 0s - loss: 0.4565 - accuracy: 0.86 - ETA: 0s - loss: 0.4891 - accuracy: 0.84 - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.4906 - accuracy: 0.84 - ETA: 0s - loss: 0.4868 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4875 - accuracy: 0.8447 - val_loss: 0.6334 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.84 - ETA: 0s - loss: 0.4657 - accuracy: 0.85 - ETA: 0s - loss: 0.4607 - accuracy: 0.87 - ETA: 0s - loss: 0.4697 - accuracy: 0.86 - ETA: 0s - loss: 0.4653 - accuracy: 0.86 - ETA: 0s - loss: 0.4648 - accuracy: 0.86 - ETA: 0s - loss: 0.4781 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8518 - val_loss: 0.6162 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.87 - ETA: 0s - loss: 0.4782 - accuracy: 0.83 - ETA: 0s - loss: 0.4910 - accuracy: 0.83 - ETA: 0s - loss: 0.5270 - accuracy: 0.83 - ETA: 0s - loss: 0.5215 - accuracy: 0.83 - ETA: 0s - loss: 0.5552 - accuracy: 0.84 - ETA: 0s - loss: 0.5403 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5442 - accuracy: 0.8440 - val_loss: 0.5918 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.87 - ETA: 0s - loss: 0.5903 - accuracy: 0.79 - ETA: 0s - loss: 0.5299 - accuracy: 0.82 - ETA: 0s - loss: 0.5420 - accuracy: 0.82 - ETA: 0s - loss: 0.5395 - accuracy: 0.82 - ETA: 0s - loss: 0.5335 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5337 - accuracy: 0.8257 - val_loss: 0.8179 - val_accuracy: 0.7403\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.65 - ETA: 0s - loss: 0.6740 - accuracy: 0.78 - ETA: 0s - loss: 0.6379 - accuracy: 0.80 - ETA: 0s - loss: 0.6063 - accuracy: 0.80 - ETA: 0s - loss: 0.6067 - accuracy: 0.81 - ETA: 0s - loss: 0.6063 - accuracy: 0.80 - ETA: 0s - loss: 0.5990 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5942 - accuracy: 0.8108 - val_loss: 1.0867 - val_accuracy: 0.7552\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.78 - ETA: 0s - loss: 0.6155 - accuracy: 0.79 - ETA: 0s - loss: 0.6032 - accuracy: 0.78 - ETA: 0s - loss: 0.5898 - accuracy: 0.79 - ETA: 0s - loss: 0.5819 - accuracy: 0.79 - ETA: 0s - loss: 0.5740 - accuracy: 0.80 - ETA: 0s - loss: 0.5724 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5724 - accuracy: 0.8055 - val_loss: 0.6139 - val_accuracy: 0.7448\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.71 - ETA: 0s - loss: 0.5741 - accuracy: 0.80 - ETA: 0s - loss: 0.5755 - accuracy: 0.81 - ETA: 0s - loss: 0.5882 - accuracy: 0.81 - ETA: 0s - loss: 0.5750 - accuracy: 0.81 - ETA: 0s - loss: 0.5754 - accuracy: 0.81 - ETA: 0s - loss: 0.5741 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5730 - accuracy: 0.8149 - val_loss: 0.6165 - val_accuracy: 0.7418\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.84 - ETA: 0s - loss: 0.5131 - accuracy: 0.83 - ETA: 0s - loss: 0.5179 - accuracy: 0.82 - ETA: 0s - loss: 0.5268 - accuracy: 0.82 - ETA: 0s - loss: 0.5225 - accuracy: 0.82 - ETA: 0s - loss: 0.5705 - accuracy: 0.81 - ETA: 0s - loss: 0.5808 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5827 - accuracy: 0.8052 - val_loss: 0.6337 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5759 - accuracy: 0.81 - ETA: 0s - loss: 0.6289 - accuracy: 0.78 - ETA: 0s - loss: 0.6194 - accuracy: 0.77 - ETA: 0s - loss: 0.6209 - accuracy: 0.78 - ETA: 0s - loss: 0.6524 - accuracy: 0.78 - ETA: 0s - loss: 0.6398 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6390 - accuracy: 0.7831 - val_loss: 0.9495 - val_accuracy: 0.7373\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.84 - ETA: 0s - loss: 0.8404 - accuracy: 0.77 - ETA: 0s - loss: 0.8169 - accuracy: 0.74 - ETA: 0s - loss: 0.7677 - accuracy: 0.71 - ETA: 0s - loss: 0.7383 - accuracy: 0.69 - ETA: 0s - loss: 0.7109 - accuracy: 0.71 - ETA: 0s - loss: 0.7009 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7009 - accuracy: 0.7159 - val_loss: 0.6443 - val_accuracy: 0.7284\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6469 - accuracy: 0.71 - ETA: 0s - loss: 0.6569 - accuracy: 0.72 - ETA: 0s - loss: 0.6553 - accuracy: 0.72 - ETA: 0s - loss: 0.6452 - accuracy: 0.73 - ETA: 0s - loss: 0.6400 - accuracy: 0.73 - ETA: 0s - loss: 0.6347 - accuracy: 0.74 - ETA: 0s - loss: 0.6300 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6287 - accuracy: 0.7495 - val_loss: 0.6525 - val_accuracy: 0.7403\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.84 - ETA: 0s - loss: 0.6146 - accuracy: 0.76 - ETA: 0s - loss: 0.6020 - accuracy: 0.77 - ETA: 0s - loss: 0.6175 - accuracy: 0.76 - ETA: 0s - loss: 0.6234 - accuracy: 0.76 - ETA: 0s - loss: 0.6184 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6253 - accuracy: 0.7581 - val_loss: 0.7949 - val_accuracy: 0.7403\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.71 - ETA: 0s - loss: 0.5876 - accuracy: 0.77 - ETA: 0s - loss: 0.6141 - accuracy: 0.76 - ETA: 0s - loss: 0.6044 - accuracy: 0.77 - ETA: 0s - loss: 0.6045 - accuracy: 0.77 - ETA: 0s - loss: 0.6167 - accuracy: 0.76 - ETA: 0s - loss: 0.6202 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6191 - accuracy: 0.7652 - val_loss: 0.6477 - val_accuracy: 0.7299\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.81 - ETA: 0s - loss: 0.6463 - accuracy: 0.74 - ETA: 0s - loss: 0.6397 - accuracy: 0.74 - ETA: 0s - loss: 0.6432 - accuracy: 0.73 - ETA: 0s - loss: 0.6434 - accuracy: 0.71 - ETA: 0s - loss: 0.6431 - accuracy: 0.72 - ETA: 0s - loss: 0.6398 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6388 - accuracy: 0.7275 - val_loss: 0.6633 - val_accuracy: 0.7343\n",
      "Epoch 28/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7087 - accuracy: 0.68 - ETA: 0s - loss: 0.6541 - accuracy: 0.73 - ETA: 0s - loss: 0.6459 - accuracy: 0.74 - ETA: 0s - loss: 0.6474 - accuracy: 0.74 - ETA: 0s - loss: 0.6537 - accuracy: 0.72 - ETA: 0s - loss: 0.6509 - accuracy: 0.72 - ETA: 0s - loss: 0.6481 - accuracy: 0.7271Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.7316 - val_loss: 0.6684 - val_accuracy: 0.7284\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.81 - ETA: 0s - loss: 2.5514 - accuracy: 0.61 - ETA: 0s - loss: 1.9456 - accuracy: 0.64 - ETA: 0s - loss: 1.4917 - accuracy: 0.65 - ETA: 0s - loss: 1.2671 - accuracy: 0.66 - ETA: 0s - loss: 1.1356 - accuracy: 0.68 - ETA: 0s - loss: 1.0346 - accuracy: 0.69 - 0s 6ms/step - loss: 0.9859 - accuracy: 0.7032 - val_loss: 0.5767 - val_accuracy: 0.7448\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.78 - ETA: 0s - loss: 0.5823 - accuracy: 0.78 - ETA: 0s - loss: 0.5680 - accuracy: 0.78 - ETA: 0s - loss: 0.5636 - accuracy: 0.77 - ETA: 0s - loss: 0.5712 - accuracy: 0.77 - ETA: 0s - loss: 0.5814 - accuracy: 0.76 - ETA: 0s - loss: 0.5832 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5807 - accuracy: 0.7697 - val_loss: 0.6031 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.75 - ETA: 0s - loss: 0.5264 - accuracy: 0.80 - ETA: 0s - loss: 0.5181 - accuracy: 0.81 - ETA: 0s - loss: 0.5173 - accuracy: 0.81 - ETA: 0s - loss: 0.5295 - accuracy: 0.80 - ETA: 0s - loss: 0.5435 - accuracy: 0.80 - ETA: 0s - loss: 0.5456 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5456 - accuracy: 0.7981 - val_loss: 0.5671 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.75 - ETA: 0s - loss: 0.5344 - accuracy: 0.79 - ETA: 0s - loss: 0.5535 - accuracy: 0.78 - ETA: 0s - loss: 0.5413 - accuracy: 0.79 - ETA: 0s - loss: 0.5378 - accuracy: 0.79 - ETA: 0s - loss: 0.5272 - accuracy: 0.80 - ETA: 0s - loss: 0.5236 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5229 - accuracy: 0.8066 - val_loss: 0.5670 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.96 - ETA: 0s - loss: 0.4477 - accuracy: 0.86 - ETA: 0s - loss: 0.5000 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - ETA: 0s - loss: 0.4979 - accuracy: 0.82 - ETA: 0s - loss: 0.5031 - accuracy: 0.82 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5045 - accuracy: 0.8246 - val_loss: 0.5696 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.87 - ETA: 0s - loss: 0.4780 - accuracy: 0.84 - ETA: 0s - loss: 0.4945 - accuracy: 0.83 - ETA: 0s - loss: 0.4828 - accuracy: 0.83 - ETA: 0s - loss: 0.4933 - accuracy: 0.83 - ETA: 0s - loss: 0.5008 - accuracy: 0.83 - ETA: 0s - loss: 0.4983 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4992 - accuracy: 0.8275 - val_loss: 0.5802 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.84 - ETA: 0s - loss: 0.4401 - accuracy: 0.86 - ETA: 0s - loss: 0.4621 - accuracy: 0.85 - ETA: 0s - loss: 0.4755 - accuracy: 0.84 - ETA: 0s - loss: 0.4762 - accuracy: 0.84 - ETA: 0s - loss: 0.4788 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4783 - accuracy: 0.8421 - val_loss: 0.5838 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.87 - ETA: 0s - loss: 0.4346 - accuracy: 0.86 - ETA: 0s - loss: 0.4614 - accuracy: 0.86 - ETA: 0s - loss: 0.4542 - accuracy: 0.86 - ETA: 0s - loss: 0.4697 - accuracy: 0.85 - ETA: 0s - loss: 0.4772 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4752 - accuracy: 0.8488 - val_loss: 0.5724 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.87 - ETA: 0s - loss: 0.4537 - accuracy: 0.85 - ETA: 0s - loss: 0.4261 - accuracy: 0.87 - ETA: 0s - loss: 0.4347 - accuracy: 0.86 - ETA: 0s - loss: 0.4582 - accuracy: 0.85 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.4687 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4642 - accuracy: 0.8526 - val_loss: 0.6051 - val_accuracy: 0.7418\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4151 - accuracy: 0.87 - ETA: 0s - loss: 0.4797 - accuracy: 0.84 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.4454 - accuracy: 0.85 - ETA: 0s - loss: 0.4664 - accuracy: 0.85 - ETA: 0s - loss: 0.4647 - accuracy: 0.85 - ETA: 0s - loss: 0.4645 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4688 - accuracy: 0.8496 - val_loss: 0.5869 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - ETA: 0s - loss: 0.4396 - accuracy: 0.86 - ETA: 0s - loss: 0.4534 - accuracy: 0.85 - ETA: 0s - loss: 0.4359 - accuracy: 0.86 - ETA: 0s - loss: 0.4539 - accuracy: 0.86 - ETA: 0s - loss: 0.4670 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8496 - val_loss: 0.5720 - val_accuracy: 0.7582\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.81 - ETA: 0s - loss: 0.5531 - accuracy: 0.81 - ETA: 0s - loss: 0.5136 - accuracy: 0.82 - ETA: 0s - loss: 0.4944 - accuracy: 0.83 - ETA: 0s - loss: 0.4828 - accuracy: 0.84 - ETA: 0s - loss: 0.4923 - accuracy: 0.84 - ETA: 0s - loss: 0.4953 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4935 - accuracy: 0.8384 - val_loss: 0.7003 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.87 - ETA: 0s - loss: 0.4714 - accuracy: 0.84 - ETA: 0s - loss: 0.4887 - accuracy: 0.84 - ETA: 0s - loss: 0.4763 - accuracy: 0.85 - ETA: 0s - loss: 0.4759 - accuracy: 0.85 - ETA: 0s - loss: 0.4775 - accuracy: 0.84 - ETA: 0s - loss: 0.4859 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4912 - accuracy: 0.8421 - val_loss: 0.6424 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.87 - ETA: 0s - loss: 0.4837 - accuracy: 0.87 - ETA: 0s - loss: 0.4825 - accuracy: 0.86 - ETA: 0s - loss: 0.4703 - accuracy: 0.86 - ETA: 0s - loss: 0.4783 - accuracy: 0.85 - ETA: 0s - loss: 0.4710 - accuracy: 0.85 - ETA: 0s - loss: 0.4737 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4742 - accuracy: 0.8555 - val_loss: 0.6800 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 1.00 - ETA: 0s - loss: 0.4549 - accuracy: 0.85 - ETA: 0s - loss: 0.4657 - accuracy: 0.85 - ETA: 0s - loss: 0.4928 - accuracy: 0.84 - ETA: 0s - loss: 0.4752 - accuracy: 0.85 - ETA: 0s - loss: 0.4635 - accuracy: 0.85 - ETA: 0s - loss: 0.4741 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4742 - accuracy: 0.8563 - val_loss: 0.7297 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4546 - accuracy: 0.84 - ETA: 0s - loss: 0.4307 - accuracy: 0.87 - ETA: 0s - loss: 0.4055 - accuracy: 0.88 - ETA: 0s - loss: 0.4292 - accuracy: 0.87 - ETA: 0s - loss: 0.4521 - accuracy: 0.86 - ETA: 0s - loss: 0.4587 - accuracy: 0.86 - ETA: 0s - loss: 0.4664 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4669 - accuracy: 0.8608 - val_loss: 0.6111 - val_accuracy: 0.7194\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.90 - ETA: 0s - loss: 0.4739 - accuracy: 0.84 - ETA: 0s - loss: 0.4655 - accuracy: 0.85 - ETA: 0s - loss: 0.4639 - accuracy: 0.85 - ETA: 0s - loss: 0.4645 - accuracy: 0.85 - ETA: 0s - loss: 0.4692 - accuracy: 0.85 - ETA: 0s - loss: 0.4602 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4602 - accuracy: 0.8600 - val_loss: 0.5513 - val_accuracy: 0.7418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.78 - ETA: 0s - loss: 0.3974 - accuracy: 0.88 - ETA: 0s - loss: 0.4193 - accuracy: 0.87 - ETA: 0s - loss: 0.4504 - accuracy: 0.86 - ETA: 0s - loss: 0.4490 - accuracy: 0.86 - ETA: 0s - loss: 0.4542 - accuracy: 0.86 - ETA: 0s - loss: 0.4418 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4441 - accuracy: 0.8679 - val_loss: 0.7493 - val_accuracy: 0.7194\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.87 - ETA: 0s - loss: 0.3770 - accuracy: 0.88 - ETA: 0s - loss: 0.4147 - accuracy: 0.88 - ETA: 0s - loss: 0.4223 - accuracy: 0.88 - ETA: 0s - loss: 0.4447 - accuracy: 0.87 - ETA: 0s - loss: 0.4561 - accuracy: 0.87 - ETA: 0s - loss: 0.5268 - accuracy: 0.86 - 0s 4ms/step - loss: 0.5259 - accuracy: 0.8652 - val_loss: 0.5882 - val_accuracy: 0.7433\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.84 - ETA: 0s - loss: 0.4904 - accuracy: 0.83 - ETA: 0s - loss: 0.4811 - accuracy: 0.84 - ETA: 0s - loss: 0.4759 - accuracy: 0.85 - ETA: 0s - loss: 0.4927 - accuracy: 0.84 - ETA: 0s - loss: 0.4990 - accuracy: 0.84 - ETA: 0s - loss: 0.4938 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4928 - accuracy: 0.8492 - val_loss: 0.7037 - val_accuracy: 0.7343\n",
      "Epoch 21/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6481 - accuracy: 0.75 - ETA: 0s - loss: 0.5398 - accuracy: 0.83 - ETA: 0s - loss: 0.4895 - accuracy: 0.84 - ETA: 0s - loss: 0.4923 - accuracy: 0.85 - ETA: 0s - loss: 0.4846 - accuracy: 0.85 - ETA: 0s - loss: 0.4819 - accuracy: 0.85 - ETA: 0s - loss: 0.4741 - accuracy: 0.8586Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4730 - accuracy: 0.8582 - val_loss: 0.6010 - val_accuracy: 0.7328\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1970 - accuracy: 0.56 - ETA: 0s - loss: 1.8006 - accuracy: 0.58 - ETA: 0s - loss: 1.5421 - accuracy: 0.59 - ETA: 0s - loss: 1.2877 - accuracy: 0.61 - ETA: 0s - loss: 1.1235 - accuracy: 0.63 - ETA: 0s - loss: 1.0189 - accuracy: 0.65 - ETA: 0s - loss: 0.9448 - accuracy: 0.67 - 1s 6ms/step - loss: 0.9192 - accuracy: 0.6771 - val_loss: 0.6096 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.75 - ETA: 0s - loss: 0.5939 - accuracy: 0.73 - ETA: 0s - loss: 0.5792 - accuracy: 0.75 - ETA: 0s - loss: 0.5715 - accuracy: 0.77 - ETA: 0s - loss: 0.5675 - accuracy: 0.76 - ETA: 0s - loss: 0.5819 - accuracy: 0.76 - ETA: 0s - loss: 0.5908 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5910 - accuracy: 0.7615 - val_loss: 0.5825 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6423 - accuracy: 0.71 - ETA: 0s - loss: 0.5210 - accuracy: 0.79 - ETA: 0s - loss: 0.5358 - accuracy: 0.78 - ETA: 0s - loss: 0.5503 - accuracy: 0.77 - ETA: 0s - loss: 0.5425 - accuracy: 0.79 - ETA: 0s - loss: 0.5447 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5506 - accuracy: 0.7846 - val_loss: 0.5546 - val_accuracy: 0.7522\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.87 - ETA: 0s - loss: 0.4830 - accuracy: 0.81 - ETA: 0s - loss: 0.4948 - accuracy: 0.81 - ETA: 0s - loss: 0.5047 - accuracy: 0.81 - ETA: 0s - loss: 0.5024 - accuracy: 0.81 - ETA: 0s - loss: 0.5107 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5205 - accuracy: 0.7996 - val_loss: 0.5914 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.84 - ETA: 0s - loss: 0.4807 - accuracy: 0.82 - ETA: 0s - loss: 0.4927 - accuracy: 0.82 - ETA: 0s - loss: 0.5016 - accuracy: 0.81 - ETA: 0s - loss: 0.4911 - accuracy: 0.81 - ETA: 0s - loss: 0.4883 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4970 - accuracy: 0.8160 - val_loss: 0.5470 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.84 - ETA: 0s - loss: 0.5314 - accuracy: 0.79 - ETA: 0s - loss: 0.5196 - accuracy: 0.80 - ETA: 0s - loss: 0.5060 - accuracy: 0.81 - ETA: 0s - loss: 0.5028 - accuracy: 0.81 - ETA: 0s - loss: 0.5038 - accuracy: 0.81 - ETA: 0s - loss: 0.5068 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5085 - accuracy: 0.8141 - val_loss: 0.5741 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5190 - accuracy: 0.78 - ETA: 0s - loss: 0.4778 - accuracy: 0.85 - ETA: 0s - loss: 0.4575 - accuracy: 0.85 - ETA: 0s - loss: 0.4602 - accuracy: 0.84 - ETA: 0s - loss: 0.5019 - accuracy: 0.83 - ETA: 0s - loss: 0.5076 - accuracy: 0.83 - ETA: 0s - loss: 0.5107 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5104 - accuracy: 0.8335 - val_loss: 0.5669 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.90 - ETA: 0s - loss: 0.5398 - accuracy: 0.82 - ETA: 0s - loss: 0.5048 - accuracy: 0.84 - ETA: 0s - loss: 0.4906 - accuracy: 0.84 - ETA: 0s - loss: 0.4825 - accuracy: 0.84 - ETA: 0s - loss: 0.4941 - accuracy: 0.83 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4975 - accuracy: 0.8343 - val_loss: 0.5963 - val_accuracy: 0.7239\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.81 - ETA: 0s - loss: 0.4457 - accuracy: 0.84 - ETA: 0s - loss: 0.4429 - accuracy: 0.86 - ETA: 0s - loss: 0.4513 - accuracy: 0.86 - ETA: 0s - loss: 0.4524 - accuracy: 0.86 - ETA: 0s - loss: 0.4634 - accuracy: 0.85 - ETA: 0s - loss: 0.4629 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4661 - accuracy: 0.8544 - val_loss: 0.5599 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.87 - ETA: 0s - loss: 0.4770 - accuracy: 0.84 - ETA: 0s - loss: 0.4550 - accuracy: 0.84 - ETA: 0s - loss: 0.4463 - accuracy: 0.85 - ETA: 0s - loss: 0.4580 - accuracy: 0.84 - ETA: 0s - loss: 0.4676 - accuracy: 0.84 - ETA: 0s - loss: 0.4704 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4690 - accuracy: 0.8503 - val_loss: 0.5998 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.90 - ETA: 0s - loss: 0.3850 - accuracy: 0.89 - ETA: 0s - loss: 0.4275 - accuracy: 0.87 - ETA: 0s - loss: 0.4190 - accuracy: 0.87 - ETA: 0s - loss: 0.4344 - accuracy: 0.87 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4540 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4522 - accuracy: 0.8570 - val_loss: 0.6598 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.90 - ETA: 0s - loss: 0.4326 - accuracy: 0.85 - ETA: 0s - loss: 0.4335 - accuracy: 0.86 - ETA: 0s - loss: 0.4269 - accuracy: 0.86 - ETA: 0s - loss: 0.4311 - accuracy: 0.86 - ETA: 0s - loss: 0.4401 - accuracy: 0.86 - ETA: 0s - loss: 0.4320 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4306 - accuracy: 0.8667 - val_loss: 0.8927 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.2018 - accuracy: 1.00 - ETA: 0s - loss: 0.3913 - accuracy: 0.88 - ETA: 0s - loss: 0.4188 - accuracy: 0.88 - ETA: 0s - loss: 0.4160 - accuracy: 0.88 - ETA: 0s - loss: 0.4351 - accuracy: 0.87 - ETA: 0s - loss: 0.4328 - accuracy: 0.8750Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.8697 - val_loss: 0.5541 - val_accuracy: 0.7373\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cf04479e4b5d2883308fd6a586087379</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7552238702774048</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7261372381448851</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_small: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9803 - accuracy: 0.40 - ETA: 0s - loss: 3.6613 - accuracy: 0.61 - ETA: 0s - loss: 2.6786 - accuracy: 0.60 - ETA: 0s - loss: 2.0353 - accuracy: 0.62 - ETA: 0s - loss: 1.7431 - accuracy: 0.61 - ETA: 0s - loss: 1.5268 - accuracy: 0.62 - 0s 6ms/step - loss: 1.4056 - accuracy: 0.6282 - val_loss: 0.5790 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.71 - ETA: 0s - loss: 0.6615 - accuracy: 0.63 - ETA: 0s - loss: 0.6083 - accuracy: 0.68 - ETA: 0s - loss: 0.6114 - accuracy: 0.68 - ETA: 0s - loss: 0.6035 - accuracy: 0.70 - ETA: 0s - loss: 0.6008 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6017 - accuracy: 0.6984 - val_loss: 0.5671 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.78 - ETA: 0s - loss: 0.5695 - accuracy: 0.72 - ETA: 0s - loss: 0.5475 - accuracy: 0.73 - ETA: 0s - loss: 0.5829 - accuracy: 0.72 - ETA: 0s - loss: 0.5662 - accuracy: 0.73 - ETA: 0s - loss: 0.5724 - accuracy: 0.73 - 0s 4ms/step - loss: 0.5673 - accuracy: 0.7387 - val_loss: 0.7412 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.6176 - accuracy: 0.84 - ETA: 0s - loss: 0.6822 - accuracy: 0.77 - ETA: 0s - loss: 0.6141 - accuracy: 0.77 - ETA: 0s - loss: 0.5793 - accuracy: 0.76 - ETA: 0s - loss: 0.5627 - accuracy: 0.76 - ETA: 0s - loss: 0.5546 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5500 - accuracy: 0.7727 - val_loss: 0.6293 - val_accuracy: 0.6940\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.75 - ETA: 0s - loss: 0.5112 - accuracy: 0.76 - ETA: 0s - loss: 0.4538 - accuracy: 0.79 - ETA: 0s - loss: 0.4470 - accuracy: 0.79 - ETA: 0s - loss: 0.4488 - accuracy: 0.79 - ETA: 0s - loss: 0.4491 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4512 - accuracy: 0.8018 - val_loss: 0.7401 - val_accuracy: 0.6761\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.4430 - accuracy: 0.81 - ETA: 0s - loss: 0.4311 - accuracy: 0.82 - ETA: 0s - loss: 0.4393 - accuracy: 0.81 - ETA: 0s - loss: 0.4335 - accuracy: 0.81 - ETA: 0s - loss: 0.4338 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8134 - val_loss: 0.8337 - val_accuracy: 0.6761\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.78 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4213 - accuracy: 0.81 - ETA: 0s - loss: 0.4228 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8182 - val_loss: 0.6173 - val_accuracy: 0.6851\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.90 - ETA: 0s - loss: 0.3192 - accuracy: 0.83 - ETA: 0s - loss: 0.3326 - accuracy: 0.83 - ETA: 0s - loss: 0.3650 - accuracy: 0.83 - ETA: 0s - loss: 0.3836 - accuracy: 0.82 - ETA: 0s - loss: 0.3822 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3785 - accuracy: 0.8302 - val_loss: 1.5242 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.93 - ETA: 0s - loss: 0.5260 - accuracy: 0.87 - ETA: 0s - loss: 0.5049 - accuracy: 0.84 - ETA: 0s - loss: 0.4881 - accuracy: 0.83 - ETA: 0s - loss: 0.4921 - accuracy: 0.82 - ETA: 0s - loss: 0.5012 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5000 - accuracy: 0.8208 - val_loss: 0.9004 - val_accuracy: 0.6701\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.78 - ETA: 0s - loss: 0.4991 - accuracy: 0.83 - ETA: 0s - loss: 0.5253 - accuracy: 0.80 - ETA: 0s - loss: 0.4881 - accuracy: 0.81 - ETA: 0s - loss: 0.4883 - accuracy: 0.80 - ETA: 0s - loss: 0.4668 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4675 - accuracy: 0.8145 - val_loss: 0.7246 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.81 - ETA: 0s - loss: 0.4378 - accuracy: 0.82 - ETA: 0s - loss: 0.4572 - accuracy: 0.80 - ETA: 0s - loss: 0.4759 - accuracy: 0.79 - ETA: 0s - loss: 0.4665 - accuracy: 0.79 - ETA: 0s - loss: 0.4547 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4616 - accuracy: 0.8052 - val_loss: 0.6294 - val_accuracy: 0.7239\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.78 - ETA: 0s - loss: 0.5934 - accuracy: 0.79 - ETA: 0s - loss: 0.5270 - accuracy: 0.79 - ETA: 0s - loss: 0.5171 - accuracy: 0.80 - ETA: 0s - loss: 0.5430 - accuracy: 0.80 - ETA: 0s - loss: 0.5304 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5151 - accuracy: 0.8100 - val_loss: 0.9460 - val_accuracy: 0.7030\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.84 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.84 - ETA: 0s - loss: 0.3775 - accuracy: 0.84 - ETA: 0s - loss: 0.3739 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3707 - accuracy: 0.8533 - val_loss: 0.8861 - val_accuracy: 0.7030\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.84 - ETA: 0s - loss: 0.2933 - accuracy: 0.86 - ETA: 0s - loss: 0.2940 - accuracy: 0.87 - ETA: 0s - loss: 0.3188 - accuracy: 0.86 - ETA: 0s - loss: 0.3182 - accuracy: 0.86 - ETA: 0s - loss: 0.3090 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3128 - accuracy: 0.8690 - val_loss: 1.2003 - val_accuracy: 0.6791\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.90 - ETA: 0s - loss: 0.3158 - accuracy: 0.85 - ETA: 0s - loss: 0.3550 - accuracy: 0.86 - ETA: 0s - loss: 0.3549 - accuracy: 0.86 - ETA: 0s - loss: 0.3815 - accuracy: 0.86 - ETA: 0s - loss: 0.3796 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8615 - val_loss: 1.7953 - val_accuracy: 0.6522\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.81 - ETA: 0s - loss: 0.3343 - accuracy: 0.88 - ETA: 0s - loss: 0.3426 - accuracy: 0.88 - ETA: 0s - loss: 0.3303 - accuracy: 0.88 - ETA: 0s - loss: 0.3280 - accuracy: 0.88 - ETA: 0s - loss: 0.3247 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3369 - accuracy: 0.8764 - val_loss: 1.7396 - val_accuracy: 0.7060\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.90 - ETA: 0s - loss: 0.2540 - accuracy: 0.90 - ETA: 0s - loss: 0.2766 - accuracy: 0.89 - ETA: 0s - loss: 0.2961 - accuracy: 0.90 - ETA: 0s - loss: 0.3074 - accuracy: 0.89 - ETA: 0s - loss: 0.3470 - accuracy: 0.88 - ETA: 0s - loss: 0.3472 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3472 - accuracy: 0.8813 - val_loss: 0.9774 - val_accuracy: 0.6866\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.90 - ETA: 0s - loss: 0.2883 - accuracy: 0.89 - ETA: 0s - loss: 0.2940 - accuracy: 0.89 - ETA: 0s - loss: 0.2967 - accuracy: 0.89 - ETA: 0s - loss: 0.2915 - accuracy: 0.89 - ETA: 0s - loss: 0.2810 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2788 - accuracy: 0.9003 - val_loss: 1.5092 - val_accuracy: 0.7254\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 1.00 - ETA: 0s - loss: 0.2893 - accuracy: 0.90 - ETA: 0s - loss: 0.3007 - accuracy: 0.91 - ETA: 0s - loss: 0.2919 - accuracy: 0.90 - ETA: 0s - loss: 0.2726 - accuracy: 0.91 - ETA: 0s - loss: 0.2622 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2689 - accuracy: 0.9149 - val_loss: 1.5253 - val_accuracy: 0.7269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - ETA: 0s - loss: 0.2061 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2240 - accuracy: 0.91 - ETA: 0s - loss: 0.2291 - accuracy: 0.91 - ETA: 0s - loss: 0.2599 - accuracy: 0.9101Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2563 - accuracy: 0.9097 - val_loss: 1.8530 - val_accuracy: 0.7239\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8763 - accuracy: 0.65 - ETA: 0s - loss: 2.5329 - accuracy: 0.60 - ETA: 0s - loss: 1.7664 - accuracy: 0.60 - ETA: 0s - loss: 1.4627 - accuracy: 0.60 - ETA: 0s - loss: 1.2833 - accuracy: 0.62 - ETA: 0s - loss: 1.1927 - accuracy: 0.63 - 0s 5ms/step - loss: 1.0998 - accuracy: 0.6465 - val_loss: 0.5834 - val_accuracy: 0.7299\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.78 - ETA: 0s - loss: 0.6353 - accuracy: 0.73 - ETA: 0s - loss: 0.6089 - accuracy: 0.74 - ETA: 0s - loss: 0.5941 - accuracy: 0.75 - ETA: 0s - loss: 0.6000 - accuracy: 0.75 - ETA: 0s - loss: 0.5992 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5931 - accuracy: 0.7495 - val_loss: 0.5425 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.78 - ETA: 0s - loss: 0.5096 - accuracy: 0.80 - ETA: 0s - loss: 0.5178 - accuracy: 0.79 - ETA: 0s - loss: 0.5380 - accuracy: 0.78 - ETA: 0s - loss: 0.5419 - accuracy: 0.77 - ETA: 0s - loss: 0.5499 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5436 - accuracy: 0.7749 - val_loss: 0.5662 - val_accuracy: 0.7493\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.84 - ETA: 0s - loss: 0.4862 - accuracy: 0.81 - ETA: 0s - loss: 0.4923 - accuracy: 0.80 - ETA: 0s - loss: 0.4821 - accuracy: 0.81 - ETA: 0s - loss: 0.4873 - accuracy: 0.81 - ETA: 0s - loss: 0.4966 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5041 - accuracy: 0.8022 - val_loss: 0.5622 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.78 - ETA: 0s - loss: 0.5083 - accuracy: 0.80 - ETA: 0s - loss: 0.5127 - accuracy: 0.79 - ETA: 0s - loss: 0.4858 - accuracy: 0.81 - ETA: 0s - loss: 0.5116 - accuracy: 0.80 - ETA: 0s - loss: 0.5001 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5026 - accuracy: 0.8156 - val_loss: 0.5468 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.84 - ETA: 0s - loss: 0.4657 - accuracy: 0.83 - ETA: 0s - loss: 0.4746 - accuracy: 0.82 - ETA: 0s - loss: 0.4745 - accuracy: 0.83 - ETA: 0s - loss: 0.4768 - accuracy: 0.83 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4872 - accuracy: 0.8253 - val_loss: 0.5745 - val_accuracy: 0.7358\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.81 - ETA: 0s - loss: 0.4903 - accuracy: 0.83 - ETA: 0s - loss: 0.4796 - accuracy: 0.83 - ETA: 0s - loss: 0.4647 - accuracy: 0.83 - ETA: 0s - loss: 0.5080 - accuracy: 0.83 - ETA: 0s - loss: 0.5075 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5059 - accuracy: 0.8320 - val_loss: 0.5696 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.78 - ETA: 0s - loss: 0.4874 - accuracy: 0.81 - ETA: 0s - loss: 0.4861 - accuracy: 0.82 - ETA: 0s - loss: 0.4817 - accuracy: 0.82 - ETA: 0s - loss: 0.5017 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4936 - accuracy: 0.8346 - val_loss: 0.9707 - val_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.9115 - accuracy: 0.81 - ETA: 0s - loss: 0.6996 - accuracy: 0.83 - ETA: 0s - loss: 0.6544 - accuracy: 0.79 - ETA: 0s - loss: 0.6287 - accuracy: 0.79 - ETA: 0s - loss: 0.5973 - accuracy: 0.79 - ETA: 0s - loss: 0.5746 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5645 - accuracy: 0.8052 - val_loss: 0.6512 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.87 - ETA: 0s - loss: 0.5038 - accuracy: 0.81 - ETA: 0s - loss: 0.4860 - accuracy: 0.82 - ETA: 0s - loss: 0.4998 - accuracy: 0.83 - ETA: 0s - loss: 0.4907 - accuracy: 0.83 - ETA: 0s - loss: 0.4883 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4892 - accuracy: 0.8358 - val_loss: 0.5504 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.71 - ETA: 0s - loss: 0.4990 - accuracy: 0.82 - ETA: 0s - loss: 0.4777 - accuracy: 0.82 - ETA: 0s - loss: 0.4775 - accuracy: 0.83 - ETA: 0s - loss: 0.4674 - accuracy: 0.84 - ETA: 0s - loss: 0.4771 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4784 - accuracy: 0.8380 - val_loss: 0.6065 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.78 - ETA: 0s - loss: 0.4670 - accuracy: 0.83 - ETA: 0s - loss: 0.4392 - accuracy: 0.83 - ETA: 0s - loss: 0.4410 - accuracy: 0.83 - ETA: 0s - loss: 0.4380 - accuracy: 0.84 - ETA: 0s - loss: 0.4367 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4567 - accuracy: 0.8387 - val_loss: 0.6214 - val_accuracy: 0.7119\n",
      "Epoch 13/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.5950 - accuracy: 0.71 - ETA: 0s - loss: 0.4630 - accuracy: 0.83 - ETA: 0s - loss: 0.4574 - accuracy: 0.83 - ETA: 0s - loss: 0.4644 - accuracy: 0.83 - ETA: 0s - loss: 0.4553 - accuracy: 0.83 - ETA: 0s - loss: 0.4508 - accuracy: 0.8413Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.8432 - val_loss: 0.7922 - val_accuracy: 0.7164\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9278 - accuracy: 0.56 - ETA: 0s - loss: 3.2446 - accuracy: 0.60 - ETA: 0s - loss: 2.7028 - accuracy: 0.62 - ETA: 0s - loss: 2.3272 - accuracy: 0.61 - ETA: 0s - loss: 1.9262 - accuracy: 0.61 - ETA: 0s - loss: 1.6690 - accuracy: 0.61 - 0s 5ms/step - loss: 1.6106 - accuracy: 0.6193 - val_loss: 0.5786 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4163 - accuracy: 0.78 - ETA: 0s - loss: 0.6258 - accuracy: 0.69 - ETA: 0s - loss: 0.6003 - accuracy: 0.69 - ETA: 0s - loss: 0.6178 - accuracy: 0.69 - ETA: 0s - loss: 0.5966 - accuracy: 0.70 - ETA: 0s - loss: 0.5964 - accuracy: 0.70 - 0s 4ms/step - loss: 0.5921 - accuracy: 0.7096 - val_loss: 0.5586 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.68 - ETA: 0s - loss: 0.5341 - accuracy: 0.74 - ETA: 0s - loss: 0.5341 - accuracy: 0.73 - ETA: 0s - loss: 0.5330 - accuracy: 0.74 - ETA: 0s - loss: 0.5317 - accuracy: 0.74 - ETA: 0s - loss: 0.5347 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5361 - accuracy: 0.7518 - val_loss: 0.5438 - val_accuracy: 0.7537\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.78 - ETA: 0s - loss: 0.5022 - accuracy: 0.79 - ETA: 0s - loss: 0.4886 - accuracy: 0.79 - ETA: 0s - loss: 0.4847 - accuracy: 0.79 - ETA: 0s - loss: 0.4954 - accuracy: 0.78 - ETA: 0s - loss: 0.5009 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5068 - accuracy: 0.7869 - val_loss: 0.7626 - val_accuracy: 0.6522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.75 - ETA: 0s - loss: 0.4761 - accuracy: 0.81 - ETA: 0s - loss: 0.4570 - accuracy: 0.81 - ETA: 0s - loss: 0.4683 - accuracy: 0.81 - ETA: 0s - loss: 0.4635 - accuracy: 0.81 - ETA: 0s - loss: 0.4681 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4811 - accuracy: 0.8059 - val_loss: 0.5791 - val_accuracy: 0.6716\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.90 - ETA: 0s - loss: 0.4633 - accuracy: 0.78 - ETA: 0s - loss: 0.4345 - accuracy: 0.81 - ETA: 0s - loss: 0.4447 - accuracy: 0.80 - ETA: 0s - loss: 0.4552 - accuracy: 0.80 - ETA: 0s - loss: 0.4560 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4567 - accuracy: 0.8052 - val_loss: 0.5452 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.3455 - accuracy: 0.86 - ETA: 0s - loss: 0.3779 - accuracy: 0.85 - ETA: 0s - loss: 0.3867 - accuracy: 0.84 - ETA: 0s - loss: 0.3913 - accuracy: 0.84 - ETA: 0s - loss: 0.4048 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4056 - accuracy: 0.8343 - val_loss: 0.6067 - val_accuracy: 0.7194\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5539 - accuracy: 0.65 - ETA: 0s - loss: 0.5084 - accuracy: 0.83 - ETA: 0s - loss: 0.4546 - accuracy: 0.83 - ETA: 0s - loss: 0.4636 - accuracy: 0.83 - ETA: 0s - loss: 0.4472 - accuracy: 0.83 - ETA: 0s - loss: 0.4396 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8373 - val_loss: 0.5702 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.93 - ETA: 0s - loss: 0.3223 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3376 - accuracy: 0.87 - ETA: 0s - loss: 0.3580 - accuracy: 0.86 - ETA: 0s - loss: 0.3677 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3733 - accuracy: 0.8555 - val_loss: 0.6621 - val_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.90 - ETA: 0s - loss: 0.3633 - accuracy: 0.85 - ETA: 0s - loss: 0.3511 - accuracy: 0.86 - ETA: 0s - loss: 0.3627 - accuracy: 0.86 - ETA: 0s - loss: 0.3737 - accuracy: 0.86 - ETA: 0s - loss: 0.3869 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3873 - accuracy: 0.8645 - val_loss: 0.5879 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.96 - ETA: 0s - loss: 0.4186 - accuracy: 0.84 - ETA: 0s - loss: 0.3763 - accuracy: 0.86 - ETA: 0s - loss: 0.3791 - accuracy: 0.86 - ETA: 0s - loss: 0.3735 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8608 - val_loss: 0.6296 - val_accuracy: 0.7149\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.3772 - accuracy: 0.87 - ETA: 0s - loss: 0.3834 - accuracy: 0.87 - ETA: 0s - loss: 0.3742 - accuracy: 0.86 - ETA: 0s - loss: 0.3742 - accuracy: 0.86 - ETA: 0s - loss: 0.3859 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8596 - val_loss: 0.8608 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2208 - accuracy: 0.90 - ETA: 0s - loss: 0.3515 - accuracy: 0.87 - ETA: 0s - loss: 0.3782 - accuracy: 0.86 - ETA: 0s - loss: 0.3762 - accuracy: 0.86 - ETA: 0s - loss: 0.3697 - accuracy: 0.86 - ETA: 0s - loss: 0.4063 - accuracy: 0.8526Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8552 - val_loss: 0.8549 - val_accuracy: 0.6597\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d85413ea4f6815c933337ec788bf5b09</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7472636898358663</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5723571024044287</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0390 - accuracy: 0.53 - ETA: 0s - loss: 1.7545 - accuracy: 0.56 - ETA: 0s - loss: 1.2406 - accuracy: 0.61 - ETA: 0s - loss: 1.0289 - accuracy: 0.64 - ETA: 0s - loss: 0.9542 - accuracy: 0.65 - ETA: 0s - loss: 0.9029 - accuracy: 0.61 - 0s 5ms/step - loss: 0.8922 - accuracy: 0.6114 - val_loss: 0.6833 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6616 - accuracy: 0.40 - ETA: 0s - loss: 0.6805 - accuracy: 0.51 - ETA: 0s - loss: 0.6786 - accuracy: 0.61 - ETA: 0s - loss: 0.6771 - accuracy: 0.60 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - 0s 4ms/step - loss: 0.6800 - accuracy: 0.5823 - val_loss: 0.6800 - val_accuracy: 0.7224\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7673 - accuracy: 0.46 - ETA: 0s - loss: 0.6950 - accuracy: 0.40 - ETA: 0s - loss: 0.6924 - accuracy: 0.43 - ETA: 0s - loss: 0.6946 - accuracy: 0.46 - ETA: 0s - loss: 0.6949 - accuracy: 0.45 - 0s 4ms/step - loss: 0.6925 - accuracy: 0.4584 - val_loss: 0.6633 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.75 - ETA: 0s - loss: 0.6981 - accuracy: 0.70 - ETA: 0s - loss: 0.7005 - accuracy: 0.61 - ETA: 0s - loss: 0.6892 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.61 - ETA: 0s - loss: 0.6787 - accuracy: 0.63 - 0s 4ms/step - loss: 0.6787 - accuracy: 0.6342 - val_loss: 0.6494 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.71 - ETA: 0s - loss: 0.6937 - accuracy: 0.64 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.7002 - accuracy: 0.57 - ETA: 0s - loss: 0.6948 - accuracy: 0.61 - 0s 3ms/step - loss: 0.6921 - accuracy: 0.6320 - val_loss: 0.6773 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5852 - accuracy: 0.84 - ETA: 0s - loss: 0.6949 - accuracy: 0.73 - ETA: 0s - loss: 0.7068 - accuracy: 0.71 - ETA: 0s - loss: 0.6986 - accuracy: 0.61 - ETA: 0s - loss: 0.6992 - accuracy: 0.61 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.6379 - val_loss: 0.6696 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.71 - ETA: 0s - loss: 0.6680 - accuracy: 0.73 - ETA: 0s - loss: 0.6903 - accuracy: 0.71 - ETA: 0s - loss: 0.6874 - accuracy: 0.71 - ETA: 0s - loss: 0.6843 - accuracy: 0.67 - 0s 3ms/step - loss: 0.6875 - accuracy: 0.6301 - val_loss: 0.7080 - val_accuracy: 0.2448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6993 - accuracy: 0.37 - ETA: 0s - loss: 0.6665 - accuracy: 0.47 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.63 - ETA: 0s - loss: 0.6685 - accuracy: 0.65 - 0s 3ms/step - loss: 0.6709 - accuracy: 0.6558 - val_loss: 0.6657 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.68 - ETA: 0s - loss: 0.6640 - accuracy: 0.38 - ETA: 0s - loss: 0.6779 - accuracy: 0.52 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.62 - ETA: 0s - loss: 0.7057 - accuracy: 0.63 - 0s 4ms/step - loss: 0.7057 - accuracy: 0.6364 - val_loss: 0.6455 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7491 - accuracy: 0.65 - ETA: 0s - loss: 0.7080 - accuracy: 0.65 - ETA: 0s - loss: 0.6973 - accuracy: 0.67 - ETA: 0s - loss: 0.6985 - accuracy: 0.65 - ETA: 0s - loss: 0.7006 - accuracy: 0.65 - 0s 3ms/step - loss: 0.6992 - accuracy: 0.6099 - val_loss: 0.6961 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.25 - ETA: 0s - loss: 0.7101 - accuracy: 0.32 - ETA: 0s - loss: 0.7036 - accuracy: 0.31 - ETA: 0s - loss: 0.6988 - accuracy: 0.34 - ETA: 0s - loss: 0.6984 - accuracy: 0.37 - ETA: 0s - loss: 0.6938 - accuracy: 0.40 - 0s 4ms/step - loss: 0.6938 - accuracy: 0.4072 - val_loss: 0.6753 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.78 - ETA: 0s - loss: 0.6820 - accuracy: 0.71 - ETA: 0s - loss: 0.6878 - accuracy: 0.70 - ETA: 0s - loss: 0.6915 - accuracy: 0.63 - ETA: 0s - loss: 0.6934 - accuracy: 0.55 - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5237 - val_loss: 0.7007 - val_accuracy: 0.3045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7387 - accuracy: 0.37 - ETA: 0s - loss: 0.7095 - accuracy: 0.37 - ETA: 0s - loss: 0.6963 - accuracy: 0.32 - ETA: 0s - loss: 0.6977 - accuracy: 0.43 - ETA: 0s - loss: 0.6983 - accuracy: 0.48 - 0s 3ms/step - loss: 0.6949 - accuracy: 0.4569 - val_loss: 0.6944 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.28 - ETA: 0s - loss: 0.6984 - accuracy: 0.67 - ETA: 0s - loss: 0.7020 - accuracy: 0.55 - ETA: 0s - loss: 0.7071 - accuracy: 0.48 - ETA: 0s - loss: 0.7003 - accuracy: 0.43 - 0s 3ms/step - loss: 0.6936 - accuracy: 0.4938 - val_loss: 0.6609 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.62 - ETA: 0s - loss: 0.7161 - accuracy: 0.67 - ETA: 0s - loss: 0.7068 - accuracy: 0.52 - ETA: 0s - loss: 0.7094 - accuracy: 0.45 - ETA: 0s - loss: 0.7049 - accuracy: 0.41 - 0s 4ms/step - loss: 0.6947 - accuracy: 0.4535 - val_loss: 0.6632 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.7069 - accuracy: 0.68 - ETA: 0s - loss: 0.6896 - accuracy: 0.70 - ETA: 0s - loss: 0.6847 - accuracy: 0.71 - ETA: 0s - loss: 0.6845 - accuracy: 0.71 - ETA: 0s - loss: 0.6933 - accuracy: 0.6676Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.6040 - val_loss: 0.7154 - val_accuracy: 0.3045\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8315 - accuracy: 0.75 - ETA: 0s - loss: 1.0877 - accuracy: 0.63 - ETA: 0s - loss: 0.9302 - accuracy: 0.65 - ETA: 0s - loss: 0.8705 - accuracy: 0.65 - ETA: 0s - loss: 0.8420 - accuracy: 0.59 - ETA: 0s - loss: 0.8112 - accuracy: 0.59 - 0s 5ms/step - loss: 0.8112 - accuracy: 0.5901 - val_loss: 0.6387 - val_accuracy: 0.7373\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.71 - ETA: 0s - loss: 0.6866 - accuracy: 0.72 - ETA: 0s - loss: 0.6753 - accuracy: 0.73 - ETA: 0s - loss: 0.6827 - accuracy: 0.71 - ETA: 0s - loss: 0.6858 - accuracy: 0.67 - ETA: 0s - loss: 0.6922 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6925 - accuracy: 0.6204 - val_loss: 0.7261 - val_accuracy: 0.3045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.34 - ETA: 0s - loss: 0.6930 - accuracy: 0.36 - ETA: 0s - loss: 0.7027 - accuracy: 0.37 - ETA: 0s - loss: 0.6944 - accuracy: 0.37 - ETA: 0s - loss: 0.6892 - accuracy: 0.37 - ETA: 0s - loss: 0.6915 - accuracy: 0.38 - 0s 4ms/step - loss: 0.6892 - accuracy: 0.3807 - val_loss: 0.6772 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7036 - accuracy: 0.40 - ETA: 0s - loss: 0.6850 - accuracy: 0.36 - ETA: 0s - loss: 0.6851 - accuracy: 0.37 - ETA: 0s - loss: 0.6883 - accuracy: 0.37 - ETA: 0s - loss: 0.6891 - accuracy: 0.36 - ETA: 0s - loss: 0.6894 - accuracy: 0.40 - 0s 4ms/step - loss: 0.6893 - accuracy: 0.4076 - val_loss: 0.6814 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.81 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6895 - accuracy: 0.47 - ETA: 0s - loss: 0.6867 - accuracy: 0.43 - ETA: 0s - loss: 0.6882 - accuracy: 0.45 - 0s 3ms/step - loss: 0.6910 - accuracy: 0.4651 - val_loss: 0.6935 - val_accuracy: 0.6627\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.31 - ETA: 0s - loss: 0.6920 - accuracy: 0.34 - ETA: 0s - loss: 0.6925 - accuracy: 0.35 - ETA: 0s - loss: 0.6925 - accuracy: 0.35 - ETA: 0s - loss: 0.6889 - accuracy: 0.38 - 0s 3ms/step - loss: 0.6920 - accuracy: 0.4013 - val_loss: 0.6993 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.37 - ETA: 0s - loss: 0.7102 - accuracy: 0.32 - ETA: 0s - loss: 0.6975 - accuracy: 0.35 - ETA: 0s - loss: 0.6994 - accuracy: 0.45 - ETA: 0s - loss: 0.6950 - accuracy: 0.46 - 0s 3ms/step - loss: 0.6945 - accuracy: 0.5103 - val_loss: 0.6853 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7236 - accuracy: 0.65 - ETA: 0s - loss: 0.6947 - accuracy: 0.40 - ETA: 0s - loss: 0.6932 - accuracy: 0.39 - ETA: 0s - loss: 0.6927 - accuracy: 0.42 - ETA: 0s - loss: 0.6952 - accuracy: 0.41 - ETA: 0s - loss: 0.6940 - accuracy: 0.41 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.4192 - val_loss: 0.6829 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.62 - ETA: 0s - loss: 0.6907 - accuracy: 0.43 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6924 - accuracy: 0.60 - ETA: 0s - loss: 0.6953 - accuracy: 0.53 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6948 - accuracy: 0.4905 - val_loss: 0.6952 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.31 - ETA: 0s - loss: 0.6937 - accuracy: 0.36 - ETA: 0s - loss: 0.6847 - accuracy: 0.50 - ETA: 0s - loss: 0.6869 - accuracy: 0.57 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - 0s 4ms/step - loss: 0.6942 - accuracy: 0.5024 - val_loss: 0.7062 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.31 - ETA: 0s - loss: 0.6840 - accuracy: 0.28 - ETA: 0s - loss: 0.6901 - accuracy: 0.47 - ETA: 0s - loss: 0.6948 - accuracy: 0.45 - ETA: 0s - loss: 0.6947 - accuracy: 0.41 - ETA: 0s - loss: 0.6943 - accuracy: 0.3957Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.3957 - val_loss: 0.6948 - val_accuracy: 0.3045\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8796 - accuracy: 0.46 - ETA: 0s - loss: 1.9977 - accuracy: 0.59 - ETA: 0s - loss: 1.3345 - accuracy: 0.62 - ETA: 0s - loss: 1.1097 - accuracy: 0.61 - ETA: 0s - loss: 0.9939 - accuracy: 0.58 - ETA: 0s - loss: 0.9234 - accuracy: 0.61 - 0s 5ms/step - loss: 0.9171 - accuracy: 0.6178 - val_loss: 0.6250 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.81 - ETA: 0s - loss: 0.6750 - accuracy: 0.72 - ETA: 0s - loss: 0.6612 - accuracy: 0.73 - ETA: 0s - loss: 0.6595 - accuracy: 0.73 - ETA: 0s - loss: 0.6583 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6626 - accuracy: 0.7335 - val_loss: 0.6456 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.56 - ETA: 0s - loss: 0.6720 - accuracy: 0.72 - ETA: 0s - loss: 0.6768 - accuracy: 0.72 - ETA: 0s - loss: 0.6695 - accuracy: 0.72 - ETA: 0s - loss: 0.6679 - accuracy: 0.71 - 0s 3ms/step - loss: 0.6622 - accuracy: 0.7234 - val_loss: 0.6024 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.68 - ETA: 0s - loss: 0.6267 - accuracy: 0.73 - ETA: 0s - loss: 0.6464 - accuracy: 0.73 - ETA: 0s - loss: 0.6432 - accuracy: 0.73 - ETA: 0s - loss: 0.6503 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6521 - accuracy: 0.7312 - val_loss: 0.6240 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5779 - accuracy: 0.81 - ETA: 0s - loss: 0.6254 - accuracy: 0.76 - ETA: 0s - loss: 0.6480 - accuracy: 0.73 - ETA: 0s - loss: 0.6389 - accuracy: 0.70 - ETA: 0s - loss: 0.6372 - accuracy: 0.71 - 0s 3ms/step - loss: 0.6406 - accuracy: 0.7126 - val_loss: 0.5899 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7658 - accuracy: 0.62 - ETA: 0s - loss: 0.6160 - accuracy: 0.76 - ETA: 0s - loss: 0.6389 - accuracy: 0.75 - ETA: 0s - loss: 0.6361 - accuracy: 0.76 - ETA: 0s - loss: 0.6379 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6414 - accuracy: 0.7566 - val_loss: 0.5835 - val_accuracy: 0.7493\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.84 - ETA: 0s - loss: 0.5799 - accuracy: 0.82 - ETA: 0s - loss: 0.6048 - accuracy: 0.78 - ETA: 0s - loss: 0.6137 - accuracy: 0.77 - ETA: 0s - loss: 0.6081 - accuracy: 0.77 - ETA: 0s - loss: 0.6083 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6085 - accuracy: 0.7716 - val_loss: 0.5929 - val_accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.78 - ETA: 0s - loss: 0.5977 - accuracy: 0.76 - ETA: 0s - loss: 0.5915 - accuracy: 0.78 - ETA: 0s - loss: 0.5934 - accuracy: 0.78 - ETA: 0s - loss: 0.6015 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6004 - accuracy: 0.7809 - val_loss: 0.5900 - val_accuracy: 0.7537\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.84 - ETA: 0s - loss: 0.5515 - accuracy: 0.79 - ETA: 0s - loss: 0.5418 - accuracy: 0.80 - ETA: 0s - loss: 0.5589 - accuracy: 0.80 - ETA: 0s - loss: 0.5684 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5646 - accuracy: 0.7973 - val_loss: 0.5946 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.78 - ETA: 0s - loss: 0.5744 - accuracy: 0.78 - ETA: 0s - loss: 0.5523 - accuracy: 0.80 - ETA: 0s - loss: 0.5486 - accuracy: 0.81 - ETA: 0s - loss: 0.5531 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5612 - accuracy: 0.8018 - val_loss: 0.6115 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.71 - ETA: 0s - loss: 0.6022 - accuracy: 0.75 - ETA: 0s - loss: 0.5944 - accuracy: 0.74 - ETA: 0s - loss: 0.5785 - accuracy: 0.75 - ETA: 0s - loss: 0.5606 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5570 - accuracy: 0.7734 - val_loss: 0.5940 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.84 - ETA: 0s - loss: 0.4730 - accuracy: 0.84 - ETA: 0s - loss: 0.5242 - accuracy: 0.81 - ETA: 0s - loss: 0.5277 - accuracy: 0.81 - ETA: 0s - loss: 0.5289 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5292 - accuracy: 0.8219 - val_loss: 0.5626 - val_accuracy: 0.7418\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.68 - ETA: 0s - loss: 0.5494 - accuracy: 0.82 - ETA: 0s - loss: 0.5669 - accuracy: 0.81 - ETA: 0s - loss: 0.5654 - accuracy: 0.81 - ETA: 0s - loss: 0.5557 - accuracy: 0.81 - ETA: 0s - loss: 0.5781 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5801 - accuracy: 0.8040 - val_loss: 0.5993 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.75 - ETA: 0s - loss: 0.6813 - accuracy: 0.73 - ETA: 0s - loss: 0.6782 - accuracy: 0.75 - ETA: 0s - loss: 0.6774 - accuracy: 0.74 - ETA: 0s - loss: 0.6708 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6694 - accuracy: 0.7480 - val_loss: 0.6251 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.71 - ETA: 0s - loss: 0.6065 - accuracy: 0.77 - ETA: 0s - loss: 0.6203 - accuracy: 0.77 - ETA: 0s - loss: 0.6259 - accuracy: 0.77 - ETA: 0s - loss: 0.6302 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6340 - accuracy: 0.7633 - val_loss: 0.6468 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.78 - ETA: 0s - loss: 0.5856 - accuracy: 0.79 - ETA: 0s - loss: 0.5842 - accuracy: 0.79 - ETA: 0s - loss: 0.5785 - accuracy: 0.79 - ETA: 0s - loss: 0.5831 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5790 - accuracy: 0.7951 - val_loss: 0.6020 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.78 - ETA: 0s - loss: 0.5446 - accuracy: 0.81 - ETA: 0s - loss: 0.5336 - accuracy: 0.82 - ETA: 0s - loss: 0.5370 - accuracy: 0.82 - ETA: 0s - loss: 0.5311 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5297 - accuracy: 0.8231 - val_loss: 0.5626 - val_accuracy: 0.7657\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.93 - ETA: 0s - loss: 0.5079 - accuracy: 0.81 - ETA: 0s - loss: 0.5173 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.82 - ETA: 0s - loss: 0.5144 - accuracy: 0.83 - ETA: 0s - loss: 0.5162 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5162 - accuracy: 0.8298 - val_loss: 0.5607 - val_accuracy: 0.7358\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.96 - ETA: 0s - loss: 0.4610 - accuracy: 0.86 - ETA: 0s - loss: 0.4885 - accuracy: 0.84 - ETA: 0s - loss: 0.4893 - accuracy: 0.84 - ETA: 0s - loss: 0.4813 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4966 - accuracy: 0.8417 - val_loss: 0.5870 - val_accuracy: 0.7179\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.87 - ETA: 0s - loss: 0.5147 - accuracy: 0.82 - ETA: 0s - loss: 0.4988 - accuracy: 0.83 - ETA: 0s - loss: 0.5107 - accuracy: 0.83 - ETA: 0s - loss: 0.5211 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5268 - accuracy: 0.8305 - val_loss: 0.5812 - val_accuracy: 0.7239\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.78 - ETA: 0s - loss: 0.5375 - accuracy: 0.81 - ETA: 0s - loss: 0.5794 - accuracy: 0.79 - ETA: 0s - loss: 0.5528 - accuracy: 0.81 - ETA: 0s - loss: 0.5394 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5369 - accuracy: 0.8246 - val_loss: 0.5658 - val_accuracy: 0.7567\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.84 - ETA: 0s - loss: 0.4735 - accuracy: 0.85 - ETA: 0s - loss: 0.5135 - accuracy: 0.83 - ETA: 0s - loss: 0.5154 - accuracy: 0.83 - ETA: 0s - loss: 0.5070 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4975 - accuracy: 0.8414 - val_loss: 0.5692 - val_accuracy: 0.7373\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.90 - ETA: 0s - loss: 0.4710 - accuracy: 0.85 - ETA: 0s - loss: 0.4872 - accuracy: 0.83 - ETA: 0s - loss: 0.4874 - accuracy: 0.84 - ETA: 0s - loss: 0.4843 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4852 - accuracy: 0.8414 - val_loss: 0.5646 - val_accuracy: 0.7418\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.78 - ETA: 0s - loss: 0.4115 - accuracy: 0.88 - ETA: 0s - loss: 0.4631 - accuracy: 0.86 - ETA: 0s - loss: 0.5000 - accuracy: 0.84 - ETA: 0s - loss: 0.5179 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5208 - accuracy: 0.8339 - val_loss: 0.5714 - val_accuracy: 0.7567\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.87 - ETA: 0s - loss: 0.4776 - accuracy: 0.84 - ETA: 0s - loss: 0.4832 - accuracy: 0.85 - ETA: 0s - loss: 0.4919 - accuracy: 0.84 - ETA: 0s - loss: 0.4948 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4974 - accuracy: 0.8421 - val_loss: 0.5913 - val_accuracy: 0.7493\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.93 - ETA: 0s - loss: 0.5238 - accuracy: 0.84 - ETA: 0s - loss: 0.5281 - accuracy: 0.83 - ETA: 0s - loss: 0.5293 - accuracy: 0.83 - ETA: 0s - loss: 0.5202 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5158 - accuracy: 0.8369 - val_loss: 0.5590 - val_accuracy: 0.7567\n",
      "Epoch 27/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4558 - accuracy: 0.81 - ETA: 0s - loss: 0.4929 - accuracy: 0.84 - ETA: 0s - loss: 0.4855 - accuracy: 0.85 - ETA: 0s - loss: 0.5029 - accuracy: 0.84 - ETA: 0s - loss: 0.4999 - accuracy: 0.8484Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.8473 - val_loss: 0.5807 - val_accuracy: 0.7448\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 380fbf077d6313fa8bdde7fc8e15aaf1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7482587297757467</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8615419636005527</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 50</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3350 - accuracy: 0.53 - ETA: 0s - loss: 10.5418 - accuracy: 0.620 - ETA: 0s - loss: 7.3637 - accuracy: 0.621 - ETA: 0s - loss: 5.4367 - accuracy: 0.64 - ETA: 0s - loss: 4.3187 - accuracy: 0.65 - ETA: 0s - loss: 3.5917 - accuracy: 0.64 - ETA: 0s - loss: 3.0902 - accuracy: 0.62 - 1s 6ms/step - loss: 3.0902 - accuracy: 0.6278 - val_loss: 0.6719 - val_accuracy: 0.6970\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7643 - accuracy: 0.53 - ETA: 0s - loss: 0.7068 - accuracy: 0.48 - ETA: 0s - loss: 0.7019 - accuracy: 0.48 - ETA: 0s - loss: 0.7079 - accuracy: 0.51 - ETA: 0s - loss: 0.6996 - accuracy: 0.49 - ETA: 0s - loss: 0.7000 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6991 - accuracy: 0.4927 - val_loss: 0.6670 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7081 - accuracy: 0.46 - ETA: 0s - loss: 0.7031 - accuracy: 0.43 - ETA: 0s - loss: 0.7060 - accuracy: 0.42 - ETA: 0s - loss: 0.7019 - accuracy: 0.44 - ETA: 0s - loss: 0.6995 - accuracy: 0.49 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - 0s 4ms/step - loss: 0.6956 - accuracy: 0.5442 - val_loss: 0.6199 - val_accuracy: 0.7373\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.68 - ETA: 0s - loss: 0.7073 - accuracy: 0.63 - ETA: 0s - loss: 0.6898 - accuracy: 0.64 - ETA: 0s - loss: 0.6950 - accuracy: 0.64 - ETA: 0s - loss: 0.6898 - accuracy: 0.64 - ETA: 0s - loss: 0.6894 - accuracy: 0.64 - ETA: 0s - loss: 0.6880 - accuracy: 0.64 - 0s 4ms/step - loss: 0.6871 - accuracy: 0.6495 - val_loss: 0.6406 - val_accuracy: 0.7104\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6756 - accuracy: 0.63 - ETA: 0s - loss: 0.6798 - accuracy: 0.64 - ETA: 0s - loss: 0.6703 - accuracy: 0.64 - ETA: 0s - loss: 0.6711 - accuracy: 0.65 - 0s 4ms/step - loss: 0.6763 - accuracy: 0.6596 - val_loss: 0.6165 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.65 - ETA: 0s - loss: 0.6675 - accuracy: 0.70 - ETA: 0s - loss: 0.6959 - accuracy: 0.66 - ETA: 0s - loss: 0.6857 - accuracy: 0.66 - ETA: 0s - loss: 0.6781 - accuracy: 0.65 - ETA: 0s - loss: 0.6691 - accuracy: 0.66 - 0s 4ms/step - loss: 0.6692 - accuracy: 0.6719 - val_loss: 0.6191 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.78 - ETA: 0s - loss: 0.6609 - accuracy: 0.71 - ETA: 0s - loss: 0.6599 - accuracy: 0.71 - ETA: 0s - loss: 0.6570 - accuracy: 0.73 - ETA: 0s - loss: 0.6655 - accuracy: 0.71 - ETA: 0s - loss: 0.6662 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6622 - accuracy: 0.7144 - val_loss: 0.6886 - val_accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7913 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.71 - ETA: 0s - loss: 0.6860 - accuracy: 0.70 - ETA: 0s - loss: 0.7083 - accuracy: 0.69 - ETA: 0s - loss: 0.7024 - accuracy: 0.69 - ETA: 0s - loss: 0.7337 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7360 - accuracy: 0.6932 - val_loss: 0.6615 - val_accuracy: 0.6985\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.71 - ETA: 0s - loss: 0.7103 - accuracy: 0.66 - ETA: 0s - loss: 0.7069 - accuracy: 0.63 - ETA: 0s - loss: 0.7025 - accuracy: 0.62 - ETA: 0s - loss: 0.7002 - accuracy: 0.62 - ETA: 0s - loss: 0.7027 - accuracy: 0.62 - 0s 4ms/step - loss: 0.7016 - accuracy: 0.6211 - val_loss: 0.6724 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.71 - ETA: 0s - loss: 0.6958 - accuracy: 0.63 - ETA: 0s - loss: 0.6923 - accuracy: 0.62 - ETA: 0s - loss: 0.6913 - accuracy: 0.62 - ETA: 0s - loss: 0.6935 - accuracy: 0.59 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5584 - val_loss: 0.6700 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7441 - accuracy: 0.62 - ETA: 0s - loss: 0.7080 - accuracy: 0.49 - ETA: 0s - loss: 0.6983 - accuracy: 0.40 - ETA: 0s - loss: 0.6973 - accuracy: 0.45 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5622 - val_loss: 0.6656 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.71 - ETA: 0s - loss: 0.6996 - accuracy: 0.67 - ETA: 0s - loss: 0.6910 - accuracy: 0.57 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.51 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5644 - val_loss: 0.6728 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.7600 - accuracy: 0.59 - ETA: 0s - loss: 0.6813 - accuracy: 0.71 - ETA: 0s - loss: 0.6789 - accuracy: 0.71 - ETA: 0s - loss: 0.6843 - accuracy: 0.70 - ETA: 0s - loss: 0.6799 - accuracy: 0.63 - ETA: 0s - loss: 0.6828 - accuracy: 0.6125Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5961 - val_loss: 0.6788 - val_accuracy: 0.7015\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2214 - accuracy: 0.68 - ETA: 0s - loss: 6.2492 - accuracy: 0.56 - ETA: 0s - loss: 5.0902 - accuracy: 0.56 - ETA: 0s - loss: 3.6398 - accuracy: 0.59 - ETA: 0s - loss: 2.8489 - accuracy: 0.61 - ETA: 0s - loss: 2.4214 - accuracy: 0.62 - 0s 5ms/step - loss: 2.1430 - accuracy: 0.6103 - val_loss: 0.6709 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7292 - accuracy: 0.56 - ETA: 0s - loss: 0.7043 - accuracy: 0.44 - ETA: 0s - loss: 0.7014 - accuracy: 0.45 - ETA: 0s - loss: 0.6937 - accuracy: 0.44 - ETA: 0s - loss: 0.7015 - accuracy: 0.44 - ETA: 0s - loss: 0.7009 - accuracy: 0.43 - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4341 - val_loss: 0.6740 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.34 - ETA: 0s - loss: 0.7251 - accuracy: 0.63 - ETA: 0s - loss: 0.7074 - accuracy: 0.66 - ETA: 0s - loss: 0.7243 - accuracy: 0.63 - ETA: 0s - loss: 0.7246 - accuracy: 0.57 - ETA: 0s - loss: 0.7395 - accuracy: 0.51 - 0s 4ms/step - loss: 0.7325 - accuracy: 0.4994 - val_loss: 0.6737 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.31 - ETA: 0s - loss: 0.7000 - accuracy: 0.59 - ETA: 0s - loss: 0.7035 - accuracy: 0.49 - ETA: 0s - loss: 0.6992 - accuracy: 0.44 - ETA: 0s - loss: 0.6978 - accuracy: 0.45 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6956 - accuracy: 0.5084 - val_loss: 0.6520 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.71 - ETA: 0s - loss: 0.7444 - accuracy: 0.65 - ETA: 0s - loss: 0.7268 - accuracy: 0.63 - ETA: 0s - loss: 0.7151 - accuracy: 0.63 - ETA: 0s - loss: 0.7133 - accuracy: 0.63 - ETA: 0s - loss: 0.7103 - accuracy: 0.64 - 0s 4ms/step - loss: 0.7087 - accuracy: 0.6540 - val_loss: 0.6464 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.68 - ETA: 0s - loss: 0.7213 - accuracy: 0.65 - ETA: 0s - loss: 0.7131 - accuracy: 0.65 - ETA: 0s - loss: 0.7049 - accuracy: 0.65 - ETA: 0s - loss: 0.7015 - accuracy: 0.64 - ETA: 0s - loss: 0.6989 - accuracy: 0.65 - 0s 4ms/step - loss: 0.7014 - accuracy: 0.6573 - val_loss: 0.6652 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.71 - ETA: 0s - loss: 0.6971 - accuracy: 0.67 - ETA: 0s - loss: 0.6971 - accuracy: 0.63 - ETA: 0s - loss: 0.6924 - accuracy: 0.55 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6945 - accuracy: 0.56 - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5737 - val_loss: 0.6765 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.75 - ETA: 0s - loss: 0.6674 - accuracy: 0.71 - ETA: 0s - loss: 0.6827 - accuracy: 0.69 - ETA: 0s - loss: 0.6924 - accuracy: 0.67 - ETA: 0s - loss: 0.6929 - accuracy: 0.61 - ETA: 0s - loss: 0.6926 - accuracy: 0.56 - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5618 - val_loss: 0.6807 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.71 - ETA: 0s - loss: 0.7030 - accuracy: 0.41 - ETA: 0s - loss: 0.7047 - accuracy: 0.38 - ETA: 0s - loss: 0.6986 - accuracy: 0.36 - ETA: 0s - loss: 0.6964 - accuracy: 0.41 - ETA: 0s - loss: 0.6910 - accuracy: 0.47 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5274 - val_loss: 0.6720 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.62 - ETA: 0s - loss: 0.6932 - accuracy: 0.66 - ETA: 0s - loss: 0.6925 - accuracy: 0.66 - ETA: 0s - loss: 0.6913 - accuracy: 0.57 - ETA: 0s - loss: 0.6908 - accuracy: 0.59 - ETA: 0s - loss: 0.6935 - accuracy: 0.56 - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5584 - val_loss: 0.6792 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.65 - ETA: 0s - loss: 0.6765 - accuracy: 0.68 - ETA: 0s - loss: 0.6727 - accuracy: 0.70 - ETA: 0s - loss: 0.6860 - accuracy: 0.68 - ETA: 0s - loss: 0.6849 - accuracy: 0.65 - ETA: 0s - loss: 0.6885 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5752 - val_loss: 0.6971 - val_accuracy: 0.3045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7067 - accuracy: 0.37 - ETA: 0s - loss: 0.7005 - accuracy: 0.35 - ETA: 0s - loss: 0.6880 - accuracy: 0.35 - ETA: 0s - loss: 0.6883 - accuracy: 0.47 - ETA: 0s - loss: 0.6864 - accuracy: 0.53 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6912 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5801 - val_loss: 0.6844 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7412 - accuracy: 0.65 - ETA: 0s - loss: 0.6953 - accuracy: 0.33 - ETA: 0s - loss: 0.6989 - accuracy: 0.34 - ETA: 0s - loss: 0.7022 - accuracy: 0.35 - ETA: 0s - loss: 0.7007 - accuracy: 0.34 - ETA: 0s - loss: 0.6952 - accuracy: 0.32 - 0s 4ms/step - loss: 0.6910 - accuracy: 0.3890 - val_loss: 0.6704 - val_accuracy: 0.6970\n",
      "Epoch 14/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.7323 - accuracy: 0.65 - ETA: 0s - loss: 0.6917 - accuracy: 0.70 - ETA: 0s - loss: 0.6968 - accuracy: 0.62 - ETA: 0s - loss: 0.6995 - accuracy: 0.53 - ETA: 0s - loss: 0.6981 - accuracy: 0.47 - ETA: 0s - loss: 0.6930 - accuracy: 0.4857Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5185 - val_loss: 0.6742 - val_accuracy: 0.6970\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9259 - accuracy: 0.68 - ETA: 0s - loss: 10.7844 - accuracy: 0.574 - ETA: 0s - loss: 6.7931 - accuracy: 0.579 - ETA: 0s - loss: 4.7579 - accuracy: 0.58 - ETA: 0s - loss: 3.7086 - accuracy: 0.55 - ETA: 0s - loss: 3.0939 - accuracy: 0.52 - ETA: 0s - loss: 2.7129 - accuracy: 0.50 - 0s 6ms/step - loss: 2.6476 - accuracy: 0.5050 - val_loss: 0.6631 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.43 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.48 - ETA: 0s - loss: 0.6927 - accuracy: 0.46 - ETA: 0s - loss: 0.6972 - accuracy: 0.46 - ETA: 0s - loss: 0.6963 - accuracy: 0.44 - ETA: 0s - loss: 0.6978 - accuracy: 0.44 - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4528 - val_loss: 0.6615 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.68 - ETA: 0s - loss: 0.7075 - accuracy: 0.46 - ETA: 0s - loss: 0.7009 - accuracy: 0.43 - ETA: 0s - loss: 0.7005 - accuracy: 0.42 - ETA: 0s - loss: 0.6961 - accuracy: 0.45 - ETA: 0s - loss: 0.7018 - accuracy: 0.48 - 0s 4ms/step - loss: 0.6983 - accuracy: 0.4651 - val_loss: 0.6723 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.31 - ETA: 0s - loss: 0.6807 - accuracy: 0.65 - ETA: 0s - loss: 0.6828 - accuracy: 0.65 - ETA: 0s - loss: 0.6775 - accuracy: 0.67 - ETA: 0s - loss: 0.6924 - accuracy: 0.65 - ETA: 0s - loss: 0.6981 - accuracy: 0.59 - ETA: 0s - loss: 0.6969 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6962 - accuracy: 0.5371 - val_loss: 0.6840 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.12 - ETA: 0s - loss: 0.6901 - accuracy: 0.57 - ETA: 0s - loss: 0.6894 - accuracy: 0.63 - ETA: 0s - loss: 0.6987 - accuracy: 0.63 - ETA: 0s - loss: 0.7002 - accuracy: 0.56 - ETA: 0s - loss: 0.6979 - accuracy: 0.51 - 0s 4ms/step - loss: 0.6969 - accuracy: 0.5174 - val_loss: 0.6660 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7505 - accuracy: 0.56 - ETA: 0s - loss: 0.6914 - accuracy: 0.67 - ETA: 0s - loss: 0.6861 - accuracy: 0.67 - ETA: 0s - loss: 0.6918 - accuracy: 0.67 - ETA: 0s - loss: 0.6927 - accuracy: 0.60 - ETA: 0s - loss: 0.6935 - accuracy: 0.55 - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5259 - val_loss: 0.7010 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.31 - ETA: 0s - loss: 0.7000 - accuracy: 0.33 - ETA: 0s - loss: 0.6960 - accuracy: 0.32 - ETA: 0s - loss: 0.6891 - accuracy: 0.44 - ETA: 0s - loss: 0.6924 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6936 - accuracy: 0.4759 - val_loss: 0.6960 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.15 - ETA: 0s - loss: 0.6959 - accuracy: 0.32 - ETA: 0s - loss: 0.7015 - accuracy: 0.44 - ETA: 0s - loss: 0.7001 - accuracy: 0.41 - ETA: 0s - loss: 0.7021 - accuracy: 0.39 - ETA: 0s - loss: 0.6991 - accuracy: 0.38 - 0s 4ms/step - loss: 0.6939 - accuracy: 0.4125 - val_loss: 0.6681 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.71 - ETA: 0s - loss: 0.6928 - accuracy: 0.68 - ETA: 0s - loss: 0.7042 - accuracy: 0.66 - ETA: 0s - loss: 0.7002 - accuracy: 0.57 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6944 - accuracy: 0.56 - ETA: 0s - loss: 0.6951 - accuracy: 0.52 - 0s 4ms/step - loss: 0.6951 - accuracy: 0.5259 - val_loss: 0.6986 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.18 - ETA: 0s - loss: 0.7001 - accuracy: 0.35 - ETA: 0s - loss: 0.6999 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.58 - ETA: 0s - loss: 0.6932 - accuracy: 0.60 - ETA: 0s - loss: 0.6956 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5782 - val_loss: 0.6985 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.7406 - accuracy: 0.31 - ETA: 0s - loss: 0.6991 - accuracy: 0.31 - ETA: 0s - loss: 0.6936 - accuracy: 0.39 - ETA: 0s - loss: 0.6972 - accuracy: 0.48 - ETA: 0s - loss: 0.6959 - accuracy: 0.44 - ETA: 0s - loss: 0.6953 - accuracy: 0.4472Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.4349 - val_loss: 0.6915 - val_accuracy: 0.6955\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d2c1dcd2544fd00107696383d2bd9973</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7194029887517294</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.975635926707118</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 60</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.81 - ETA: 0s - loss: 1.5693 - accuracy: 0.58 - ETA: 0s - loss: 1.1489 - accuracy: 0.62 - ETA: 0s - loss: 0.9660 - accuracy: 0.66 - ETA: 0s - loss: 0.8785 - accuracy: 0.69 - ETA: 0s - loss: 0.8429 - accuracy: 0.69 - 0s 6ms/step - loss: 0.8045 - accuracy: 0.7029 - val_loss: 0.6079 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.68 - ETA: 0s - loss: 0.5931 - accuracy: 0.72 - ETA: 0s - loss: 0.6036 - accuracy: 0.72 - ETA: 0s - loss: 0.5926 - accuracy: 0.73 - ETA: 0s - loss: 0.5860 - accuracy: 0.74 - ETA: 0s - loss: 0.5834 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5832 - accuracy: 0.7492 - val_loss: 0.7345 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4449 - accuracy: 0.87 - ETA: 0s - loss: 0.5505 - accuracy: 0.79 - ETA: 0s - loss: 0.5504 - accuracy: 0.79 - ETA: 0s - loss: 0.5540 - accuracy: 0.79 - ETA: 0s - loss: 0.5618 - accuracy: 0.78 - ETA: 0s - loss: 0.5542 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5540 - accuracy: 0.7786 - val_loss: 0.5422 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.87 - ETA: 0s - loss: 0.5435 - accuracy: 0.78 - ETA: 0s - loss: 0.5267 - accuracy: 0.79 - ETA: 0s - loss: 0.5179 - accuracy: 0.80 - ETA: 0s - loss: 0.5088 - accuracy: 0.80 - ETA: 0s - loss: 0.5196 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5187 - accuracy: 0.8040 - val_loss: 0.9080 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.78 - ETA: 0s - loss: 0.5458 - accuracy: 0.79 - ETA: 0s - loss: 0.5302 - accuracy: 0.80 - ETA: 0s - loss: 0.5189 - accuracy: 0.80 - ETA: 0s - loss: 0.5264 - accuracy: 0.80 - ETA: 0s - loss: 0.5405 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5375 - accuracy: 0.7999 - val_loss: 0.6164 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.71 - ETA: 0s - loss: 0.5149 - accuracy: 0.81 - ETA: 0s - loss: 0.5206 - accuracy: 0.81 - ETA: 0s - loss: 0.5261 - accuracy: 0.82 - ETA: 0s - loss: 0.5405 - accuracy: 0.82 - ETA: 0s - loss: 0.5355 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5315 - accuracy: 0.8219 - val_loss: 0.8300 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.84 - ETA: 0s - loss: 0.5496 - accuracy: 0.81 - ETA: 0s - loss: 0.5162 - accuracy: 0.82 - ETA: 0s - loss: 0.5029 - accuracy: 0.82 - ETA: 0s - loss: 0.5159 - accuracy: 0.82 - ETA: 0s - loss: 0.5568 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5555 - accuracy: 0.8037 - val_loss: 0.6167 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.90 - ETA: 0s - loss: 0.5564 - accuracy: 0.79 - ETA: 0s - loss: 0.5878 - accuracy: 0.78 - ETA: 0s - loss: 0.6232 - accuracy: 0.77 - ETA: 0s - loss: 0.6059 - accuracy: 0.78 - ETA: 0s - loss: 0.6131 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6107 - accuracy: 0.7850 - val_loss: 0.7948 - val_accuracy: 0.7313\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.87 - ETA: 0s - loss: 0.5872 - accuracy: 0.79 - ETA: 0s - loss: 0.5748 - accuracy: 0.80 - ETA: 0s - loss: 0.5609 - accuracy: 0.81 - ETA: 0s - loss: 0.5677 - accuracy: 0.81 - ETA: 0s - loss: 0.5791 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5810 - accuracy: 0.8078 - val_loss: 0.5736 - val_accuracy: 0.7552\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.87 - ETA: 0s - loss: 0.5846 - accuracy: 0.80 - ETA: 0s - loss: 0.5634 - accuracy: 0.80 - ETA: 0s - loss: 0.5606 - accuracy: 0.80 - ETA: 0s - loss: 0.5767 - accuracy: 0.80 - ETA: 0s - loss: 0.5913 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5932 - accuracy: 0.7966 - val_loss: 0.8544 - val_accuracy: 0.7522\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6264 - accuracy: 0.81 - ETA: 0s - loss: 0.5912 - accuracy: 0.79 - ETA: 0s - loss: 0.6250 - accuracy: 0.77 - ETA: 0s - loss: 0.6245 - accuracy: 0.77 - ETA: 0s - loss: 0.6283 - accuracy: 0.76 - ETA: 0s - loss: 0.6246 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6271 - accuracy: 0.7495 - val_loss: 0.6755 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.78 - ETA: 0s - loss: 0.6376 - accuracy: 0.74 - ETA: 0s - loss: 0.6222 - accuracy: 0.75 - ETA: 0s - loss: 0.6090 - accuracy: 0.77 - ETA: 0s - loss: 0.6017 - accuracy: 0.78 - ETA: 0s - loss: 0.5935 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5944 - accuracy: 0.7887 - val_loss: 0.6311 - val_accuracy: 0.7493\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.78 - ETA: 0s - loss: 0.5629 - accuracy: 0.79 - ETA: 0s - loss: 0.6485 - accuracy: 0.78 - ETA: 0s - loss: 0.6511 - accuracy: 0.78 - ETA: 0s - loss: 0.6322 - accuracy: 0.79 - ETA: 0s - loss: 0.6134 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6098 - accuracy: 0.7992 - val_loss: 1.0953 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.65 - ETA: 0s - loss: 0.5849 - accuracy: 0.78 - ETA: 0s - loss: 0.5602 - accuracy: 0.79 - ETA: 0s - loss: 0.6093 - accuracy: 0.80 - ETA: 0s - loss: 0.6158 - accuracy: 0.80 - ETA: 0s - loss: 0.6314 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7999 - val_loss: 0.6026 - val_accuracy: 0.7418\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.75 - ETA: 0s - loss: 0.5974 - accuracy: 0.79 - ETA: 0s - loss: 0.5773 - accuracy: 0.80 - ETA: 0s - loss: 0.5877 - accuracy: 0.79 - ETA: 0s - loss: 0.6089 - accuracy: 0.76 - ETA: 0s - loss: 0.6232 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6230 - accuracy: 0.7663 - val_loss: 0.6516 - val_accuracy: 0.7060\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6414 - accuracy: 0.78 - ETA: 0s - loss: 0.6703 - accuracy: 0.73 - ETA: 0s - loss: 0.6839 - accuracy: 0.64 - ETA: 0s - loss: 0.7339 - accuracy: 0.62 - ETA: 0s - loss: 0.7231 - accuracy: 0.59 - ETA: 0s - loss: 0.7135 - accuracy: 0.57 - 0s 4ms/step - loss: 0.7126 - accuracy: 0.5827 - val_loss: 0.6772 - val_accuracy: 0.6985\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5892 - accuracy: 0.78 - ETA: 0s - loss: 0.6901 - accuracy: 0.69 - ETA: 0s - loss: 0.6847 - accuracy: 0.62 - ETA: 0s - loss: 0.6954 - accuracy: 0.60 - ETA: 0s - loss: 0.6948 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5614 - val_loss: 0.6745 - val_accuracy: 0.6985\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.78 - ETA: 0s - loss: 0.6795 - accuracy: 0.71 - ETA: 0s - loss: 0.6818 - accuracy: 0.71 - ETA: 0s - loss: 0.6896 - accuracy: 0.68 - ETA: 0s - loss: 0.6902 - accuracy: 0.59 - ETA: 0s - loss: 0.6882 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5767 - val_loss: 0.6839 - val_accuracy: 0.6985\n",
      "Epoch 19/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6792 - accuracy: 0.68 - ETA: 0s - loss: 0.6924 - accuracy: 0.64 - ETA: 0s - loss: 0.6908 - accuracy: 0.59 - ETA: 0s - loss: 0.6911 - accuracy: 0.58 - ETA: 0s - loss: 0.6914 - accuracy: 0.57 - ETA: 0s - loss: 0.6903 - accuracy: 0.5586Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5599 - val_loss: 0.7197 - val_accuracy: 0.6985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8421 - accuracy: 0.62 - ETA: 0s - loss: 1.6041 - accuracy: 0.64 - ETA: 0s - loss: 1.1052 - accuracy: 0.65 - ETA: 0s - loss: 0.9391 - accuracy: 0.69 - ETA: 0s - loss: 0.8529 - accuracy: 0.70 - ETA: 0s - loss: 0.8077 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7902 - accuracy: 0.7126 - val_loss: 0.5993 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6641 - accuracy: 0.68 - ETA: 0s - loss: 0.5310 - accuracy: 0.79 - ETA: 0s - loss: 0.5559 - accuracy: 0.77 - ETA: 0s - loss: 0.5780 - accuracy: 0.77 - ETA: 0s - loss: 0.5786 - accuracy: 0.77 - ETA: 0s - loss: 0.5866 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5870 - accuracy: 0.7716 - val_loss: 0.6065 - val_accuracy: 0.6836\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.75 - ETA: 0s - loss: 0.5202 - accuracy: 0.78 - ETA: 0s - loss: 0.5219 - accuracy: 0.78 - ETA: 0s - loss: 0.5310 - accuracy: 0.77 - ETA: 0s - loss: 0.5402 - accuracy: 0.77 - ETA: 0s - loss: 0.5574 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5544 - accuracy: 0.7786 - val_loss: 0.5332 - val_accuracy: 0.7612\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.75 - ETA: 0s - loss: 0.5394 - accuracy: 0.82 - ETA: 0s - loss: 0.5292 - accuracy: 0.80 - ETA: 0s - loss: 0.5381 - accuracy: 0.79 - ETA: 0s - loss: 0.5311 - accuracy: 0.80 - ETA: 0s - loss: 0.5355 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5369 - accuracy: 0.8018 - val_loss: 0.6730 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.90 - ETA: 0s - loss: 0.6515 - accuracy: 0.79 - ETA: 0s - loss: 0.5732 - accuracy: 0.81 - ETA: 0s - loss: 0.5481 - accuracy: 0.81 - ETA: 0s - loss: 0.5314 - accuracy: 0.82 - ETA: 0s - loss: 0.5222 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5178 - accuracy: 0.8219 - val_loss: 0.6853 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.81 - ETA: 0s - loss: 0.4722 - accuracy: 0.83 - ETA: 0s - loss: 0.4930 - accuracy: 0.82 - ETA: 0s - loss: 0.5005 - accuracy: 0.82 - ETA: 0s - loss: 0.4927 - accuracy: 0.82 - ETA: 0s - loss: 0.4858 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4844 - accuracy: 0.8253 - val_loss: 0.7965 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4925 - accuracy: 0.81 - ETA: 0s - loss: 0.4614 - accuracy: 0.83 - ETA: 0s - loss: 0.4620 - accuracy: 0.83 - ETA: 0s - loss: 0.4579 - accuracy: 0.83 - ETA: 0s - loss: 0.4513 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4585 - accuracy: 0.8425 - val_loss: 0.6247 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.87 - ETA: 0s - loss: 0.5182 - accuracy: 0.84 - ETA: 0s - loss: 0.4644 - accuracy: 0.85 - ETA: 0s - loss: 0.4580 - accuracy: 0.85 - ETA: 0s - loss: 0.4632 - accuracy: 0.85 - ETA: 0s - loss: 0.4922 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4985 - accuracy: 0.8429 - val_loss: 0.6979 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.81 - ETA: 0s - loss: 0.5030 - accuracy: 0.81 - ETA: 0s - loss: 0.4833 - accuracy: 0.82 - ETA: 0s - loss: 0.4880 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.83 - ETA: 0s - loss: 0.4945 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4911 - accuracy: 0.8324 - val_loss: 0.7691 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5110 - accuracy: 0.81 - ETA: 0s - loss: 0.4941 - accuracy: 0.84 - ETA: 0s - loss: 0.4520 - accuracy: 0.86 - ETA: 0s - loss: 0.4555 - accuracy: 0.86 - ETA: 0s - loss: 0.4614 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4554 - accuracy: 0.8585 - val_loss: 0.5921 - val_accuracy: 0.7328\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.87 - ETA: 0s - loss: 0.3916 - accuracy: 0.88 - ETA: 0s - loss: 0.4112 - accuracy: 0.87 - ETA: 0s - loss: 0.4145 - accuracy: 0.87 - ETA: 0s - loss: 0.4219 - accuracy: 0.87 - ETA: 0s - loss: 0.4385 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4396 - accuracy: 0.8634 - val_loss: 0.5629 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4035 - accuracy: 0.90 - ETA: 0s - loss: 0.4308 - accuracy: 0.87 - ETA: 0s - loss: 0.4486 - accuracy: 0.85 - ETA: 0s - loss: 0.4387 - accuracy: 0.86 - ETA: 0s - loss: 0.4285 - accuracy: 0.86 - ETA: 0s - loss: 0.4220 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8697 - val_loss: 0.6235 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4175 - accuracy: 0.87 - ETA: 0s - loss: 0.4576 - accuracy: 0.85 - ETA: 0s - loss: 0.4174 - accuracy: 0.87 - ETA: 0s - loss: 0.4141 - accuracy: 0.87 - ETA: 0s - loss: 0.4218 - accuracy: 0.86 - ETA: 0s - loss: 0.4312 - accuracy: 0.8650Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.8649 - val_loss: 0.7308 - val_accuracy: 0.7134\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.68 - ETA: 0s - loss: 1.3839 - accuracy: 0.62 - ETA: 0s - loss: 1.0635 - accuracy: 0.63 - ETA: 0s - loss: 0.8839 - accuracy: 0.67 - ETA: 0s - loss: 0.8092 - accuracy: 0.68 - ETA: 0s - loss: 0.7618 - accuracy: 0.69 - 0s 5ms/step - loss: 0.7446 - accuracy: 0.7014 - val_loss: 0.5841 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.84 - ETA: 0s - loss: 0.6141 - accuracy: 0.75 - ETA: 0s - loss: 0.5899 - accuracy: 0.73 - ETA: 0s - loss: 0.5747 - accuracy: 0.74 - ETA: 0s - loss: 0.5735 - accuracy: 0.74 - ETA: 0s - loss: 0.5794 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5762 - accuracy: 0.7473 - val_loss: 0.5920 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.78 - ETA: 0s - loss: 0.4988 - accuracy: 0.76 - ETA: 0s - loss: 0.4646 - accuracy: 0.78 - ETA: 0s - loss: 0.5027 - accuracy: 0.77 - ETA: 0s - loss: 0.5161 - accuracy: 0.77 - ETA: 0s - loss: 0.5320 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5343 - accuracy: 0.7686 - val_loss: 0.7163 - val_accuracy: 0.6896\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8420 - accuracy: 0.78 - ETA: 0s - loss: 0.4862 - accuracy: 0.81 - ETA: 0s - loss: 0.4883 - accuracy: 0.80 - ETA: 0s - loss: 0.4940 - accuracy: 0.80 - ETA: 0s - loss: 0.4911 - accuracy: 0.79 - ETA: 0s - loss: 0.4898 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4853 - accuracy: 0.7910 - val_loss: 0.6533 - val_accuracy: 0.6970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.75 - ETA: 0s - loss: 0.4417 - accuracy: 0.85 - ETA: 0s - loss: 0.4398 - accuracy: 0.84 - ETA: 0s - loss: 0.4583 - accuracy: 0.82 - ETA: 0s - loss: 0.4690 - accuracy: 0.81 - ETA: 0s - loss: 0.4806 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4890 - accuracy: 0.8059 - val_loss: 0.8421 - val_accuracy: 0.6627\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.93 - ETA: 0s - loss: 0.4496 - accuracy: 0.83 - ETA: 0s - loss: 0.4665 - accuracy: 0.82 - ETA: 0s - loss: 0.5272 - accuracy: 0.82 - ETA: 0s - loss: 0.5326 - accuracy: 0.80 - ETA: 0s - loss: 0.5789 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5669 - accuracy: 0.8070 - val_loss: 0.8793 - val_accuracy: 0.6910\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.78 - ETA: 0s - loss: 0.6619 - accuracy: 0.80 - ETA: 0s - loss: 0.5771 - accuracy: 0.79 - ETA: 0s - loss: 0.5490 - accuracy: 0.79 - ETA: 0s - loss: 0.5226 - accuracy: 0.80 - ETA: 0s - loss: 0.5087 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5098 - accuracy: 0.8100 - val_loss: 0.6890 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.71 - ETA: 0s - loss: 0.4600 - accuracy: 0.83 - ETA: 0s - loss: 0.4475 - accuracy: 0.82 - ETA: 0s - loss: 0.4342 - accuracy: 0.83 - ETA: 0s - loss: 0.4162 - accuracy: 0.84 - ETA: 0s - loss: 0.4145 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4214 - accuracy: 0.8324 - val_loss: 1.7931 - val_accuracy: 0.6552\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.81 - ETA: 0s - loss: 0.4815 - accuracy: 0.82 - ETA: 0s - loss: 0.4901 - accuracy: 0.80 - ETA: 0s - loss: 0.5362 - accuracy: 0.79 - ETA: 0s - loss: 0.5416 - accuracy: 0.77 - ETA: 0s - loss: 0.5277 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5254 - accuracy: 0.7846 - val_loss: 0.7059 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.90 - ETA: 0s - loss: 0.3704 - accuracy: 0.86 - ETA: 0s - loss: 0.3871 - accuracy: 0.85 - ETA: 0s - loss: 0.3725 - accuracy: 0.86 - ETA: 0s - loss: 0.3927 - accuracy: 0.84 - ETA: 0s - loss: 0.3962 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4009 - accuracy: 0.8365 - val_loss: 0.8322 - val_accuracy: 0.6313\n",
      "Epoch 11/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.5838 - accuracy: 0.68 - ETA: 0s - loss: 0.5279 - accuracy: 0.76 - ETA: 0s - loss: 0.5022 - accuracy: 0.76 - ETA: 0s - loss: 0.4758 - accuracy: 0.78 - ETA: 0s - loss: 0.4747 - accuracy: 0.79 - ETA: 0s - loss: 0.4518 - accuracy: 0.8105Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.8141 - val_loss: 0.7088 - val_accuracy: 0.6716\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ee3bb1b7217d634f4a28a99a688ebb16</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7472636898358663</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5975110247535671</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.50 - ETA: 0s - loss: 1.6177 - accuracy: 0.66 - ETA: 0s - loss: 1.1305 - accuracy: 0.65 - ETA: 0s - loss: 0.9650 - accuracy: 0.66 - ETA: 0s - loss: 0.8584 - accuracy: 0.69 - ETA: 0s - loss: 0.8161 - accuracy: 0.70 - 0s 5ms/step - loss: 0.8093 - accuracy: 0.7018 - val_loss: 0.5865 - val_accuracy: 0.7104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.81 - ETA: 0s - loss: 0.5885 - accuracy: 0.74 - ETA: 0s - loss: 0.5745 - accuracy: 0.75 - ETA: 0s - loss: 0.5841 - accuracy: 0.74 - ETA: 0s - loss: 0.5782 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5789 - accuracy: 0.7540 - val_loss: 0.6209 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.71 - ETA: 0s - loss: 0.5418 - accuracy: 0.77 - ETA: 0s - loss: 0.5242 - accuracy: 0.79 - ETA: 0s - loss: 0.5140 - accuracy: 0.79 - ETA: 0s - loss: 0.5220 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5287 - accuracy: 0.7928 - val_loss: 0.7321 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.75 - ETA: 0s - loss: 0.6044 - accuracy: 0.79 - ETA: 0s - loss: 0.5888 - accuracy: 0.79 - ETA: 0s - loss: 0.5727 - accuracy: 0.79 - ETA: 0s - loss: 0.5548 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5464 - accuracy: 0.8029 - val_loss: 0.6225 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3962 - accuracy: 0.84 - ETA: 0s - loss: 0.4586 - accuracy: 0.83 - ETA: 0s - loss: 0.4722 - accuracy: 0.83 - ETA: 0s - loss: 0.4961 - accuracy: 0.82 - ETA: 0s - loss: 0.5537 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5601 - accuracy: 0.8216 - val_loss: 0.5644 - val_accuracy: 0.7478\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.81 - ETA: 0s - loss: 0.5059 - accuracy: 0.81 - ETA: 0s - loss: 0.5057 - accuracy: 0.81 - ETA: 0s - loss: 0.4934 - accuracy: 0.82 - ETA: 0s - loss: 0.5067 - accuracy: 0.81 - ETA: 0s - loss: 0.5107 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5111 - accuracy: 0.8182 - val_loss: 0.9027 - val_accuracy: 0.6910\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5964 - accuracy: 0.81 - ETA: 0s - loss: 0.4746 - accuracy: 0.84 - ETA: 0s - loss: 0.4758 - accuracy: 0.84 - ETA: 0s - loss: 0.4912 - accuracy: 0.83 - ETA: 0s - loss: 0.4897 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4922 - accuracy: 0.8294 - val_loss: 0.7546 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0045 - accuracy: 0.78 - ETA: 0s - loss: 0.4825 - accuracy: 0.82 - ETA: 0s - loss: 0.4490 - accuracy: 0.84 - ETA: 0s - loss: 0.4513 - accuracy: 0.84 - ETA: 0s - loss: 0.4524 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4497 - accuracy: 0.8470 - val_loss: 0.9903 - val_accuracy: 0.7000\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.87 - ETA: 0s - loss: 0.4017 - accuracy: 0.86 - ETA: 0s - loss: 0.3960 - accuracy: 0.86 - ETA: 0s - loss: 0.3991 - accuracy: 0.87 - ETA: 0s - loss: 0.4318 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8541 - val_loss: 0.7616 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.87 - ETA: 0s - loss: 0.4844 - accuracy: 0.82 - ETA: 0s - loss: 0.4676 - accuracy: 0.82 - ETA: 0s - loss: 0.4422 - accuracy: 0.84 - ETA: 0s - loss: 0.4374 - accuracy: 0.84 - ETA: 0s - loss: 0.4409 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4437 - accuracy: 0.8466 - val_loss: 0.6627 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.90 - ETA: 0s - loss: 0.4554 - accuracy: 0.83 - ETA: 0s - loss: 0.4328 - accuracy: 0.85 - ETA: 0s - loss: 0.4136 - accuracy: 0.86 - ETA: 0s - loss: 0.4243 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4178 - accuracy: 0.8630 - val_loss: 0.8086 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.93 - ETA: 0s - loss: 0.3744 - accuracy: 0.87 - ETA: 0s - loss: 0.3938 - accuracy: 0.87 - ETA: 0s - loss: 0.4191 - accuracy: 0.87 - ETA: 0s - loss: 0.4193 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8679 - val_loss: 0.7207 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.93 - ETA: 0s - loss: 0.4275 - accuracy: 0.86 - ETA: 0s - loss: 0.4197 - accuracy: 0.86 - ETA: 0s - loss: 0.4081 - accuracy: 0.86 - ETA: 0s - loss: 0.4155 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8611 - val_loss: 0.7289 - val_accuracy: 0.7343\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.87 - ETA: 0s - loss: 0.5917 - accuracy: 0.80 - ETA: 0s - loss: 0.5453 - accuracy: 0.81 - ETA: 0s - loss: 0.5064 - accuracy: 0.83 - ETA: 0s - loss: 0.4784 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4730 - accuracy: 0.8451 - val_loss: 0.6795 - val_accuracy: 0.7478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.87 - ETA: 0s - loss: 0.3859 - accuracy: 0.88 - ETA: 0s - loss: 0.3966 - accuracy: 0.88 - ETA: 0s - loss: 0.3890 - accuracy: 0.88 - ETA: 0s - loss: 0.3934 - accuracy: 0.87 - ETA: 0s - loss: 0.3992 - accuracy: 0.8753Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3992 - accuracy: 0.8753 - val_loss: 1.0394 - val_accuracy: 0.7134\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8592 - accuracy: 0.50 - ETA: 0s - loss: 2.3520 - accuracy: 0.61 - ETA: 0s - loss: 1.5033 - accuracy: 0.64 - ETA: 0s - loss: 1.1764 - accuracy: 0.67 - ETA: 0s - loss: 1.0607 - accuracy: 0.67 - ETA: 0s - loss: 0.9525 - accuracy: 0.70 - 0s 5ms/step - loss: 0.9298 - accuracy: 0.7044 - val_loss: 0.6780 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.75 - ETA: 0s - loss: 0.5467 - accuracy: 0.76 - ETA: 0s - loss: 0.5647 - accuracy: 0.75 - ETA: 0s - loss: 0.5664 - accuracy: 0.75 - ETA: 0s - loss: 0.5672 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7622 - val_loss: 0.6481 - val_accuracy: 0.6851\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.75 - ETA: 0s - loss: 0.5217 - accuracy: 0.80 - ETA: 0s - loss: 0.5089 - accuracy: 0.81 - ETA: 0s - loss: 0.5196 - accuracy: 0.80 - ETA: 0s - loss: 0.5197 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5234 - accuracy: 0.8018 - val_loss: 0.6036 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.78 - ETA: 0s - loss: 0.5091 - accuracy: 0.81 - ETA: 0s - loss: 0.5069 - accuracy: 0.81 - ETA: 0s - loss: 0.4956 - accuracy: 0.81 - ETA: 0s - loss: 0.4925 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4930 - accuracy: 0.8234 - val_loss: 0.7788 - val_accuracy: 0.6925\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.81 - ETA: 0s - loss: 0.4500 - accuracy: 0.85 - ETA: 0s - loss: 0.4610 - accuracy: 0.84 - ETA: 0s - loss: 0.4614 - accuracy: 0.84 - ETA: 0s - loss: 0.4620 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4738 - accuracy: 0.8358 - val_loss: 0.6045 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.84 - ETA: 0s - loss: 0.4321 - accuracy: 0.85 - ETA: 0s - loss: 0.4721 - accuracy: 0.83 - ETA: 0s - loss: 0.4602 - accuracy: 0.83 - ETA: 0s - loss: 0.4561 - accuracy: 0.84 - ETA: 0s - loss: 0.4660 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4632 - accuracy: 0.8384 - val_loss: 0.6433 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.81 - ETA: 0s - loss: 0.4579 - accuracy: 0.84 - ETA: 0s - loss: 0.4575 - accuracy: 0.84 - ETA: 0s - loss: 0.4657 - accuracy: 0.83 - ETA: 0s - loss: 0.4574 - accuracy: 0.83 - ETA: 0s - loss: 0.4541 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4541 - accuracy: 0.8395 - val_loss: 0.8327 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.84 - ETA: 0s - loss: 0.4456 - accuracy: 0.87 - ETA: 0s - loss: 0.4244 - accuracy: 0.87 - ETA: 0s - loss: 0.4281 - accuracy: 0.86 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4409 - accuracy: 0.8544 - val_loss: 0.9540 - val_accuracy: 0.6866\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.78 - ETA: 0s - loss: 0.4260 - accuracy: 0.86 - ETA: 0s - loss: 0.4539 - accuracy: 0.86 - ETA: 0s - loss: 0.4458 - accuracy: 0.85 - ETA: 0s - loss: 0.4495 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8451 - val_loss: 0.7133 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.96 - ETA: 0s - loss: 0.4505 - accuracy: 0.86 - ETA: 0s - loss: 0.4589 - accuracy: 0.85 - ETA: 0s - loss: 0.4452 - accuracy: 0.86 - ETA: 0s - loss: 0.4403 - accuracy: 0.85 - ETA: 0s - loss: 0.4345 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8615 - val_loss: 0.9814 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.93 - ETA: 0s - loss: 0.3633 - accuracy: 0.89 - ETA: 0s - loss: 0.4183 - accuracy: 0.88 - ETA: 0s - loss: 0.4179 - accuracy: 0.87 - ETA: 0s - loss: 0.4312 - accuracy: 0.86 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8626 - val_loss: 0.7649 - val_accuracy: 0.7075\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.84 - ETA: 0s - loss: 0.4551 - accuracy: 0.87 - ETA: 0s - loss: 0.4074 - accuracy: 0.88 - ETA: 0s - loss: 0.4030 - accuracy: 0.88 - ETA: 0s - loss: 0.4058 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4076 - accuracy: 0.8802 - val_loss: 0.7793 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.78 - ETA: 0s - loss: 0.4116 - accuracy: 0.86 - ETA: 0s - loss: 0.4066 - accuracy: 0.87 - ETA: 0s - loss: 0.4253 - accuracy: 0.86 - ETA: 0s - loss: 0.4453 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4659 - accuracy: 0.8570 - val_loss: 1.1173 - val_accuracy: 0.7582\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.96 - ETA: 0s - loss: 0.6615 - accuracy: 0.81 - ETA: 0s - loss: 0.5887 - accuracy: 0.82 - ETA: 0s - loss: 0.5747 - accuracy: 0.82 - ETA: 0s - loss: 0.5733 - accuracy: 0.82 - ETA: 0s - loss: 0.6130 - accuracy: 0.83 - 0s 4ms/step - loss: 0.6130 - accuracy: 0.8346 - val_loss: 0.9130 - val_accuracy: 0.7104\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 1.00 - ETA: 0s - loss: 0.4587 - accuracy: 0.84 - ETA: 0s - loss: 0.4651 - accuracy: 0.85 - ETA: 0s - loss: 0.4521 - accuracy: 0.86 - ETA: 0s - loss: 0.4393 - accuracy: 0.86 - ETA: 0s - loss: 0.4493 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8623 - val_loss: 0.9538 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.96 - ETA: 0s - loss: 0.4344 - accuracy: 0.88 - ETA: 0s - loss: 0.4273 - accuracy: 0.87 - ETA: 0s - loss: 0.4326 - accuracy: 0.87 - ETA: 0s - loss: 0.4324 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8727 - val_loss: 0.8233 - val_accuracy: 0.7448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.87 - ETA: 0s - loss: 0.4488 - accuracy: 0.85 - ETA: 0s - loss: 0.5247 - accuracy: 0.85 - ETA: 0s - loss: 0.4871 - accuracy: 0.86 - ETA: 0s - loss: 0.4828 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4640 - accuracy: 0.8667 - val_loss: 0.6809 - val_accuracy: 0.7478\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.87 - ETA: 0s - loss: 0.5131 - accuracy: 0.83 - ETA: 0s - loss: 0.5355 - accuracy: 0.84 - ETA: 0s - loss: 0.5170 - accuracy: 0.85 - ETA: 0s - loss: 0.5001 - accuracy: 0.85 - ETA: 0s - loss: 0.4846 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4836 - accuracy: 0.8537 - val_loss: 0.6865 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.87 - ETA: 0s - loss: 0.4458 - accuracy: 0.86 - ETA: 0s - loss: 0.4234 - accuracy: 0.86 - ETA: 0s - loss: 0.4313 - accuracy: 0.87 - ETA: 0s - loss: 0.4271 - accuracy: 0.87 - ETA: 0s - loss: 0.4306 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4291 - accuracy: 0.8764 - val_loss: 1.2427 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.84 - ETA: 0s - loss: 0.4658 - accuracy: 0.87 - ETA: 0s - loss: 0.4871 - accuracy: 0.85 - ETA: 0s - loss: 0.4779 - accuracy: 0.85 - ETA: 0s - loss: 0.4882 - accuracy: 0.85 - ETA: 0s - loss: 0.4813 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4819 - accuracy: 0.8600 - val_loss: 0.8208 - val_accuracy: 0.7433\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7347 - accuracy: 0.71 - ETA: 0s - loss: 0.4658 - accuracy: 0.85 - ETA: 0s - loss: 0.4777 - accuracy: 0.85 - ETA: 0s - loss: 0.4704 - accuracy: 0.85 - ETA: 0s - loss: 0.4609 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8667 - val_loss: 0.8265 - val_accuracy: 0.7343\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.84 - ETA: 0s - loss: 0.3699 - accuracy: 0.89 - ETA: 0s - loss: 0.3697 - accuracy: 0.90 - ETA: 0s - loss: 0.3804 - accuracy: 0.89 - ETA: 0s - loss: 0.4062 - accuracy: 0.88 - ETA: 0s - loss: 0.3969 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3967 - accuracy: 0.8899 - val_loss: 0.8619 - val_accuracy: 0.7239\n",
      "Epoch 23/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.1908 - accuracy: 0.96 - ETA: 0s - loss: 0.3928 - accuracy: 0.87 - ETA: 0s - loss: 0.3718 - accuracy: 0.89 - ETA: 0s - loss: 0.3927 - accuracy: 0.89 - ETA: 0s - loss: 0.3857 - accuracy: 0.89 - ETA: 0s - loss: 0.3839 - accuracy: 0.8944Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8940 - val_loss: 1.0379 - val_accuracy: 0.7254\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.8749 - accuracy: 0.59 - ETA: 0s - loss: 2.1894 - accuracy: 0.59 - ETA: 0s - loss: 1.5961 - accuracy: 0.63 - ETA: 0s - loss: 1.2850 - accuracy: 0.66 - ETA: 0s - loss: 1.1261 - accuracy: 0.67 - ETA: 0s - loss: 1.0195 - accuracy: 0.68 - 0s 5ms/step - loss: 0.9782 - accuracy: 0.6928 - val_loss: 0.5882 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.71 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5937 - accuracy: 0.71 - ETA: 0s - loss: 0.5770 - accuracy: 0.73 - ETA: 0s - loss: 0.5684 - accuracy: 0.73 - ETA: 0s - loss: 0.5681 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5664 - accuracy: 0.7492 - val_loss: 0.5930 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.78 - ETA: 0s - loss: 0.5103 - accuracy: 0.79 - ETA: 0s - loss: 0.4931 - accuracy: 0.80 - ETA: 0s - loss: 0.5162 - accuracy: 0.78 - ETA: 0s - loss: 0.5124 - accuracy: 0.79 - ETA: 0s - loss: 0.5186 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7932 - val_loss: 0.6077 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.75 - ETA: 0s - loss: 0.5653 - accuracy: 0.83 - ETA: 0s - loss: 0.5529 - accuracy: 0.80 - ETA: 0s - loss: 0.5561 - accuracy: 0.81 - ETA: 0s - loss: 0.5444 - accuracy: 0.80 - ETA: 0s - loss: 0.5520 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5521 - accuracy: 0.8070 - val_loss: 0.5817 - val_accuracy: 0.7463\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.68 - ETA: 0s - loss: 0.5413 - accuracy: 0.80 - ETA: 0s - loss: 0.5439 - accuracy: 0.80 - ETA: 0s - loss: 0.5196 - accuracy: 0.81 - ETA: 0s - loss: 0.5467 - accuracy: 0.80 - ETA: 0s - loss: 0.5396 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5356 - accuracy: 0.8100 - val_loss: 0.6206 - val_accuracy: 0.7328\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.87 - ETA: 0s - loss: 0.5440 - accuracy: 0.80 - ETA: 0s - loss: 0.4997 - accuracy: 0.81 - ETA: 0s - loss: 0.5171 - accuracy: 0.81 - ETA: 0s - loss: 0.5261 - accuracy: 0.80 - ETA: 0s - loss: 0.5287 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5309 - accuracy: 0.8066 - val_loss: 0.5751 - val_accuracy: 0.7627\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.75 - ETA: 0s - loss: 0.5207 - accuracy: 0.82 - ETA: 0s - loss: 0.5329 - accuracy: 0.83 - ETA: 0s - loss: 0.5156 - accuracy: 0.83 - ETA: 0s - loss: 0.5071 - accuracy: 0.83 - ETA: 0s - loss: 0.5189 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5268 - accuracy: 0.8328 - val_loss: 0.8756 - val_accuracy: 0.6701\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.75 - ETA: 0s - loss: 0.4452 - accuracy: 0.86 - ETA: 0s - loss: 0.5119 - accuracy: 0.83 - ETA: 0s - loss: 0.4877 - accuracy: 0.84 - ETA: 0s - loss: 0.4857 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4841 - accuracy: 0.8402 - val_loss: 0.9202 - val_accuracy: 0.7313\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.84 - ETA: 0s - loss: 0.4626 - accuracy: 0.86 - ETA: 0s - loss: 0.4705 - accuracy: 0.84 - ETA: 0s - loss: 0.4630 - accuracy: 0.85 - ETA: 0s - loss: 0.4567 - accuracy: 0.85 - ETA: 0s - loss: 0.4491 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4480 - accuracy: 0.8555 - val_loss: 0.7173 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4181 - accuracy: 0.90 - ETA: 0s - loss: 0.4137 - accuracy: 0.87 - ETA: 0s - loss: 0.4171 - accuracy: 0.86 - ETA: 0s - loss: 0.4145 - accuracy: 0.86 - ETA: 0s - loss: 0.4244 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4395 - accuracy: 0.8559 - val_loss: 0.9588 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.87 - ETA: 0s - loss: 0.4172 - accuracy: 0.87 - ETA: 0s - loss: 0.4109 - accuracy: 0.87 - ETA: 0s - loss: 0.4114 - accuracy: 0.87 - ETA: 0s - loss: 0.4429 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4420 - accuracy: 0.8694 - val_loss: 0.6542 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.84 - ETA: 0s - loss: 0.4327 - accuracy: 0.86 - ETA: 0s - loss: 0.4434 - accuracy: 0.85 - ETA: 0s - loss: 0.4487 - accuracy: 0.85 - ETA: 0s - loss: 0.4446 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4438 - accuracy: 0.8593 - val_loss: 0.7792 - val_accuracy: 0.7104\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.75 - ETA: 0s - loss: 0.3999 - accuracy: 0.86 - ETA: 0s - loss: 0.4065 - accuracy: 0.87 - ETA: 0s - loss: 0.4064 - accuracy: 0.86 - ETA: 0s - loss: 0.3953 - accuracy: 0.87 - ETA: 0s - loss: 0.3990 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8731 - val_loss: 0.7298 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.87 - ETA: 0s - loss: 0.5864 - accuracy: 0.87 - ETA: 0s - loss: 0.4845 - accuracy: 0.87 - ETA: 0s - loss: 0.4511 - accuracy: 0.87 - ETA: 0s - loss: 0.4224 - accuracy: 0.88 - ETA: 0s - loss: 0.4254 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8772 - val_loss: 0.9747 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.93 - ETA: 0s - loss: 0.3692 - accuracy: 0.89 - ETA: 0s - loss: 0.3863 - accuracy: 0.88 - ETA: 0s - loss: 0.4143 - accuracy: 0.88 - ETA: 0s - loss: 0.3881 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8918 - val_loss: 0.9313 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.3114 - accuracy: 0.87 - ETA: 0s - loss: 0.3744 - accuracy: 0.88 - ETA: 0s - loss: 0.3662 - accuracy: 0.89 - ETA: 0s - loss: 0.3830 - accuracy: 0.88 - ETA: 0s - loss: 0.3964 - accuracy: 0.8828Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4130 - accuracy: 0.8824 - val_loss: 0.7120 - val_accuracy: 0.7403\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 839fefdd2803a7db387e75d5c7a913e4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7562188903490702</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.38019002396404455</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 105</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0221 - accuracy: 0.56 - ETA: 0s - loss: 0.8483 - accuracy: 0.66 - ETA: 0s - loss: 0.7096 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6846 - accuracy: 0.7059 - val_loss: 0.5842 - val_accuracy: 0.7045\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.87 - ETA: 0s - loss: 0.5300 - accuracy: 0.78 - ETA: 0s - loss: 0.5311 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7745 - val_loss: 0.5562 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.87 - ETA: 0s - loss: 0.4862 - accuracy: 0.81 - ETA: 0s - loss: 0.5069 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8007 - val_loss: 0.7521 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.90 - ETA: 0s - loss: 0.4976 - accuracy: 0.81 - ETA: 0s - loss: 0.5008 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7988 - val_loss: 0.7823 - val_accuracy: 0.6418\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.75 - ETA: 0s - loss: 0.4334 - accuracy: 0.83 - ETA: 0s - loss: 0.4390 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4496 - accuracy: 0.8328 - val_loss: 0.7206 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.78 - ETA: 0s - loss: 0.4283 - accuracy: 0.83 - ETA: 0s - loss: 0.4270 - accuracy: 0.83 - ETA: 0s - loss: 0.4410 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8331 - val_loss: 0.6888 - val_accuracy: 0.7239\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.65 - ETA: 0s - loss: 0.4449 - accuracy: 0.83 - ETA: 0s - loss: 0.4304 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4265 - accuracy: 0.8443 - val_loss: 0.6544 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5137 - accuracy: 0.84 - ETA: 0s - loss: 0.3459 - accuracy: 0.87 - ETA: 0s - loss: 0.3775 - accuracy: 0.86 - ETA: 0s - loss: 0.3811 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8645 - val_loss: 0.7909 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.84 - ETA: 0s - loss: 0.3437 - accuracy: 0.87 - ETA: 0s - loss: 0.3433 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8761 - val_loss: 1.0644 - val_accuracy: 0.7060\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.93 - ETA: 0s - loss: 0.3194 - accuracy: 0.90 - ETA: 0s - loss: 0.3513 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8835 - val_loss: 0.6491 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.84 - ETA: 0s - loss: 0.3449 - accuracy: 0.88 - ETA: 0s - loss: 0.3644 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8794 - val_loss: 0.7438 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "58/84 [===================>..........] - ETA: 0s - loss: 0.3744 - accuracy: 0.93 - ETA: 0s - loss: 0.3235 - accuracy: 0.89 - ETA: 0s - loss: 0.3695 - accuracy: 0.8885Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8876 - val_loss: 1.1854 - val_accuracy: 0.7015\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.43 - ETA: 0s - loss: 0.8755 - accuracy: 0.63 - ETA: 0s - loss: 0.7632 - accuracy: 0.66 - 0s 3ms/step - loss: 0.7245 - accuracy: 0.6809 - val_loss: 0.6731 - val_accuracy: 0.6836\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.81 - ETA: 0s - loss: 0.4869 - accuracy: 0.77 - ETA: 0s - loss: 0.4926 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7607 - val_loss: 0.7319 - val_accuracy: 0.6672\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.84 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4254 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4256 - accuracy: 0.8093 - val_loss: 0.6983 - val_accuracy: 0.6761\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.81 - ETA: 0s - loss: 0.3459 - accuracy: 0.84 - ETA: 0s - loss: 0.3760 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8234 - val_loss: 0.8803 - val_accuracy: 0.6388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.81 - ETA: 0s - loss: 0.3251 - accuracy: 0.83 - ETA: 0s - loss: 0.3422 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8343 - val_loss: 0.8036 - val_accuracy: 0.6642\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.84 - ETA: 0s - loss: 0.3325 - accuracy: 0.85 - ETA: 0s - loss: 0.3309 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8440 - val_loss: 0.8401 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.90 - ETA: 0s - loss: 0.2757 - accuracy: 0.87 - ETA: 0s - loss: 0.2852 - accuracy: 0.86 - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8649 - val_loss: 0.8995 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.93 - ETA: 0s - loss: 0.2703 - accuracy: 0.88 - ETA: 0s - loss: 0.2867 - accuracy: 0.87 - ETA: 0s - loss: 0.2990 - accuracy: 0.87 - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8708 - val_loss: 0.6955 - val_accuracy: 0.7075\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.90 - ETA: 0s - loss: 0.2585 - accuracy: 0.87 - ETA: 0s - loss: 0.2687 - accuracy: 0.86 - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8664 - val_loss: 1.2062 - val_accuracy: 0.6851\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.96 - ETA: 0s - loss: 0.2650 - accuracy: 0.88 - ETA: 0s - loss: 0.2697 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2693 - accuracy: 0.8880 - val_loss: 1.4350 - val_accuracy: 0.7045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.93 - ETA: 0s - loss: 0.2194 - accuracy: 0.90 - ETA: 0s - loss: 0.2415 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2399 - accuracy: 0.8962 - val_loss: 1.9933 - val_accuracy: 0.7075\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.96 - ETA: 0s - loss: 0.2651 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8876 - val_loss: 1.9289 - val_accuracy: 0.6866\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.75 - ETA: 0s - loss: 0.1872 - accuracy: 0.91 - ETA: 0s - loss: 0.1905 - accuracy: 0.91 - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9071 - val_loss: 1.2627 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.87 - ETA: 0s - loss: 0.2655 - accuracy: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2385 - accuracy: 0.8985 - val_loss: 1.0346 - val_accuracy: 0.7045\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.96 - ETA: 0s - loss: 0.1820 - accuracy: 0.91 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2261 - accuracy: 0.8988 - val_loss: 1.5697 - val_accuracy: 0.6657\n",
      "Epoch 16/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.2648 - accuracy: 0.87 - ETA: 0s - loss: 0.1908 - accuracy: 0.91 - ETA: 0s - loss: 0.1889 - accuracy: 0.9219Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9168 - val_loss: 1.6466 - val_accuracy: 0.7149\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8026 - accuracy: 0.50 - ETA: 0s - loss: 0.8063 - accuracy: 0.61 - ETA: 0s - loss: 0.7559 - accuracy: 0.64 - ETA: 0s - loss: 0.6884 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6843 - accuracy: 0.6838 - val_loss: 0.6531 - val_accuracy: 0.6701\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.75 - ETA: 0s - loss: 0.5303 - accuracy: 0.73 - ETA: 0s - loss: 0.5577 - accuracy: 0.73 - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7395 - val_loss: 0.5799 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.71 - ETA: 0s - loss: 0.4898 - accuracy: 0.77 - ETA: 0s - loss: 0.4818 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7876 - val_loss: 0.7553 - val_accuracy: 0.6597\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.87 - ETA: 0s - loss: 0.4053 - accuracy: 0.80 - ETA: 0s - loss: 0.3968 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8145 - val_loss: 0.6114 - val_accuracy: 0.6836\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.81 - ETA: 0s - loss: 0.3916 - accuracy: 0.83 - ETA: 0s - loss: 0.3832 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8272 - val_loss: 0.7900 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.87 - ETA: 0s - loss: 0.3640 - accuracy: 0.84 - ETA: 0s - loss: 0.3675 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8380 - val_loss: 0.9041 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.84 - ETA: 0s - loss: 0.3837 - accuracy: 0.84 - ETA: 0s - loss: 0.3515 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8529 - val_loss: 0.8523 - val_accuracy: 0.6716\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.59 - ETA: 0s - loss: 0.3392 - accuracy: 0.84 - ETA: 0s - loss: 0.3268 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8623 - val_loss: 1.1087 - val_accuracy: 0.7134\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.96 - ETA: 0s - loss: 0.2404 - accuracy: 0.90 - ETA: 0s - loss: 0.2786 - accuracy: 0.87 - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8731 - val_loss: 0.9602 - val_accuracy: 0.6940\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.87 - ETA: 0s - loss: 0.2795 - accuracy: 0.89 - ETA: 0s - loss: 0.2884 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8895 - val_loss: 0.7587 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.90 - ETA: 0s - loss: 0.2490 - accuracy: 0.88 - ETA: 0s - loss: 0.2581 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2563 - accuracy: 0.8899 - val_loss: 1.2496 - val_accuracy: 0.6896\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.96 - ETA: 0s - loss: 0.2248 - accuracy: 0.91 - ETA: 0s - loss: 0.2726 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2622 - accuracy: 0.9003 - val_loss: 1.3511 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.96 - ETA: 0s - loss: 0.2228 - accuracy: 0.91 - ETA: 0s - loss: 0.2017 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9149 - val_loss: 1.7682 - val_accuracy: 0.7075\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.87 - ETA: 0s - loss: 0.2413 - accuracy: 0.90 - ETA: 0s - loss: 0.2366 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9115 - val_loss: 1.5011 - val_accuracy: 0.6776\n",
      "Epoch 15/50\n",
      "60/84 [====================>.........] - ETA: 0s - loss: 0.1333 - accuracy: 0.93 - ETA: 0s - loss: 0.1836 - accuracy: 0.93 - ETA: 0s - loss: 0.1767 - accuracy: 0.9339Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1802 - accuracy: 0.9313 - val_loss: 1.7519 - val_accuracy: 0.6955\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 080db5689a0818c6feb9d935a63a494b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7318408091862997</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.12799336398664463</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9153 - accuracy: 0.62 - ETA: 0s - loss: 1.7709 - accuracy: 0.59 - ETA: 0s - loss: 1.2217 - accuracy: 0.65 - ETA: 0s - loss: 1.0317 - accuracy: 0.66 - ETA: 0s - loss: 0.9458 - accuracy: 0.68 - ETA: 0s - loss: 0.8769 - accuracy: 0.69 - ETA: 0s - loss: 0.8243 - accuracy: 0.70 - 1s 7ms/step - loss: 0.8010 - accuracy: 0.7006 - val_loss: 0.5751 - val_accuracy: 0.7388\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.78 - ETA: 0s - loss: 0.5071 - accuracy: 0.78 - ETA: 0s - loss: 0.5278 - accuracy: 0.79 - ETA: 0s - loss: 0.5357 - accuracy: 0.78 - ETA: 0s - loss: 0.5316 - accuracy: 0.78 - ETA: 0s - loss: 0.5373 - accuracy: 0.78 - ETA: 0s - loss: 0.5329 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5385 - accuracy: 0.7783 - val_loss: 0.5835 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.81 - ETA: 0s - loss: 0.4636 - accuracy: 0.81 - ETA: 0s - loss: 0.4497 - accuracy: 0.82 - ETA: 0s - loss: 0.4806 - accuracy: 0.81 - ETA: 0s - loss: 0.4892 - accuracy: 0.81 - ETA: 0s - loss: 0.4940 - accuracy: 0.81 - ETA: 0s - loss: 0.4968 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5016 - accuracy: 0.8074 - val_loss: 0.5519 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.78 - ETA: 0s - loss: 0.5155 - accuracy: 0.79 - ETA: 0s - loss: 0.5174 - accuracy: 0.79 - ETA: 0s - loss: 0.5107 - accuracy: 0.80 - ETA: 0s - loss: 0.5004 - accuracy: 0.80 - ETA: 0s - loss: 0.4963 - accuracy: 0.80 - ETA: 0s - loss: 0.4933 - accuracy: 0.81 - ETA: 0s - loss: 0.4926 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4926 - accuracy: 0.8104 - val_loss: 0.6398 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.81 - ETA: 0s - loss: 0.4873 - accuracy: 0.82 - ETA: 0s - loss: 0.4770 - accuracy: 0.83 - ETA: 0s - loss: 0.4782 - accuracy: 0.83 - ETA: 0s - loss: 0.4679 - accuracy: 0.84 - ETA: 0s - loss: 0.4633 - accuracy: 0.84 - ETA: 0s - loss: 0.4848 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4886 - accuracy: 0.8376 - val_loss: 0.6278 - val_accuracy: 0.7000\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3772 - accuracy: 0.90 - ETA: 0s - loss: 0.5860 - accuracy: 0.83 - ETA: 0s - loss: 0.5859 - accuracy: 0.81 - ETA: 0s - loss: 0.5822 - accuracy: 0.80 - ETA: 0s - loss: 0.5704 - accuracy: 0.80 - ETA: 0s - loss: 0.5620 - accuracy: 0.80 - ETA: 0s - loss: 0.5474 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5444 - accuracy: 0.8190 - val_loss: 0.6672 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.84 - ETA: 0s - loss: 0.5142 - accuracy: 0.80 - ETA: 0s - loss: 0.4956 - accuracy: 0.82 - ETA: 0s - loss: 0.4887 - accuracy: 0.83 - ETA: 0s - loss: 0.4889 - accuracy: 0.83 - ETA: 0s - loss: 0.4893 - accuracy: 0.83 - ETA: 0s - loss: 0.4902 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4881 - accuracy: 0.8309 - val_loss: 0.6324 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.90 - ETA: 0s - loss: 0.4058 - accuracy: 0.86 - ETA: 0s - loss: 0.4735 - accuracy: 0.84 - ETA: 0s - loss: 0.5800 - accuracy: 0.83 - ETA: 0s - loss: 0.5574 - accuracy: 0.83 - ETA: 0s - loss: 0.5410 - accuracy: 0.83 - ETA: 0s - loss: 0.5242 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5125 - accuracy: 0.8369 - val_loss: 0.6469 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.90 - ETA: 0s - loss: 0.4387 - accuracy: 0.88 - ETA: 0s - loss: 0.4361 - accuracy: 0.87 - ETA: 0s - loss: 0.4481 - accuracy: 0.86 - ETA: 0s - loss: 0.4614 - accuracy: 0.85 - ETA: 0s - loss: 0.4553 - accuracy: 0.85 - ETA: 0s - loss: 0.4580 - accuracy: 0.85 - ETA: 0s - loss: 0.4616 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4616 - accuracy: 0.8496 - val_loss: 0.9612 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.90 - ETA: 0s - loss: 0.3914 - accuracy: 0.88 - ETA: 0s - loss: 0.4239 - accuracy: 0.85 - ETA: 0s - loss: 0.4457 - accuracy: 0.84 - ETA: 0s - loss: 0.4478 - accuracy: 0.85 - ETA: 0s - loss: 0.4661 - accuracy: 0.84 - ETA: 0s - loss: 0.4685 - accuracy: 0.84 - ETA: 0s - loss: 0.4664 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4662 - accuracy: 0.8485 - val_loss: 0.5649 - val_accuracy: 0.7537\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.81 - ETA: 0s - loss: 0.4923 - accuracy: 0.85 - ETA: 0s - loss: 0.4843 - accuracy: 0.85 - ETA: 0s - loss: 0.4628 - accuracy: 0.85 - ETA: 0s - loss: 0.4558 - accuracy: 0.86 - ETA: 0s - loss: 0.4603 - accuracy: 0.85 - ETA: 0s - loss: 0.4595 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4568 - accuracy: 0.8563 - val_loss: 0.6724 - val_accuracy: 0.7478\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.96 - ETA: 0s - loss: 0.4224 - accuracy: 0.88 - ETA: 0s - loss: 0.5134 - accuracy: 0.84 - ETA: 0s - loss: 0.4912 - accuracy: 0.84 - ETA: 0s - loss: 0.4822 - accuracy: 0.84 - ETA: 0s - loss: 0.4653 - accuracy: 0.85 - ETA: 0s - loss: 0.5078 - accuracy: 0.84 - ETA: 0s - loss: 0.5114 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5174 - accuracy: 0.8443 - val_loss: 0.6165 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.84 - ETA: 0s - loss: 0.5898 - accuracy: 0.83 - ETA: 0s - loss: 0.5353 - accuracy: 0.83 - ETA: 0s - loss: 0.5160 - accuracy: 0.83 - ETA: 0s - loss: 0.5047 - accuracy: 0.83 - ETA: 0s - loss: 0.4991 - accuracy: 0.83 - ETA: 0s - loss: 0.4855 - accuracy: 0.84 - ETA: 0s - loss: 0.4766 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4766 - accuracy: 0.8443 - val_loss: 0.7835 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.87 - ETA: 0s - loss: 0.4060 - accuracy: 0.87 - ETA: 0s - loss: 0.4008 - accuracy: 0.87 - ETA: 0s - loss: 0.4058 - accuracy: 0.87 - ETA: 0s - loss: 0.4005 - accuracy: 0.88 - ETA: 0s - loss: 0.4186 - accuracy: 0.87 - ETA: 0s - loss: 0.4119 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4181 - accuracy: 0.8776 - val_loss: 0.6991 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.90 - ETA: 0s - loss: 0.3914 - accuracy: 0.88 - ETA: 0s - loss: 0.3791 - accuracy: 0.89 - ETA: 0s - loss: 0.3809 - accuracy: 0.89 - ETA: 0s - loss: 0.3900 - accuracy: 0.88 - ETA: 0s - loss: 0.3965 - accuracy: 0.88 - ETA: 0s - loss: 0.4054 - accuracy: 0.88 - ETA: 0s - loss: 0.4086 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4109 - accuracy: 0.8783 - val_loss: 0.6754 - val_accuracy: 0.7731\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - ETA: 0s - loss: 0.4659 - accuracy: 0.85 - ETA: 0s - loss: 0.4335 - accuracy: 0.86 - ETA: 0s - loss: 0.4244 - accuracy: 0.87 - ETA: 0s - loss: 0.4418 - accuracy: 0.86 - ETA: 0s - loss: 0.4395 - accuracy: 0.86 - ETA: 0s - loss: 0.4352 - accuracy: 0.86 - ETA: 0s - loss: 0.4461 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4439 - accuracy: 0.8604 - val_loss: 0.8009 - val_accuracy: 0.7284\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.78 - ETA: 0s - loss: 0.4330 - accuracy: 0.86 - ETA: 0s - loss: 0.4233 - accuracy: 0.86 - ETA: 0s - loss: 0.4360 - accuracy: 0.86 - ETA: 0s - loss: 0.4309 - accuracy: 0.87 - ETA: 0s - loss: 0.4211 - accuracy: 0.87 - ETA: 0s - loss: 0.4363 - accuracy: 0.86 - ETA: 0s - loss: 0.4606 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8626 - val_loss: 0.6734 - val_accuracy: 0.7463\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.81 - ETA: 0s - loss: 0.5918 - accuracy: 0.79 - ETA: 0s - loss: 0.5651 - accuracy: 0.81 - ETA: 0s - loss: 0.5300 - accuracy: 0.83 - ETA: 0s - loss: 0.5274 - accuracy: 0.83 - ETA: 0s - loss: 0.5091 - accuracy: 0.84 - ETA: 0s - loss: 0.5044 - accuracy: 0.84 - ETA: 0s - loss: 0.5105 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5103 - accuracy: 0.8406 - val_loss: 0.7280 - val_accuracy: 0.7403\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5431 - accuracy: 0.81 - ETA: 0s - loss: 0.4961 - accuracy: 0.83 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.5009 - accuracy: 0.83 - ETA: 0s - loss: 0.5373 - accuracy: 0.83 - ETA: 0s - loss: 0.5251 - accuracy: 0.83 - ETA: 0s - loss: 0.5278 - accuracy: 0.82 - ETA: 0s - loss: 0.5253 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5245 - accuracy: 0.8279 - val_loss: 0.6720 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.81 - ETA: 0s - loss: 0.4203 - accuracy: 0.86 - ETA: 0s - loss: 0.4471 - accuracy: 0.85 - ETA: 0s - loss: 0.4548 - accuracy: 0.85 - ETA: 0s - loss: 0.4752 - accuracy: 0.85 - ETA: 0s - loss: 0.5189 - accuracy: 0.85 - ETA: 0s - loss: 0.5178 - accuracy: 0.85 - ETA: 0s - loss: 0.5131 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5157 - accuracy: 0.8537 - val_loss: 0.7024 - val_accuracy: 0.7507\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.93 - ETA: 0s - loss: 0.6387 - accuracy: 0.79 - ETA: 0s - loss: 0.6472 - accuracy: 0.77 - ETA: 0s - loss: 0.6517 - accuracy: 0.76 - ETA: 0s - loss: 0.7290 - accuracy: 0.76 - ETA: 0s - loss: 0.7120 - accuracy: 0.76 - ETA: 0s - loss: 0.7212 - accuracy: 0.76 - 0s 5ms/step - loss: 0.7146 - accuracy: 0.7630 - val_loss: 0.7698 - val_accuracy: 0.7507\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.84 - ETA: 0s - loss: 0.5417 - accuracy: 0.84 - ETA: 0s - loss: 0.6106 - accuracy: 0.80 - ETA: 0s - loss: 0.6318 - accuracy: 0.78 - ETA: 0s - loss: 0.6853 - accuracy: 0.76 - ETA: 0s - loss: 0.6859 - accuracy: 0.75 - ETA: 0s - loss: 0.6922 - accuracy: 0.73 - ETA: 0s - loss: 0.6909 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6901 - accuracy: 0.7346 - val_loss: 0.6786 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6953 - accuracy: 0.69 - ETA: 0s - loss: 0.6939 - accuracy: 0.69 - ETA: 0s - loss: 0.6900 - accuracy: 0.70 - ETA: 0s - loss: 0.6883 - accuracy: 0.70 - ETA: 0s - loss: 0.6952 - accuracy: 0.69 - ETA: 0s - loss: 0.6942 - accuracy: 0.63 - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5875 - val_loss: 0.6934 - val_accuracy: 0.3045\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.28 - ETA: 0s - loss: 0.6706 - accuracy: 0.55 - ETA: 0s - loss: 0.6927 - accuracy: 0.60 - ETA: 0s - loss: 0.6951 - accuracy: 0.62 - ETA: 0s - loss: 0.6957 - accuracy: 0.55 - ETA: 0s - loss: 0.6973 - accuracy: 0.50 - ETA: 0s - loss: 0.6992 - accuracy: 0.47 - ETA: 0s - loss: 0.6967 - accuracy: 0.45 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4337 - val_loss: 0.6974 - val_accuracy: 0.3045\n",
      "Epoch 25/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.7597 - accuracy: 0.40 - ETA: 0s - loss: 0.6858 - accuracy: 0.43 - ETA: 0s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6894 - accuracy: 0.60 - ETA: 0s - loss: 0.6831 - accuracy: 0.64 - ETA: 0s - loss: 0.6901 - accuracy: 0.64 - ETA: 0s - loss: 0.6919 - accuracy: 0.6246Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.6010 - val_loss: 0.6972 - val_accuracy: 0.3045\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6729 - accuracy: 0.75 - ETA: 0s - loss: 2.2209 - accuracy: 0.63 - ETA: 0s - loss: 1.4817 - accuracy: 0.66 - ETA: 0s - loss: 1.2424 - accuracy: 0.66 - ETA: 0s - loss: 1.1041 - accuracy: 0.67 - ETA: 0s - loss: 0.9999 - accuracy: 0.69 - ETA: 0s - loss: 0.9367 - accuracy: 0.70 - ETA: 0s - loss: 0.8942 - accuracy: 0.70 - 1s 6ms/step - loss: 0.8942 - accuracy: 0.7059 - val_loss: 0.6651 - val_accuracy: 0.6493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.78 - ETA: 0s - loss: 0.5751 - accuracy: 0.72 - ETA: 0s - loss: 0.5450 - accuracy: 0.76 - ETA: 0s - loss: 0.5488 - accuracy: 0.75 - ETA: 0s - loss: 0.5630 - accuracy: 0.75 - ETA: 0s - loss: 0.5622 - accuracy: 0.75 - ETA: 0s - loss: 0.5632 - accuracy: 0.75 - ETA: 0s - loss: 0.5742 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5752 - accuracy: 0.7514 - val_loss: 0.6195 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.78 - ETA: 0s - loss: 0.6442 - accuracy: 0.80 - ETA: 0s - loss: 0.5901 - accuracy: 0.77 - ETA: 0s - loss: 0.5637 - accuracy: 0.78 - ETA: 0s - loss: 0.5512 - accuracy: 0.78 - ETA: 0s - loss: 0.5556 - accuracy: 0.78 - ETA: 0s - loss: 0.5419 - accuracy: 0.79 - ETA: 0s - loss: 0.5636 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7969 - val_loss: 0.9089 - val_accuracy: 0.6358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.71 - ETA: 0s - loss: 0.6383 - accuracy: 0.80 - ETA: 0s - loss: 0.5832 - accuracy: 0.81 - ETA: 0s - loss: 0.5810 - accuracy: 0.80 - ETA: 0s - loss: 0.5656 - accuracy: 0.80 - ETA: 0s - loss: 0.5421 - accuracy: 0.81 - ETA: 0s - loss: 0.5369 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5364 - accuracy: 0.8182 - val_loss: 0.5993 - val_accuracy: 0.7269\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.87 - ETA: 0s - loss: 0.4777 - accuracy: 0.83 - ETA: 0s - loss: 0.4730 - accuracy: 0.83 - ETA: 0s - loss: 0.4680 - accuracy: 0.84 - ETA: 0s - loss: 0.4775 - accuracy: 0.83 - ETA: 0s - loss: 0.4846 - accuracy: 0.83 - ETA: 0s - loss: 0.4884 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4862 - accuracy: 0.8317 - val_loss: 0.7503 - val_accuracy: 0.7060\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.84 - ETA: 0s - loss: 0.4223 - accuracy: 0.85 - ETA: 0s - loss: 0.4449 - accuracy: 0.84 - ETA: 0s - loss: 0.4413 - accuracy: 0.84 - ETA: 0s - loss: 0.4427 - accuracy: 0.85 - ETA: 0s - loss: 0.4415 - accuracy: 0.85 - ETA: 0s - loss: 0.4464 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4571 - accuracy: 0.8432 - val_loss: 0.6668 - val_accuracy: 0.7119\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.81 - ETA: 0s - loss: 0.4646 - accuracy: 0.83 - ETA: 0s - loss: 0.4378 - accuracy: 0.84 - ETA: 0s - loss: 0.4352 - accuracy: 0.85 - ETA: 0s - loss: 0.4255 - accuracy: 0.85 - ETA: 0s - loss: 0.4290 - accuracy: 0.85 - ETA: 0s - loss: 0.4346 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4393 - accuracy: 0.8544 - val_loss: 0.6812 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.93 - ETA: 0s - loss: 0.3880 - accuracy: 0.86 - ETA: 0s - loss: 0.4116 - accuracy: 0.86 - ETA: 0s - loss: 0.4222 - accuracy: 0.86 - ETA: 0s - loss: 0.4325 - accuracy: 0.86 - ETA: 0s - loss: 0.4434 - accuracy: 0.85 - ETA: 0s - loss: 0.4408 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8608 - val_loss: 0.9783 - val_accuracy: 0.7119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.75 - ETA: 0s - loss: 0.4043 - accuracy: 0.87 - ETA: 0s - loss: 0.3856 - accuracy: 0.87 - ETA: 0s - loss: 0.4063 - accuracy: 0.87 - ETA: 0s - loss: 0.4076 - accuracy: 0.86 - ETA: 0s - loss: 0.4213 - accuracy: 0.86 - ETA: 0s - loss: 0.4232 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4264 - accuracy: 0.8626 - val_loss: 0.6122 - val_accuracy: 0.7418\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.81 - ETA: 0s - loss: 0.4335 - accuracy: 0.86 - ETA: 0s - loss: 0.4498 - accuracy: 0.85 - ETA: 0s - loss: 0.4589 - accuracy: 0.84 - ETA: 0s - loss: 0.4664 - accuracy: 0.84 - ETA: 0s - loss: 0.4620 - accuracy: 0.84 - ETA: 0s - loss: 0.4763 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4824 - accuracy: 0.8436 - val_loss: 1.0719 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.90 - ETA: 0s - loss: 0.4403 - accuracy: 0.86 - ETA: 0s - loss: 0.5254 - accuracy: 0.83 - ETA: 0s - loss: 0.5477 - accuracy: 0.82 - ETA: 0s - loss: 0.5371 - accuracy: 0.82 - ETA: 0s - loss: 0.5289 - accuracy: 0.82 - ETA: 0s - loss: 0.5337 - accuracy: 0.82 - ETA: 0s - loss: 0.5216 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5239 - accuracy: 0.8305 - val_loss: 1.2847 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 1.00 - ETA: 0s - loss: 0.8929 - accuracy: 0.73 - ETA: 0s - loss: 0.8858 - accuracy: 0.69 - ETA: 0s - loss: 0.8182 - accuracy: 0.70 - ETA: 0s - loss: 0.7952 - accuracy: 0.70 - ETA: 0s - loss: 0.7752 - accuracy: 0.70 - ETA: 0s - loss: 0.7565 - accuracy: 0.70 - ETA: 0s - loss: 0.7515 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7507 - accuracy: 0.7040 - val_loss: 0.6700 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.75 - ETA: 0s - loss: 0.7036 - accuracy: 0.68 - ETA: 0s - loss: 0.6920 - accuracy: 0.70 - ETA: 0s - loss: 0.6910 - accuracy: 0.70 - ETA: 0s - loss: 0.6976 - accuracy: 0.69 - ETA: 0s - loss: 0.6982 - accuracy: 0.66 - ETA: 0s - loss: 0.6959 - accuracy: 0.60 - ETA: 0s - loss: 0.6939 - accuracy: 0.57 - 0s 5ms/step - loss: 0.6939 - accuracy: 0.5752 - val_loss: 0.6915 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.68 - ETA: 0s - loss: 0.7026 - accuracy: 0.68 - ETA: 0s - loss: 0.7035 - accuracy: 0.58 - ETA: 0s - loss: 0.6992 - accuracy: 0.49 - ETA: 0s - loss: 0.6929 - accuracy: 0.44 - ETA: 0s - loss: 0.6948 - accuracy: 0.48 - ETA: 0s - loss: 0.6950 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.54 - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5491 - val_loss: 0.6902 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.71 - ETA: 0s - loss: 0.6922 - accuracy: 0.70 - ETA: 0s - loss: 0.6839 - accuracy: 0.71 - ETA: 0s - loss: 0.6858 - accuracy: 0.71 - ETA: 0s - loss: 0.6884 - accuracy: 0.70 - ETA: 0s - loss: 0.6896 - accuracy: 0.70 - ETA: 0s - loss: 0.6901 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6934 - accuracy: 0.6991 - val_loss: 0.6921 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.68 - ETA: 0s - loss: 0.6890 - accuracy: 0.32 - ETA: 0s - loss: 0.6959 - accuracy: 0.32 - ETA: 0s - loss: 0.7027 - accuracy: 0.32 - ETA: 0s - loss: 0.7030 - accuracy: 0.32 - ETA: 0s - loss: 0.6980 - accuracy: 0.31 - ETA: 0s - loss: 0.6972 - accuracy: 0.31 - 0s 5ms/step - loss: 0.6949 - accuracy: 0.3281 - val_loss: 0.6916 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.87 - ETA: 0s - loss: 0.6985 - accuracy: 0.69 - ETA: 0s - loss: 0.6758 - accuracy: 0.72 - ETA: 0s - loss: 0.6833 - accuracy: 0.71 - ETA: 0s - loss: 0.6941 - accuracy: 0.69 - ETA: 0s - loss: 0.6963 - accuracy: 0.67 - ETA: 0s - loss: 0.6931 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6940 - accuracy: 0.5719 - val_loss: 0.6964 - val_accuracy: 0.3045\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.25 - ETA: 0s - loss: 0.6758 - accuracy: 0.33 - ETA: 0s - loss: 0.6922 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.56 - ETA: 0s - loss: 0.6931 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.45 - ETA: 0s - loss: 0.6932 - accuracy: 0.43 - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4319 - val_loss: 0.6926 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6390 - accuracy: 0.78 - ETA: 0s - loss: 0.7115 - accuracy: 0.43 - ETA: 0s - loss: 0.6992 - accuracy: 0.36 - ETA: 0s - loss: 0.6906 - accuracy: 0.33 - ETA: 0s - loss: 0.6881 - accuracy: 0.37 - ETA: 0s - loss: 0.6878 - accuracy: 0.43 - ETA: 0s - loss: 0.6902 - accuracy: 0.47 - ETA: 0s - loss: 0.6911 - accuracy: 0.5088Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7201 - accuracy: 0.5185 - val_loss: 0.6932 - val_accuracy: 0.3045\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7474 - accuracy: 0.75 - ETA: 0s - loss: 1.9869 - accuracy: 0.65 - ETA: 0s - loss: 1.3336 - accuracy: 0.68 - ETA: 0s - loss: 1.0957 - accuracy: 0.70 - ETA: 0s - loss: 0.9869 - accuracy: 0.71 - ETA: 0s - loss: 0.9097 - accuracy: 0.71 - ETA: 0s - loss: 0.8599 - accuracy: 0.72 - ETA: 0s - loss: 0.8256 - accuracy: 0.72 - 1s 6ms/step - loss: 0.8183 - accuracy: 0.7275 - val_loss: 0.6333 - val_accuracy: 0.6925\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.78 - ETA: 0s - loss: 0.5537 - accuracy: 0.75 - ETA: 0s - loss: 0.5466 - accuracy: 0.76 - ETA: 0s - loss: 0.5431 - accuracy: 0.77 - ETA: 0s - loss: 0.5436 - accuracy: 0.78 - ETA: 0s - loss: 0.5493 - accuracy: 0.77 - ETA: 0s - loss: 0.5512 - accuracy: 0.77 - ETA: 0s - loss: 0.5594 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7630 - val_loss: 0.5843 - val_accuracy: 0.7209\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.84 - ETA: 0s - loss: 0.5460 - accuracy: 0.73 - ETA: 0s - loss: 0.5477 - accuracy: 0.75 - ETA: 0s - loss: 0.5418 - accuracy: 0.76 - ETA: 0s - loss: 0.5455 - accuracy: 0.77 - ETA: 0s - loss: 0.5465 - accuracy: 0.77 - ETA: 0s - loss: 0.5377 - accuracy: 0.77 - ETA: 0s - loss: 0.5381 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5311 - accuracy: 0.7861 - val_loss: 0.6255 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.81 - ETA: 0s - loss: 0.4719 - accuracy: 0.82 - ETA: 0s - loss: 0.4438 - accuracy: 0.83 - ETA: 0s - loss: 0.4576 - accuracy: 0.82 - ETA: 0s - loss: 0.4752 - accuracy: 0.82 - ETA: 0s - loss: 0.4802 - accuracy: 0.81 - ETA: 0s - loss: 0.4955 - accuracy: 0.80 - ETA: 0s - loss: 0.5113 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5129 - accuracy: 0.8100 - val_loss: 0.5679 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4972 - accuracy: 0.84 - ETA: 0s - loss: 0.5438 - accuracy: 0.81 - ETA: 0s - loss: 0.5404 - accuracy: 0.81 - ETA: 0s - loss: 0.5160 - accuracy: 0.82 - ETA: 0s - loss: 0.5223 - accuracy: 0.82 - ETA: 0s - loss: 0.5164 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.81 - ETA: 0s - loss: 0.5108 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5074 - accuracy: 0.8193 - val_loss: 0.6231 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8986 - accuracy: 0.62 - ETA: 0s - loss: 0.4564 - accuracy: 0.85 - ETA: 0s - loss: 0.4667 - accuracy: 0.85 - ETA: 0s - loss: 0.4792 - accuracy: 0.84 - ETA: 0s - loss: 0.4699 - accuracy: 0.84 - ETA: 0s - loss: 0.4685 - accuracy: 0.84 - ETA: 0s - loss: 0.4828 - accuracy: 0.83 - ETA: 0s - loss: 0.4860 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4888 - accuracy: 0.8305 - val_loss: 0.5869 - val_accuracy: 0.7149\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.84 - ETA: 0s - loss: 0.4212 - accuracy: 0.85 - ETA: 0s - loss: 0.4483 - accuracy: 0.84 - ETA: 0s - loss: 0.4538 - accuracy: 0.84 - ETA: 0s - loss: 0.4479 - accuracy: 0.84 - ETA: 0s - loss: 0.4506 - accuracy: 0.84 - ETA: 0s - loss: 0.4606 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4554 - accuracy: 0.8499 - val_loss: 0.6831 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.78 - ETA: 0s - loss: 0.4150 - accuracy: 0.86 - ETA: 0s - loss: 0.4173 - accuracy: 0.86 - ETA: 0s - loss: 0.4312 - accuracy: 0.86 - ETA: 0s - loss: 0.4383 - accuracy: 0.86 - ETA: 0s - loss: 0.4397 - accuracy: 0.85 - ETA: 0s - loss: 0.4400 - accuracy: 0.85 - ETA: 0s - loss: 0.4436 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4434 - accuracy: 0.8544 - val_loss: 0.7391 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.84 - ETA: 0s - loss: 0.4592 - accuracy: 0.84 - ETA: 0s - loss: 0.4280 - accuracy: 0.86 - ETA: 0s - loss: 0.4327 - accuracy: 0.86 - ETA: 0s - loss: 0.4417 - accuracy: 0.86 - ETA: 0s - loss: 0.4415 - accuracy: 0.86 - ETA: 0s - loss: 0.4496 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4449 - accuracy: 0.8585 - val_loss: 0.7729 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.87 - ETA: 0s - loss: 0.4176 - accuracy: 0.85 - ETA: 0s - loss: 0.4039 - accuracy: 0.87 - ETA: 0s - loss: 0.4699 - accuracy: 0.85 - ETA: 0s - loss: 0.4819 - accuracy: 0.84 - ETA: 0s - loss: 0.4792 - accuracy: 0.84 - ETA: 0s - loss: 0.4755 - accuracy: 0.84 - ETA: 0s - loss: 0.4809 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4821 - accuracy: 0.8391 - val_loss: 0.7756 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4026 - accuracy: 0.87 - ETA: 0s - loss: 0.4063 - accuracy: 0.87 - ETA: 0s - loss: 0.4343 - accuracy: 0.86 - ETA: 0s - loss: 0.4686 - accuracy: 0.85 - ETA: 0s - loss: 0.4626 - accuracy: 0.85 - ETA: 0s - loss: 0.4681 - accuracy: 0.84 - ETA: 0s - loss: 0.4764 - accuracy: 0.84 - ETA: 0s - loss: 0.4841 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4835 - accuracy: 0.8443 - val_loss: 0.5932 - val_accuracy: 0.7597\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4385 - accuracy: 0.87 - ETA: 0s - loss: 0.4558 - accuracy: 0.84 - ETA: 0s - loss: 0.4733 - accuracy: 0.84 - ETA: 0s - loss: 0.4534 - accuracy: 0.85 - ETA: 0s - loss: 0.4511 - accuracy: 0.85 - ETA: 0s - loss: 0.4598 - accuracy: 0.85 - ETA: 0s - loss: 0.4616 - accuracy: 0.85 - ETA: 0s - loss: 0.4644 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4646 - accuracy: 0.8511 - val_loss: 0.5728 - val_accuracy: 0.7642\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.93 - ETA: 0s - loss: 0.4481 - accuracy: 0.86 - ETA: 0s - loss: 0.4339 - accuracy: 0.87 - ETA: 0s - loss: 0.4338 - accuracy: 0.87 - ETA: 0s - loss: 0.4346 - accuracy: 0.87 - ETA: 0s - loss: 0.4240 - accuracy: 0.87 - ETA: 0s - loss: 0.4208 - accuracy: 0.87 - ETA: 0s - loss: 0.4170 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4150 - accuracy: 0.8791 - val_loss: 0.9018 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.87 - ETA: 0s - loss: 0.3478 - accuracy: 0.90 - ETA: 0s - loss: 0.3778 - accuracy: 0.89 - ETA: 0s - loss: 0.3932 - accuracy: 0.89 - ETA: 0s - loss: 0.4065 - accuracy: 0.88 - ETA: 0s - loss: 0.3992 - accuracy: 0.88 - ETA: 0s - loss: 0.4034 - accuracy: 0.88 - ETA: 0s - loss: 0.4046 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4074 - accuracy: 0.8802 - val_loss: 0.8079 - val_accuracy: 0.7418\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.90 - ETA: 0s - loss: 0.3596 - accuracy: 0.88 - ETA: 0s - loss: 0.3872 - accuracy: 0.88 - ETA: 0s - loss: 0.3828 - accuracy: 0.88 - ETA: 0s - loss: 0.3899 - accuracy: 0.88 - ETA: 0s - loss: 0.3883 - accuracy: 0.88 - ETA: 0s - loss: 0.3969 - accuracy: 0.87 - ETA: 0s - loss: 0.3979 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4013 - accuracy: 0.8764 - val_loss: 0.9189 - val_accuracy: 0.7582\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.87 - ETA: 0s - loss: 0.3744 - accuracy: 0.90 - ETA: 0s - loss: 0.3863 - accuracy: 0.89 - ETA: 0s - loss: 0.3608 - accuracy: 0.90 - ETA: 0s - loss: 0.3774 - accuracy: 0.89 - ETA: 0s - loss: 0.3720 - accuracy: 0.89 - ETA: 0s - loss: 0.3692 - accuracy: 0.89 - ETA: 0s - loss: 0.3765 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3796 - accuracy: 0.8895 - val_loss: 0.7262 - val_accuracy: 0.7254\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.96 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - ETA: 0s - loss: 0.4730 - accuracy: 0.84 - ETA: 0s - loss: 0.4466 - accuracy: 0.86 - ETA: 0s - loss: 0.4372 - accuracy: 0.86 - ETA: 0s - loss: 0.4220 - accuracy: 0.86 - ETA: 0s - loss: 0.4050 - accuracy: 0.87 - ETA: 0s - loss: 0.3971 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4013 - accuracy: 0.8820 - val_loss: 0.8019 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4668 - accuracy: 0.87 - ETA: 0s - loss: 0.4167 - accuracy: 0.88 - ETA: 0s - loss: 0.4113 - accuracy: 0.88 - ETA: 0s - loss: 0.4072 - accuracy: 0.88 - ETA: 0s - loss: 0.4008 - accuracy: 0.88 - ETA: 0s - loss: 0.4030 - accuracy: 0.88 - ETA: 0s - loss: 0.4042 - accuracy: 0.88 - ETA: 0s - loss: 0.4006 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3994 - accuracy: 0.8910 - val_loss: 0.9748 - val_accuracy: 0.7373\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.75 - ETA: 0s - loss: 0.3733 - accuracy: 0.89 - ETA: 0s - loss: 0.3701 - accuracy: 0.89 - ETA: 0s - loss: 0.3593 - accuracy: 0.89 - ETA: 0s - loss: 0.3595 - accuracy: 0.89 - ETA: 0s - loss: 0.3510 - accuracy: 0.90 - ETA: 0s - loss: 0.3602 - accuracy: 0.89 - ETA: 0s - loss: 0.3688 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8955 - val_loss: 0.6312 - val_accuracy: 0.7448\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4309 - accuracy: 0.87 - ETA: 0s - loss: 0.3868 - accuracy: 0.89 - ETA: 0s - loss: 0.3728 - accuracy: 0.89 - ETA: 0s - loss: 0.3812 - accuracy: 0.89 - ETA: 0s - loss: 0.3870 - accuracy: 0.89 - ETA: 0s - loss: 0.3866 - accuracy: 0.89 - ETA: 0s - loss: 0.3740 - accuracy: 0.89 - ETA: 0s - loss: 0.3756 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3762 - accuracy: 0.8977 - val_loss: 1.4730 - val_accuracy: 0.7358\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.87 - ETA: 0s - loss: 0.3789 - accuracy: 0.89 - ETA: 0s - loss: 0.3504 - accuracy: 0.90 - ETA: 0s - loss: 0.3675 - accuracy: 0.89 - ETA: 0s - loss: 0.3741 - accuracy: 0.89 - ETA: 0s - loss: 0.3775 - accuracy: 0.89 - ETA: 0s - loss: 0.3792 - accuracy: 0.89 - ETA: 0s - loss: 0.3823 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3835 - accuracy: 0.8910 - val_loss: 1.1621 - val_accuracy: 0.7403\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/84 [============================>.] - ETA: 0s - loss: 0.4571 - accuracy: 0.84 - ETA: 0s - loss: 0.3972 - accuracy: 0.88 - ETA: 0s - loss: 0.3923 - accuracy: 0.88 - ETA: 0s - loss: 0.3773 - accuracy: 0.89 - ETA: 0s - loss: 0.3816 - accuracy: 0.89 - ETA: 0s - loss: 0.3919 - accuracy: 0.88 - ETA: 0s - loss: 0.4086 - accuracy: 0.88 - ETA: 0s - loss: 0.4105 - accuracy: 0.8822Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4140 - accuracy: 0.8824 - val_loss: 1.4883 - val_accuracy: 0.7522\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f9afa533d51204f9aa52d0fed8a0b7dd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7597015102704366</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.312365988850128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.75 - ETA: 0s - loss: 2.3244 - accuracy: 0.67 - ETA: 0s - loss: 1.4195 - accuracy: 0.70 - ETA: 0s - loss: 1.1417 - accuracy: 0.72 - ETA: 0s - loss: 1.0053 - accuracy: 0.72 - ETA: 0s - loss: 0.9123 - accuracy: 0.72 - 1s 6ms/step - loss: 0.8777 - accuracy: 0.7268 - val_loss: 0.6099 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5561 - accuracy: 0.78 - ETA: 0s - loss: 0.5612 - accuracy: 0.75 - ETA: 0s - loss: 0.5873 - accuracy: 0.73 - ETA: 0s - loss: 0.5752 - accuracy: 0.75 - ETA: 0s - loss: 0.5600 - accuracy: 0.76 - ETA: 0s - loss: 0.5576 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5648 - accuracy: 0.7626 - val_loss: 0.5717 - val_accuracy: 0.7418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6294 - accuracy: 0.75 - ETA: 0s - loss: 0.5294 - accuracy: 0.79 - ETA: 0s - loss: 0.5270 - accuracy: 0.78 - ETA: 0s - loss: 0.5218 - accuracy: 0.79 - ETA: 0s - loss: 0.5262 - accuracy: 0.79 - ETA: 0s - loss: 0.5278 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5325 - accuracy: 0.7872 - val_loss: 0.5806 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.78 - ETA: 0s - loss: 0.4855 - accuracy: 0.82 - ETA: 0s - loss: 0.4889 - accuracy: 0.83 - ETA: 0s - loss: 0.5021 - accuracy: 0.82 - ETA: 0s - loss: 0.5043 - accuracy: 0.82 - ETA: 0s - loss: 0.5147 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5173 - accuracy: 0.8160 - val_loss: 0.6123 - val_accuracy: 0.7209\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.84 - ETA: 0s - loss: 0.5093 - accuracy: 0.80 - ETA: 0s - loss: 0.4924 - accuracy: 0.81 - ETA: 0s - loss: 0.4875 - accuracy: 0.82 - ETA: 0s - loss: 0.4812 - accuracy: 0.83 - ETA: 0s - loss: 0.4912 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4908 - accuracy: 0.8261 - val_loss: 0.7202 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.78 - ETA: 0s - loss: 0.5181 - accuracy: 0.80 - ETA: 0s - loss: 0.4951 - accuracy: 0.82 - ETA: 0s - loss: 0.4847 - accuracy: 0.83 - ETA: 0s - loss: 0.4811 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4842 - accuracy: 0.8309 - val_loss: 0.6986 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.87 - ETA: 0s - loss: 0.4735 - accuracy: 0.84 - ETA: 0s - loss: 0.4626 - accuracy: 0.84 - ETA: 0s - loss: 0.4627 - accuracy: 0.83 - ETA: 0s - loss: 0.4661 - accuracy: 0.84 - ETA: 0s - loss: 0.4696 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4832 - accuracy: 0.8417 - val_loss: 0.8521 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.93 - ETA: 0s - loss: 0.5062 - accuracy: 0.83 - ETA: 0s - loss: 0.4890 - accuracy: 0.84 - ETA: 0s - loss: 0.5220 - accuracy: 0.83 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.5217 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5219 - accuracy: 0.8335 - val_loss: 0.6565 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.87 - ETA: 0s - loss: 0.5349 - accuracy: 0.83 - ETA: 0s - loss: 0.4991 - accuracy: 0.83 - ETA: 0s - loss: 0.5963 - accuracy: 0.83 - ETA: 0s - loss: 0.6047 - accuracy: 0.82 - ETA: 0s - loss: 0.5971 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5902 - accuracy: 0.8186 - val_loss: 0.7712 - val_accuracy: 0.7507\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6626 - accuracy: 0.75 - ETA: 0s - loss: 0.5781 - accuracy: 0.81 - ETA: 0s - loss: 0.5372 - accuracy: 0.82 - ETA: 0s - loss: 0.5383 - accuracy: 0.82 - ETA: 0s - loss: 0.5598 - accuracy: 0.81 - ETA: 0s - loss: 0.5690 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5676 - accuracy: 0.8093 - val_loss: 0.7882 - val_accuracy: 0.7507\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.93 - ETA: 0s - loss: 0.5145 - accuracy: 0.83 - ETA: 0s - loss: 0.5360 - accuracy: 0.81 - ETA: 0s - loss: 0.5308 - accuracy: 0.82 - ETA: 0s - loss: 0.5416 - accuracy: 0.82 - ETA: 0s - loss: 0.5368 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5380 - accuracy: 0.8227 - val_loss: 0.6264 - val_accuracy: 0.7522\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.87 - ETA: 0s - loss: 0.5421 - accuracy: 0.81 - ETA: 0s - loss: 0.5051 - accuracy: 0.83 - ETA: 0s - loss: 0.5053 - accuracy: 0.83 - ETA: 0s - loss: 0.5000 - accuracy: 0.83 - ETA: 0s - loss: 0.4928 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4939 - accuracy: 0.8384 - val_loss: 0.5925 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.81 - ETA: 0s - loss: 0.4742 - accuracy: 0.84 - ETA: 0s - loss: 0.4651 - accuracy: 0.85 - ETA: 0s - loss: 0.4712 - accuracy: 0.84 - ETA: 0s - loss: 0.4725 - accuracy: 0.85 - ETA: 0s - loss: 0.4612 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4671 - accuracy: 0.8559 - val_loss: 0.5715 - val_accuracy: 0.7642\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.78 - ETA: 0s - loss: 0.4276 - accuracy: 0.86 - ETA: 0s - loss: 0.4348 - accuracy: 0.86 - ETA: 0s - loss: 0.4337 - accuracy: 0.86 - ETA: 0s - loss: 0.4479 - accuracy: 0.85 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - ETA: 0s - loss: 0.4580 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8585 - val_loss: 0.8528 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.90 - ETA: 0s - loss: 0.4483 - accuracy: 0.86 - ETA: 0s - loss: 0.4224 - accuracy: 0.86 - ETA: 0s - loss: 0.4251 - accuracy: 0.86 - ETA: 0s - loss: 0.4307 - accuracy: 0.86 - ETA: 0s - loss: 0.4339 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4371 - accuracy: 0.8608 - val_loss: 0.6050 - val_accuracy: 0.7388\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.87 - ETA: 0s - loss: 0.4955 - accuracy: 0.83 - ETA: 0s - loss: 0.4387 - accuracy: 0.86 - ETA: 0s - loss: 0.4232 - accuracy: 0.87 - ETA: 0s - loss: 0.4342 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4404 - accuracy: 0.8682 - val_loss: 0.7604 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.87 - ETA: 0s - loss: 0.4297 - accuracy: 0.85 - ETA: 0s - loss: 0.4735 - accuracy: 0.85 - ETA: 0s - loss: 0.5069 - accuracy: 0.84 - ETA: 0s - loss: 0.5131 - accuracy: 0.83 - ETA: 0s - loss: 0.5138 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5110 - accuracy: 0.8317 - val_loss: 0.8846 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.78 - ETA: 0s - loss: 0.8350 - accuracy: 0.80 - ETA: 0s - loss: 0.6720 - accuracy: 0.82 - ETA: 0s - loss: 0.6328 - accuracy: 0.82 - ETA: 0s - loss: 0.6237 - accuracy: 0.82 - ETA: 0s - loss: 0.6095 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6108 - accuracy: 0.8216 - val_loss: 1.1317 - val_accuracy: 0.7463\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.90 - ETA: 0s - loss: 0.7057 - accuracy: 0.83 - ETA: 0s - loss: 0.6232 - accuracy: 0.82 - ETA: 0s - loss: 0.6018 - accuracy: 0.81 - ETA: 0s - loss: 0.5784 - accuracy: 0.81 - ETA: 0s - loss: 0.5912 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5881 - accuracy: 0.8137 - val_loss: 0.6392 - val_accuracy: 0.7418\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7871 - accuracy: 0.62 - ETA: 0s - loss: 0.5925 - accuracy: 0.78 - ETA: 0s - loss: 0.5750 - accuracy: 0.79 - ETA: 0s - loss: 0.5491 - accuracy: 0.81 - ETA: 0s - loss: 0.5451 - accuracy: 0.81 - ETA: 0s - loss: 0.5499 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5484 - accuracy: 0.8063 - val_loss: 0.7427 - val_accuracy: 0.7522\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.81 - ETA: 0s - loss: 0.5526 - accuracy: 0.79 - ETA: 0s - loss: 0.5347 - accuracy: 0.81 - ETA: 0s - loss: 0.5290 - accuracy: 0.82 - ETA: 0s - loss: 0.5366 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5358 - accuracy: 0.8145 - val_loss: 1.0180 - val_accuracy: 0.7448\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.81 - ETA: 0s - loss: 0.5681 - accuracy: 0.79 - ETA: 0s - loss: 0.5621 - accuracy: 0.79 - ETA: 0s - loss: 0.5428 - accuracy: 0.80 - ETA: 0s - loss: 0.5354 - accuracy: 0.80 - ETA: 0s - loss: 0.5217 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5209 - accuracy: 0.8156 - val_loss: 0.6404 - val_accuracy: 0.7567\n",
      "Epoch 23/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6014 - accuracy: 0.75 - ETA: 0s - loss: 0.4954 - accuracy: 0.82 - ETA: 0s - loss: 0.4871 - accuracy: 0.83 - ETA: 0s - loss: 0.4930 - accuracy: 0.84 - ETA: 0s - loss: 0.5031 - accuracy: 0.83 - ETA: 0s - loss: 0.5028 - accuracy: 0.8349Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5029 - accuracy: 0.8354 - val_loss: 0.7241 - val_accuracy: 0.7343\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7599 - accuracy: 0.62 - ETA: 0s - loss: 2.3900 - accuracy: 0.64 - ETA: 0s - loss: 1.5922 - accuracy: 0.68 - ETA: 0s - loss: 1.2333 - accuracy: 0.70 - ETA: 0s - loss: 1.0730 - accuracy: 0.70 - ETA: 0s - loss: 0.9813 - accuracy: 0.70 - 0s 5ms/step - loss: 0.9126 - accuracy: 0.7044 - val_loss: 0.5971 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.65 - ETA: 0s - loss: 0.5518 - accuracy: 0.74 - ETA: 0s - loss: 0.5639 - accuracy: 0.75 - ETA: 0s - loss: 0.5649 - accuracy: 0.75 - ETA: 0s - loss: 0.5533 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5545 - accuracy: 0.7585 - val_loss: 0.6344 - val_accuracy: 0.6925\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.68 - ETA: 0s - loss: 0.5221 - accuracy: 0.77 - ETA: 0s - loss: 0.5042 - accuracy: 0.79 - ETA: 0s - loss: 0.4973 - accuracy: 0.79 - ETA: 0s - loss: 0.4930 - accuracy: 0.79 - ETA: 0s - loss: 0.5003 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5074 - accuracy: 0.7943 - val_loss: 0.5796 - val_accuracy: 0.7373\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.75 - ETA: 0s - loss: 0.5018 - accuracy: 0.81 - ETA: 0s - loss: 0.5151 - accuracy: 0.81 - ETA: 0s - loss: 0.4970 - accuracy: 0.82 - ETA: 0s - loss: 0.5069 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5004 - accuracy: 0.8197 - val_loss: 0.6781 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.81 - ETA: 0s - loss: 0.4593 - accuracy: 0.83 - ETA: 0s - loss: 0.4586 - accuracy: 0.83 - ETA: 0s - loss: 0.4521 - accuracy: 0.83 - ETA: 0s - loss: 0.4580 - accuracy: 0.83 - ETA: 0s - loss: 0.4688 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4666 - accuracy: 0.8302 - val_loss: 0.6273 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.87 - ETA: 0s - loss: 0.4004 - accuracy: 0.85 - ETA: 0s - loss: 0.4276 - accuracy: 0.84 - ETA: 0s - loss: 0.4377 - accuracy: 0.83 - ETA: 0s - loss: 0.4429 - accuracy: 0.84 - ETA: 0s - loss: 0.4575 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4572 - accuracy: 0.8350 - val_loss: 0.6901 - val_accuracy: 0.6701\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.78 - ETA: 0s - loss: 0.4065 - accuracy: 0.84 - ETA: 0s - loss: 0.4211 - accuracy: 0.85 - ETA: 0s - loss: 0.4402 - accuracy: 0.84 - ETA: 0s - loss: 0.4405 - accuracy: 0.84 - ETA: 0s - loss: 0.4461 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4531 - accuracy: 0.8406 - val_loss: 0.5762 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.84 - ETA: 0s - loss: 0.6812 - accuracy: 0.82 - ETA: 0s - loss: 0.5963 - accuracy: 0.82 - ETA: 0s - loss: 0.5594 - accuracy: 0.82 - ETA: 0s - loss: 0.5467 - accuracy: 0.82 - ETA: 0s - loss: 0.5393 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5366 - accuracy: 0.8197 - val_loss: 0.6030 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4557 - accuracy: 0.81 - ETA: 0s - loss: 0.4248 - accuracy: 0.84 - ETA: 0s - loss: 0.4327 - accuracy: 0.85 - ETA: 0s - loss: 0.4323 - accuracy: 0.85 - ETA: 0s - loss: 0.4472 - accuracy: 0.84 - ETA: 0s - loss: 0.4436 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8492 - val_loss: 0.7422 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4358 - accuracy: 0.84 - ETA: 0s - loss: 0.4506 - accuracy: 0.87 - ETA: 0s - loss: 0.4801 - accuracy: 0.84 - ETA: 0s - loss: 0.4610 - accuracy: 0.85 - ETA: 0s - loss: 0.4596 - accuracy: 0.85 - ETA: 0s - loss: 0.4724 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5110 - accuracy: 0.8462 - val_loss: 1.0760 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.78 - ETA: 0s - loss: 0.4018 - accuracy: 0.88 - ETA: 0s - loss: 0.8427 - accuracy: 0.87 - ETA: 0s - loss: 0.7286 - accuracy: 0.86 - ETA: 0s - loss: 0.6586 - accuracy: 0.86 - ETA: 0s - loss: 0.6299 - accuracy: 0.85 - 0s 4ms/step - loss: 0.6127 - accuracy: 0.8533 - val_loss: 0.7241 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 1.00 - ETA: 0s - loss: 0.3946 - accuracy: 0.87 - ETA: 0s - loss: 0.3691 - accuracy: 0.88 - ETA: 0s - loss: 0.4075 - accuracy: 0.86 - ETA: 0s - loss: 0.4086 - accuracy: 0.86 - ETA: 0s - loss: 0.4094 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4157 - accuracy: 0.8604 - val_loss: 0.6418 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.90 - ETA: 0s - loss: 0.6124 - accuracy: 0.87 - ETA: 0s - loss: 0.7423 - accuracy: 0.86 - ETA: 0s - loss: 0.7415 - accuracy: 0.84 - ETA: 0s - loss: 0.6677 - accuracy: 0.84 - 0s 3ms/step - loss: 0.6363 - accuracy: 0.8451 - val_loss: 1.0422 - val_accuracy: 0.7254\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.93 - ETA: 0s - loss: 0.4356 - accuracy: 0.85 - ETA: 0s - loss: 0.4219 - accuracy: 0.86 - ETA: 0s - loss: 0.4345 - accuracy: 0.86 - ETA: 0s - loss: 0.4277 - accuracy: 0.86 - ETA: 0s - loss: 0.4261 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4298 - accuracy: 0.8667 - val_loss: 0.7539 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.96 - ETA: 0s - loss: 0.3207 - accuracy: 0.90 - ETA: 0s - loss: 0.3606 - accuracy: 0.89 - ETA: 0s - loss: 0.4865 - accuracy: 0.87 - ETA: 0s - loss: 0.4871 - accuracy: 0.86 - ETA: 0s - loss: 0.4867 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4849 - accuracy: 0.8645 - val_loss: 0.7980 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.75 - ETA: 0s - loss: 0.4379 - accuracy: 0.85 - ETA: 0s - loss: 0.4289 - accuracy: 0.86 - ETA: 0s - loss: 0.4331 - accuracy: 0.85 - ETA: 0s - loss: 0.4298 - accuracy: 0.86 - ETA: 0s - loss: 0.4321 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4332 - accuracy: 0.8619 - val_loss: 0.6872 - val_accuracy: 0.7582\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.81 - ETA: 0s - loss: 0.4266 - accuracy: 0.86 - ETA: 0s - loss: 0.4252 - accuracy: 0.87 - ETA: 0s - loss: 0.4518 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4459 - accuracy: 0.8675 - val_loss: 0.7543 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 1.00 - ETA: 0s - loss: 0.3918 - accuracy: 0.88 - ETA: 0s - loss: 0.3939 - accuracy: 0.88 - ETA: 0s - loss: 0.3946 - accuracy: 0.88 - ETA: 0s - loss: 0.4360 - accuracy: 0.87 - ETA: 0s - loss: 0.4592 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4683 - accuracy: 0.8652 - val_loss: 0.9484 - val_accuracy: 0.7478\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.90 - ETA: 0s - loss: 0.5236 - accuracy: 0.84 - ETA: 0s - loss: 0.5052 - accuracy: 0.84 - ETA: 0s - loss: 0.4784 - accuracy: 0.85 - ETA: 0s - loss: 0.4694 - accuracy: 0.85 - ETA: 0s - loss: 0.4617 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4589 - accuracy: 0.8593 - val_loss: 0.7563 - val_accuracy: 0.7373\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.90 - ETA: 0s - loss: 0.4207 - accuracy: 0.88 - ETA: 0s - loss: 0.4159 - accuracy: 0.87 - ETA: 0s - loss: 0.3994 - accuracy: 0.87 - ETA: 0s - loss: 0.4077 - accuracy: 0.87 - ETA: 0s - loss: 0.4133 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4144 - accuracy: 0.8776 - val_loss: 0.6428 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3815 - accuracy: 0.93 - ETA: 0s - loss: 0.4776 - accuracy: 0.86 - ETA: 0s - loss: 0.4648 - accuracy: 0.87 - ETA: 0s - loss: 0.4708 - accuracy: 0.86 - ETA: 0s - loss: 0.4597 - accuracy: 0.86 - ETA: 0s - loss: 0.4381 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8731 - val_loss: 1.0699 - val_accuracy: 0.7164\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.81 - ETA: 0s - loss: 0.4331 - accuracy: 0.86 - ETA: 0s - loss: 0.4224 - accuracy: 0.87 - ETA: 0s - loss: 0.4001 - accuracy: 0.87 - ETA: 0s - loss: 0.4246 - accuracy: 0.87 - ETA: 0s - loss: 0.4206 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8772 - val_loss: 0.7463 - val_accuracy: 0.7328\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3215 - accuracy: 0.90 - ETA: 0s - loss: 0.3944 - accuracy: 0.89 - ETA: 0s - loss: 0.4035 - accuracy: 0.88 - ETA: 0s - loss: 0.3980 - accuracy: 0.88 - ETA: 0s - loss: 0.3860 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8880 - val_loss: 0.8479 - val_accuracy: 0.7493\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.90 - ETA: 0s - loss: 0.3850 - accuracy: 0.88 - ETA: 0s - loss: 0.5129 - accuracy: 0.88 - ETA: 0s - loss: 0.5767 - accuracy: 0.86 - ETA: 0s - loss: 0.5542 - accuracy: 0.86 - ETA: 0s - loss: 0.5256 - accuracy: 0.86 - 0s 4ms/step - loss: 0.5156 - accuracy: 0.8686 - val_loss: 0.9414 - val_accuracy: 0.7403\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3706 - accuracy: 0.90 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - ETA: 0s - loss: 0.4469 - accuracy: 0.86 - ETA: 0s - loss: 0.4995 - accuracy: 0.85 - ETA: 0s - loss: 0.5044 - accuracy: 0.85 - ETA: 0s - loss: 0.5069 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5069 - accuracy: 0.8485 - val_loss: 0.6618 - val_accuracy: 0.7522\n",
      "Epoch 26/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5544 - accuracy: 0.75 - ETA: 0s - loss: 0.4918 - accuracy: 0.84 - ETA: 0s - loss: 0.4946 - accuracy: 0.84 - ETA: 0s - loss: 0.4986 - accuracy: 0.83 - ETA: 0s - loss: 0.4883 - accuracy: 0.84 - ETA: 0s - loss: 0.4785 - accuracy: 0.8489Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4781 - accuracy: 0.8503 - val_loss: 1.1100 - val_accuracy: 0.7373\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.81 - ETA: 0s - loss: 1.8317 - accuracy: 0.68 - ETA: 0s - loss: 1.2550 - accuracy: 0.69 - ETA: 0s - loss: 1.0339 - accuracy: 0.71 - ETA: 0s - loss: 0.9137 - accuracy: 0.71 - ETA: 0s - loss: 0.8445 - accuracy: 0.71 - 0s 5ms/step - loss: 0.8390 - accuracy: 0.7178 - val_loss: 0.5697 - val_accuracy: 0.7299\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.96 - ETA: 0s - loss: 0.5397 - accuracy: 0.75 - ETA: 0s - loss: 0.5500 - accuracy: 0.73 - ETA: 0s - loss: 0.5503 - accuracy: 0.73 - ETA: 0s - loss: 0.5383 - accuracy: 0.75 - ETA: 0s - loss: 0.5376 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5419 - accuracy: 0.7563 - val_loss: 0.6026 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.84 - ETA: 0s - loss: 0.4698 - accuracy: 0.82 - ETA: 0s - loss: 0.4919 - accuracy: 0.80 - ETA: 0s - loss: 0.5024 - accuracy: 0.79 - ETA: 0s - loss: 0.4931 - accuracy: 0.80 - ETA: 0s - loss: 0.4964 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4992 - accuracy: 0.8022 - val_loss: 0.5445 - val_accuracy: 0.7507\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.87 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - ETA: 0s - loss: 0.4702 - accuracy: 0.81 - ETA: 0s - loss: 0.4667 - accuracy: 0.81 - ETA: 0s - loss: 0.4747 - accuracy: 0.81 - ETA: 0s - loss: 0.4864 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4891 - accuracy: 0.8130 - val_loss: 0.6122 - val_accuracy: 0.7179\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.68 - ETA: 0s - loss: 0.4157 - accuracy: 0.84 - ETA: 0s - loss: 0.4322 - accuracy: 0.83 - ETA: 0s - loss: 0.4466 - accuracy: 0.83 - ETA: 0s - loss: 0.4536 - accuracy: 0.83 - ETA: 0s - loss: 0.4588 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4677 - accuracy: 0.8272 - val_loss: 0.8351 - val_accuracy: 0.7164\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6575 - accuracy: 0.75 - ETA: 0s - loss: 0.4601 - accuracy: 0.83 - ETA: 0s - loss: 0.4472 - accuracy: 0.84 - ETA: 0s - loss: 0.4419 - accuracy: 0.84 - ETA: 0s - loss: 0.4320 - accuracy: 0.84 - ETA: 0s - loss: 0.4369 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4370 - accuracy: 0.8458 - val_loss: 0.6190 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.84 - ETA: 0s - loss: 0.4101 - accuracy: 0.85 - ETA: 0s - loss: 0.4294 - accuracy: 0.85 - ETA: 0s - loss: 0.4502 - accuracy: 0.84 - ETA: 0s - loss: 0.4575 - accuracy: 0.84 - ETA: 0s - loss: 0.4501 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4516 - accuracy: 0.8425 - val_loss: 0.6256 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.84 - ETA: 0s - loss: 0.3944 - accuracy: 0.87 - ETA: 0s - loss: 0.3804 - accuracy: 0.87 - ETA: 0s - loss: 0.3825 - accuracy: 0.87 - ETA: 0s - loss: 0.3964 - accuracy: 0.87 - ETA: 0s - loss: 0.4091 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4139 - accuracy: 0.8585 - val_loss: 0.5785 - val_accuracy: 0.7418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.90 - ETA: 0s - loss: 0.3832 - accuracy: 0.89 - ETA: 0s - loss: 0.4171 - accuracy: 0.87 - ETA: 0s - loss: 0.4045 - accuracy: 0.87 - ETA: 0s - loss: 0.4050 - accuracy: 0.87 - ETA: 0s - loss: 0.4086 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4097 - accuracy: 0.8652 - val_loss: 0.6153 - val_accuracy: 0.7343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.87 - ETA: 0s - loss: 0.3415 - accuracy: 0.88 - ETA: 0s - loss: 0.3678 - accuracy: 0.88 - ETA: 0s - loss: 0.3767 - accuracy: 0.87 - ETA: 0s - loss: 0.3855 - accuracy: 0.87 - ETA: 0s - loss: 0.3919 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8757 - val_loss: 0.7465 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.93 - ETA: 0s - loss: 0.4117 - accuracy: 0.89 - ETA: 0s - loss: 0.3978 - accuracy: 0.88 - ETA: 0s - loss: 0.4179 - accuracy: 0.87 - ETA: 0s - loss: 0.3963 - accuracy: 0.87 - ETA: 0s - loss: 0.3967 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3908 - accuracy: 0.8813 - val_loss: 0.7548 - val_accuracy: 0.7448\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.84 - ETA: 0s - loss: 0.3436 - accuracy: 0.90 - ETA: 0s - loss: 0.3564 - accuracy: 0.89 - ETA: 0s - loss: 0.3739 - accuracy: 0.88 - ETA: 0s - loss: 0.3766 - accuracy: 0.88 - ETA: 0s - loss: 0.3716 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3719 - accuracy: 0.8891 - val_loss: 0.7883 - val_accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.87 - ETA: 0s - loss: 0.3702 - accuracy: 0.89 - ETA: 0s - loss: 0.3648 - accuracy: 0.89 - ETA: 0s - loss: 0.4817 - accuracy: 0.89 - ETA: 0s - loss: 0.4604 - accuracy: 0.89 - ETA: 0s - loss: 0.4602 - accuracy: 0.8873Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4602 - accuracy: 0.8873 - val_loss: 0.6508 - val_accuracy: 0.7507\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 20f91d88cbc44bfd43badc8697caff4c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7577114303906759</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.19136057491531744</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 65</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.75 - ETA: 0s - loss: 1.7737 - accuracy: 0.62 - ETA: 0s - loss: 1.3521 - accuracy: 0.63 - ETA: 0s - loss: 1.1800 - accuracy: 0.64 - ETA: 0s - loss: 1.0408 - accuracy: 0.66 - ETA: 0s - loss: 0.9706 - accuracy: 0.66 - 0s 6ms/step - loss: 0.9324 - accuracy: 0.6760 - val_loss: 0.5703 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7356 - accuracy: 0.71 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6378 - accuracy: 0.67 - ETA: 0s - loss: 0.5935 - accuracy: 0.70 - ETA: 0s - loss: 0.5896 - accuracy: 0.70 - ETA: 0s - loss: 0.5821 - accuracy: 0.71 - ETA: 0s - loss: 0.5785 - accuracy: 0.72 - 0s 5ms/step - loss: 0.5741 - accuracy: 0.7346 - val_loss: 0.5534 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.78 - ETA: 0s - loss: 0.5236 - accuracy: 0.81 - ETA: 0s - loss: 0.5089 - accuracy: 0.80 - ETA: 0s - loss: 0.4908 - accuracy: 0.80 - ETA: 0s - loss: 0.4897 - accuracy: 0.80 - ETA: 0s - loss: 0.4976 - accuracy: 0.79 - ETA: 0s - loss: 0.5015 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5058 - accuracy: 0.7936 - val_loss: 0.5742 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.71 - ETA: 0s - loss: 0.5101 - accuracy: 0.79 - ETA: 0s - loss: 0.5051 - accuracy: 0.79 - ETA: 0s - loss: 0.5092 - accuracy: 0.79 - ETA: 0s - loss: 0.4992 - accuracy: 0.80 - ETA: 0s - loss: 0.5066 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5060 - accuracy: 0.7984 - val_loss: 0.5487 - val_accuracy: 0.7463\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.71 - ETA: 0s - loss: 0.5484 - accuracy: 0.78 - ETA: 0s - loss: 0.6692 - accuracy: 0.77 - ETA: 0s - loss: 0.7410 - accuracy: 0.79 - ETA: 0s - loss: 0.7301 - accuracy: 0.79 - ETA: 0s - loss: 0.7047 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6838 - accuracy: 0.7936 - val_loss: 0.6056 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6417 - accuracy: 0.75 - ETA: 0s - loss: 0.4923 - accuracy: 0.80 - ETA: 0s - loss: 0.4814 - accuracy: 0.81 - ETA: 0s - loss: 0.5011 - accuracy: 0.81 - ETA: 0s - loss: 0.5055 - accuracy: 0.80 - ETA: 0s - loss: 0.5054 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5136 - accuracy: 0.8025 - val_loss: 0.5882 - val_accuracy: 0.7299\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.81 - ETA: 0s - loss: 0.9053 - accuracy: 0.80 - ETA: 0s - loss: 0.7355 - accuracy: 0.81 - ETA: 0s - loss: 0.6730 - accuracy: 0.80 - ETA: 0s - loss: 0.6275 - accuracy: 0.81 - ETA: 0s - loss: 0.5980 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5860 - accuracy: 0.8175 - val_loss: 0.6238 - val_accuracy: 0.7000\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3768 - accuracy: 0.84 - ETA: 0s - loss: 0.5312 - accuracy: 0.80 - ETA: 0s - loss: 0.5136 - accuracy: 0.81 - ETA: 0s - loss: 0.4924 - accuracy: 0.81 - ETA: 0s - loss: 0.4698 - accuracy: 0.82 - ETA: 0s - loss: 0.4654 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4622 - accuracy: 0.8287 - val_loss: 0.6670 - val_accuracy: 0.7104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.81 - ETA: 0s - loss: 0.4516 - accuracy: 0.84 - ETA: 0s - loss: 0.4272 - accuracy: 0.86 - ETA: 0s - loss: 0.4679 - accuracy: 0.84 - ETA: 0s - loss: 0.4764 - accuracy: 0.84 - ETA: 0s - loss: 0.4767 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4674 - accuracy: 0.8443 - val_loss: 0.7372 - val_accuracy: 0.7269\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5788 - accuracy: 0.81 - ETA: 0s - loss: 0.4599 - accuracy: 0.84 - ETA: 0s - loss: 0.4346 - accuracy: 0.85 - ETA: 0s - loss: 0.4615 - accuracy: 0.85 - ETA: 0s - loss: 0.4624 - accuracy: 0.85 - ETA: 0s - loss: 0.4698 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4617 - accuracy: 0.8567 - val_loss: 0.9636 - val_accuracy: 0.7269\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.90 - ETA: 0s - loss: 0.3744 - accuracy: 0.88 - ETA: 0s - loss: 0.4058 - accuracy: 0.86 - ETA: 0s - loss: 0.4104 - accuracy: 0.87 - ETA: 0s - loss: 0.3982 - accuracy: 0.87 - ETA: 0s - loss: 0.4054 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3979 - accuracy: 0.8686 - val_loss: 0.8785 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 1.00 - ETA: 0s - loss: 0.4304 - accuracy: 0.86 - ETA: 0s - loss: 0.3841 - accuracy: 0.88 - ETA: 0s - loss: 0.4059 - accuracy: 0.87 - ETA: 0s - loss: 0.4126 - accuracy: 0.86 - ETA: 0s - loss: 0.4121 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4203 - accuracy: 0.8686 - val_loss: 0.7837 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.87 - ETA: 0s - loss: 0.4007 - accuracy: 0.87 - ETA: 0s - loss: 0.4303 - accuracy: 0.87 - ETA: 0s - loss: 0.4417 - accuracy: 0.86 - ETA: 0s - loss: 0.4391 - accuracy: 0.86 - ETA: 0s - loss: 0.4389 - accuracy: 0.86 - ETA: 0s - loss: 0.4363 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4363 - accuracy: 0.8574 - val_loss: 0.7258 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.2996 - accuracy: 0.84 - ETA: 0s - loss: 0.3282 - accuracy: 0.88 - ETA: 0s - loss: 0.3214 - accuracy: 0.89 - ETA: 0s - loss: 0.3546 - accuracy: 0.88 - ETA: 0s - loss: 0.4034 - accuracy: 0.87 - ETA: 0s - loss: 0.4072 - accuracy: 0.8683Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8626 - val_loss: 0.5804 - val_accuracy: 0.7194\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2540 - accuracy: 0.62 - ETA: 0s - loss: 1.5718 - accuracy: 0.56 - ETA: 0s - loss: 1.2176 - accuracy: 0.60 - ETA: 0s - loss: 1.0706 - accuracy: 0.63 - ETA: 0s - loss: 0.9792 - accuracy: 0.64 - ETA: 0s - loss: 0.9175 - accuracy: 0.65 - ETA: 0s - loss: 0.8761 - accuracy: 0.66 - 0s 5ms/step - loss: 0.8731 - accuracy: 0.6693 - val_loss: 0.6015 - val_accuracy: 0.6985\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.62 - ETA: 0s - loss: 0.5602 - accuracy: 0.71 - ETA: 0s - loss: 0.5594 - accuracy: 0.73 - ETA: 0s - loss: 0.5652 - accuracy: 0.73 - ETA: 0s - loss: 0.5635 - accuracy: 0.73 - ETA: 0s - loss: 0.5642 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5649 - accuracy: 0.7451 - val_loss: 0.5534 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.71 - ETA: 0s - loss: 0.4506 - accuracy: 0.81 - ETA: 0s - loss: 0.4805 - accuracy: 0.80 - ETA: 0s - loss: 0.5053 - accuracy: 0.78 - ETA: 0s - loss: 0.5074 - accuracy: 0.79 - ETA: 0s - loss: 0.5141 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5227 - accuracy: 0.7846 - val_loss: 0.5695 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.87 - ETA: 0s - loss: 0.4614 - accuracy: 0.78 - ETA: 0s - loss: 0.4884 - accuracy: 0.80 - ETA: 0s - loss: 0.4929 - accuracy: 0.80 - ETA: 0s - loss: 0.5001 - accuracy: 0.80 - ETA: 0s - loss: 0.5122 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5250 - accuracy: 0.8014 - val_loss: 0.6129 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.87 - ETA: 0s - loss: 0.5163 - accuracy: 0.80 - ETA: 0s - loss: 0.4845 - accuracy: 0.80 - ETA: 0s - loss: 0.5120 - accuracy: 0.80 - ETA: 0s - loss: 0.5666 - accuracy: 0.80 - ETA: 0s - loss: 0.5684 - accuracy: 0.79 - ETA: 0s - loss: 0.5617 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5617 - accuracy: 0.8003 - val_loss: 0.6091 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4862 - accuracy: 0.87 - ETA: 0s - loss: 0.5085 - accuracy: 0.80 - ETA: 0s - loss: 0.4895 - accuracy: 0.81 - ETA: 0s - loss: 0.4865 - accuracy: 0.82 - ETA: 0s - loss: 0.4899 - accuracy: 0.82 - ETA: 0s - loss: 0.4941 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4980 - accuracy: 0.8231 - val_loss: 0.5585 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.90 - ETA: 0s - loss: 0.4859 - accuracy: 0.84 - ETA: 0s - loss: 0.4846 - accuracy: 0.84 - ETA: 0s - loss: 0.4938 - accuracy: 0.83 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5171 - accuracy: 0.83 - ETA: 0s - loss: 0.5456 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5588 - accuracy: 0.8242 - val_loss: 0.6465 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7131 - accuracy: 0.71 - ETA: 0s - loss: 0.6155 - accuracy: 0.77 - ETA: 0s - loss: 0.5751 - accuracy: 0.79 - ETA: 0s - loss: 0.5586 - accuracy: 0.79 - ETA: 0s - loss: 0.5491 - accuracy: 0.79 - ETA: 0s - loss: 0.6013 - accuracy: 0.81 - ETA: 0s - loss: 0.6015 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5982 - accuracy: 0.8018 - val_loss: 0.5975 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4653 - accuracy: 0.82 - ETA: 0s - loss: 0.4883 - accuracy: 0.82 - ETA: 0s - loss: 0.4822 - accuracy: 0.82 - ETA: 0s - loss: 0.4821 - accuracy: 0.82 - ETA: 0s - loss: 0.4829 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5170 - accuracy: 0.8268 - val_loss: 0.7908 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.87 - ETA: 0s - loss: 0.4529 - accuracy: 0.84 - ETA: 0s - loss: 0.4494 - accuracy: 0.84 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - ETA: 0s - loss: 0.4480 - accuracy: 0.85 - ETA: 0s - loss: 0.4564 - accuracy: 0.84 - 1s 6ms/step - loss: 0.4496 - accuracy: 0.8507 - val_loss: 0.7093 - val_accuracy: 0.7448\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5861 - accuracy: 0.81 - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4320 - accuracy: 0.85 - ETA: 0s - loss: 0.4408 - accuracy: 0.85 - ETA: 0s - loss: 0.4376 - accuracy: 0.85 - ETA: 0s - loss: 0.4270 - accuracy: 0.86 - ETA: 0s - loss: 0.4254 - accuracy: 0.86 - ETA: 0s - loss: 0.4204 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4204 - accuracy: 0.8667 - val_loss: 0.7079 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.96 - ETA: 0s - loss: 0.4106 - accuracy: 0.87 - ETA: 0s - loss: 0.3888 - accuracy: 0.87 - ETA: 0s - loss: 0.4082 - accuracy: 0.87 - ETA: 0s - loss: 0.4253 - accuracy: 0.87 - ETA: 0s - loss: 0.4262 - accuracy: 0.87 - ETA: 0s - loss: 0.4314 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4295 - accuracy: 0.8705 - val_loss: 0.8425 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.96 - ETA: 0s - loss: 0.3896 - accuracy: 0.88 - ETA: 0s - loss: 0.4135 - accuracy: 0.87 - ETA: 0s - loss: 0.4103 - accuracy: 0.87 - ETA: 0s - loss: 0.4043 - accuracy: 0.87 - ETA: 0s - loss: 0.4041 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4016 - accuracy: 0.8746 - val_loss: 0.7387 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.93 - ETA: 0s - loss: 0.3672 - accuracy: 0.89 - ETA: 0s - loss: 0.3720 - accuracy: 0.89 - ETA: 0s - loss: 0.3896 - accuracy: 0.88 - ETA: 0s - loss: 0.3775 - accuracy: 0.89 - ETA: 0s - loss: 0.4859 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4724 - accuracy: 0.8850 - val_loss: 0.6282 - val_accuracy: 0.7448\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.90 - ETA: 0s - loss: 0.3445 - accuracy: 0.89 - ETA: 0s - loss: 0.3560 - accuracy: 0.89 - ETA: 0s - loss: 0.3708 - accuracy: 0.88 - ETA: 0s - loss: 0.3861 - accuracy: 0.88 - ETA: 0s - loss: 0.3918 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3903 - accuracy: 0.8835 - val_loss: 0.9761 - val_accuracy: 0.7313\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.93 - ETA: 0s - loss: 0.4053 - accuracy: 0.87 - ETA: 0s - loss: 0.3950 - accuracy: 0.88 - ETA: 0s - loss: 0.4140 - accuracy: 0.87 - ETA: 0s - loss: 0.4221 - accuracy: 0.87 - ETA: 0s - loss: 0.4047 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4064 - accuracy: 0.8783 - val_loss: 0.6973 - val_accuracy: 0.7313\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.81 - ETA: 0s - loss: 0.4398 - accuracy: 0.86 - ETA: 0s - loss: 0.4478 - accuracy: 0.85 - ETA: 0s - loss: 0.4419 - accuracy: 0.86 - ETA: 0s - loss: 0.4321 - accuracy: 0.86 - ETA: 0s - loss: 0.4276 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4299 - accuracy: 0.8705 - val_loss: 1.1009 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.93 - ETA: 0s - loss: 0.3081 - accuracy: 0.91 - ETA: 0s - loss: 0.3414 - accuracy: 0.90 - ETA: 0s - loss: 0.3609 - accuracy: 0.89 - ETA: 0s - loss: 0.4047 - accuracy: 0.89 - ETA: 0s - loss: 0.4047 - accuracy: 0.89 - 0s 4ms/step - loss: 0.4863 - accuracy: 0.8802 - val_loss: 0.6456 - val_accuracy: 0.7194\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.90 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.4540 - accuracy: 0.86 - ETA: 0s - loss: 0.4543 - accuracy: 0.87 - ETA: 0s - loss: 0.4607 - accuracy: 0.86 - ETA: 0s - loss: 0.4539 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4609 - accuracy: 0.8641 - val_loss: 0.6804 - val_accuracy: 0.7418\n",
      "Epoch 20/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.3488 - accuracy: 0.93 - ETA: 0s - loss: 0.3994 - accuracy: 0.88 - ETA: 0s - loss: 0.4168 - accuracy: 0.87 - ETA: 0s - loss: 0.4277 - accuracy: 0.87 - ETA: 0s - loss: 0.4136 - accuracy: 0.87 - ETA: 0s - loss: 0.4180 - accuracy: 0.8772Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4150 - accuracy: 0.8791 - val_loss: 1.1923 - val_accuracy: 0.7299\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.50 - ETA: 0s - loss: 1.9322 - accuracy: 0.58 - ETA: 0s - loss: 1.3540 - accuracy: 0.61 - ETA: 0s - loss: 1.1815 - accuracy: 0.62 - ETA: 0s - loss: 1.0511 - accuracy: 0.63 - ETA: 0s - loss: 0.9618 - accuracy: 0.65 - ETA: 0s - loss: 0.8940 - accuracy: 0.66 - 0s 6ms/step - loss: 0.8855 - accuracy: 0.6708 - val_loss: 0.5853 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.90 - ETA: 0s - loss: 0.5228 - accuracy: 0.80 - ETA: 0s - loss: 0.5327 - accuracy: 0.78 - ETA: 0s - loss: 0.5304 - accuracy: 0.78 - ETA: 0s - loss: 0.5456 - accuracy: 0.77 - ETA: 0s - loss: 0.5420 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5455 - accuracy: 0.7757 - val_loss: 0.5826 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.75 - ETA: 0s - loss: 0.4377 - accuracy: 0.82 - ETA: 0s - loss: 0.4446 - accuracy: 0.80 - ETA: 0s - loss: 0.4691 - accuracy: 0.79 - ETA: 0s - loss: 0.4677 - accuracy: 0.80 - ETA: 0s - loss: 0.4737 - accuracy: 0.80 - ETA: 0s - loss: 0.4731 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4863 - accuracy: 0.7996 - val_loss: 0.5923 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.90 - ETA: 0s - loss: 0.4594 - accuracy: 0.82 - ETA: 0s - loss: 0.4759 - accuracy: 0.82 - ETA: 0s - loss: 0.4657 - accuracy: 0.82 - ETA: 0s - loss: 0.4665 - accuracy: 0.82 - ETA: 0s - loss: 0.4755 - accuracy: 0.81 - ETA: 0s - loss: 0.4838 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4817 - accuracy: 0.8193 - val_loss: 0.8071 - val_accuracy: 0.6821\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.93 - ETA: 0s - loss: 0.4338 - accuracy: 0.81 - ETA: 0s - loss: 0.4718 - accuracy: 0.82 - ETA: 0s - loss: 0.4718 - accuracy: 0.82 - ETA: 0s - loss: 0.4813 - accuracy: 0.81 - ETA: 0s - loss: 0.4886 - accuracy: 0.81 - ETA: 0s - loss: 0.4796 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4814 - accuracy: 0.8193 - val_loss: 0.9350 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.87 - ETA: 0s - loss: 0.3995 - accuracy: 0.85 - ETA: 0s - loss: 0.4194 - accuracy: 0.84 - ETA: 0s - loss: 0.4486 - accuracy: 0.82 - ETA: 0s - loss: 0.4540 - accuracy: 0.82 - ETA: 0s - loss: 0.4540 - accuracy: 0.82 - ETA: 0s - loss: 0.4882 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4882 - accuracy: 0.8186 - val_loss: 0.5308 - val_accuracy: 0.7448\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.78 - ETA: 0s - loss: 0.4881 - accuracy: 0.84 - ETA: 0s - loss: 0.4585 - accuracy: 0.85 - ETA: 0s - loss: 0.4842 - accuracy: 0.84 - ETA: 0s - loss: 0.4882 - accuracy: 0.84 - ETA: 0s - loss: 0.4922 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4906 - accuracy: 0.8402 - val_loss: 0.6013 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.90 - ETA: 0s - loss: 0.4402 - accuracy: 0.83 - ETA: 0s - loss: 0.4854 - accuracy: 0.82 - ETA: 0s - loss: 0.4837 - accuracy: 0.82 - ETA: 0s - loss: 0.4711 - accuracy: 0.83 - ETA: 0s - loss: 0.4593 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4653 - accuracy: 0.8406 - val_loss: 0.9006 - val_accuracy: 0.7015\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.90 - ETA: 0s - loss: 0.4738 - accuracy: 0.83 - ETA: 0s - loss: 0.4530 - accuracy: 0.84 - ETA: 0s - loss: 0.4663 - accuracy: 0.83 - ETA: 0s - loss: 0.4469 - accuracy: 0.85 - ETA: 0s - loss: 0.4451 - accuracy: 0.85 - ETA: 0s - loss: 0.4388 - accuracy: 0.85 - ETA: 0s - loss: 0.4353 - accuracy: 0.85 - ETA: 0s - loss: 0.4339 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4339 - accuracy: 0.8582 - val_loss: 0.6521 - val_accuracy: 0.6910\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.78 - ETA: 0s - loss: 0.4914 - accuracy: 0.82 - ETA: 0s - loss: 0.5342 - accuracy: 0.81 - ETA: 0s - loss: 0.5440 - accuracy: 0.81 - ETA: 0s - loss: 0.5385 - accuracy: 0.82 - ETA: 0s - loss: 0.5219 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5127 - accuracy: 0.8328 - val_loss: 0.6073 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3592 - accuracy: 0.93 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4787 - accuracy: 0.84 - ETA: 0s - loss: 0.4564 - accuracy: 0.86 - ETA: 0s - loss: 0.4500 - accuracy: 0.85 - ETA: 0s - loss: 0.4494 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4380 - accuracy: 0.8634 - val_loss: 0.6249 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.84 - ETA: 0s - loss: 0.3805 - accuracy: 0.88 - ETA: 0s - loss: 0.3821 - accuracy: 0.88 - ETA: 0s - loss: 0.4023 - accuracy: 0.88 - ETA: 0s - loss: 0.3949 - accuracy: 0.88 - ETA: 0s - loss: 0.4255 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4247 - accuracy: 0.8664 - val_loss: 0.6294 - val_accuracy: 0.7179\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.87 - ETA: 0s - loss: 0.5658 - accuracy: 0.77 - ETA: 0s - loss: 0.5799 - accuracy: 0.77 - ETA: 0s - loss: 0.5503 - accuracy: 0.79 - ETA: 0s - loss: 0.5315 - accuracy: 0.80 - ETA: 0s - loss: 0.5116 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5124 - accuracy: 0.8178 - val_loss: 0.8148 - val_accuracy: 0.7239\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.87 - ETA: 0s - loss: 0.4168 - accuracy: 0.86 - ETA: 0s - loss: 0.4576 - accuracy: 0.86 - ETA: 0s - loss: 0.6025 - accuracy: 0.85 - ETA: 0s - loss: 0.5750 - accuracy: 0.85 - ETA: 0s - loss: 0.5595 - accuracy: 0.84 - ETA: 0s - loss: 0.5728 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5723 - accuracy: 0.8455 - val_loss: 0.5643 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.96 - ETA: 0s - loss: 0.4234 - accuracy: 0.87 - ETA: 0s - loss: 0.4373 - accuracy: 0.86 - ETA: 0s - loss: 0.4421 - accuracy: 0.86 - ETA: 0s - loss: 0.4229 - accuracy: 0.86 - ETA: 0s - loss: 0.4181 - accuracy: 0.86 - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8671 - val_loss: 0.6231 - val_accuracy: 0.7358\n",
      "Epoch 16/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.4590 - accuracy: 0.87 - ETA: 0s - loss: 0.4180 - accuracy: 0.86 - ETA: 0s - loss: 0.4499 - accuracy: 0.85 - ETA: 0s - loss: 0.4527 - accuracy: 0.85 - ETA: 0s - loss: 0.4348 - accuracy: 0.85 - ETA: 0s - loss: 0.4314 - accuracy: 0.8577Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.8567 - val_loss: 0.6434 - val_accuracy: 0.6940\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a1e354b452083dd83ca01dc1923f9fec</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7452736298243204</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5426282487996653</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 195</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8466 - accuracy: 0.46 - ETA: 0s - loss: 1.5604 - accuracy: 0.64 - ETA: 0s - loss: 1.1189 - accuracy: 0.68 - ETA: 0s - loss: 0.9755 - accuracy: 0.69 - ETA: 0s - loss: 0.8782 - accuracy: 0.71 - ETA: 0s - loss: 0.8305 - accuracy: 0.71 - ETA: 0s - loss: 0.7931 - accuracy: 0.71 - 1s 6ms/step - loss: 0.7845 - accuracy: 0.7148 - val_loss: 0.5965 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.71 - ETA: 0s - loss: 0.5399 - accuracy: 0.77 - ETA: 0s - loss: 0.5620 - accuracy: 0.76 - ETA: 0s - loss: 0.5619 - accuracy: 0.76 - ETA: 0s - loss: 0.5602 - accuracy: 0.76 - ETA: 0s - loss: 0.5568 - accuracy: 0.76 - ETA: 0s - loss: 0.5607 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5610 - accuracy: 0.7641 - val_loss: 0.5895 - val_accuracy: 0.7269\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.81 - ETA: 0s - loss: 0.5765 - accuracy: 0.76 - ETA: 0s - loss: 0.6139 - accuracy: 0.75 - ETA: 0s - loss: 0.5821 - accuracy: 0.77 - ETA: 0s - loss: 0.5688 - accuracy: 0.77 - ETA: 0s - loss: 0.5613 - accuracy: 0.78 - ETA: 0s - loss: 0.5531 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5531 - accuracy: 0.7876 - val_loss: 0.6309 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.81 - ETA: 0s - loss: 0.5077 - accuracy: 0.80 - ETA: 0s - loss: 0.4997 - accuracy: 0.80 - ETA: 0s - loss: 0.5073 - accuracy: 0.80 - ETA: 0s - loss: 0.5125 - accuracy: 0.80 - ETA: 0s - loss: 0.5075 - accuracy: 0.80 - ETA: 0s - loss: 0.5077 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5074 - accuracy: 0.8078 - val_loss: 0.5785 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.84 - ETA: 0s - loss: 0.4888 - accuracy: 0.83 - ETA: 0s - loss: 0.5469 - accuracy: 0.82 - ETA: 0s - loss: 0.5429 - accuracy: 0.80 - ETA: 0s - loss: 0.5424 - accuracy: 0.80 - ETA: 0s - loss: 0.5404 - accuracy: 0.79 - ETA: 0s - loss: 0.5370 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5363 - accuracy: 0.8022 - val_loss: 0.5818 - val_accuracy: 0.7567\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.78 - ETA: 0s - loss: 0.4714 - accuracy: 0.84 - ETA: 0s - loss: 0.4639 - accuracy: 0.84 - ETA: 0s - loss: 0.4846 - accuracy: 0.83 - ETA: 0s - loss: 0.5020 - accuracy: 0.82 - ETA: 0s - loss: 0.5258 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5364 - accuracy: 0.8171 - val_loss: 0.7031 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.5078 - accuracy: 0.83 - ETA: 0s - loss: 0.5126 - accuracy: 0.82 - ETA: 0s - loss: 0.5364 - accuracy: 0.80 - ETA: 0s - loss: 0.5463 - accuracy: 0.80 - ETA: 0s - loss: 0.5444 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5326 - accuracy: 0.8052 - val_loss: 0.6259 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.81 - ETA: 0s - loss: 0.4836 - accuracy: 0.82 - ETA: 0s - loss: 0.4904 - accuracy: 0.82 - ETA: 0s - loss: 0.5163 - accuracy: 0.81 - ETA: 0s - loss: 0.5059 - accuracy: 0.81 - ETA: 0s - loss: 0.4971 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4905 - accuracy: 0.8249 - val_loss: 0.6192 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.87 - ETA: 0s - loss: 0.4600 - accuracy: 0.83 - ETA: 0s - loss: 0.4590 - accuracy: 0.84 - ETA: 0s - loss: 0.5471 - accuracy: 0.84 - ETA: 0s - loss: 0.5319 - accuracy: 0.84 - ETA: 0s - loss: 0.5371 - accuracy: 0.83 - ETA: 0s - loss: 0.5542 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5534 - accuracy: 0.8268 - val_loss: 0.6082 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.87 - ETA: 0s - loss: 0.4818 - accuracy: 0.83 - ETA: 0s - loss: 0.5544 - accuracy: 0.83 - ETA: 0s - loss: 0.5317 - accuracy: 0.83 - ETA: 0s - loss: 0.5934 - accuracy: 0.82 - ETA: 0s - loss: 0.5682 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5699 - accuracy: 0.8287 - val_loss: 0.6190 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.75 - ETA: 0s - loss: 0.4288 - accuracy: 0.85 - ETA: 0s - loss: 0.4525 - accuracy: 0.85 - ETA: 0s - loss: 0.4691 - accuracy: 0.84 - ETA: 0s - loss: 0.4681 - accuracy: 0.85 - ETA: 0s - loss: 0.4697 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4662 - accuracy: 0.8488 - val_loss: 0.7194 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.87 - ETA: 0s - loss: 0.4333 - accuracy: 0.86 - ETA: 0s - loss: 0.4474 - accuracy: 0.85 - ETA: 0s - loss: 0.4454 - accuracy: 0.86 - ETA: 0s - loss: 0.4385 - accuracy: 0.86 - ETA: 0s - loss: 0.4548 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8552 - val_loss: 0.5994 - val_accuracy: 0.7493\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.90 - ETA: 0s - loss: 0.4234 - accuracy: 0.87 - ETA: 0s - loss: 0.4291 - accuracy: 0.86 - ETA: 0s - loss: 0.4524 - accuracy: 0.86 - ETA: 0s - loss: 0.4571 - accuracy: 0.85 - ETA: 0s - loss: 0.4564 - accuracy: 0.85 - ETA: 0s - loss: 0.4519 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4510 - accuracy: 0.8585 - val_loss: 0.7121 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.87 - ETA: 0s - loss: 0.3584 - accuracy: 0.89 - ETA: 0s - loss: 0.3735 - accuracy: 0.89 - ETA: 0s - loss: 0.3812 - accuracy: 0.88 - ETA: 0s - loss: 0.3950 - accuracy: 0.88 - ETA: 0s - loss: 0.4270 - accuracy: 0.87 - ETA: 0s - loss: 0.4297 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4272 - accuracy: 0.8723 - val_loss: 0.6019 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7439 - accuracy: 0.75 - ETA: 0s - loss: 0.4467 - accuracy: 0.86 - ETA: 0s - loss: 0.4686 - accuracy: 0.86 - ETA: 0s - loss: 0.5159 - accuracy: 0.85 - ETA: 0s - loss: 0.5243 - accuracy: 0.85 - ETA: 0s - loss: 0.5326 - accuracy: 0.84 - ETA: 0s - loss: 0.5483 - accuracy: 0.8302Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5483 - accuracy: 0.8302 - val_loss: 0.5659 - val_accuracy: 0.7299\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.81 - ETA: 0s - loss: 3.1120 - accuracy: 0.60 - ETA: 0s - loss: 1.8816 - accuracy: 0.63 - ETA: 0s - loss: 1.4781 - accuracy: 0.66 - ETA: 0s - loss: 1.2541 - accuracy: 0.68 - ETA: 0s - loss: 1.1361 - accuracy: 0.70 - ETA: 0s - loss: 1.0406 - accuracy: 0.70 - 0s 6ms/step - loss: 1.0164 - accuracy: 0.7133 - val_loss: 0.6568 - val_accuracy: 0.7104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.78 - ETA: 0s - loss: 0.6073 - accuracy: 0.74 - ETA: 0s - loss: 0.6000 - accuracy: 0.74 - ETA: 0s - loss: 0.5905 - accuracy: 0.74 - ETA: 0s - loss: 0.5866 - accuracy: 0.74 - ETA: 0s - loss: 0.5758 - accuracy: 0.75 - ETA: 0s - loss: 0.5780 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5799 - accuracy: 0.7525 - val_loss: 0.5631 - val_accuracy: 0.7522\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5684 - accuracy: 0.81 - ETA: 0s - loss: 0.5042 - accuracy: 0.83 - ETA: 0s - loss: 0.5584 - accuracy: 0.79 - ETA: 0s - loss: 0.5550 - accuracy: 0.78 - ETA: 0s - loss: 0.5681 - accuracy: 0.78 - ETA: 0s - loss: 0.5779 - accuracy: 0.77 - ETA: 0s - loss: 0.5680 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5688 - accuracy: 0.7839 - val_loss: 0.6244 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.78 - ETA: 0s - loss: 0.4944 - accuracy: 0.82 - ETA: 0s - loss: 0.5139 - accuracy: 0.81 - ETA: 0s - loss: 0.5303 - accuracy: 0.80 - ETA: 0s - loss: 0.5410 - accuracy: 0.80 - ETA: 0s - loss: 0.5363 - accuracy: 0.80 - ETA: 0s - loss: 0.5353 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5376 - accuracy: 0.7992 - val_loss: 0.5880 - val_accuracy: 0.7418\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.78 - ETA: 0s - loss: 0.4803 - accuracy: 0.82 - ETA: 0s - loss: 0.5016 - accuracy: 0.82 - ETA: 0s - loss: 0.4950 - accuracy: 0.82 - ETA: 0s - loss: 0.4895 - accuracy: 0.82 - ETA: 0s - loss: 0.5088 - accuracy: 0.81 - ETA: 0s - loss: 0.5174 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5202 - accuracy: 0.8134 - val_loss: 0.6266 - val_accuracy: 0.7478\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.78 - ETA: 0s - loss: 0.5444 - accuracy: 0.80 - ETA: 0s - loss: 0.5195 - accuracy: 0.81 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.4954 - accuracy: 0.83 - ETA: 0s - loss: 0.5029 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5083 - accuracy: 0.8246 - val_loss: 0.5766 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3744 - accuracy: 0.90 - ETA: 0s - loss: 0.4700 - accuracy: 0.84 - ETA: 0s - loss: 0.4709 - accuracy: 0.83 - ETA: 0s - loss: 0.4933 - accuracy: 0.82 - ETA: 0s - loss: 0.4979 - accuracy: 0.82 - ETA: 0s - loss: 0.5060 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5024 - accuracy: 0.8219 - val_loss: 0.6058 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.75 - ETA: 0s - loss: 0.5133 - accuracy: 0.81 - ETA: 0s - loss: 0.4819 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.83 - ETA: 0s - loss: 0.4807 - accuracy: 0.83 - ETA: 0s - loss: 0.4747 - accuracy: 0.83 - ETA: 0s - loss: 0.4703 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4710 - accuracy: 0.8410 - val_loss: 0.6664 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.96 - ETA: 0s - loss: 0.4338 - accuracy: 0.86 - ETA: 0s - loss: 0.4524 - accuracy: 0.85 - ETA: 0s - loss: 0.4491 - accuracy: 0.85 - ETA: 0s - loss: 0.4636 - accuracy: 0.85 - ETA: 0s - loss: 0.4779 - accuracy: 0.84 - ETA: 0s - loss: 0.4716 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4756 - accuracy: 0.8429 - val_loss: 0.5705 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.81 - ETA: 0s - loss: 0.4636 - accuracy: 0.84 - ETA: 0s - loss: 0.4353 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.5240 - accuracy: 0.84 - ETA: 0s - loss: 0.5699 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5621 - accuracy: 0.8436 - val_loss: 0.6809 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.90 - ETA: 0s - loss: 0.5037 - accuracy: 0.81 - ETA: 0s - loss: 0.4798 - accuracy: 0.83 - ETA: 0s - loss: 0.5079 - accuracy: 0.82 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - ETA: 0s - loss: 0.5281 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5385 - accuracy: 0.8178 - val_loss: 0.5793 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.65 - ETA: 0s - loss: 0.5579 - accuracy: 0.81 - ETA: 0s - loss: 0.5426 - accuracy: 0.81 - ETA: 0s - loss: 0.5364 - accuracy: 0.81 - ETA: 0s - loss: 0.5331 - accuracy: 0.82 - ETA: 0s - loss: 0.5342 - accuracy: 0.82 - ETA: 0s - loss: 0.5299 - accuracy: 0.8216Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5299 - accuracy: 0.8216 - val_loss: 0.7539 - val_accuracy: 0.7418\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.53 - ETA: 0s - loss: 1.7833 - accuracy: 0.67 - ETA: 0s - loss: 1.2042 - accuracy: 0.69 - ETA: 0s - loss: 1.0037 - accuracy: 0.71 - ETA: 0s - loss: 0.9064 - accuracy: 0.72 - ETA: 0s - loss: 0.8352 - accuracy: 0.73 - ETA: 0s - loss: 0.8019 - accuracy: 0.73 - 0s 5ms/step - loss: 0.8019 - accuracy: 0.7350 - val_loss: 0.6058 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.71 - ETA: 0s - loss: 0.6281 - accuracy: 0.74 - ETA: 0s - loss: 0.5940 - accuracy: 0.75 - ETA: 0s - loss: 0.5930 - accuracy: 0.75 - ETA: 0s - loss: 0.5896 - accuracy: 0.76 - ETA: 0s - loss: 0.5844 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5854 - accuracy: 0.7693 - val_loss: 0.6288 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.68 - ETA: 0s - loss: 0.5955 - accuracy: 0.75 - ETA: 0s - loss: 0.5709 - accuracy: 0.77 - ETA: 0s - loss: 0.5832 - accuracy: 0.77 - ETA: 0s - loss: 0.5709 - accuracy: 0.77 - ETA: 0s - loss: 0.5693 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5653 - accuracy: 0.7824 - val_loss: 0.6466 - val_accuracy: 0.7373\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.81 - ETA: 0s - loss: 0.5197 - accuracy: 0.81 - ETA: 0s - loss: 0.5058 - accuracy: 0.82 - ETA: 0s - loss: 0.5452 - accuracy: 0.80 - ETA: 0s - loss: 0.5468 - accuracy: 0.79 - ETA: 0s - loss: 0.5529 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5547 - accuracy: 0.7928 - val_loss: 0.5771 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.78 - ETA: 0s - loss: 0.5313 - accuracy: 0.80 - ETA: 0s - loss: 0.5025 - accuracy: 0.82 - ETA: 0s - loss: 0.5035 - accuracy: 0.82 - ETA: 0s - loss: 0.5020 - accuracy: 0.82 - ETA: 0s - loss: 0.5066 - accuracy: 0.81 - ETA: 0s - loss: 0.5138 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5152 - accuracy: 0.8152 - val_loss: 0.6247 - val_accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.84 - ETA: 0s - loss: 0.5552 - accuracy: 0.80 - ETA: 0s - loss: 0.5259 - accuracy: 0.81 - ETA: 0s - loss: 0.5381 - accuracy: 0.82 - ETA: 0s - loss: 0.5274 - accuracy: 0.82 - ETA: 0s - loss: 0.5346 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5315 - accuracy: 0.8178 - val_loss: 0.5847 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.75 - ETA: 0s - loss: 0.5855 - accuracy: 0.78 - ETA: 0s - loss: 0.5529 - accuracy: 0.80 - ETA: 0s - loss: 0.5307 - accuracy: 0.81 - ETA: 0s - loss: 0.5251 - accuracy: 0.81 - ETA: 0s - loss: 0.5339 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5329 - accuracy: 0.8186 - val_loss: 0.6094 - val_accuracy: 0.7478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.87 - ETA: 0s - loss: 0.6253 - accuracy: 0.79 - ETA: 0s - loss: 0.5942 - accuracy: 0.79 - ETA: 0s - loss: 0.5641 - accuracy: 0.79 - ETA: 0s - loss: 0.5582 - accuracy: 0.80 - ETA: 0s - loss: 0.5542 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5546 - accuracy: 0.8040 - val_loss: 0.7067 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.84 - ETA: 0s - loss: 0.5464 - accuracy: 0.78 - ETA: 0s - loss: 0.5359 - accuracy: 0.79 - ETA: 0s - loss: 0.5096 - accuracy: 0.81 - ETA: 0s - loss: 0.5330 - accuracy: 0.81 - ETA: 0s - loss: 0.5287 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5224 - accuracy: 0.8201 - val_loss: 0.6568 - val_accuracy: 0.7284\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.87 - ETA: 0s - loss: 0.4758 - accuracy: 0.84 - ETA: 0s - loss: 0.5014 - accuracy: 0.83 - ETA: 0s - loss: 0.4957 - accuracy: 0.83 - ETA: 0s - loss: 0.4900 - accuracy: 0.83 - ETA: 0s - loss: 0.4888 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4907 - accuracy: 0.8406 - val_loss: 0.6990 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.90 - ETA: 0s - loss: 0.4477 - accuracy: 0.86 - ETA: 0s - loss: 0.4664 - accuracy: 0.84 - ETA: 0s - loss: 0.4868 - accuracy: 0.83 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - ETA: 0s - loss: 0.4772 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4731 - accuracy: 0.8417 - val_loss: 0.7945 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.75 - ETA: 0s - loss: 0.4923 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.4812 - accuracy: 0.84 - ETA: 0s - loss: 0.4710 - accuracy: 0.84 - ETA: 0s - loss: 0.4704 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4717 - accuracy: 0.8462 - val_loss: 0.6790 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.87 - ETA: 0s - loss: 0.4902 - accuracy: 0.82 - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4635 - accuracy: 0.84 - ETA: 0s - loss: 0.4698 - accuracy: 0.84 - ETA: 0s - loss: 0.4611 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4608 - accuracy: 0.8537 - val_loss: 0.6604 - val_accuracy: 0.7493\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.87 - ETA: 0s - loss: 0.4467 - accuracy: 0.86 - ETA: 0s - loss: 0.4448 - accuracy: 0.85 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - ETA: 0s - loss: 0.4557 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4626 - accuracy: 0.8488 - val_loss: 0.7276 - val_accuracy: 0.7373\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.81 - ETA: 0s - loss: 0.4715 - accuracy: 0.85 - ETA: 0s - loss: 0.4854 - accuracy: 0.84 - ETA: 0s - loss: 0.5088 - accuracy: 0.83 - ETA: 0s - loss: 0.5098 - accuracy: 0.82 - ETA: 0s - loss: 0.4921 - accuracy: 0.83 - ETA: 0s - loss: 0.4881 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4889 - accuracy: 0.8406 - val_loss: 0.8755 - val_accuracy: 0.7358\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.81 - ETA: 0s - loss: 0.6680 - accuracy: 0.82 - ETA: 0s - loss: 0.5824 - accuracy: 0.83 - ETA: 0s - loss: 0.5261 - accuracy: 0.84 - ETA: 0s - loss: 0.4946 - accuracy: 0.85 - ETA: 0s - loss: 0.5018 - accuracy: 0.84 - ETA: 0s - loss: 0.4894 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4897 - accuracy: 0.8499 - val_loss: 0.8143 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.84 - ETA: 0s - loss: 0.4322 - accuracy: 0.86 - ETA: 0s - loss: 0.4778 - accuracy: 0.85 - ETA: 0s - loss: 0.4961 - accuracy: 0.84 - ETA: 0s - loss: 0.4844 - accuracy: 0.85 - ETA: 0s - loss: 0.4909 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4934 - accuracy: 0.8447 - val_loss: 0.7714 - val_accuracy: 0.7194\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.81 - ETA: 0s - loss: 0.4435 - accuracy: 0.87 - ETA: 0s - loss: 0.4497 - accuracy: 0.86 - ETA: 0s - loss: 0.4590 - accuracy: 0.86 - ETA: 0s - loss: 0.4554 - accuracy: 0.86 - ETA: 0s - loss: 0.4598 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4709 - accuracy: 0.8544 - val_loss: 0.7968 - val_accuracy: 0.7507\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.87 - ETA: 0s - loss: 0.4619 - accuracy: 0.85 - ETA: 0s - loss: 0.4692 - accuracy: 0.84 - ETA: 0s - loss: 0.4669 - accuracy: 0.84 - ETA: 0s - loss: 0.4654 - accuracy: 0.85 - ETA: 0s - loss: 0.4663 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4658 - accuracy: 0.8541 - val_loss: 0.7468 - val_accuracy: 0.7388\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.84 - ETA: 0s - loss: 0.4640 - accuracy: 0.85 - ETA: 0s - loss: 0.4971 - accuracy: 0.84 - ETA: 0s - loss: 0.4909 - accuracy: 0.84 - ETA: 0s - loss: 0.4824 - accuracy: 0.85 - ETA: 0s - loss: 0.4834 - accuracy: 0.85 - ETA: 0s - loss: 0.4900 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4903 - accuracy: 0.8485 - val_loss: 0.7028 - val_accuracy: 0.7403\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.87 - ETA: 0s - loss: 0.4839 - accuracy: 0.84 - ETA: 0s - loss: 0.4820 - accuracy: 0.84 - ETA: 0s - loss: 0.4917 - accuracy: 0.83 - ETA: 0s - loss: 0.4925 - accuracy: 0.83 - ETA: 0s - loss: 0.4906 - accuracy: 0.83 - ETA: 0s - loss: 0.4881 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4856 - accuracy: 0.8443 - val_loss: 0.9726 - val_accuracy: 0.7358\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.87 - ETA: 0s - loss: 0.5608 - accuracy: 0.87 - ETA: 0s - loss: 0.5262 - accuracy: 0.86 - ETA: 0s - loss: 0.5189 - accuracy: 0.86 - ETA: 0s - loss: 0.5232 - accuracy: 0.85 - ETA: 0s - loss: 0.5249 - accuracy: 0.84 - ETA: 0s - loss: 0.5495 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5495 - accuracy: 0.8414 - val_loss: 0.7331 - val_accuracy: 0.7284\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.87 - ETA: 0s - loss: 0.5018 - accuracy: 0.84 - ETA: 0s - loss: 0.5083 - accuracy: 0.84 - ETA: 0s - loss: 0.5129 - accuracy: 0.83 - ETA: 0s - loss: 0.5243 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5116 - accuracy: 0.8309 - val_loss: 0.7031 - val_accuracy: 0.7493\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.81 - ETA: 0s - loss: 0.4786 - accuracy: 0.84 - ETA: 0s - loss: 0.4585 - accuracy: 0.86 - ETA: 0s - loss: 0.4629 - accuracy: 0.86 - ETA: 0s - loss: 0.4582 - accuracy: 0.86 - ETA: 0s - loss: 0.4534 - accuracy: 0.86 - ETA: 0s - loss: 0.4510 - accuracy: 0.86 - ETA: 0s - loss: 0.4499 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4512 - accuracy: 0.8611 - val_loss: 0.7123 - val_accuracy: 0.7522\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.87 - ETA: 0s - loss: 0.5480 - accuracy: 0.83 - ETA: 0s - loss: 0.5177 - accuracy: 0.83 - ETA: 0s - loss: 0.5145 - accuracy: 0.84 - ETA: 0s - loss: 0.5010 - accuracy: 0.84 - ETA: 0s - loss: 0.4898 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4881 - accuracy: 0.8507 - val_loss: 1.0358 - val_accuracy: 0.7403\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.90 - ETA: 0s - loss: 0.4100 - accuracy: 0.87 - ETA: 0s - loss: 0.4329 - accuracy: 0.87 - ETA: 0s - loss: 0.4248 - accuracy: 0.87 - ETA: 0s - loss: 0.4492 - accuracy: 0.87 - ETA: 0s - loss: 0.4636 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4635 - accuracy: 0.8660 - val_loss: 0.8731 - val_accuracy: 0.7358\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.90 - ETA: 0s - loss: 0.4830 - accuracy: 0.85 - ETA: 0s - loss: 0.4552 - accuracy: 0.86 - ETA: 0s - loss: 0.4514 - accuracy: 0.86 - ETA: 0s - loss: 0.4564 - accuracy: 0.86 - ETA: 0s - loss: 0.4535 - accuracy: 0.86 - ETA: 0s - loss: 0.4533 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4534 - accuracy: 0.8638 - val_loss: 0.8956 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.81 - ETA: 0s - loss: 0.4365 - accuracy: 0.85 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.4521 - accuracy: 0.86 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - ETA: 0s - loss: 0.4390 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4367 - accuracy: 0.8686 - val_loss: 0.8985 - val_accuracy: 0.7388\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4542 - accuracy: 0.87 - ETA: 0s - loss: 0.4551 - accuracy: 0.86 - ETA: 0s - loss: 0.4673 - accuracy: 0.86 - ETA: 0s - loss: 0.4621 - accuracy: 0.86 - ETA: 0s - loss: 0.4701 - accuracy: 0.86 - ETA: 0s - loss: 0.4671 - accuracy: 0.86 - ETA: 0s - loss: 0.4707 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4707 - accuracy: 0.8589 - val_loss: 2.8971 - val_accuracy: 0.7522\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.90 - ETA: 0s - loss: 0.4938 - accuracy: 0.84 - ETA: 0s - loss: 0.5062 - accuracy: 0.84 - ETA: 0s - loss: 0.4926 - accuracy: 0.85 - ETA: 0s - loss: 0.4856 - accuracy: 0.85 - ETA: 0s - loss: 0.5316 - accuracy: 0.84 - ETA: 0s - loss: 0.5398 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5398 - accuracy: 0.8399 - val_loss: 1.2816 - val_accuracy: 0.7075\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.84 - ETA: 0s - loss: 0.5422 - accuracy: 0.82 - ETA: 0s - loss: 0.5050 - accuracy: 0.83 - ETA: 0s - loss: 0.5053 - accuracy: 0.83 - ETA: 0s - loss: 0.5110 - accuracy: 0.83 - ETA: 0s - loss: 0.5084 - accuracy: 0.83 - ETA: 0s - loss: 0.5046 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5033 - accuracy: 0.8331 - val_loss: 1.2867 - val_accuracy: 0.7328\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4104 - accuracy: 0.84 - ETA: 0s - loss: 0.5524 - accuracy: 0.82 - ETA: 0s - loss: 0.4989 - accuracy: 0.84 - ETA: 0s - loss: 0.4698 - accuracy: 0.86 - ETA: 0s - loss: 0.4627 - accuracy: 0.86 - ETA: 0s - loss: 0.4627 - accuracy: 0.86 - ETA: 0s - loss: 0.4592 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4572 - accuracy: 0.8630 - val_loss: 0.8249 - val_accuracy: 0.7313\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3815 - accuracy: 0.90 - ETA: 0s - loss: 0.3957 - accuracy: 0.89 - ETA: 0s - loss: 0.3819 - accuracy: 0.89 - ETA: 0s - loss: 0.3902 - accuracy: 0.89 - ETA: 0s - loss: 0.3949 - accuracy: 0.88 - ETA: 0s - loss: 0.4224 - accuracy: 0.88 - ETA: 0s - loss: 0.4277 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8757 - val_loss: 0.8785 - val_accuracy: 0.7448\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3850 - accuracy: 0.90 - ETA: 0s - loss: 0.4443 - accuracy: 0.87 - ETA: 0s - loss: 0.4414 - accuracy: 0.87 - ETA: 0s - loss: 0.4448 - accuracy: 0.86 - ETA: 0s - loss: 0.4365 - accuracy: 0.87 - ETA: 0s - loss: 0.4268 - accuracy: 0.87 - ETA: 0s - loss: 0.4248 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4248 - accuracy: 0.8761 - val_loss: 0.9886 - val_accuracy: 0.7373\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.84 - ETA: 0s - loss: 0.4053 - accuracy: 0.88 - ETA: 0s - loss: 0.3784 - accuracy: 0.89 - ETA: 0s - loss: 0.3945 - accuracy: 0.89 - ETA: 0s - loss: 0.4131 - accuracy: 0.88 - ETA: 0s - loss: 0.4184 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8828 - val_loss: 0.9769 - val_accuracy: 0.7433\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.90 - ETA: 0s - loss: 0.3792 - accuracy: 0.90 - ETA: 0s - loss: 0.4338 - accuracy: 0.86 - ETA: 0s - loss: 0.4394 - accuracy: 0.87 - ETA: 0s - loss: 0.4269 - accuracy: 0.87 - ETA: 0s - loss: 0.4270 - accuracy: 0.87 - ETA: 0s - loss: 0.4438 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4456 - accuracy: 0.8731 - val_loss: 0.7463 - val_accuracy: 0.7403\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.87 - ETA: 0s - loss: 0.4731 - accuracy: 0.85 - ETA: 0s - loss: 0.4657 - accuracy: 0.85 - ETA: 0s - loss: 0.4632 - accuracy: 0.86 - ETA: 0s - loss: 0.4767 - accuracy: 0.85 - ETA: 0s - loss: 0.4916 - accuracy: 0.85 - ETA: 0s - loss: 0.5042 - accuracy: 0.8436Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5042 - accuracy: 0.8436 - val_loss: 0.9445 - val_accuracy: 0.7284\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d39d089371492ccef98415d46c180854</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7547263701756796</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.37331234863731333</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 310</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 55</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.50 - ETA: 0s - loss: 2.6477 - accuracy: 0.60 - ETA: 0s - loss: 1.7783 - accuracy: 0.60 - ETA: 0s - loss: 1.3910 - accuracy: 0.63 - ETA: 0s - loss: 1.2294 - accuracy: 0.65 - 0s 5ms/step - loss: 1.1629 - accuracy: 0.6596 - val_loss: 0.5837 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.75 - ETA: 0s - loss: 0.5721 - accuracy: 0.76 - ETA: 0s - loss: 0.5924 - accuracy: 0.74 - ETA: 0s - loss: 0.6057 - accuracy: 0.74 - ETA: 0s - loss: 0.6113 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6117 - accuracy: 0.7376 - val_loss: 0.5958 - val_accuracy: 0.7463\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.65 - ETA: 0s - loss: 0.5972 - accuracy: 0.75 - ETA: 0s - loss: 0.5767 - accuracy: 0.77 - ETA: 0s - loss: 0.5725 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5762 - accuracy: 0.7798 - val_loss: 0.5889 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.84 - ETA: 0s - loss: 0.5738 - accuracy: 0.78 - ETA: 0s - loss: 0.5700 - accuracy: 0.79 - ETA: 0s - loss: 0.5689 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5642 - accuracy: 0.7940 - val_loss: 0.6599 - val_accuracy: 0.7478\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7098 - accuracy: 0.68 - ETA: 0s - loss: 0.5366 - accuracy: 0.79 - ETA: 0s - loss: 0.5238 - accuracy: 0.81 - ETA: 0s - loss: 0.5230 - accuracy: 0.80 - ETA: 0s - loss: 0.5293 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5269 - accuracy: 0.8085 - val_loss: 0.6932 - val_accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.78 - ETA: 0s - loss: 0.5781 - accuracy: 0.80 - ETA: 0s - loss: 0.5554 - accuracy: 0.80 - ETA: 0s - loss: 0.5351 - accuracy: 0.80 - ETA: 0s - loss: 0.5328 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5345 - accuracy: 0.8044 - val_loss: 0.6459 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.84 - ETA: 0s - loss: 0.4898 - accuracy: 0.82 - ETA: 0s - loss: 0.4873 - accuracy: 0.83 - ETA: 0s - loss: 0.4941 - accuracy: 0.82 - ETA: 0s - loss: 0.4986 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4965 - accuracy: 0.8227 - val_loss: 0.5872 - val_accuracy: 0.7448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - ETA: 0s - loss: 0.5433 - accuracy: 0.81 - ETA: 0s - loss: 0.5175 - accuracy: 0.83 - ETA: 0s - loss: 0.5203 - accuracy: 0.83 - ETA: 0s - loss: 0.5233 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5276 - accuracy: 0.8272 - val_loss: 0.6307 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.90 - ETA: 0s - loss: 0.6228 - accuracy: 0.81 - ETA: 0s - loss: 0.5924 - accuracy: 0.81 - ETA: 0s - loss: 0.5803 - accuracy: 0.80 - ETA: 0s - loss: 0.5783 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5773 - accuracy: 0.8055 - val_loss: 0.6242 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4436 - accuracy: 0.84 - ETA: 0s - loss: 0.4526 - accuracy: 0.85 - ETA: 0s - loss: 0.4620 - accuracy: 0.84 - ETA: 0s - loss: 0.4949 - accuracy: 0.83 - ETA: 0s - loss: 0.4898 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5057 - accuracy: 0.8290 - val_loss: 0.7479 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.78 - ETA: 0s - loss: 0.5851 - accuracy: 0.83 - ETA: 0s - loss: 0.5153 - accuracy: 0.84 - ETA: 0s - loss: 0.5150 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5087 - accuracy: 0.8384 - val_loss: 0.5714 - val_accuracy: 0.7448\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.84 - ETA: 0s - loss: 0.4988 - accuracy: 0.84 - ETA: 0s - loss: 0.5222 - accuracy: 0.83 - ETA: 0s - loss: 0.5574 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5787 - accuracy: 0.8156 - val_loss: 0.5679 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.9179 - accuracy: 0.75 - ETA: 0s - loss: 0.5866 - accuracy: 0.82 - ETA: 0s - loss: 0.5563 - accuracy: 0.82 - ETA: 0s - loss: 0.5255 - accuracy: 0.83 - ETA: 0s - loss: 0.5525 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5546 - accuracy: 0.8249 - val_loss: 0.7469 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 1.1966 - accuracy: 0.78 - ETA: 0s - loss: 0.6378 - accuracy: 0.81 - ETA: 0s - loss: 0.5819 - accuracy: 0.81 - ETA: 0s - loss: 0.5472 - accuracy: 0.82 - ETA: 0s - loss: 0.5375 - accuracy: 0.8287Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5377 - accuracy: 0.8283 - val_loss: 0.7276 - val_accuracy: 0.7060\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9324 - accuracy: 0.62 - ETA: 0s - loss: 2.4398 - accuracy: 0.58 - ETA: 0s - loss: 1.5278 - accuracy: 0.62 - ETA: 0s - loss: 1.2226 - accuracy: 0.64 - ETA: 0s - loss: 1.0770 - accuracy: 0.65 - 0s 4ms/step - loss: 1.0672 - accuracy: 0.6581 - val_loss: 0.5846 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.71 - ETA: 0s - loss: 0.6028 - accuracy: 0.74 - ETA: 0s - loss: 0.5979 - accuracy: 0.75 - ETA: 0s - loss: 0.6061 - accuracy: 0.73 - 0s 3ms/step - loss: 0.5932 - accuracy: 0.7462 - val_loss: 0.5528 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.71 - ETA: 0s - loss: 0.5763 - accuracy: 0.77 - ETA: 0s - loss: 0.5680 - accuracy: 0.77 - ETA: 0s - loss: 0.5646 - accuracy: 0.77 - ETA: 0s - loss: 0.5504 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7813 - val_loss: 0.6699 - val_accuracy: 0.6731\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.90 - ETA: 0s - loss: 0.5135 - accuracy: 0.83 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.5224 - accuracy: 0.82 - ETA: 0s - loss: 0.5278 - accuracy: 0.81 - ETA: 0s - loss: 0.5466 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5446 - accuracy: 0.7943 - val_loss: 0.6358 - val_accuracy: 0.7134\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7349 - accuracy: 0.68 - ETA: 0s - loss: 0.5370 - accuracy: 0.76 - ETA: 0s - loss: 0.5201 - accuracy: 0.79 - ETA: 0s - loss: 0.5256 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5144 - accuracy: 0.8033 - val_loss: 0.5944 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.75 - ETA: 0s - loss: 0.4527 - accuracy: 0.84 - ETA: 0s - loss: 0.4593 - accuracy: 0.84 - ETA: 0s - loss: 0.5027 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5020 - accuracy: 0.8160 - val_loss: 0.6214 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.78 - ETA: 0s - loss: 0.4871 - accuracy: 0.81 - ETA: 0s - loss: 0.5033 - accuracy: 0.80 - ETA: 0s - loss: 0.4875 - accuracy: 0.81 - ETA: 0s - loss: 0.4940 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4963 - accuracy: 0.8201 - val_loss: 0.9139 - val_accuracy: 0.6910\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.75 - ETA: 0s - loss: 0.4266 - accuracy: 0.86 - ETA: 0s - loss: 0.4386 - accuracy: 0.85 - ETA: 0s - loss: 0.4598 - accuracy: 0.84 - ETA: 0s - loss: 0.4563 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4600 - accuracy: 0.8518 - val_loss: 0.8407 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.87 - ETA: 0s - loss: 0.4225 - accuracy: 0.85 - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4471 - accuracy: 0.83 - ETA: 0s - loss: 0.4496 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4547 - accuracy: 0.8391 - val_loss: 0.9898 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.87 - ETA: 0s - loss: 0.3688 - accuracy: 0.88 - ETA: 0s - loss: 0.3836 - accuracy: 0.88 - ETA: 0s - loss: 0.4095 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8716 - val_loss: 0.7834 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.90 - ETA: 0s - loss: 0.3867 - accuracy: 0.87 - ETA: 0s - loss: 0.3963 - accuracy: 0.87 - ETA: 0s - loss: 0.4815 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4814 - accuracy: 0.8593 - val_loss: 1.3805 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.93 - ETA: 0s - loss: 0.4824 - accuracy: 0.86 - ETA: 0s - loss: 0.5385 - accuracy: 0.84 - ETA: 0s - loss: 0.5245 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5174 - accuracy: 0.8518 - val_loss: 0.6414 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.78 - ETA: 0s - loss: 0.4874 - accuracy: 0.83 - ETA: 0s - loss: 0.5612 - accuracy: 0.82 - ETA: 0s - loss: 0.5889 - accuracy: 0.81 - ETA: 0s - loss: 0.5829 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5773 - accuracy: 0.8115 - val_loss: 1.0762 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.93 - ETA: 0s - loss: 0.4833 - accuracy: 0.83 - ETA: 0s - loss: 0.5399 - accuracy: 0.81 - ETA: 0s - loss: 0.5234 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5162 - accuracy: 0.8268 - val_loss: 0.9132 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.71 - ETA: 0s - loss: 0.5910 - accuracy: 0.82 - ETA: 0s - loss: 0.6386 - accuracy: 0.82 - ETA: 0s - loss: 0.6010 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6030 - accuracy: 0.8320 - val_loss: 1.3049 - val_accuracy: 0.7388\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.81 - ETA: 0s - loss: 0.5817 - accuracy: 0.79 - ETA: 0s - loss: 0.5473 - accuracy: 0.82 - ETA: 0s - loss: 0.5396 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5365 - accuracy: 0.8242 - val_loss: 0.9561 - val_accuracy: 0.7448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.71 - ETA: 0s - loss: 0.5423 - accuracy: 0.84 - ETA: 0s - loss: 0.5540 - accuracy: 0.84 - ETA: 0s - loss: 0.5072 - accuracy: 0.85 - ETA: 0s - loss: 0.4912 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8604 - val_loss: 1.7343 - val_accuracy: 0.7030\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.87 - ETA: 0s - loss: 0.4397 - accuracy: 0.87 - ETA: 0s - loss: 0.4991 - accuracy: 0.88 - ETA: 0s - loss: 0.4736 - accuracy: 0.88 - ETA: 0s - loss: 0.4632 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4570 - accuracy: 0.8779 - val_loss: 1.1273 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.84 - ETA: 0s - loss: 0.3930 - accuracy: 0.87 - ETA: 0s - loss: 0.3724 - accuracy: 0.89 - ETA: 0s - loss: 0.3634 - accuracy: 0.89 - ETA: 0s - loss: 0.3583 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8932 - val_loss: 1.2422 - val_accuracy: 0.7104\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.90 - ETA: 0s - loss: 0.3691 - accuracy: 0.88 - ETA: 0s - loss: 0.3724 - accuracy: 0.89 - ETA: 0s - loss: 0.4115 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4157 - accuracy: 0.8880 - val_loss: 0.8705 - val_accuracy: 0.7269\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.87 - ETA: 0s - loss: 0.3137 - accuracy: 0.91 - ETA: 0s - loss: 0.3512 - accuracy: 0.90 - ETA: 0s - loss: 0.3721 - accuracy: 0.89 - ETA: 0s - loss: 0.3921 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8959 - val_loss: 1.2556 - val_accuracy: 0.7045\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.81 - ETA: 0s - loss: 0.4489 - accuracy: 0.88 - ETA: 0s - loss: 0.3998 - accuracy: 0.89 - ETA: 0s - loss: 0.4569 - accuracy: 0.89 - ETA: 0s - loss: 0.4305 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4274 - accuracy: 0.8903 - val_loss: 0.8822 - val_accuracy: 0.7194\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0617 - accuracy: 0.87 - ETA: 0s - loss: 0.4802 - accuracy: 0.88 - ETA: 0s - loss: 0.5893 - accuracy: 0.89 - ETA: 0s - loss: 0.6397 - accuracy: 0.88 - ETA: 0s - loss: 0.6665 - accuracy: 0.86 - 0s 3ms/step - loss: 0.7131 - accuracy: 0.8541 - val_loss: 1.1628 - val_accuracy: 0.7000\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1417 - accuracy: 0.71 - ETA: 0s - loss: 1.1396 - accuracy: 0.74 - ETA: 0s - loss: 0.9820 - accuracy: 0.73 - ETA: 0s - loss: 0.8691 - accuracy: 0.75 - ETA: 0s - loss: 0.8125 - accuracy: 0.75 - 0s 3ms/step - loss: 0.7888 - accuracy: 0.7589 - val_loss: 1.1463 - val_accuracy: 0.7090\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0424 - accuracy: 0.81 - ETA: 0s - loss: 0.6175 - accuracy: 0.85 - ETA: 0s - loss: 0.5447 - accuracy: 0.86 - ETA: 0s - loss: 0.5115 - accuracy: 0.86 - ETA: 0s - loss: 0.4950 - accuracy: 0.86 - 0s 3ms/step - loss: 0.5089 - accuracy: 0.8656 - val_loss: 0.9191 - val_accuracy: 0.7254\n",
      "Epoch 26/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5163 - accuracy: 0.90 - ETA: 0s - loss: 0.5795 - accuracy: 0.87 - ETA: 0s - loss: 0.5157 - accuracy: 0.87 - ETA: 0s - loss: 0.4755 - accuracy: 0.87 - ETA: 0s - loss: 0.4600 - accuracy: 0.87 - ETA: 0s - loss: 0.4488 - accuracy: 0.8789Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.8772 - val_loss: 1.1649 - val_accuracy: 0.7179\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.62 - ETA: 0s - loss: 2.1288 - accuracy: 0.62 - ETA: 0s - loss: 1.4237 - accuracy: 0.66 - ETA: 0s - loss: 1.1930 - accuracy: 0.67 - ETA: 0s - loss: 1.0672 - accuracy: 0.68 - 0s 5ms/step - loss: 1.0589 - accuracy: 0.6805 - val_loss: 0.5827 - val_accuracy: 0.7507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.71 - ETA: 0s - loss: 0.6217 - accuracy: 0.73 - ETA: 0s - loss: 0.6109 - accuracy: 0.75 - ETA: 0s - loss: 0.6086 - accuracy: 0.74 - ETA: 0s - loss: 0.6098 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6091 - accuracy: 0.7477 - val_loss: 0.5837 - val_accuracy: 0.7403\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.81 - ETA: 0s - loss: 0.5351 - accuracy: 0.78 - ETA: 0s - loss: 0.5349 - accuracy: 0.78 - ETA: 0s - loss: 0.5280 - accuracy: 0.79 - ETA: 0s - loss: 0.5321 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7906 - val_loss: 0.5881 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.81 - ETA: 0s - loss: 0.5615 - accuracy: 0.80 - ETA: 0s - loss: 0.5601 - accuracy: 0.80 - ETA: 0s - loss: 0.5670 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5639 - accuracy: 0.7999 - val_loss: 0.5556 - val_accuracy: 0.7537\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.93 - ETA: 0s - loss: 0.4869 - accuracy: 0.82 - ETA: 0s - loss: 0.5039 - accuracy: 0.82 - ETA: 0s - loss: 0.5188 - accuracy: 0.81 - ETA: 0s - loss: 0.5322 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5350 - accuracy: 0.8066 - val_loss: 0.5698 - val_accuracy: 0.7493\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.71 - ETA: 0s - loss: 0.5202 - accuracy: 0.81 - ETA: 0s - loss: 0.5386 - accuracy: 0.80 - ETA: 0s - loss: 0.5268 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5244 - accuracy: 0.8093 - val_loss: 0.6833 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.71 - ETA: 0s - loss: 0.4936 - accuracy: 0.82 - ETA: 0s - loss: 0.5146 - accuracy: 0.82 - ETA: 0s - loss: 0.5106 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5188 - accuracy: 0.8275 - val_loss: 0.6004 - val_accuracy: 0.7552\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3388 - accuracy: 0.90 - ETA: 0s - loss: 0.5296 - accuracy: 0.81 - ETA: 0s - loss: 0.5011 - accuracy: 0.83 - ETA: 0s - loss: 0.5059 - accuracy: 0.83 - ETA: 0s - loss: 0.5119 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5226 - accuracy: 0.8320 - val_loss: 0.8432 - val_accuracy: 0.6687\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.78 - ETA: 0s - loss: 0.5425 - accuracy: 0.84 - ETA: 0s - loss: 0.5045 - accuracy: 0.84 - ETA: 0s - loss: 0.4813 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5024 - accuracy: 0.8335 - val_loss: 0.8079 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.84 - ETA: 0s - loss: 0.4886 - accuracy: 0.83 - ETA: 0s - loss: 0.4773 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8384 - val_loss: 0.6815 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.87 - ETA: 0s - loss: 0.4140 - accuracy: 0.86 - ETA: 0s - loss: 0.4358 - accuracy: 0.85 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4837 - accuracy: 0.8507 - val_loss: 1.1064 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.84 - ETA: 0s - loss: 0.5189 - accuracy: 0.84 - ETA: 0s - loss: 0.5227 - accuracy: 0.83 - ETA: 0s - loss: 0.5189 - accuracy: 0.83 - ETA: 0s - loss: 0.5047 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5022 - accuracy: 0.8384 - val_loss: 0.7521 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8834 - accuracy: 0.71 - ETA: 0s - loss: 0.5040 - accuracy: 0.83 - ETA: 0s - loss: 0.4787 - accuracy: 0.84 - ETA: 0s - loss: 0.4744 - accuracy: 0.85 - ETA: 0s - loss: 0.4809 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4731 - accuracy: 0.8507 - val_loss: 1.0074 - val_accuracy: 0.7134\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.81 - ETA: 0s - loss: 0.5228 - accuracy: 0.83 - ETA: 0s - loss: 0.4877 - accuracy: 0.84 - ETA: 0s - loss: 0.4804 - accuracy: 0.84 - ETA: 0s - loss: 0.4655 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4683 - accuracy: 0.8518 - val_loss: 0.8109 - val_accuracy: 0.7269\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.84 - ETA: 0s - loss: 0.5123 - accuracy: 0.85 - ETA: 0s - loss: 0.5239 - accuracy: 0.83 - ETA: 0s - loss: 0.5159 - accuracy: 0.83 - ETA: 0s - loss: 0.4806 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4918 - accuracy: 0.8473 - val_loss: 0.8805 - val_accuracy: 0.7179\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.90 - ETA: 0s - loss: 0.4614 - accuracy: 0.85 - ETA: 0s - loss: 0.4376 - accuracy: 0.86 - ETA: 0s - loss: 0.4430 - accuracy: 0.86 - ETA: 0s - loss: 0.4409 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8671 - val_loss: 0.7006 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.4041 - accuracy: 0.90 - ETA: 0s - loss: 0.4427 - accuracy: 0.87 - ETA: 0s - loss: 0.4295 - accuracy: 0.87 - ETA: 0s - loss: 0.4203 - accuracy: 0.8760Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8742 - val_loss: 1.2518 - val_accuracy: 0.7299\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 9d80e3db698e69972eb63fa7b63f37cf</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7492537299791971</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7908023517346229</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9603 - accuracy: 0.43 - ETA: 0s - loss: 3.4438 - accuracy: 0.62 - ETA: 0s - loss: 2.2658 - accuracy: 0.62 - ETA: 0s - loss: 1.7205 - accuracy: 0.65 - ETA: 0s - loss: 1.4707 - accuracy: 0.65 - ETA: 0s - loss: 1.3114 - accuracy: 0.65 - 0s 6ms/step - loss: 1.2203 - accuracy: 0.6603 - val_loss: 0.6045 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.71 - ETA: 0s - loss: 0.6592 - accuracy: 0.73 - ETA: 0s - loss: 0.6440 - accuracy: 0.73 - ETA: 0s - loss: 0.6203 - accuracy: 0.75 - ETA: 0s - loss: 0.6036 - accuracy: 0.75 - ETA: 0s - loss: 0.6041 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6023 - accuracy: 0.7518 - val_loss: 0.5795 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.84 - ETA: 0s - loss: 0.5651 - accuracy: 0.75 - ETA: 0s - loss: 0.5585 - accuracy: 0.77 - ETA: 0s - loss: 0.5575 - accuracy: 0.77 - ETA: 0s - loss: 0.5551 - accuracy: 0.78 - ETA: 0s - loss: 0.5505 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5476 - accuracy: 0.7884 - val_loss: 0.5683 - val_accuracy: 0.7403\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.81 - ETA: 0s - loss: 0.4554 - accuracy: 0.83 - ETA: 0s - loss: 0.4874 - accuracy: 0.82 - ETA: 0s - loss: 0.4952 - accuracy: 0.82 - ETA: 0s - loss: 0.5217 - accuracy: 0.81 - ETA: 0s - loss: 0.5313 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5312 - accuracy: 0.8055 - val_loss: 0.5669 - val_accuracy: 0.7537\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.93 - ETA: 0s - loss: 0.4767 - accuracy: 0.83 - ETA: 0s - loss: 0.4853 - accuracy: 0.81 - ETA: 0s - loss: 0.4843 - accuracy: 0.81 - ETA: 0s - loss: 0.4887 - accuracy: 0.81 - ETA: 0s - loss: 0.4979 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4976 - accuracy: 0.8137 - val_loss: 0.7232 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.78 - ETA: 0s - loss: 0.4810 - accuracy: 0.82 - ETA: 0s - loss: 0.4821 - accuracy: 0.83 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - ETA: 0s - loss: 0.4934 - accuracy: 0.83 - ETA: 0s - loss: 0.5049 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5045 - accuracy: 0.8335 - val_loss: 0.5957 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.90 - ETA: 0s - loss: 0.4273 - accuracy: 0.85 - ETA: 0s - loss: 0.4873 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.83 - ETA: 0s - loss: 0.5062 - accuracy: 0.82 - ETA: 0s - loss: 0.4907 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4843 - accuracy: 0.8343 - val_loss: 0.7490 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.87 - ETA: 0s - loss: 0.4711 - accuracy: 0.84 - ETA: 0s - loss: 0.4594 - accuracy: 0.85 - ETA: 0s - loss: 0.4646 - accuracy: 0.85 - ETA: 0s - loss: 0.4606 - accuracy: 0.85 - ETA: 0s - loss: 0.4646 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4661 - accuracy: 0.8499 - val_loss: 0.7235 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.84 - ETA: 0s - loss: 0.4087 - accuracy: 0.87 - ETA: 0s - loss: 0.4290 - accuracy: 0.86 - ETA: 0s - loss: 0.4695 - accuracy: 0.85 - ETA: 0s - loss: 0.4927 - accuracy: 0.85 - ETA: 0s - loss: 0.5061 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5046 - accuracy: 0.8436 - val_loss: 0.5811 - val_accuracy: 0.7597\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8711 - accuracy: 0.78 - ETA: 0s - loss: 0.5723 - accuracy: 0.84 - ETA: 0s - loss: 0.5037 - accuracy: 0.85 - ETA: 0s - loss: 0.4996 - accuracy: 0.85 - ETA: 0s - loss: 0.5057 - accuracy: 0.84 - ETA: 0s - loss: 0.5223 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5164 - accuracy: 0.8384 - val_loss: 0.6646 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.78 - ETA: 0s - loss: 0.5926 - accuracy: 0.78 - ETA: 0s - loss: 0.5617 - accuracy: 0.79 - ETA: 0s - loss: 0.6044 - accuracy: 0.81 - ETA: 0s - loss: 0.5889 - accuracy: 0.81 - ETA: 0s - loss: 0.5733 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5713 - accuracy: 0.8182 - val_loss: 0.8441 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7324 - accuracy: 0.68 - ETA: 0s - loss: 0.5142 - accuracy: 0.82 - ETA: 0s - loss: 0.5539 - accuracy: 0.81 - ETA: 0s - loss: 0.5495 - accuracy: 0.82 - ETA: 0s - loss: 0.5379 - accuracy: 0.82 - ETA: 0s - loss: 0.5378 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5320 - accuracy: 0.8287 - val_loss: 1.0078 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.87 - ETA: 0s - loss: 0.5396 - accuracy: 0.83 - ETA: 0s - loss: 0.5562 - accuracy: 0.82 - ETA: 0s - loss: 0.5706 - accuracy: 0.83 - ETA: 0s - loss: 0.5738 - accuracy: 0.83 - ETA: 0s - loss: 0.5984 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5988 - accuracy: 0.8294 - val_loss: 1.1253 - val_accuracy: 0.7209\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.75 - ETA: 0s - loss: 0.4990 - accuracy: 0.83 - ETA: 0s - loss: 0.6315 - accuracy: 0.81 - ETA: 0s - loss: 0.6924 - accuracy: 0.80 - ETA: 0s - loss: 0.7556 - accuracy: 0.80 - ETA: 0s - loss: 0.7281 - accuracy: 0.79 - ETA: 0s - loss: 0.7217 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7208 - accuracy: 0.7891 - val_loss: 0.7225 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7681 - accuracy: 0.81 - ETA: 0s - loss: 0.8662 - accuracy: 0.72 - ETA: 0s - loss: 0.7848 - accuracy: 0.75 - ETA: 0s - loss: 0.7219 - accuracy: 0.77 - ETA: 0s - loss: 0.6985 - accuracy: 0.77 - ETA: 0s - loss: 0.6848 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6831 - accuracy: 0.7760 - val_loss: 0.7381 - val_accuracy: 0.7522\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.90 - ETA: 0s - loss: 0.4737 - accuracy: 0.85 - ETA: 0s - loss: 0.5564 - accuracy: 0.81 - ETA: 0s - loss: 0.5770 - accuracy: 0.80 - ETA: 0s - loss: 0.5768 - accuracy: 0.80 - ETA: 0s - loss: 0.5704 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5706 - accuracy: 0.8081 - val_loss: 1.0325 - val_accuracy: 0.7254\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.87 - ETA: 0s - loss: 0.6892 - accuracy: 0.76 - ETA: 0s - loss: 0.6258 - accuracy: 0.79 - ETA: 0s - loss: 0.6130 - accuracy: 0.79 - ETA: 0s - loss: 0.5813 - accuracy: 0.80 - ETA: 0s - loss: 0.5822 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5795 - accuracy: 0.8081 - val_loss: 0.7216 - val_accuracy: 0.7612\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.75 - ETA: 0s - loss: 0.5690 - accuracy: 0.81 - ETA: 0s - loss: 0.5748 - accuracy: 0.81 - ETA: 0s - loss: 0.5761 - accuracy: 0.81 - ETA: 0s - loss: 0.5733 - accuracy: 0.82 - ETA: 0s - loss: 0.5730 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5771 - accuracy: 0.8126 - val_loss: 1.2738 - val_accuracy: 0.7403\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5743 - accuracy: 0.78 - ETA: 0s - loss: 0.7844 - accuracy: 0.76 - ETA: 0s - loss: 0.6868 - accuracy: 0.78 - ETA: 0s - loss: 0.6948 - accuracy: 0.78 - ETA: 0s - loss: 0.6888 - accuracy: 0.77 - ETA: 0s - loss: 0.7053 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7324 - accuracy: 0.7801 - val_loss: 1.0391 - val_accuracy: 0.7090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6014 - accuracy: 0.78 - ETA: 0s - loss: 0.8934 - accuracy: 0.74 - ETA: 0s - loss: 0.8423 - accuracy: 0.74 - ETA: 0s - loss: 0.9033 - accuracy: 0.73 - ETA: 0s - loss: 0.8789 - accuracy: 0.72 - ETA: 0s - loss: 0.8690 - accuracy: 0.72 - 0s 4ms/step - loss: 0.8554 - accuracy: 0.7197 - val_loss: 0.6618 - val_accuracy: 0.7060\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.78 - ETA: 0s - loss: 0.6552 - accuracy: 0.73 - ETA: 0s - loss: 0.6659 - accuracy: 0.71 - ETA: 0s - loss: 0.6698 - accuracy: 0.70 - ETA: 0s - loss: 0.6643 - accuracy: 0.70 - ETA: 0s - loss: 0.6646 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6657 - accuracy: 0.7040 - val_loss: 0.7133 - val_accuracy: 0.7090\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6754 - accuracy: 0.75 - ETA: 0s - loss: 0.6803 - accuracy: 0.69 - ETA: 0s - loss: 0.6756 - accuracy: 0.69 - ETA: 0s - loss: 0.6705 - accuracy: 0.70 - ETA: 0s - loss: 0.6702 - accuracy: 0.70 - ETA: 0s - loss: 0.6678 - accuracy: 0.71 - ETA: 0s - loss: 0.6696 - accuracy: 0.70 - ETA: 0s - loss: 0.6644 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6608 - accuracy: 0.7200 - val_loss: 0.6876 - val_accuracy: 0.7313\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.75 - ETA: 0s - loss: 0.6193 - accuracy: 0.77 - ETA: 0s - loss: 0.6316 - accuracy: 0.79 - ETA: 0s - loss: 0.6285 - accuracy: 0.78 - ETA: 0s - loss: 0.6341 - accuracy: 0.77 - ETA: 0s - loss: 0.6350 - accuracy: 0.77 - ETA: 0s - loss: 0.6314 - accuracy: 0.77 - ETA: 0s - loss: 0.6318 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6335 - accuracy: 0.7645 - val_loss: 0.7451 - val_accuracy: 0.7269\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.68 - ETA: 0s - loss: 0.6171 - accuracy: 0.76 - ETA: 0s - loss: 0.5987 - accuracy: 0.77 - ETA: 0s - loss: 0.5943 - accuracy: 0.78 - ETA: 0s - loss: 0.5988 - accuracy: 0.77 - ETA: 0s - loss: 0.6023 - accuracy: 0.77 - ETA: 0s - loss: 0.5973 - accuracy: 0.78 - ETA: 0s - loss: 0.6068 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6068 - accuracy: 0.7742 - val_loss: 0.8073 - val_accuracy: 0.7179\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.71 - ETA: 0s - loss: 0.6409 - accuracy: 0.74 - ETA: 0s - loss: 0.7136 - accuracy: 0.75 - ETA: 0s - loss: 0.7114 - accuracy: 0.76 - ETA: 0s - loss: 0.6907 - accuracy: 0.76 - ETA: 0s - loss: 0.6773 - accuracy: 0.76 - ETA: 0s - loss: 0.6688 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6650 - accuracy: 0.7626 - val_loss: 0.9522 - val_accuracy: 0.7358\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.87 - ETA: 0s - loss: 0.5891 - accuracy: 0.77 - ETA: 0s - loss: 0.7609 - accuracy: 0.76 - ETA: 0s - loss: 0.7266 - accuracy: 0.75 - ETA: 0s - loss: 0.7012 - accuracy: 0.75 - ETA: 0s - loss: 0.7231 - accuracy: 0.74 - 0s 4ms/step - loss: 0.7157 - accuracy: 0.7492 - val_loss: 0.7057 - val_accuracy: 0.6970\n",
      "Epoch 27/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.6510 - accuracy: 0.75 - ETA: 0s - loss: 0.7258 - accuracy: 0.74 - ETA: 0s - loss: 0.6900 - accuracy: 0.76 - ETA: 0s - loss: 0.6835 - accuracy: 0.76 - ETA: 0s - loss: 0.6813 - accuracy: 0.75 - ETA: 0s - loss: 0.6700 - accuracy: 0.7571Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6721 - accuracy: 0.7521 - val_loss: 0.7836 - val_accuracy: 0.7328\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3817 - accuracy: 0.59 - ETA: 0s - loss: 2.8189 - accuracy: 0.61 - ETA: 0s - loss: 1.9083 - accuracy: 0.62 - ETA: 0s - loss: 1.5303 - accuracy: 0.64 - ETA: 0s - loss: 1.3211 - accuracy: 0.65 - ETA: 0s - loss: 1.1888 - accuracy: 0.66 - ETA: 0s - loss: 1.0995 - accuracy: 0.66 - 1s 7ms/step - loss: 1.0976 - accuracy: 0.6652 - val_loss: 0.5926 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.68 - ETA: 0s - loss: 0.6005 - accuracy: 0.70 - ETA: 0s - loss: 0.5881 - accuracy: 0.71 - ETA: 0s - loss: 0.5705 - accuracy: 0.73 - ETA: 0s - loss: 0.5914 - accuracy: 0.73 - ETA: 0s - loss: 0.5794 - accuracy: 0.74 - ETA: 0s - loss: 0.5774 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5792 - accuracy: 0.7518 - val_loss: 0.6123 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.78 - ETA: 0s - loss: 0.5237 - accuracy: 0.79 - ETA: 0s - loss: 0.5200 - accuracy: 0.79 - ETA: 0s - loss: 0.5098 - accuracy: 0.80 - ETA: 0s - loss: 0.5099 - accuracy: 0.80 - ETA: 0s - loss: 0.5232 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5262 - accuracy: 0.7902 - val_loss: 0.5423 - val_accuracy: 0.7403\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.78 - ETA: 0s - loss: 0.4903 - accuracy: 0.80 - ETA: 0s - loss: 0.4883 - accuracy: 0.82 - ETA: 0s - loss: 0.5011 - accuracy: 0.81 - ETA: 0s - loss: 0.5102 - accuracy: 0.81 - ETA: 0s - loss: 0.5130 - accuracy: 0.81 - ETA: 0s - loss: 0.5095 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5113 - accuracy: 0.8130 - val_loss: 0.7080 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.87 - ETA: 0s - loss: 0.5572 - accuracy: 0.81 - ETA: 0s - loss: 0.5102 - accuracy: 0.82 - ETA: 0s - loss: 0.4854 - accuracy: 0.82 - ETA: 0s - loss: 0.4761 - accuracy: 0.82 - ETA: 0s - loss: 0.4874 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4950 - accuracy: 0.8205 - val_loss: 0.6587 - val_accuracy: 0.7060\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3708 - accuracy: 0.90 - ETA: 0s - loss: 0.4486 - accuracy: 0.84 - ETA: 0s - loss: 0.4377 - accuracy: 0.85 - ETA: 0s - loss: 0.4432 - accuracy: 0.84 - ETA: 0s - loss: 0.4643 - accuracy: 0.84 - ETA: 0s - loss: 0.4702 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4819 - accuracy: 0.8335 - val_loss: 0.5534 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.81 - ETA: 0s - loss: 0.4551 - accuracy: 0.84 - ETA: 0s - loss: 0.4705 - accuracy: 0.84 - ETA: 0s - loss: 0.4877 - accuracy: 0.83 - ETA: 0s - loss: 0.5030 - accuracy: 0.82 - ETA: 0s - loss: 0.4972 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4971 - accuracy: 0.8331 - val_loss: 0.5646 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.90 - ETA: 0s - loss: 0.4949 - accuracy: 0.85 - ETA: 0s - loss: 0.4927 - accuracy: 0.84 - ETA: 0s - loss: 0.4872 - accuracy: 0.84 - ETA: 0s - loss: 0.5059 - accuracy: 0.83 - ETA: 0s - loss: 0.5230 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5226 - accuracy: 0.8294 - val_loss: 0.5988 - val_accuracy: 0.7448\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.81 - ETA: 0s - loss: 0.5465 - accuracy: 0.80 - ETA: 0s - loss: 0.5676 - accuracy: 0.78 - ETA: 0s - loss: 0.5472 - accuracy: 0.80 - ETA: 0s - loss: 0.5600 - accuracy: 0.80 - ETA: 0s - loss: 0.6177 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6137 - accuracy: 0.8081 - val_loss: 1.2096 - val_accuracy: 0.6776\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0063 - accuracy: 0.65 - ETA: 0s - loss: 0.6302 - accuracy: 0.77 - ETA: 0s - loss: 0.6494 - accuracy: 0.77 - ETA: 0s - loss: 0.6453 - accuracy: 0.77 - ETA: 0s - loss: 0.6372 - accuracy: 0.78 - ETA: 0s - loss: 0.6626 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6664 - accuracy: 0.7671 - val_loss: 1.0027 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.87 - ETA: 0s - loss: 0.6451 - accuracy: 0.79 - ETA: 0s - loss: 0.6227 - accuracy: 0.78 - ETA: 0s - loss: 0.6247 - accuracy: 0.78 - ETA: 0s - loss: 0.6273 - accuracy: 0.79 - ETA: 0s - loss: 0.6935 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6966 - accuracy: 0.7801 - val_loss: 0.7865 - val_accuracy: 0.7060\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.78 - ETA: 0s - loss: 0.8256 - accuracy: 0.71 - ETA: 0s - loss: 0.7602 - accuracy: 0.73 - ETA: 0s - loss: 0.7364 - accuracy: 0.73 - ETA: 0s - loss: 0.7063 - accuracy: 0.73 - ETA: 0s - loss: 0.6937 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6900 - accuracy: 0.7342 - val_loss: 0.6441 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6043 - accuracy: 0.75 - ETA: 0s - loss: 0.6635 - accuracy: 0.75 - ETA: 0s - loss: 0.6808 - accuracy: 0.73 - ETA: 0s - loss: 0.6717 - accuracy: 0.74 - ETA: 0s - loss: 0.6565 - accuracy: 0.75 - ETA: 0s - loss: 0.6497 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6471 - accuracy: 0.7581 - val_loss: 0.6469 - val_accuracy: 0.7403\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.75 - ETA: 0s - loss: 0.6023 - accuracy: 0.77 - ETA: 0s - loss: 0.5957 - accuracy: 0.78 - ETA: 0s - loss: 0.5957 - accuracy: 0.78 - ETA: 0s - loss: 0.6472 - accuracy: 0.77 - ETA: 0s - loss: 0.6412 - accuracy: 0.77 - ETA: 0s - loss: 0.6456 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6434 - accuracy: 0.7686 - val_loss: 0.8573 - val_accuracy: 0.7209\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.81 - ETA: 0s - loss: 0.9745 - accuracy: 0.74 - ETA: 0s - loss: 0.8159 - accuracy: 0.74 - ETA: 0s - loss: 0.7373 - accuracy: 0.76 - ETA: 0s - loss: 0.7546 - accuracy: 0.74 - ETA: 0s - loss: 0.7248 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7049 - accuracy: 0.7536 - val_loss: 1.1142 - val_accuracy: 0.7373\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6400 - accuracy: 0.71 - ETA: 0s - loss: 0.6959 - accuracy: 0.75 - ETA: 0s - loss: 0.7405 - accuracy: 0.76 - ETA: 0s - loss: 0.7318 - accuracy: 0.75 - ETA: 0s - loss: 0.7186 - accuracy: 0.75 - ETA: 0s - loss: 0.6977 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6969 - accuracy: 0.7499 - val_loss: 1.0211 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.81 - ETA: 0s - loss: 0.6110 - accuracy: 0.78 - ETA: 0s - loss: 0.6803 - accuracy: 0.76 - ETA: 0s - loss: 0.6835 - accuracy: 0.76 - ETA: 0s - loss: 0.6814 - accuracy: 0.76 - ETA: 0s - loss: 0.6788 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6767 - accuracy: 0.7533 - val_loss: 0.8000 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.75 - ETA: 0s - loss: 0.6084 - accuracy: 0.77 - ETA: 0s - loss: 0.6280 - accuracy: 0.76 - ETA: 0s - loss: 0.6607 - accuracy: 0.75 - ETA: 0s - loss: 0.6767 - accuracy: 0.75 - ETA: 0s - loss: 0.6573 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6657 - accuracy: 0.7581 - val_loss: 0.8622 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.71 - ETA: 0s - loss: 0.6466 - accuracy: 0.76 - ETA: 0s - loss: 0.6740 - accuracy: 0.75 - ETA: 0s - loss: 0.6880 - accuracy: 0.74 - ETA: 0s - loss: 0.6869 - accuracy: 0.73 - ETA: 0s - loss: 0.6772 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6769 - accuracy: 0.7234 - val_loss: 1.4842 - val_accuracy: 0.7224\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7584 - accuracy: 0.62 - ETA: 0s - loss: 0.6430 - accuracy: 0.71 - ETA: 0s - loss: 0.6595 - accuracy: 0.70 - ETA: 0s - loss: 0.6568 - accuracy: 0.70 - ETA: 0s - loss: 0.6525 - accuracy: 0.70 - ETA: 0s - loss: 0.6513 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6490 - accuracy: 0.7178 - val_loss: 1.6878 - val_accuracy: 0.7224\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.78 - ETA: 0s - loss: 0.6324 - accuracy: 0.72 - ETA: 0s - loss: 0.6438 - accuracy: 0.72 - ETA: 0s - loss: 0.6901 - accuracy: 0.72 - ETA: 0s - loss: 0.6946 - accuracy: 0.72 - ETA: 0s - loss: 0.6903 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6897 - accuracy: 0.7212 - val_loss: 1.0873 - val_accuracy: 0.7030\n",
      "Epoch 22/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.6676 - accuracy: 0.70 - ETA: 0s - loss: 0.6852 - accuracy: 0.68 - ETA: 0s - loss: 0.6809 - accuracy: 0.68 - ETA: 0s - loss: 0.6815 - accuracy: 0.68 - ETA: 0s - loss: 0.6743 - accuracy: 0.6879Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.6850 - val_loss: 1.4402 - val_accuracy: 0.7075\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5953 - accuracy: 0.65 - ETA: 0s - loss: 2.1665 - accuracy: 0.63 - ETA: 0s - loss: 1.6238 - accuracy: 0.63 - ETA: 0s - loss: 1.3208 - accuracy: 0.65 - ETA: 0s - loss: 1.1619 - accuracy: 0.66 - ETA: 0s - loss: 1.0606 - accuracy: 0.67 - 0s 5ms/step - loss: 1.0178 - accuracy: 0.6865 - val_loss: 0.5891 - val_accuracy: 0.7388\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.71 - ETA: 0s - loss: 0.5683 - accuracy: 0.75 - ETA: 0s - loss: 0.5989 - accuracy: 0.74 - ETA: 0s - loss: 0.6020 - accuracy: 0.74 - ETA: 0s - loss: 0.5927 - accuracy: 0.75 - ETA: 0s - loss: 0.5946 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5954 - accuracy: 0.7507 - val_loss: 0.6097 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.75 - ETA: 0s - loss: 0.6043 - accuracy: 0.80 - ETA: 0s - loss: 0.5911 - accuracy: 0.79 - ETA: 0s - loss: 0.5889 - accuracy: 0.79 - ETA: 0s - loss: 0.5874 - accuracy: 0.78 - ETA: 0s - loss: 0.5794 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5775 - accuracy: 0.7828 - val_loss: 0.5802 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.78 - ETA: 0s - loss: 0.5587 - accuracy: 0.76 - ETA: 0s - loss: 0.5334 - accuracy: 0.79 - ETA: 0s - loss: 0.5171 - accuracy: 0.80 - ETA: 0s - loss: 0.5263 - accuracy: 0.80 - ETA: 0s - loss: 0.5299 - accuracy: 0.81 - ETA: 0s - loss: 0.5345 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5345 - accuracy: 0.8078 - val_loss: 0.5533 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7388 - accuracy: 0.84 - ETA: 0s - loss: 0.5355 - accuracy: 0.83 - ETA: 0s - loss: 0.5442 - accuracy: 0.82 - ETA: 0s - loss: 0.5369 - accuracy: 0.82 - ETA: 0s - loss: 0.5254 - accuracy: 0.82 - ETA: 0s - loss: 0.5196 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5208 - accuracy: 0.8219 - val_loss: 0.5607 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5539 - accuracy: 0.75 - ETA: 0s - loss: 0.5025 - accuracy: 0.82 - ETA: 0s - loss: 0.5072 - accuracy: 0.81 - ETA: 0s - loss: 0.4994 - accuracy: 0.82 - ETA: 0s - loss: 0.4935 - accuracy: 0.82 - ETA: 0s - loss: 0.5106 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5172 - accuracy: 0.8205 - val_loss: 0.5942 - val_accuracy: 0.7119\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.78 - ETA: 0s - loss: 0.5506 - accuracy: 0.80 - ETA: 0s - loss: 0.5074 - accuracy: 0.82 - ETA: 0s - loss: 0.5133 - accuracy: 0.82 - ETA: 0s - loss: 0.5000 - accuracy: 0.82 - ETA: 0s - loss: 0.5005 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4977 - accuracy: 0.8268 - val_loss: 0.5954 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.87 - ETA: 0s - loss: 0.4347 - accuracy: 0.86 - ETA: 0s - loss: 0.4712 - accuracy: 0.84 - ETA: 0s - loss: 0.4704 - accuracy: 0.84 - ETA: 0s - loss: 0.4862 - accuracy: 0.83 - ETA: 0s - loss: 0.5032 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5028 - accuracy: 0.8324 - val_loss: 0.6002 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.78 - ETA: 0s - loss: 0.4379 - accuracy: 0.85 - ETA: 0s - loss: 0.4411 - accuracy: 0.85 - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - ETA: 0s - loss: 0.4743 - accuracy: 0.85 - ETA: 0s - loss: 0.5113 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5156 - accuracy: 0.8384 - val_loss: 0.6398 - val_accuracy: 0.7313\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.78 - ETA: 0s - loss: 0.5253 - accuracy: 0.82 - ETA: 0s - loss: 0.5235 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.82 - ETA: 0s - loss: 0.4993 - accuracy: 0.83 - ETA: 0s - loss: 0.4976 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4999 - accuracy: 0.8313 - val_loss: 0.6328 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.71 - ETA: 0s - loss: 0.4941 - accuracy: 0.86 - ETA: 0s - loss: 0.5141 - accuracy: 0.84 - ETA: 0s - loss: 0.5143 - accuracy: 0.83 - ETA: 0s - loss: 0.5134 - accuracy: 0.83 - ETA: 0s - loss: 0.5200 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5174 - accuracy: 0.8320 - val_loss: 0.7515 - val_accuracy: 0.7463\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.78 - ETA: 0s - loss: 0.4916 - accuracy: 0.84 - ETA: 0s - loss: 0.4839 - accuracy: 0.84 - ETA: 0s - loss: 0.4801 - accuracy: 0.84 - ETA: 0s - loss: 0.4812 - accuracy: 0.84 - ETA: 0s - loss: 0.4714 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4750 - accuracy: 0.8507 - val_loss: 0.8163 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7379 - accuracy: 0.90 - ETA: 0s - loss: 0.4912 - accuracy: 0.85 - ETA: 0s - loss: 0.4925 - accuracy: 0.85 - ETA: 0s - loss: 0.4859 - accuracy: 0.85 - ETA: 0s - loss: 0.4755 - accuracy: 0.85 - ETA: 0s - loss: 0.4714 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4686 - accuracy: 0.8567 - val_loss: 0.7758 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.90 - ETA: 0s - loss: 0.4182 - accuracy: 0.87 - ETA: 0s - loss: 0.4605 - accuracy: 0.87 - ETA: 0s - loss: 0.4752 - accuracy: 0.87 - ETA: 0s - loss: 0.4611 - accuracy: 0.86 - ETA: 0s - loss: 0.4646 - accuracy: 0.8693Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4631 - accuracy: 0.8694 - val_loss: 0.7149 - val_accuracy: 0.7299\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: db6a7c765083ce6fbb1461d1d24b3d56</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7522388100624084</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8231202626059463</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 60</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.75 - ETA: 0s - loss: 4.6359 - accuracy: 0.57 - ETA: 0s - loss: 2.9695 - accuracy: 0.61 - ETA: 0s - loss: 2.2001 - accuracy: 0.63 - ETA: 0s - loss: 1.8750 - accuracy: 0.62 - ETA: 0s - loss: 1.6694 - accuracy: 0.61 - ETA: 0s - loss: 1.4955 - accuracy: 0.63 - ETA: 0s - loss: 1.3731 - accuracy: 0.64 - 1s 7ms/step - loss: 1.3679 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.6940\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.81 - ETA: 0s - loss: 0.6480 - accuracy: 0.74 - ETA: 0s - loss: 0.6399 - accuracy: 0.72 - ETA: 0s - loss: 0.6250 - accuracy: 0.73 - ETA: 0s - loss: 0.6174 - accuracy: 0.74 - ETA: 0s - loss: 0.6144 - accuracy: 0.74 - ETA: 0s - loss: 0.6119 - accuracy: 0.74 - ETA: 0s - loss: 0.6009 - accuracy: 0.75 - 0s 6ms/step - loss: 0.6009 - accuracy: 0.7525 - val_loss: 0.5743 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.68 - ETA: 0s - loss: 0.5328 - accuracy: 0.79 - ETA: 0s - loss: 0.5501 - accuracy: 0.77 - ETA: 0s - loss: 0.5493 - accuracy: 0.77 - ETA: 0s - loss: 0.5567 - accuracy: 0.77 - ETA: 0s - loss: 0.5596 - accuracy: 0.77 - ETA: 0s - loss: 0.5583 - accuracy: 0.77 - ETA: 0s - loss: 0.5628 - accuracy: 0.77 - ETA: 0s - loss: 0.5685 - accuracy: 0.77 - 1s 7ms/step - loss: 0.5661 - accuracy: 0.7742 - val_loss: 0.5681 - val_accuracy: 0.7493\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.68 - ETA: 0s - loss: 0.5429 - accuracy: 0.79 - ETA: 0s - loss: 0.5645 - accuracy: 0.78 - ETA: 0s - loss: 0.5209 - accuracy: 0.81 - ETA: 0s - loss: 0.5249 - accuracy: 0.81 - ETA: 0s - loss: 0.5287 - accuracy: 0.80 - ETA: 0s - loss: 0.5363 - accuracy: 0.80 - ETA: 0s - loss: 0.5461 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5483 - accuracy: 0.8059 - val_loss: 0.5662 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.87 - ETA: 0s - loss: 0.5441 - accuracy: 0.81 - ETA: 0s - loss: 0.5632 - accuracy: 0.80 - ETA: 0s - loss: 0.5595 - accuracy: 0.80 - ETA: 0s - loss: 0.5497 - accuracy: 0.80 - ETA: 0s - loss: 0.5564 - accuracy: 0.80 - ETA: 0s - loss: 0.5457 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5548 - accuracy: 0.8037 - val_loss: 0.5656 - val_accuracy: 0.7493\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.93 - ETA: 0s - loss: 0.4826 - accuracy: 0.84 - ETA: 0s - loss: 0.5253 - accuracy: 0.81 - ETA: 0s - loss: 0.5285 - accuracy: 0.80 - ETA: 0s - loss: 0.5339 - accuracy: 0.80 - ETA: 0s - loss: 0.5570 - accuracy: 0.80 - ETA: 0s - loss: 0.5601 - accuracy: 0.80 - ETA: 0s - loss: 0.5572 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5596 - accuracy: 0.8070 - val_loss: 0.6640 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.75 - ETA: 0s - loss: 0.5247 - accuracy: 0.82 - ETA: 0s - loss: 0.5335 - accuracy: 0.81 - ETA: 0s - loss: 0.5162 - accuracy: 0.82 - ETA: 0s - loss: 0.5089 - accuracy: 0.83 - ETA: 0s - loss: 0.5170 - accuracy: 0.82 - ETA: 0s - loss: 0.5232 - accuracy: 0.82 - ETA: 0s - loss: 0.5375 - accuracy: 0.81 - ETA: 0s - loss: 0.5425 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5450 - accuracy: 0.8130 - val_loss: 0.6141 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.87 - ETA: 0s - loss: 0.5010 - accuracy: 0.83 - ETA: 0s - loss: 0.5016 - accuracy: 0.83 - ETA: 0s - loss: 0.5162 - accuracy: 0.83 - ETA: 0s - loss: 0.5211 - accuracy: 0.83 - ETA: 0s - loss: 0.5190 - accuracy: 0.82 - ETA: 0s - loss: 0.5315 - accuracy: 0.81 - ETA: 0s - loss: 0.5509 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5582 - accuracy: 0.8100 - val_loss: 0.8259 - val_accuracy: 0.7463\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.81 - ETA: 0s - loss: 0.6518 - accuracy: 0.75 - ETA: 0s - loss: 0.6508 - accuracy: 0.76 - ETA: 0s - loss: 0.6774 - accuracy: 0.76 - ETA: 0s - loss: 0.6710 - accuracy: 0.76 - ETA: 0s - loss: 0.6653 - accuracy: 0.76 - ETA: 0s - loss: 0.6483 - accuracy: 0.77 - ETA: 0s - loss: 0.6586 - accuracy: 0.77 - 1s 6ms/step - loss: 0.6632 - accuracy: 0.7730 - val_loss: 0.5782 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.9102 - accuracy: 0.78 - ETA: 0s - loss: 0.7382 - accuracy: 0.77 - ETA: 0s - loss: 0.7205 - accuracy: 0.76 - ETA: 0s - loss: 0.6877 - accuracy: 0.77 - ETA: 0s - loss: 0.6462 - accuracy: 0.79 - ETA: 0s - loss: 0.6655 - accuracy: 0.78 - ETA: 0s - loss: 0.6574 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6454 - accuracy: 0.7809 - val_loss: 0.6453 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.84 - ETA: 0s - loss: 0.6475 - accuracy: 0.77 - ETA: 0s - loss: 0.6138 - accuracy: 0.77 - ETA: 0s - loss: 0.6107 - accuracy: 0.77 - ETA: 0s - loss: 0.6046 - accuracy: 0.78 - ETA: 0s - loss: 0.6253 - accuracy: 0.77 - ETA: 0s - loss: 0.6291 - accuracy: 0.77 - ETA: 0s - loss: 0.6360 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6365 - accuracy: 0.7753 - val_loss: 0.6013 - val_accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.71 - ETA: 0s - loss: 0.7948 - accuracy: 0.72 - ETA: 0s - loss: 0.7322 - accuracy: 0.74 - ETA: 0s - loss: 0.6979 - accuracy: 0.75 - ETA: 0s - loss: 0.6955 - accuracy: 0.75 - ETA: 0s - loss: 0.6774 - accuracy: 0.76 - ETA: 0s - loss: 0.6731 - accuracy: 0.76 - ETA: 0s - loss: 0.6891 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6894 - accuracy: 0.7626 - val_loss: 0.6369 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.81 - ETA: 0s - loss: 0.6393 - accuracy: 0.71 - ETA: 0s - loss: 0.6377 - accuracy: 0.71 - ETA: 0s - loss: 0.6400 - accuracy: 0.71 - ETA: 0s - loss: 0.6386 - accuracy: 0.71 - ETA: 0s - loss: 0.6343 - accuracy: 0.70 - ETA: 0s - loss: 0.6381 - accuracy: 0.71 - ETA: 0s - loss: 0.6393 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6410 - accuracy: 0.7126 - val_loss: 0.6249 - val_accuracy: 0.7269\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8086 - accuracy: 0.53 - ETA: 0s - loss: 0.6260 - accuracy: 0.73 - ETA: 0s - loss: 0.6453 - accuracy: 0.73 - ETA: 0s - loss: 0.6442 - accuracy: 0.71 - ETA: 0s - loss: 0.6529 - accuracy: 0.69 - ETA: 0s - loss: 0.6606 - accuracy: 0.67 - ETA: 0s - loss: 0.6632 - accuracy: 0.66 - 0s 5ms/step - loss: 0.6590 - accuracy: 0.6719 - val_loss: 0.6859 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.84 - ETA: 0s - loss: 0.6319 - accuracy: 0.76 - ETA: 0s - loss: 0.6148 - accuracy: 0.77 - ETA: 0s - loss: 0.6328 - accuracy: 0.76 - ETA: 0s - loss: 0.6322 - accuracy: 0.75 - ETA: 0s - loss: 0.6340 - accuracy: 0.75 - ETA: 0s - loss: 0.6330 - accuracy: 0.75 - ETA: 0s - loss: 0.6309 - accuracy: 0.75 - ETA: 0s - loss: 0.6267 - accuracy: 0.75 - ETA: 0s - loss: 0.6202 - accuracy: 0.76 - 1s 6ms/step - loss: 0.6202 - accuracy: 0.7611 - val_loss: 0.7656 - val_accuracy: 0.7403\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.65 - ETA: 0s - loss: 0.6265 - accuracy: 0.75 - ETA: 0s - loss: 0.6288 - accuracy: 0.74 - ETA: 0s - loss: 0.6798 - accuracy: 0.76 - ETA: 0s - loss: 0.6584 - accuracy: 0.76 - ETA: 0s - loss: 0.6597 - accuracy: 0.76 - ETA: 0s - loss: 0.6568 - accuracy: 0.75 - ETA: 0s - loss: 0.6542 - accuracy: 0.75 - ETA: 0s - loss: 0.6579 - accuracy: 0.75 - 1s 6ms/step - loss: 0.6536 - accuracy: 0.7600 - val_loss: 1.6660 - val_accuracy: 0.7060\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.75 - ETA: 0s - loss: 1.0111 - accuracy: 0.75 - ETA: 0s - loss: 0.8998 - accuracy: 0.74 - ETA: 0s - loss: 0.8185 - accuracy: 0.74 - ETA: 0s - loss: 0.7858 - accuracy: 0.72 - ETA: 0s - loss: 0.7561 - accuracy: 0.69 - ETA: 0s - loss: 0.7331 - accuracy: 0.69 - ETA: 0s - loss: 0.7236 - accuracy: 0.69 - ETA: 0s - loss: 0.7122 - accuracy: 0.69 - 1s 6ms/step - loss: 0.7099 - accuracy: 0.6932 - val_loss: 0.9855 - val_accuracy: 0.7313\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.68 - ETA: 0s - loss: 0.6528 - accuracy: 0.59 - ETA: 0s - loss: 0.6370 - accuracy: 0.61 - ETA: 0s - loss: 0.6422 - accuracy: 0.65 - ETA: 0s - loss: 0.6406 - accuracy: 0.67 - ETA: 0s - loss: 0.6270 - accuracy: 0.69 - ETA: 0s - loss: 0.8425 - accuracy: 0.70 - ETA: 0s - loss: 0.8211 - accuracy: 0.70 - 0s 5ms/step - loss: 0.8159 - accuracy: 0.7021 - val_loss: 0.6595 - val_accuracy: 0.6970\n",
      "Epoch 19/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7805 - accuracy: 0.53 - ETA: 0s - loss: 0.7469 - accuracy: 0.53 - ETA: 0s - loss: 0.7183 - accuracy: 0.50 - ETA: 0s - loss: 0.7071 - accuracy: 0.50 - ETA: 0s - loss: 0.7015 - accuracy: 0.54 - ETA: 0s - loss: 0.6975 - accuracy: 0.57 - ETA: 0s - loss: 0.6966 - accuracy: 0.58 - ETA: 0s - loss: 0.6971 - accuracy: 0.58 - ETA: 0s - loss: 0.6987 - accuracy: 0.5810Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.5801 - val_loss: 0.6800 - val_accuracy: 0.6955\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1324 - accuracy: 0.50 - ETA: 0s - loss: 4.2867 - accuracy: 0.54 - ETA: 0s - loss: 2.7746 - accuracy: 0.60 - ETA: 0s - loss: 2.2600 - accuracy: 0.61 - ETA: 0s - loss: 1.8628 - accuracy: 0.63 - ETA: 0s - loss: 1.6097 - accuracy: 0.65 - ETA: 0s - loss: 1.4358 - accuracy: 0.66 - ETA: 0s - loss: 1.3080 - accuracy: 0.67 - 1s 7ms/step - loss: 1.2266 - accuracy: 0.6820 - val_loss: 0.5909 - val_accuracy: 0.7373\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5229 - accuracy: 0.87 - ETA: 0s - loss: 0.6291 - accuracy: 0.75 - ETA: 0s - loss: 0.6185 - accuracy: 0.75 - ETA: 0s - loss: 0.6196 - accuracy: 0.75 - ETA: 0s - loss: 0.6080 - accuracy: 0.76 - ETA: 0s - loss: 0.6100 - accuracy: 0.76 - ETA: 0s - loss: 0.6070 - accuracy: 0.76 - ETA: 0s - loss: 0.6071 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6067 - accuracy: 0.7622 - val_loss: 0.5702 - val_accuracy: 0.7493\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.68 - ETA: 0s - loss: 0.6134 - accuracy: 0.74 - ETA: 0s - loss: 0.5904 - accuracy: 0.77 - ETA: 0s - loss: 0.5815 - accuracy: 0.78 - ETA: 0s - loss: 0.5893 - accuracy: 0.77 - ETA: 0s - loss: 0.5847 - accuracy: 0.77 - ETA: 0s - loss: 0.5785 - accuracy: 0.78 - ETA: 0s - loss: 0.5802 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5898 - accuracy: 0.7760 - val_loss: 0.6282 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.84 - ETA: 0s - loss: 0.5121 - accuracy: 0.83 - ETA: 0s - loss: 0.5402 - accuracy: 0.81 - ETA: 0s - loss: 0.5768 - accuracy: 0.79 - ETA: 0s - loss: 0.5756 - accuracy: 0.78 - ETA: 0s - loss: 0.5784 - accuracy: 0.78 - ETA: 0s - loss: 0.5774 - accuracy: 0.78 - ETA: 0s - loss: 0.5961 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6012 - accuracy: 0.7880 - val_loss: 0.6481 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.75 - ETA: 0s - loss: 0.5669 - accuracy: 0.77 - ETA: 0s - loss: 0.5794 - accuracy: 0.76 - ETA: 0s - loss: 0.5999 - accuracy: 0.76 - ETA: 0s - loss: 0.5944 - accuracy: 0.77 - ETA: 0s - loss: 0.6019 - accuracy: 0.76 - ETA: 0s - loss: 0.5873 - accuracy: 0.77 - ETA: 0s - loss: 0.5851 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5886 - accuracy: 0.7768 - val_loss: 0.6053 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.76 - ETA: 0s - loss: 0.6407 - accuracy: 0.76 - ETA: 0s - loss: 0.6358 - accuracy: 0.76 - ETA: 0s - loss: 0.6129 - accuracy: 0.78 - ETA: 0s - loss: 0.6020 - accuracy: 0.78 - ETA: 0s - loss: 0.6025 - accuracy: 0.78 - ETA: 0s - loss: 0.5956 - accuracy: 0.79 - 0s 6ms/step - loss: 0.6036 - accuracy: 0.7846 - val_loss: 0.5695 - val_accuracy: 0.7657\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.75 - ETA: 0s - loss: 0.5061 - accuracy: 0.83 - ETA: 0s - loss: 0.5287 - accuracy: 0.82 - ETA: 0s - loss: 0.5334 - accuracy: 0.81 - ETA: 0s - loss: 0.5381 - accuracy: 0.81 - ETA: 0s - loss: 0.5587 - accuracy: 0.80 - ETA: 0s - loss: 0.5700 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5702 - accuracy: 0.7981 - val_loss: 0.5854 - val_accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6014 - accuracy: 0.71 - ETA: 0s - loss: 0.5806 - accuracy: 0.77 - ETA: 0s - loss: 0.5900 - accuracy: 0.77 - ETA: 0s - loss: 0.6036 - accuracy: 0.78 - ETA: 0s - loss: 0.6264 - accuracy: 0.77 - ETA: 0s - loss: 0.6207 - accuracy: 0.77 - ETA: 0s - loss: 0.6264 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6310 - accuracy: 0.7723 - val_loss: 0.5942 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3068 - accuracy: 0.65 - ETA: 0s - loss: 0.6864 - accuracy: 0.74 - ETA: 0s - loss: 0.6609 - accuracy: 0.74 - ETA: 0s - loss: 0.7095 - accuracy: 0.73 - ETA: 0s - loss: 0.6904 - accuracy: 0.74 - ETA: 0s - loss: 0.6717 - accuracy: 0.74 - ETA: 0s - loss: 0.6562 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6606 - accuracy: 0.7592 - val_loss: 0.5968 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.81 - ETA: 0s - loss: 0.6253 - accuracy: 0.79 - ETA: 0s - loss: 0.6651 - accuracy: 0.76 - ETA: 0s - loss: 0.6544 - accuracy: 0.75 - ETA: 0s - loss: 0.6457 - accuracy: 0.74 - ETA: 0s - loss: 0.6687 - accuracy: 0.74 - ETA: 0s - loss: 0.6607 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6607 - accuracy: 0.7488 - val_loss: 0.7287 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0222 - accuracy: 0.65 - ETA: 0s - loss: 0.6024 - accuracy: 0.78 - ETA: 0s - loss: 0.6130 - accuracy: 0.78 - ETA: 0s - loss: 0.6462 - accuracy: 0.75 - ETA: 0s - loss: 0.6507 - accuracy: 0.73 - ETA: 0s - loss: 0.6471 - accuracy: 0.72 - ETA: 0s - loss: 0.6454 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6408 - accuracy: 0.7372 - val_loss: 0.6359 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.84 - ETA: 0s - loss: 0.6747 - accuracy: 0.73 - ETA: 0s - loss: 0.6875 - accuracy: 0.72 - ETA: 0s - loss: 0.6606 - accuracy: 0.74 - ETA: 0s - loss: 0.6592 - accuracy: 0.74 - ETA: 0s - loss: 0.6478 - accuracy: 0.74 - ETA: 0s - loss: 0.6384 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6950 - accuracy: 0.7577 - val_loss: 0.6089 - val_accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.84 - ETA: 0s - loss: 0.5973 - accuracy: 0.81 - ETA: 0s - loss: 0.6652 - accuracy: 0.76 - ETA: 0s - loss: 0.6990 - accuracy: 0.75 - ETA: 0s - loss: 0.6810 - accuracy: 0.72 - ETA: 0s - loss: 0.6747 - accuracy: 0.71 - ETA: 0s - loss: 0.6767 - accuracy: 0.71 - ETA: 0s - loss: 0.6786 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6834 - accuracy: 0.7137 - val_loss: 0.6370 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7334 - accuracy: 0.59 - ETA: 0s - loss: 0.6836 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6911 - accuracy: 0.64 - ETA: 0s - loss: 0.6785 - accuracy: 0.66 - ETA: 0s - loss: 0.6786 - accuracy: 0.67 - ETA: 0s - loss: 0.6736 - accuracy: 0.68 - ETA: 0s - loss: 0.6659 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6716 - accuracy: 0.6902 - val_loss: 0.6328 - val_accuracy: 0.7194\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6504 - accuracy: 0.71 - ETA: 0s - loss: 0.6688 - accuracy: 0.72 - ETA: 0s - loss: 0.6796 - accuracy: 0.71 - ETA: 0s - loss: 0.6823 - accuracy: 0.71 - ETA: 0s - loss: 0.6997 - accuracy: 0.67 - ETA: 0s - loss: 0.6937 - accuracy: 0.62 - ETA: 0s - loss: 0.6954 - accuracy: 0.60 - ETA: 0s - loss: 0.6933 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6925 - accuracy: 0.6032 - val_loss: 0.6695 - val_accuracy: 0.6970\n",
      "Epoch 16/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6557 - accuracy: 0.65 - ETA: 0s - loss: 0.6841 - accuracy: 0.61 - ETA: 0s - loss: 0.6882 - accuracy: 0.63 - ETA: 0s - loss: 0.6726 - accuracy: 0.65 - ETA: 0s - loss: 0.6908 - accuracy: 0.65 - ETA: 0s - loss: 0.6893 - accuracy: 0.62 - ETA: 0s - loss: 0.6901 - accuracy: 0.61 - ETA: 0s - loss: 0.6906 - accuracy: 0.6030Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.5999 - val_loss: 0.6757 - val_accuracy: 0.6985\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.68 - ETA: 0s - loss: 4.2008 - accuracy: 0.61 - ETA: 0s - loss: 2.8648 - accuracy: 0.61 - ETA: 0s - loss: 2.3164 - accuracy: 0.61 - ETA: 0s - loss: 1.9951 - accuracy: 0.62 - ETA: 0s - loss: 1.7442 - accuracy: 0.63 - ETA: 0s - loss: 1.5686 - accuracy: 0.64 - ETA: 0s - loss: 1.4276 - accuracy: 0.65 - 1s 6ms/step - loss: 1.3937 - accuracy: 0.6581 - val_loss: 0.5933 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7280 - accuracy: 0.65 - ETA: 0s - loss: 0.6119 - accuracy: 0.74 - ETA: 0s - loss: 0.6103 - accuracy: 0.74 - ETA: 0s - loss: 0.6015 - accuracy: 0.75 - ETA: 0s - loss: 0.6005 - accuracy: 0.74 - ETA: 0s - loss: 0.5945 - accuracy: 0.75 - ETA: 0s - loss: 0.5930 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5948 - accuracy: 0.7555 - val_loss: 0.5749 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6054 - accuracy: 0.81 - ETA: 0s - loss: 0.5554 - accuracy: 0.79 - ETA: 0s - loss: 0.5276 - accuracy: 0.80 - ETA: 0s - loss: 0.5553 - accuracy: 0.79 - ETA: 0s - loss: 0.5963 - accuracy: 0.77 - ETA: 0s - loss: 0.5991 - accuracy: 0.76 - ETA: 0s - loss: 0.5980 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5915 - accuracy: 0.7637 - val_loss: 0.5890 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.68 - ETA: 0s - loss: 0.5079 - accuracy: 0.79 - ETA: 0s - loss: 0.5372 - accuracy: 0.78 - ETA: 0s - loss: 0.5397 - accuracy: 0.79 - ETA: 0s - loss: 0.5349 - accuracy: 0.79 - ETA: 0s - loss: 0.5339 - accuracy: 0.79 - ETA: 0s - loss: 0.5439 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5485 - accuracy: 0.7962 - val_loss: 0.5634 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.4981 - accuracy: 0.82 - ETA: 0s - loss: 0.4962 - accuracy: 0.82 - ETA: 0s - loss: 0.5073 - accuracy: 0.82 - ETA: 0s - loss: 0.5060 - accuracy: 0.82 - ETA: 0s - loss: 0.5193 - accuracy: 0.81 - ETA: 0s - loss: 0.5182 - accuracy: 0.81 - ETA: 0s - loss: 0.5208 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5209 - accuracy: 0.8201 - val_loss: 0.5684 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.87 - ETA: 0s - loss: 0.4855 - accuracy: 0.83 - ETA: 0s - loss: 0.4982 - accuracy: 0.83 - ETA: 0s - loss: 0.5156 - accuracy: 0.82 - ETA: 0s - loss: 0.5337 - accuracy: 0.82 - ETA: 0s - loss: 0.5564 - accuracy: 0.81 - ETA: 0s - loss: 0.5661 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5653 - accuracy: 0.8111 - val_loss: 0.5755 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.87 - ETA: 0s - loss: 0.6040 - accuracy: 0.80 - ETA: 0s - loss: 0.6594 - accuracy: 0.79 - ETA: 0s - loss: 0.6579 - accuracy: 0.78 - ETA: 0s - loss: 0.6427 - accuracy: 0.78 - ETA: 0s - loss: 0.6420 - accuracy: 0.78 - ETA: 0s - loss: 0.6348 - accuracy: 0.79 - ETA: 0s - loss: 0.6252 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6250 - accuracy: 0.7969 - val_loss: 0.5754 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.93 - ETA: 0s - loss: 0.5734 - accuracy: 0.81 - ETA: 0s - loss: 0.5838 - accuracy: 0.79 - ETA: 0s - loss: 0.5730 - accuracy: 0.80 - ETA: 0s - loss: 0.5668 - accuracy: 0.80 - ETA: 0s - loss: 0.5625 - accuracy: 0.80 - ETA: 0s - loss: 0.5830 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5832 - accuracy: 0.8029 - val_loss: 0.6416 - val_accuracy: 0.7463\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.87 - ETA: 0s - loss: 0.5846 - accuracy: 0.80 - ETA: 0s - loss: 0.5793 - accuracy: 0.80 - ETA: 0s - loss: 0.5907 - accuracy: 0.80 - ETA: 0s - loss: 0.5860 - accuracy: 0.80 - ETA: 0s - loss: 0.5806 - accuracy: 0.80 - ETA: 0s - loss: 0.5878 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5959 - accuracy: 0.7981 - val_loss: 0.6766 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4924 - accuracy: 0.81 - ETA: 0s - loss: 0.5564 - accuracy: 0.81 - ETA: 0s - loss: 0.6503 - accuracy: 0.80 - ETA: 0s - loss: 0.6547 - accuracy: 0.80 - ETA: 0s - loss: 0.6602 - accuracy: 0.80 - ETA: 0s - loss: 0.6751 - accuracy: 0.79 - ETA: 0s - loss: 0.7088 - accuracy: 0.79 - ETA: 0s - loss: 0.7080 - accuracy: 0.79 - 0s 5ms/step - loss: 0.7104 - accuracy: 0.7936 - val_loss: 0.5912 - val_accuracy: 0.7493\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.75 - ETA: 0s - loss: 0.5945 - accuracy: 0.79 - ETA: 0s - loss: 0.6499 - accuracy: 0.77 - ETA: 0s - loss: 0.6512 - accuracy: 0.75 - ETA: 0s - loss: 0.6565 - accuracy: 0.73 - ETA: 0s - loss: 0.6787 - accuracy: 0.71 - ETA: 0s - loss: 0.6887 - accuracy: 0.71 - ETA: 0s - loss: 0.6855 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6873 - accuracy: 0.6894 - val_loss: 0.6364 - val_accuracy: 0.7060\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.48 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.61 - ETA: 0s - loss: 0.6768 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6711 - accuracy: 0.62 - 0s 5ms/step - loss: 0.6710 - accuracy: 0.6286 - val_loss: 0.6129 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7909 - accuracy: 0.53 - ETA: 0s - loss: 0.6327 - accuracy: 0.63 - ETA: 0s - loss: 0.6256 - accuracy: 0.66 - ETA: 0s - loss: 0.6631 - accuracy: 0.65 - ETA: 0s - loss: 0.6614 - accuracy: 0.65 - ETA: 0s - loss: 0.6562 - accuracy: 0.64 - ETA: 0s - loss: 0.6678 - accuracy: 0.64 - ETA: 0s - loss: 0.6701 - accuracy: 0.65 - 0s 5ms/step - loss: 0.6688 - accuracy: 0.6529 - val_loss: 0.6371 - val_accuracy: 0.7030\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6752 - accuracy: 0.71 - ETA: 0s - loss: 0.6905 - accuracy: 0.69 - ETA: 0s - loss: 0.6667 - accuracy: 0.69 - ETA: 0s - loss: 0.6606 - accuracy: 0.69 - ETA: 0s - loss: 0.6491 - accuracy: 0.68 - ETA: 0s - loss: 0.6429 - accuracy: 0.68 - ETA: 0s - loss: 0.6465 - accuracy: 0.68 - ETA: 0s - loss: 0.6453 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6458 - accuracy: 0.6857 - val_loss: 0.6864 - val_accuracy: 0.7522\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4053 - accuracy: 0.50 - ETA: 0s - loss: 0.7082 - accuracy: 0.67 - ETA: 0s - loss: 0.6707 - accuracy: 0.70 - ETA: 0s - loss: 0.6625 - accuracy: 0.71 - ETA: 0s - loss: 0.6710 - accuracy: 0.71 - ETA: 0s - loss: 0.6677 - accuracy: 0.72 - ETA: 0s - loss: 0.6769 - accuracy: 0.71 - ETA: 0s - loss: 0.7057 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7057 - accuracy: 0.7223 - val_loss: 0.6419 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.78 - ETA: 0s - loss: 0.6387 - accuracy: 0.74 - ETA: 0s - loss: 0.6561 - accuracy: 0.71 - ETA: 0s - loss: 0.6689 - accuracy: 0.68 - ETA: 0s - loss: 0.6703 - accuracy: 0.65 - ETA: 0s - loss: 0.6700 - accuracy: 0.65 - ETA: 0s - loss: 0.6667 - accuracy: 0.63 - ETA: 0s - loss: 0.6671 - accuracy: 0.63 - 0s 5ms/step - loss: 0.6660 - accuracy: 0.6405 - val_loss: 0.6552 - val_accuracy: 0.7149\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.65 - ETA: 0s - loss: 0.6387 - accuracy: 0.71 - ETA: 0s - loss: 0.6517 - accuracy: 0.70 - ETA: 0s - loss: 0.6589 - accuracy: 0.70 - ETA: 0s - loss: 0.6556 - accuracy: 0.68 - ETA: 0s - loss: 0.6485 - accuracy: 0.68 - ETA: 0s - loss: 0.6493 - accuracy: 0.68 - ETA: 0s - loss: 0.6526 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6537 - accuracy: 0.6879 - val_loss: 0.6818 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5930 - accuracy: 0.71 - ETA: 0s - loss: 0.6902 - accuracy: 0.61 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.7051 - accuracy: 0.64 - 0s 5ms/step - loss: 0.7071 - accuracy: 0.6469 - val_loss: 0.6594 - val_accuracy: 0.7060\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7664 - accuracy: 0.62 - ETA: 0s - loss: 0.6813 - accuracy: 0.67 - ETA: 0s - loss: 0.6782 - accuracy: 0.65 - ETA: 0s - loss: 0.6775 - accuracy: 0.66 - ETA: 0s - loss: 0.6812 - accuracy: 0.64 - ETA: 0s - loss: 0.6847 - accuracy: 0.61 - ETA: 0s - loss: 0.6925 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5965 - val_loss: 0.6719 - val_accuracy: 0.7045\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.53 - ETA: 0s - loss: 0.7247 - accuracy: 0.58 - ETA: 0s - loss: 0.7044 - accuracy: 0.58 - ETA: 0s - loss: 0.6829 - accuracy: 0.61 - ETA: 0s - loss: 0.7368 - accuracy: 0.63 - ETA: 0s - loss: 0.7476 - accuracy: 0.64 - ETA: 0s - loss: 0.7370 - accuracy: 0.62 - ETA: 0s - loss: 0.7305 - accuracy: 0.62 - 0s 5ms/step - loss: 0.7305 - accuracy: 0.6226 - val_loss: 0.6527 - val_accuracy: 0.7030\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.56 - ETA: 0s - loss: 0.6723 - accuracy: 0.62 - ETA: 0s - loss: 0.6671 - accuracy: 0.65 - ETA: 0s - loss: 0.6835 - accuracy: 0.64 - ETA: 0s - loss: 0.6822 - accuracy: 0.63 - ETA: 0s - loss: 0.6860 - accuracy: 0.62 - ETA: 0s - loss: 0.6881 - accuracy: 0.60 - ETA: 0s - loss: 0.6892 - accuracy: 0.59 - 0s 5ms/step - loss: 0.6894 - accuracy: 0.5920 - val_loss: 0.6691 - val_accuracy: 0.7030\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7365 - accuracy: 0.43 - ETA: 0s - loss: 0.8519 - accuracy: 0.46 - ETA: 0s - loss: 0.7813 - accuracy: 0.48 - ETA: 0s - loss: 0.7420 - accuracy: 0.51 - ETA: 0s - loss: 0.7218 - accuracy: 0.54 - ETA: 0s - loss: 0.8490 - accuracy: 0.56 - ETA: 0s - loss: 0.8352 - accuracy: 0.56 - ETA: 0s - loss: 0.8143 - accuracy: 0.56 - 0s 5ms/step - loss: 0.8131 - accuracy: 0.5629 - val_loss: 0.6657 - val_accuracy: 0.7000\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.60 - ETA: 0s - loss: 0.7034 - accuracy: 0.59 - ETA: 0s - loss: 0.7015 - accuracy: 0.54 - ETA: 0s - loss: 0.7061 - accuracy: 0.53 - ETA: 0s - loss: 0.7018 - accuracy: 0.50 - ETA: 0s - loss: 0.6951 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5558 - val_loss: 0.7194 - val_accuracy: 0.7075\n",
      "Epoch 24/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.7794 - accuracy: 0.62 - ETA: 0s - loss: 0.7012 - accuracy: 0.63 - ETA: 0s - loss: 0.6987 - accuracy: 0.58 - ETA: 0s - loss: 0.6958 - accuracy: 0.56 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6863 - accuracy: 0.5810Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6859 - accuracy: 0.5909 - val_loss: 0.6685 - val_accuracy: 0.7000\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 2b53e17369dcff825d4a5efc41b6c84b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7572139501571655</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8053926371501441</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 195</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8424 - accuracy: 0.59 - ETA: 0s - loss: 2.8207 - accuracy: 0.59 - ETA: 0s - loss: 2.0856 - accuracy: 0.62 - ETA: 0s - loss: 1.5763 - accuracy: 0.65 - ETA: 0s - loss: 1.3186 - accuracy: 0.66 - ETA: 0s - loss: 1.1687 - accuracy: 0.68 - 0s 6ms/step - loss: 1.0817 - accuracy: 0.6857 - val_loss: 0.6098 - val_accuracy: 0.7104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6539 - accuracy: 0.75 - ETA: 0s - loss: 0.5801 - accuracy: 0.78 - ETA: 0s - loss: 0.6135 - accuracy: 0.76 - ETA: 0s - loss: 0.6212 - accuracy: 0.76 - ETA: 0s - loss: 0.6112 - accuracy: 0.76 - ETA: 0s - loss: 0.6010 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6011 - accuracy: 0.7719 - val_loss: 0.5884 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.81 - ETA: 0s - loss: 0.6108 - accuracy: 0.77 - ETA: 0s - loss: 0.6151 - accuracy: 0.77 - ETA: 0s - loss: 0.5974 - accuracy: 0.78 - ETA: 0s - loss: 0.6009 - accuracy: 0.77 - ETA: 0s - loss: 0.5994 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5959 - accuracy: 0.7775 - val_loss: 0.5799 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5227 - accuracy: 0.78 - ETA: 0s - loss: 0.5736 - accuracy: 0.78 - ETA: 0s - loss: 0.5672 - accuracy: 0.79 - ETA: 0s - loss: 0.5645 - accuracy: 0.79 - ETA: 0s - loss: 0.5740 - accuracy: 0.78 - ETA: 0s - loss: 0.5746 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5798 - accuracy: 0.7895 - val_loss: 0.5838 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.84 - ETA: 0s - loss: 0.5259 - accuracy: 0.83 - ETA: 0s - loss: 0.5246 - accuracy: 0.82 - ETA: 0s - loss: 0.5415 - accuracy: 0.82 - ETA: 0s - loss: 0.5524 - accuracy: 0.81 - ETA: 0s - loss: 0.5566 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5622 - accuracy: 0.8085 - val_loss: 0.5703 - val_accuracy: 0.7597\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.84 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5273 - accuracy: 0.82 - ETA: 0s - loss: 0.5443 - accuracy: 0.82 - ETA: 0s - loss: 0.5649 - accuracy: 0.81 - ETA: 0s - loss: 0.5804 - accuracy: 0.80 - ETA: 0s - loss: 0.5781 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5831 - accuracy: 0.7984 - val_loss: 0.5838 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7099 - accuracy: 0.78 - ETA: 0s - loss: 0.6192 - accuracy: 0.77 - ETA: 0s - loss: 0.5798 - accuracy: 0.80 - ETA: 0s - loss: 0.6117 - accuracy: 0.81 - ETA: 0s - loss: 0.6136 - accuracy: 0.80 - ETA: 0s - loss: 0.6314 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6356 - accuracy: 0.7891 - val_loss: 0.5948 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8154 - accuracy: 0.68 - ETA: 0s - loss: 0.7622 - accuracy: 0.76 - ETA: 0s - loss: 0.7029 - accuracy: 0.77 - ETA: 0s - loss: 0.6956 - accuracy: 0.76 - ETA: 0s - loss: 0.6851 - accuracy: 0.76 - ETA: 0s - loss: 0.6842 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6792 - accuracy: 0.7559 - val_loss: 0.6112 - val_accuracy: 0.7119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.65 - ETA: 0s - loss: 0.6267 - accuracy: 0.74 - ETA: 0s - loss: 0.6371 - accuracy: 0.74 - ETA: 0s - loss: 0.6234 - accuracy: 0.74 - ETA: 0s - loss: 0.6201 - accuracy: 0.75 - ETA: 0s - loss: 0.6219 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6544 - accuracy: 0.7484 - val_loss: 0.5866 - val_accuracy: 0.7597\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.78 - ETA: 0s - loss: 0.6642 - accuracy: 0.72 - ETA: 0s - loss: 0.6992 - accuracy: 0.70 - ETA: 0s - loss: 0.7041 - accuracy: 0.71 - ETA: 0s - loss: 0.7009 - accuracy: 0.73 - ETA: 0s - loss: 0.6959 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6961 - accuracy: 0.7309 - val_loss: 0.5986 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7572 - accuracy: 0.65 - ETA: 0s - loss: 0.6518 - accuracy: 0.72 - ETA: 0s - loss: 0.6499 - accuracy: 0.73 - ETA: 0s - loss: 0.6456 - accuracy: 0.71 - ETA: 0s - loss: 0.6467 - accuracy: 0.70 - ETA: 0s - loss: 0.6540 - accuracy: 0.68 - ETA: 0s - loss: 0.6912 - accuracy: 0.65 - 0s 5ms/step - loss: 0.6837 - accuracy: 0.6674 - val_loss: 0.6179 - val_accuracy: 0.7194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.68 - ETA: 0s - loss: 0.8349 - accuracy: 0.71 - ETA: 0s - loss: 0.7485 - accuracy: 0.66 - ETA: 0s - loss: 0.7190 - accuracy: 0.65 - ETA: 0s - loss: 0.7222 - accuracy: 0.65 - ETA: 0s - loss: 0.7148 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7148 - accuracy: 0.6719 - val_loss: 0.6390 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.62 - ETA: 0s - loss: 0.7091 - accuracy: 0.66 - ETA: 0s - loss: 0.6985 - accuracy: 0.68 - ETA: 0s - loss: 0.7190 - accuracy: 0.66 - ETA: 0s - loss: 0.7136 - accuracy: 0.64 - ETA: 0s - loss: 0.7071 - accuracy: 0.62 - 0s 4ms/step - loss: 0.7071 - accuracy: 0.6219 - val_loss: 0.6737 - val_accuracy: 0.6970\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.78 - ETA: 0s - loss: 0.6911 - accuracy: 0.61 - ETA: 0s - loss: 0.7216 - accuracy: 0.60 - ETA: 0s - loss: 0.7136 - accuracy: 0.57 - ETA: 0s - loss: 0.7154 - accuracy: 0.56 - ETA: 0s - loss: 0.7104 - accuracy: 0.53 - 0s 4ms/step - loss: 0.7070 - accuracy: 0.5345 - val_loss: 0.6745 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.6911 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.63 - ETA: 0s - loss: 0.6888 - accuracy: 0.62 - ETA: 0s - loss: 0.6872 - accuracy: 0.60 - ETA: 0s - loss: 0.6963 - accuracy: 0.59 - ETA: 0s - loss: 0.6942 - accuracy: 0.5858Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5834 - val_loss: 0.6738 - val_accuracy: 0.6955\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7172 - accuracy: 0.65 - ETA: 0s - loss: 3.3468 - accuracy: 0.60 - ETA: 0s - loss: 2.1251 - accuracy: 0.63 - ETA: 0s - loss: 1.6142 - accuracy: 0.66 - ETA: 0s - loss: 1.3696 - accuracy: 0.68 - ETA: 0s - loss: 1.2297 - accuracy: 0.69 - ETA: 0s - loss: 1.1203 - accuracy: 0.70 - 0s 6ms/step - loss: 1.1106 - accuracy: 0.7010 - val_loss: 0.5947 - val_accuracy: 0.7373\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.87 - ETA: 0s - loss: 0.5573 - accuracy: 0.80 - ETA: 0s - loss: 0.5880 - accuracy: 0.78 - ETA: 0s - loss: 0.6058 - accuracy: 0.76 - ETA: 0s - loss: 0.6109 - accuracy: 0.76 - ETA: 0s - loss: 0.6086 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6165 - accuracy: 0.7637 - val_loss: 0.5920 - val_accuracy: 0.7507\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.87 - ETA: 0s - loss: 0.5791 - accuracy: 0.77 - ETA: 0s - loss: 0.5969 - accuracy: 0.76 - ETA: 0s - loss: 0.5919 - accuracy: 0.77 - ETA: 0s - loss: 0.5839 - accuracy: 0.77 - ETA: 0s - loss: 0.5944 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5965 - accuracy: 0.7693 - val_loss: 0.5892 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4395 - accuracy: 0.87 - ETA: 0s - loss: 0.6057 - accuracy: 0.78 - ETA: 0s - loss: 0.5793 - accuracy: 0.79 - ETA: 0s - loss: 0.5858 - accuracy: 0.78 - ETA: 0s - loss: 0.5787 - accuracy: 0.79 - ETA: 0s - loss: 0.5865 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5863 - accuracy: 0.7906 - val_loss: 0.5902 - val_accuracy: 0.7418\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6604 - accuracy: 0.75 - ETA: 0s - loss: 0.6171 - accuracy: 0.77 - ETA: 0s - loss: 0.5998 - accuracy: 0.78 - ETA: 0s - loss: 0.5878 - accuracy: 0.77 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - ETA: 0s - loss: 0.5790 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5806 - accuracy: 0.7906 - val_loss: 0.5756 - val_accuracy: 0.7507\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4548 - accuracy: 0.84 - ETA: 0s - loss: 0.5400 - accuracy: 0.81 - ETA: 0s - loss: 0.5632 - accuracy: 0.80 - ETA: 0s - loss: 0.5560 - accuracy: 0.80 - ETA: 0s - loss: 0.5498 - accuracy: 0.81 - ETA: 0s - loss: 0.5477 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5502 - accuracy: 0.8100 - val_loss: 0.5633 - val_accuracy: 0.7612\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.84 - ETA: 0s - loss: 0.5607 - accuracy: 0.80 - ETA: 0s - loss: 0.5511 - accuracy: 0.81 - ETA: 0s - loss: 0.5603 - accuracy: 0.80 - ETA: 0s - loss: 0.5598 - accuracy: 0.80 - ETA: 0s - loss: 0.5618 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5616 - accuracy: 0.8063 - val_loss: 0.6611 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.71 - ETA: 0s - loss: 0.6068 - accuracy: 0.78 - ETA: 0s - loss: 0.6102 - accuracy: 0.79 - ETA: 0s - loss: 0.5771 - accuracy: 0.80 - ETA: 0s - loss: 0.5766 - accuracy: 0.80 - ETA: 0s - loss: 0.5716 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5716 - accuracy: 0.8052 - val_loss: 0.5817 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4958 - accuracy: 0.81 - ETA: 0s - loss: 0.5233 - accuracy: 0.80 - ETA: 0s - loss: 0.5219 - accuracy: 0.82 - ETA: 0s - loss: 0.5387 - accuracy: 0.81 - ETA: 0s - loss: 0.5627 - accuracy: 0.80 - ETA: 0s - loss: 0.5726 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5787 - accuracy: 0.7984 - val_loss: 0.5927 - val_accuracy: 0.7313\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.84 - ETA: 0s - loss: 0.6417 - accuracy: 0.79 - ETA: 0s - loss: 0.6441 - accuracy: 0.79 - ETA: 0s - loss: 0.6873 - accuracy: 0.78 - ETA: 0s - loss: 0.6672 - accuracy: 0.78 - ETA: 0s - loss: 0.6671 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6642 - accuracy: 0.7671 - val_loss: 0.6405 - val_accuracy: 0.6701\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4411 - accuracy: 0.71 - ETA: 0s - loss: 0.9854 - accuracy: 0.73 - ETA: 0s - loss: 0.8178 - accuracy: 0.75 - ETA: 0s - loss: 0.8097 - accuracy: 0.74 - ETA: 0s - loss: 0.7847 - accuracy: 0.72 - ETA: 0s - loss: 0.7700 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7547 - accuracy: 0.6842 - val_loss: 0.6209 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.71 - ETA: 0s - loss: 0.7036 - accuracy: 0.69 - ETA: 0s - loss: 0.6780 - accuracy: 0.68 - ETA: 0s - loss: 0.6863 - accuracy: 0.66 - ETA: 0s - loss: 0.6754 - accuracy: 0.66 - ETA: 0s - loss: 0.6735 - accuracy: 0.66 - ETA: 0s - loss: 0.6761 - accuracy: 0.66 - 0s 4ms/step - loss: 0.7162 - accuracy: 0.6711 - val_loss: 1.1102 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.7412 - accuracy: 0.66 - ETA: 0s - loss: 0.7221 - accuracy: 0.61 - ETA: 0s - loss: 0.7089 - accuracy: 0.59 - ETA: 0s - loss: 0.7058 - accuracy: 0.58 - ETA: 0s - loss: 0.6935 - accuracy: 0.59 - ETA: 0s - loss: 0.7124 - accuracy: 0.61 - ETA: 0s - loss: 0.7129 - accuracy: 0.62 - 0s 5ms/step - loss: 0.7135 - accuracy: 0.6234 - val_loss: 0.6397 - val_accuracy: 0.7030\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.65 - ETA: 0s - loss: 0.6569 - accuracy: 0.65 - ETA: 0s - loss: 0.6662 - accuracy: 0.64 - ETA: 0s - loss: 0.6697 - accuracy: 0.65 - ETA: 0s - loss: 0.6839 - accuracy: 0.64 - ETA: 0s - loss: 0.6894 - accuracy: 0.62 - ETA: 0s - loss: 0.6886 - accuracy: 0.61 - ETA: 0s - loss: 0.6911 - accuracy: 0.59 - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5950 - val_loss: 0.6526 - val_accuracy: 0.7030\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7189 - accuracy: 0.62 - ETA: 0s - loss: 0.7167 - accuracy: 0.52 - ETA: 0s - loss: 0.7085 - accuracy: 0.52 - ETA: 0s - loss: 0.6966 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6945 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - 0s 5ms/step - loss: 0.6875 - accuracy: 0.5760 - val_loss: 0.6974 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.78 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6981 - accuracy: 0.65 - ETA: 0s - loss: 0.6889 - accuracy: 0.66 - ETA: 0s - loss: 0.6966 - accuracy: 0.65 - ETA: 0s - loss: 0.6952 - accuracy: 0.63 - ETA: 0s - loss: 0.7132 - accuracy: 0.6002Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.6002 - val_loss: 0.6642 - val_accuracy: 0.7000\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.68 - ETA: 0s - loss: 3.3132 - accuracy: 0.58 - ETA: 0s - loss: 2.1265 - accuracy: 0.63 - ETA: 0s - loss: 1.6663 - accuracy: 0.65 - ETA: 0s - loss: 1.4193 - accuracy: 0.67 - ETA: 0s - loss: 1.2677 - accuracy: 0.68 - 0s 5ms/step - loss: 1.2098 - accuracy: 0.6898 - val_loss: 0.5973 - val_accuracy: 0.7299\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.78 - ETA: 0s - loss: 0.5869 - accuracy: 0.78 - ETA: 0s - loss: 0.5915 - accuracy: 0.78 - ETA: 0s - loss: 0.5973 - accuracy: 0.77 - ETA: 0s - loss: 0.6023 - accuracy: 0.77 - ETA: 0s - loss: 0.6131 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6252 - accuracy: 0.7641 - val_loss: 0.6089 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.78 - ETA: 0s - loss: 0.6139 - accuracy: 0.79 - ETA: 0s - loss: 0.6227 - accuracy: 0.76 - ETA: 0s - loss: 0.6149 - accuracy: 0.76 - ETA: 0s - loss: 0.6159 - accuracy: 0.76 - ETA: 0s - loss: 0.6161 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6105 - accuracy: 0.7682 - val_loss: 0.5856 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.81 - ETA: 0s - loss: 0.5665 - accuracy: 0.79 - ETA: 0s - loss: 0.5651 - accuracy: 0.79 - ETA: 0s - loss: 0.5816 - accuracy: 0.79 - ETA: 0s - loss: 0.5932 - accuracy: 0.78 - ETA: 0s - loss: 0.6035 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6048 - accuracy: 0.7786 - val_loss: 0.5977 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.81 - ETA: 0s - loss: 0.5764 - accuracy: 0.77 - ETA: 0s - loss: 0.5744 - accuracy: 0.78 - ETA: 0s - loss: 0.6095 - accuracy: 0.78 - ETA: 0s - loss: 0.6097 - accuracy: 0.77 - ETA: 0s - loss: 0.6119 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6106 - accuracy: 0.7716 - val_loss: 0.5868 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.75 - ETA: 0s - loss: 0.5940 - accuracy: 0.76 - ETA: 0s - loss: 0.5860 - accuracy: 0.77 - ETA: 0s - loss: 0.5902 - accuracy: 0.77 - ETA: 0s - loss: 0.6582 - accuracy: 0.76 - ETA: 0s - loss: 0.6655 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6632 - accuracy: 0.7641 - val_loss: 0.6249 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.71 - ETA: 0s - loss: 0.6077 - accuracy: 0.73 - ETA: 0s - loss: 0.6567 - accuracy: 0.72 - ETA: 0s - loss: 0.6504 - accuracy: 0.71 - ETA: 0s - loss: 0.6530 - accuracy: 0.70 - ETA: 0s - loss: 0.6527 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6506 - accuracy: 0.7107 - val_loss: 0.6347 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7084 - accuracy: 0.68 - ETA: 0s - loss: 0.6158 - accuracy: 0.71 - ETA: 0s - loss: 0.6327 - accuracy: 0.72 - ETA: 0s - loss: 0.6320 - accuracy: 0.73 - ETA: 0s - loss: 0.6293 - accuracy: 0.74 - ETA: 0s - loss: 0.6446 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6413 - accuracy: 0.7488 - val_loss: 0.6159 - val_accuracy: 0.7463\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.84 - ETA: 0s - loss: 0.6469 - accuracy: 0.76 - ETA: 0s - loss: 0.6296 - accuracy: 0.75 - ETA: 0s - loss: 0.6455 - accuracy: 0.71 - ETA: 0s - loss: 0.6457 - accuracy: 0.69 - ETA: 0s - loss: 0.6452 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6486 - accuracy: 0.7055 - val_loss: 0.6207 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.84 - ETA: 0s - loss: 0.6606 - accuracy: 0.67 - ETA: 0s - loss: 0.6526 - accuracy: 0.67 - ETA: 0s - loss: 0.6553 - accuracy: 0.66 - ETA: 0s - loss: 0.6631 - accuracy: 0.68 - ETA: 0s - loss: 0.6683 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6750 - accuracy: 0.6980 - val_loss: 0.6529 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5923 - accuracy: 0.81 - ETA: 0s - loss: 0.7004 - accuracy: 0.70 - ETA: 0s - loss: 0.6660 - accuracy: 0.72 - ETA: 0s - loss: 0.6698 - accuracy: 0.69 - ETA: 0s - loss: 0.6691 - accuracy: 0.67 - ETA: 0s - loss: 0.6611 - accuracy: 0.67 - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6775 - val_loss: 0.6926 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.62 - ETA: 0s - loss: 0.6151 - accuracy: 0.71 - ETA: 0s - loss: 0.6212 - accuracy: 0.73 - ETA: 0s - loss: 0.6490 - accuracy: 0.73 - ETA: 0s - loss: 0.6586 - accuracy: 0.71 - ETA: 0s - loss: 0.6647 - accuracy: 0.69 - ETA: 0s - loss: 0.6603 - accuracy: 0.67 - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6749 - val_loss: 0.6496 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.5352 - accuracy: 0.75 - ETA: 0s - loss: 0.7191 - accuracy: 0.69 - ETA: 0s - loss: 0.7009 - accuracy: 0.70 - ETA: 0s - loss: 0.6880 - accuracy: 0.72 - ETA: 0s - loss: 0.6884 - accuracy: 0.71 - ETA: 0s - loss: 0.6825 - accuracy: 0.7038Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.6894 - val_loss: 0.6719 - val_accuracy: 0.7015\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 1b7baa97414c204a3f386bd76880e3f6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064703305563</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8838076771990834</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0655 - accuracy: 0.37 - ETA: 0s - loss: 2.8287 - accuracy: 0.53 - ETA: 0s - loss: 1.7568 - accuracy: 0.61 - ETA: 0s - loss: 1.5050 - accuracy: 0.64 - ETA: 0s - loss: 1.3042 - accuracy: 0.66 - ETA: 0s - loss: 1.1655 - accuracy: 0.68 - ETA: 0s - loss: 1.0700 - accuracy: 0.69 - ETA: 0s - loss: 1.0261 - accuracy: 0.70 - 1s 7ms/step - loss: 0.9862 - accuracy: 0.7018 - val_loss: 0.6047 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.71 - ETA: 0s - loss: 0.5696 - accuracy: 0.73 - ETA: 0s - loss: 0.6281 - accuracy: 0.73 - ETA: 0s - loss: 0.6123 - accuracy: 0.74 - ETA: 0s - loss: 0.5985 - accuracy: 0.75 - ETA: 0s - loss: 0.5984 - accuracy: 0.75 - ETA: 0s - loss: 0.6168 - accuracy: 0.75 - ETA: 0s - loss: 0.6272 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6229 - accuracy: 0.7402 - val_loss: 0.6766 - val_accuracy: 0.6881\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.87 - ETA: 0s - loss: 0.4998 - accuracy: 0.81 - ETA: 0s - loss: 0.5183 - accuracy: 0.78 - ETA: 0s - loss: 0.5646 - accuracy: 0.77 - ETA: 0s - loss: 0.5569 - accuracy: 0.78 - ETA: 0s - loss: 0.5769 - accuracy: 0.77 - ETA: 0s - loss: 0.5956 - accuracy: 0.75 - ETA: 0s - loss: 0.6091 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6185 - accuracy: 0.7432 - val_loss: 0.6311 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.53 - ETA: 0s - loss: 0.6825 - accuracy: 0.71 - ETA: 0s - loss: 0.6936 - accuracy: 0.71 - ETA: 0s - loss: 0.6872 - accuracy: 0.72 - ETA: 0s - loss: 0.6992 - accuracy: 0.72 - ETA: 0s - loss: 0.6869 - accuracy: 0.72 - ETA: 0s - loss: 0.6759 - accuracy: 0.73 - ETA: 0s - loss: 0.6961 - accuracy: 0.72 - 1s 6ms/step - loss: 0.6816 - accuracy: 0.7357 - val_loss: 0.7168 - val_accuracy: 0.7478\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.75 - ETA: 0s - loss: 0.6189 - accuracy: 0.78 - ETA: 0s - loss: 0.6086 - accuracy: 0.79 - ETA: 0s - loss: 0.6186 - accuracy: 0.80 - ETA: 0s - loss: 0.6182 - accuracy: 0.80 - ETA: 0s - loss: 0.6219 - accuracy: 0.79 - ETA: 0s - loss: 0.6419 - accuracy: 0.79 - ETA: 0s - loss: 0.6653 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6588 - accuracy: 0.7809 - val_loss: 0.6420 - val_accuracy: 0.6716\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.75 - ETA: 0s - loss: 0.6257 - accuracy: 0.76 - ETA: 0s - loss: 0.6028 - accuracy: 0.77 - ETA: 0s - loss: 0.5900 - accuracy: 0.78 - ETA: 0s - loss: 0.5836 - accuracy: 0.78 - ETA: 0s - loss: 0.5786 - accuracy: 0.78 - ETA: 0s - loss: 0.5859 - accuracy: 0.78 - ETA: 0s - loss: 0.5939 - accuracy: 0.77 - 0s 6ms/step - loss: 0.5901 - accuracy: 0.7813 - val_loss: 0.6336 - val_accuracy: 0.7522\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.87 - ETA: 0s - loss: 0.5334 - accuracy: 0.82 - ETA: 0s - loss: 0.5998 - accuracy: 0.80 - ETA: 0s - loss: 0.5777 - accuracy: 0.80 - ETA: 0s - loss: 0.5647 - accuracy: 0.80 - ETA: 0s - loss: 0.5615 - accuracy: 0.79 - ETA: 0s - loss: 0.5485 - accuracy: 0.80 - ETA: 0s - loss: 0.5428 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5461 - accuracy: 0.8111 - val_loss: 0.6457 - val_accuracy: 0.6940\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.84 - ETA: 0s - loss: 0.5596 - accuracy: 0.80 - ETA: 0s - loss: 0.5565 - accuracy: 0.80 - ETA: 0s - loss: 0.5311 - accuracy: 0.82 - ETA: 0s - loss: 0.5191 - accuracy: 0.82 - ETA: 0s - loss: 0.5138 - accuracy: 0.82 - ETA: 0s - loss: 0.5181 - accuracy: 0.82 - ETA: 0s - loss: 0.5189 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5211 - accuracy: 0.8160 - val_loss: 0.7690 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.96 - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4794 - accuracy: 0.83 - ETA: 0s - loss: 0.4696 - accuracy: 0.83 - ETA: 0s - loss: 0.4907 - accuracy: 0.82 - ETA: 0s - loss: 0.5088 - accuracy: 0.82 - ETA: 0s - loss: 0.5196 - accuracy: 0.82 - ETA: 0s - loss: 0.5295 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5306 - accuracy: 0.8227 - val_loss: 0.7738 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.93 - ETA: 0s - loss: 0.4836 - accuracy: 0.84 - ETA: 0s - loss: 0.4925 - accuracy: 0.84 - ETA: 0s - loss: 0.5049 - accuracy: 0.84 - ETA: 0s - loss: 0.5269 - accuracy: 0.83 - ETA: 0s - loss: 0.5473 - accuracy: 0.82 - ETA: 0s - loss: 0.5508 - accuracy: 0.82 - ETA: 0s - loss: 0.5430 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5444 - accuracy: 0.8238 - val_loss: 0.7319 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.81 - ETA: 0s - loss: 0.8993 - accuracy: 0.83 - ETA: 0s - loss: 0.6885 - accuracy: 0.84 - ETA: 0s - loss: 0.6623 - accuracy: 0.82 - ETA: 0s - loss: 0.6280 - accuracy: 0.82 - ETA: 0s - loss: 0.6161 - accuracy: 0.82 - ETA: 0s - loss: 0.6155 - accuracy: 0.81 - ETA: 0s - loss: 0.6060 - accuracy: 0.81 - 0s 5ms/step - loss: 0.6055 - accuracy: 0.8130 - val_loss: 0.6916 - val_accuracy: 0.7507\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.75 - ETA: 0s - loss: 0.5237 - accuracy: 0.80 - ETA: 0s - loss: 0.5067 - accuracy: 0.82 - ETA: 0s - loss: 0.5125 - accuracy: 0.81 - ETA: 0s - loss: 0.5228 - accuracy: 0.81 - ETA: 0s - loss: 0.5192 - accuracy: 0.82 - ETA: 0s - loss: 0.5224 - accuracy: 0.82 - ETA: 0s - loss: 0.5299 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5331 - accuracy: 0.8178 - val_loss: 1.0725 - val_accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.84 - ETA: 0s - loss: 0.5740 - accuracy: 0.78 - ETA: 0s - loss: 0.5653 - accuracy: 0.80 - ETA: 0s - loss: 0.5325 - accuracy: 0.82 - ETA: 0s - loss: 0.5178 - accuracy: 0.82 - ETA: 0s - loss: 0.5162 - accuracy: 0.83 - ETA: 0s - loss: 0.5166 - accuracy: 0.82 - ETA: 0s - loss: 0.5113 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5125 - accuracy: 0.8298 - val_loss: 1.0235 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.78 - ETA: 0s - loss: 0.4984 - accuracy: 0.83 - ETA: 0s - loss: 0.5037 - accuracy: 0.83 - ETA: 0s - loss: 0.5360 - accuracy: 0.82 - ETA: 0s - loss: 0.5534 - accuracy: 0.82 - ETA: 0s - loss: 0.5597 - accuracy: 0.81 - ETA: 0s - loss: 0.5474 - accuracy: 0.82 - ETA: 0s - loss: 0.5492 - accuracy: 0.82 - ETA: 0s - loss: 0.5478 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5478 - accuracy: 0.8223 - val_loss: 1.0010 - val_accuracy: 0.7269\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8647 - accuracy: 0.71 - ETA: 0s - loss: 0.4869 - accuracy: 0.85 - ETA: 0s - loss: 0.5276 - accuracy: 0.82 - ETA: 0s - loss: 0.5207 - accuracy: 0.82 - ETA: 0s - loss: 0.5247 - accuracy: 0.82 - ETA: 0s - loss: 0.5627 - accuracy: 0.82 - ETA: 0s - loss: 0.5860 - accuracy: 0.82 - ETA: 0s - loss: 0.5753 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5751 - accuracy: 0.8190 - val_loss: 0.7303 - val_accuracy: 0.7433\n",
      "Epoch 16/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.90 - ETA: 0s - loss: 0.6084 - accuracy: 0.79 - ETA: 0s - loss: 0.5540 - accuracy: 0.81 - ETA: 0s - loss: 0.5360 - accuracy: 0.81 - ETA: 0s - loss: 0.5334 - accuracy: 0.81 - ETA: 0s - loss: 0.5330 - accuracy: 0.82 - ETA: 0s - loss: 0.5371 - accuracy: 0.81 - ETA: 0s - loss: 0.5242 - accuracy: 0.82 - ETA: 0s - loss: 0.5191 - accuracy: 0.8277Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5168 - accuracy: 0.8290 - val_loss: 0.8553 - val_accuracy: 0.7358\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0020 - accuracy: 0.56 - ETA: 0s - loss: 3.6684 - accuracy: 0.60 - ETA: 0s - loss: 2.2426 - accuracy: 0.63 - ETA: 0s - loss: 1.7015 - accuracy: 0.64 - ETA: 0s - loss: 1.4468 - accuracy: 0.65 - ETA: 0s - loss: 1.2883 - accuracy: 0.66 - ETA: 0s - loss: 1.1755 - accuracy: 0.67 - ETA: 0s - loss: 1.1121 - accuracy: 0.68 - 1s 7ms/step - loss: 1.0629 - accuracy: 0.6887 - val_loss: 0.6484 - val_accuracy: 0.6806\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.65 - ETA: 0s - loss: 0.5393 - accuracy: 0.70 - ETA: 0s - loss: 0.5536 - accuracy: 0.73 - ETA: 0s - loss: 0.5832 - accuracy: 0.74 - ETA: 0s - loss: 0.5680 - accuracy: 0.75 - ETA: 0s - loss: 0.5797 - accuracy: 0.74 - ETA: 0s - loss: 0.5785 - accuracy: 0.74 - ETA: 0s - loss: 0.5790 - accuracy: 0.74 - 0s 5ms/step - loss: 0.5819 - accuracy: 0.7443 - val_loss: 0.5824 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.84 - ETA: 0s - loss: 0.8292 - accuracy: 0.78 - ETA: 0s - loss: 0.6809 - accuracy: 0.79 - ETA: 0s - loss: 0.6421 - accuracy: 0.78 - ETA: 0s - loss: 0.6008 - accuracy: 0.79 - ETA: 0s - loss: 0.5847 - accuracy: 0.79 - ETA: 0s - loss: 0.5793 - accuracy: 0.79 - ETA: 0s - loss: 0.5738 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5734 - accuracy: 0.7902 - val_loss: 0.6298 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.78 - ETA: 0s - loss: 0.5534 - accuracy: 0.82 - ETA: 0s - loss: 0.5691 - accuracy: 0.80 - ETA: 0s - loss: 0.5817 - accuracy: 0.79 - ETA: 0s - loss: 0.5791 - accuracy: 0.78 - ETA: 0s - loss: 0.5789 - accuracy: 0.78 - ETA: 0s - loss: 0.5813 - accuracy: 0.78 - ETA: 0s - loss: 0.5881 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5850 - accuracy: 0.7813 - val_loss: 0.6160 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7654 - accuracy: 0.78 - ETA: 0s - loss: 0.6276 - accuracy: 0.79 - ETA: 0s - loss: 0.5814 - accuracy: 0.80 - ETA: 0s - loss: 0.5820 - accuracy: 0.80 - ETA: 0s - loss: 0.5725 - accuracy: 0.80 - ETA: 0s - loss: 0.5610 - accuracy: 0.80 - ETA: 0s - loss: 0.5708 - accuracy: 0.80 - ETA: 0s - loss: 0.5774 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5767 - accuracy: 0.7973 - val_loss: 0.9330 - val_accuracy: 0.7209\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.93 - ETA: 0s - loss: 0.4990 - accuracy: 0.83 - ETA: 0s - loss: 0.7068 - accuracy: 0.79 - ETA: 0s - loss: 0.6724 - accuracy: 0.78 - ETA: 0s - loss: 0.6608 - accuracy: 0.79 - ETA: 0s - loss: 0.6488 - accuracy: 0.79 - ETA: 0s - loss: 0.6364 - accuracy: 0.79 - ETA: 0s - loss: 0.6248 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6185 - accuracy: 0.7969 - val_loss: 0.8948 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.90 - ETA: 0s - loss: 0.3947 - accuracy: 0.89 - ETA: 0s - loss: 0.4808 - accuracy: 0.85 - ETA: 0s - loss: 0.5277 - accuracy: 0.83 - ETA: 0s - loss: 0.5465 - accuracy: 0.82 - ETA: 0s - loss: 0.5436 - accuracy: 0.82 - ETA: 0s - loss: 0.5425 - accuracy: 0.82 - ETA: 0s - loss: 0.5353 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5393 - accuracy: 0.8216 - val_loss: 0.7167 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.71 - ETA: 0s - loss: 0.5217 - accuracy: 0.81 - ETA: 0s - loss: 0.5080 - accuracy: 0.81 - ETA: 0s - loss: 0.4966 - accuracy: 0.81 - ETA: 0s - loss: 0.4873 - accuracy: 0.81 - ETA: 0s - loss: 0.4845 - accuracy: 0.81 - ETA: 0s - loss: 0.4778 - accuracy: 0.82 - ETA: 0s - loss: 0.4714 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4749 - accuracy: 0.8287 - val_loss: 0.6117 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.84 - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - ETA: 0s - loss: 0.4743 - accuracy: 0.84 - ETA: 0s - loss: 0.4873 - accuracy: 0.83 - ETA: 0s - loss: 0.4627 - accuracy: 0.84 - ETA: 0s - loss: 0.4429 - accuracy: 0.85 - ETA: 0s - loss: 0.4465 - accuracy: 0.85 - ETA: 0s - loss: 0.4469 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4469 - accuracy: 0.8567 - val_loss: 0.9164 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.93 - ETA: 0s - loss: 0.4231 - accuracy: 0.87 - ETA: 0s - loss: 0.4124 - accuracy: 0.87 - ETA: 0s - loss: 0.4317 - accuracy: 0.87 - ETA: 0s - loss: 0.4775 - accuracy: 0.86 - ETA: 0s - loss: 0.4795 - accuracy: 0.85 - ETA: 0s - loss: 0.4852 - accuracy: 0.84 - ETA: 0s - loss: 0.4849 - accuracy: 0.84 - ETA: 0s - loss: 0.4789 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4789 - accuracy: 0.8432 - val_loss: 0.7705 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.87 - ETA: 0s - loss: 0.3841 - accuracy: 0.88 - ETA: 0s - loss: 0.3982 - accuracy: 0.87 - ETA: 0s - loss: 0.4065 - accuracy: 0.87 - ETA: 0s - loss: 0.4191 - accuracy: 0.86 - ETA: 0s - loss: 0.4170 - accuracy: 0.86 - ETA: 0s - loss: 0.4201 - accuracy: 0.86 - ETA: 0s - loss: 0.4257 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4241 - accuracy: 0.8664 - val_loss: 1.0172 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.84 - ETA: 0s - loss: 0.4195 - accuracy: 0.86 - ETA: 0s - loss: 0.3899 - accuracy: 0.88 - ETA: 0s - loss: 0.4083 - accuracy: 0.87 - ETA: 0s - loss: 0.4163 - accuracy: 0.87 - ETA: 0s - loss: 0.4152 - accuracy: 0.86 - ETA: 0s - loss: 0.4201 - accuracy: 0.86 - ETA: 0s - loss: 0.4157 - accuracy: 0.87 - ETA: 0s - loss: 0.4197 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8675 - val_loss: 0.7682 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.96 - ETA: 0s - loss: 0.4043 - accuracy: 0.88 - ETA: 0s - loss: 0.4199 - accuracy: 0.87 - ETA: 0s - loss: 0.4252 - accuracy: 0.87 - ETA: 0s - loss: 0.4124 - accuracy: 0.87 - ETA: 0s - loss: 0.4063 - accuracy: 0.87 - ETA: 0s - loss: 0.4162 - accuracy: 0.87 - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - ETA: 0s - loss: 0.4104 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4089 - accuracy: 0.8753 - val_loss: 0.8296 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5824 - accuracy: 0.84 - ETA: 0s - loss: 0.3923 - accuracy: 0.89 - ETA: 0s - loss: 0.4128 - accuracy: 0.88 - ETA: 0s - loss: 0.3921 - accuracy: 0.88 - ETA: 0s - loss: 0.3853 - accuracy: 0.88 - ETA: 0s - loss: 0.3844 - accuracy: 0.88 - ETA: 0s - loss: 0.3792 - accuracy: 0.88 - ETA: 0s - loss: 0.3803 - accuracy: 0.88 - ETA: 0s - loss: 0.3870 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8854 - val_loss: 0.9168 - val_accuracy: 0.7060\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.84 - ETA: 0s - loss: 0.4153 - accuracy: 0.87 - ETA: 0s - loss: 0.3960 - accuracy: 0.88 - ETA: 0s - loss: 0.3902 - accuracy: 0.89 - ETA: 0s - loss: 0.3812 - accuracy: 0.89 - ETA: 0s - loss: 0.3889 - accuracy: 0.89 - ETA: 0s - loss: 0.3785 - accuracy: 0.89 - ETA: 0s - loss: 0.3866 - accuracy: 0.89 - ETA: 0s - loss: 0.3905 - accuracy: 0.88 - 0s 6ms/step - loss: 0.3904 - accuracy: 0.8873 - val_loss: 1.0668 - val_accuracy: 0.7284\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.87 - ETA: 0s - loss: 0.3751 - accuracy: 0.90 - ETA: 0s - loss: 0.3807 - accuracy: 0.89 - ETA: 0s - loss: 0.4058 - accuracy: 0.88 - ETA: 0s - loss: 0.4226 - accuracy: 0.87 - ETA: 0s - loss: 0.4118 - accuracy: 0.88 - ETA: 0s - loss: 0.4382 - accuracy: 0.88 - ETA: 0s - loss: 0.4386 - accuracy: 0.88 - ETA: 0s - loss: 0.4303 - accuracy: 0.88 - 1s 6ms/step - loss: 0.4272 - accuracy: 0.8828 - val_loss: 1.0013 - val_accuracy: 0.7060\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.84 - ETA: 0s - loss: 0.3847 - accuracy: 0.89 - ETA: 0s - loss: 0.3995 - accuracy: 0.89 - ETA: 0s - loss: 0.3969 - accuracy: 0.89 - ETA: 0s - loss: 0.4075 - accuracy: 0.88 - ETA: 0s - loss: 0.3946 - accuracy: 0.88 - ETA: 0s - loss: 0.3943 - accuracy: 0.88 - ETA: 0s - loss: 0.3922 - accuracy: 0.88 - ETA: 0s - loss: 0.3860 - accuracy: 0.89 - 0s 6ms/step - loss: 0.3875 - accuracy: 0.8895 - val_loss: 1.1624 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.96 - ETA: 0s - loss: 0.3442 - accuracy: 0.90 - ETA: 0s - loss: 0.3693 - accuracy: 0.89 - ETA: 0s - loss: 0.3598 - accuracy: 0.90 - ETA: 0s - loss: 0.3800 - accuracy: 0.89 - ETA: 0s - loss: 0.3895 - accuracy: 0.88 - ETA: 0s - loss: 0.3956 - accuracy: 0.88 - ETA: 0s - loss: 0.3899 - accuracy: 0.88 - ETA: 0s - loss: 0.3854 - accuracy: 0.88 - ETA: 0s - loss: 0.3968 - accuracy: 0.88 - 1s 7ms/step - loss: 0.3961 - accuracy: 0.8876 - val_loss: 0.7588 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.81 - ETA: 0s - loss: 0.4755 - accuracy: 0.87 - ETA: 0s - loss: 0.4040 - accuracy: 0.89 - ETA: 0s - loss: 0.3832 - accuracy: 0.89 - ETA: 0s - loss: 0.3753 - accuracy: 0.90 - ETA: 0s - loss: 0.3583 - accuracy: 0.90 - ETA: 0s - loss: 0.3721 - accuracy: 0.89 - ETA: 0s - loss: 0.3791 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3727 - accuracy: 0.8988 - val_loss: 1.0428 - val_accuracy: 0.7104\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.90 - ETA: 0s - loss: 0.3155 - accuracy: 0.91 - ETA: 0s - loss: 0.3004 - accuracy: 0.92 - ETA: 0s - loss: 0.3205 - accuracy: 0.91 - ETA: 0s - loss: 0.3299 - accuracy: 0.91 - ETA: 0s - loss: 0.3327 - accuracy: 0.91 - ETA: 0s - loss: 0.3526 - accuracy: 0.90 - ETA: 0s - loss: 0.3579 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3692 - accuracy: 0.9018 - val_loss: 0.7348 - val_accuracy: 0.7284\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.90 - ETA: 0s - loss: 0.3271 - accuracy: 0.90 - ETA: 0s - loss: 0.3604 - accuracy: 0.89 - ETA: 0s - loss: 0.3476 - accuracy: 0.90 - ETA: 0s - loss: 0.3543 - accuracy: 0.90 - ETA: 0s - loss: 0.3624 - accuracy: 0.89 - ETA: 0s - loss: 0.3880 - accuracy: 0.89 - ETA: 0s - loss: 0.4045 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4051 - accuracy: 0.8858 - val_loss: 0.7104 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.81 - ETA: 0s - loss: 0.4532 - accuracy: 0.86 - ETA: 0s - loss: 0.4280 - accuracy: 0.86 - ETA: 0s - loss: 0.4543 - accuracy: 0.86 - ETA: 0s - loss: 0.4712 - accuracy: 0.86 - ETA: 0s - loss: 0.4705 - accuracy: 0.86 - ETA: 0s - loss: 0.4738 - accuracy: 0.86 - ETA: 0s - loss: 0.4675 - accuracy: 0.86 - ETA: 0s - loss: 0.4607 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4620 - accuracy: 0.8660 - val_loss: 1.2239 - val_accuracy: 0.7149\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.90 - ETA: 0s - loss: 0.5045 - accuracy: 0.86 - ETA: 0s - loss: 0.4853 - accuracy: 0.86 - ETA: 0s - loss: 0.4611 - accuracy: 0.87 - ETA: 0s - loss: 0.4831 - accuracy: 0.86 - ETA: 0s - loss: 0.4740 - accuracy: 0.86 - ETA: 0s - loss: 0.4666 - accuracy: 0.86 - ETA: 0s - loss: 0.4650 - accuracy: 0.87 - ETA: 0s - loss: 0.4624 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4617 - accuracy: 0.8686 - val_loss: 1.1491 - val_accuracy: 0.7194\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.81 - ETA: 0s - loss: 0.4243 - accuracy: 0.86 - ETA: 0s - loss: 0.5668 - accuracy: 0.86 - ETA: 0s - loss: 0.5162 - accuracy: 0.86 - ETA: 0s - loss: 0.4845 - accuracy: 0.87 - ETA: 0s - loss: 0.4727 - accuracy: 0.87 - ETA: 0s - loss: 0.4778 - accuracy: 0.87 - ETA: 0s - loss: 0.4722 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4714 - accuracy: 0.8731 - val_loss: 0.8859 - val_accuracy: 0.7418\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.90 - ETA: 0s - loss: 0.3914 - accuracy: 0.89 - ETA: 0s - loss: 0.3701 - accuracy: 0.89 - ETA: 0s - loss: 0.3678 - accuracy: 0.89 - ETA: 0s - loss: 0.3794 - accuracy: 0.89 - ETA: 0s - loss: 0.3864 - accuracy: 0.89 - ETA: 0s - loss: 0.3900 - accuracy: 0.89 - ETA: 0s - loss: 0.3874 - accuracy: 0.89 - ETA: 0s - loss: 0.3950 - accuracy: 0.88 - 0s 6ms/step - loss: 0.3972 - accuracy: 0.8884 - val_loss: 1.6242 - val_accuracy: 0.7224\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.84 - ETA: 0s - loss: 0.3818 - accuracy: 0.89 - ETA: 0s - loss: 0.4073 - accuracy: 0.89 - ETA: 0s - loss: 0.4065 - accuracy: 0.88 - ETA: 0s - loss: 0.4110 - accuracy: 0.88 - ETA: 0s - loss: 0.4178 - accuracy: 0.87 - ETA: 0s - loss: 0.4010 - accuracy: 0.88 - ETA: 0s - loss: 0.3947 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4009 - accuracy: 0.8884 - val_loss: 0.9698 - val_accuracy: 0.7209\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.87 - ETA: 0s - loss: 0.3562 - accuracy: 0.90 - ETA: 0s - loss: 0.3870 - accuracy: 0.88 - ETA: 0s - loss: 0.3681 - accuracy: 0.89 - ETA: 0s - loss: 0.3779 - accuracy: 0.89 - ETA: 0s - loss: 0.3966 - accuracy: 0.88 - ETA: 0s - loss: 0.3942 - accuracy: 0.88 - ETA: 0s - loss: 0.3944 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3922 - accuracy: 0.8873 - val_loss: 1.2826 - val_accuracy: 0.7060\n",
      "Epoch 28/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4543 - accuracy: 0.87 - ETA: 0s - loss: 0.3764 - accuracy: 0.90 - ETA: 0s - loss: 0.3545 - accuracy: 0.90 - ETA: 0s - loss: 0.3620 - accuracy: 0.90 - ETA: 0s - loss: 0.3568 - accuracy: 0.90 - ETA: 0s - loss: 0.3602 - accuracy: 0.90 - ETA: 0s - loss: 0.3655 - accuracy: 0.90 - ETA: 0s - loss: 0.3641 - accuracy: 0.9026Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3602 - accuracy: 0.9041 - val_loss: 1.8965 - val_accuracy: 0.7299\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.71 - ETA: 0s - loss: 2.1918 - accuracy: 0.63 - ETA: 0s - loss: 1.5272 - accuracy: 0.63 - ETA: 0s - loss: 1.2526 - accuracy: 0.64 - ETA: 0s - loss: 1.1327 - accuracy: 0.65 - ETA: 0s - loss: 1.0385 - accuracy: 0.67 - ETA: 0s - loss: 0.9664 - accuracy: 0.69 - ETA: 0s - loss: 0.9140 - accuracy: 0.69 - 1s 6ms/step - loss: 0.9034 - accuracy: 0.6988 - val_loss: 0.6022 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.81 - ETA: 0s - loss: 0.6228 - accuracy: 0.73 - ETA: 0s - loss: 0.6695 - accuracy: 0.73 - ETA: 0s - loss: 0.6445 - accuracy: 0.74 - ETA: 0s - loss: 0.6353 - accuracy: 0.75 - ETA: 0s - loss: 0.6487 - accuracy: 0.74 - ETA: 0s - loss: 0.6370 - accuracy: 0.74 - ETA: 0s - loss: 0.6379 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6373 - accuracy: 0.7447 - val_loss: 0.5801 - val_accuracy: 0.7284\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.65 - ETA: 0s - loss: 0.5500 - accuracy: 0.78 - ETA: 0s - loss: 0.5297 - accuracy: 0.80 - ETA: 0s - loss: 0.5507 - accuracy: 0.78 - ETA: 0s - loss: 0.5519 - accuracy: 0.77 - ETA: 0s - loss: 0.5525 - accuracy: 0.77 - ETA: 0s - loss: 0.5569 - accuracy: 0.77 - ETA: 0s - loss: 0.5575 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5571 - accuracy: 0.7772 - val_loss: 0.7139 - val_accuracy: 0.6537\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.78 - ETA: 0s - loss: 0.5524 - accuracy: 0.77 - ETA: 0s - loss: 0.5516 - accuracy: 0.78 - ETA: 0s - loss: 0.5346 - accuracy: 0.79 - ETA: 0s - loss: 0.5273 - accuracy: 0.80 - ETA: 0s - loss: 0.5381 - accuracy: 0.80 - ETA: 0s - loss: 0.5416 - accuracy: 0.80 - ETA: 0s - loss: 0.5311 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5319 - accuracy: 0.8025 - val_loss: 0.6429 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.87 - ETA: 0s - loss: 0.4662 - accuracy: 0.81 - ETA: 0s - loss: 0.4476 - accuracy: 0.83 - ETA: 0s - loss: 0.4769 - accuracy: 0.83 - ETA: 0s - loss: 0.5029 - accuracy: 0.83 - ETA: 0s - loss: 0.5271 - accuracy: 0.82 - ETA: 0s - loss: 0.5292 - accuracy: 0.82 - ETA: 0s - loss: 0.5351 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5319 - accuracy: 0.8201 - val_loss: 0.6105 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.87 - ETA: 0s - loss: 0.5839 - accuracy: 0.79 - ETA: 0s - loss: 0.5615 - accuracy: 0.81 - ETA: 0s - loss: 0.5390 - accuracy: 0.82 - ETA: 0s - loss: 0.5507 - accuracy: 0.81 - ETA: 0s - loss: 0.5596 - accuracy: 0.81 - ETA: 0s - loss: 0.5718 - accuracy: 0.80 - ETA: 0s - loss: 0.5696 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5696 - accuracy: 0.8096 - val_loss: 0.6103 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.93 - ETA: 0s - loss: 0.4617 - accuracy: 0.85 - ETA: 0s - loss: 0.4784 - accuracy: 0.83 - ETA: 0s - loss: 0.4984 - accuracy: 0.83 - ETA: 0s - loss: 0.5028 - accuracy: 0.83 - ETA: 0s - loss: 0.4983 - accuracy: 0.83 - ETA: 0s - loss: 0.5082 - accuracy: 0.83 - ETA: 0s - loss: 0.5089 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5073 - accuracy: 0.8343 - val_loss: 0.7791 - val_accuracy: 0.7060\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.93 - ETA: 0s - loss: 0.5162 - accuracy: 0.81 - ETA: 0s - loss: 0.5205 - accuracy: 0.82 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - ETA: 0s - loss: 0.5052 - accuracy: 0.83 - ETA: 0s - loss: 0.5012 - accuracy: 0.83 - ETA: 0s - loss: 0.5184 - accuracy: 0.82 - ETA: 0s - loss: 0.5221 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5221 - accuracy: 0.8290 - val_loss: 0.6799 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.78 - ETA: 0s - loss: 0.4360 - accuracy: 0.85 - ETA: 0s - loss: 0.4329 - accuracy: 0.85 - ETA: 0s - loss: 0.4397 - accuracy: 0.85 - ETA: 0s - loss: 0.4741 - accuracy: 0.84 - ETA: 0s - loss: 0.4807 - accuracy: 0.83 - ETA: 0s - loss: 0.4877 - accuracy: 0.82 - ETA: 0s - loss: 0.4905 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4889 - accuracy: 0.8294 - val_loss: 0.7319 - val_accuracy: 0.7075\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.87 - ETA: 0s - loss: 0.4632 - accuracy: 0.81 - ETA: 0s - loss: 0.4433 - accuracy: 0.83 - ETA: 0s - loss: 0.4398 - accuracy: 0.84 - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - ETA: 0s - loss: 0.4415 - accuracy: 0.84 - ETA: 0s - loss: 0.4416 - accuracy: 0.84 - ETA: 0s - loss: 0.4446 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4446 - accuracy: 0.8447 - val_loss: 0.7491 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.84 - ETA: 0s - loss: 0.4322 - accuracy: 0.85 - ETA: 0s - loss: 0.4248 - accuracy: 0.85 - ETA: 0s - loss: 0.4247 - accuracy: 0.85 - ETA: 0s - loss: 0.4165 - accuracy: 0.86 - ETA: 0s - loss: 0.4226 - accuracy: 0.85 - ETA: 0s - loss: 0.4180 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4335 - accuracy: 0.8507 - val_loss: 0.7850 - val_accuracy: 0.7269\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.84 - ETA: 0s - loss: 0.4186 - accuracy: 0.88 - ETA: 0s - loss: 0.4333 - accuracy: 0.87 - ETA: 0s - loss: 0.4413 - accuracy: 0.86 - ETA: 0s - loss: 0.4442 - accuracy: 0.86 - ETA: 0s - loss: 0.4509 - accuracy: 0.86 - ETA: 0s - loss: 0.4626 - accuracy: 0.85 - ETA: 0s - loss: 0.4550 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4550 - accuracy: 0.8574 - val_loss: 0.7298 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.90 - ETA: 0s - loss: 0.3677 - accuracy: 0.88 - ETA: 0s - loss: 0.4657 - accuracy: 0.86 - ETA: 0s - loss: 0.4744 - accuracy: 0.85 - ETA: 0s - loss: 0.4680 - accuracy: 0.85 - ETA: 0s - loss: 0.4618 - accuracy: 0.85 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4493 - accuracy: 0.8619 - val_loss: 0.8340 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.90 - ETA: 0s - loss: 0.3995 - accuracy: 0.87 - ETA: 0s - loss: 0.4080 - accuracy: 0.87 - ETA: 0s - loss: 0.4088 - accuracy: 0.87 - ETA: 0s - loss: 0.4127 - accuracy: 0.87 - ETA: 0s - loss: 0.4225 - accuracy: 0.87 - ETA: 0s - loss: 0.4325 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4713 - accuracy: 0.8675 - val_loss: 0.6288 - val_accuracy: 0.7493\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.78 - ETA: 0s - loss: 0.7395 - accuracy: 0.80 - ETA: 0s - loss: 0.7290 - accuracy: 0.78 - ETA: 0s - loss: 0.6855 - accuracy: 0.79 - ETA: 0s - loss: 0.6680 - accuracy: 0.79 - ETA: 0s - loss: 0.6627 - accuracy: 0.78 - ETA: 0s - loss: 0.7405 - accuracy: 0.78 - ETA: 0s - loss: 0.7988 - accuracy: 0.78 - 0s 5ms/step - loss: 0.8536 - accuracy: 0.7764 - val_loss: 0.6229 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6651 - accuracy: 0.71 - ETA: 0s - loss: 0.8415 - accuracy: 0.72 - ETA: 0s - loss: 0.7639 - accuracy: 0.73 - ETA: 0s - loss: 0.7585 - accuracy: 0.75 - ETA: 0s - loss: 0.9026 - accuracy: 0.75 - ETA: 0s - loss: 0.9262 - accuracy: 0.75 - ETA: 0s - loss: 0.9080 - accuracy: 0.74 - ETA: 0s - loss: 0.8802 - accuracy: 0.74 - 0s 5ms/step - loss: 0.8737 - accuracy: 0.7454 - val_loss: 0.7478 - val_accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.84 - ETA: 0s - loss: 0.6289 - accuracy: 0.77 - ETA: 0s - loss: 0.6289 - accuracy: 0.75 - ETA: 0s - loss: 0.8126 - accuracy: 0.75 - ETA: 0s - loss: 0.8027 - accuracy: 0.74 - ETA: 0s - loss: 0.7783 - accuracy: 0.74 - ETA: 0s - loss: 0.7661 - accuracy: 0.73 - ETA: 0s - loss: 0.7540 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7499 - accuracy: 0.7238 - val_loss: 0.6796 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.78 - ETA: 0s - loss: 0.7263 - accuracy: 0.66 - ETA: 0s - loss: 0.7009 - accuracy: 0.69 - ETA: 0s - loss: 0.7011 - accuracy: 0.71 - ETA: 0s - loss: 0.6941 - accuracy: 0.71 - ETA: 0s - loss: 0.6967 - accuracy: 0.71 - ETA: 0s - loss: 0.7029 - accuracy: 0.70 - ETA: 0s - loss: 0.7018 - accuracy: 0.68 - 0s 5ms/step - loss: 0.7015 - accuracy: 0.6495 - val_loss: 0.6973 - val_accuracy: 0.3045\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.28 - ETA: 0s - loss: 0.6924 - accuracy: 0.29 - ETA: 0s - loss: 0.6885 - accuracy: 0.29 - ETA: 0s - loss: 0.6919 - accuracy: 0.29 - ETA: 0s - loss: 0.6941 - accuracy: 0.30 - ETA: 0s - loss: 0.6937 - accuracy: 0.30 - ETA: 0s - loss: 0.6945 - accuracy: 0.30 - ETA: 0s - loss: 0.6911 - accuracy: 0.29 - 0s 5ms/step - loss: 0.6925 - accuracy: 0.3191 - val_loss: 0.6916 - val_accuracy: 0.6955\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.68 - ETA: 0s - loss: 0.7083 - accuracy: 0.40 - ETA: 0s - loss: 0.7054 - accuracy: 0.36 - ETA: 0s - loss: 0.7022 - accuracy: 0.34 - ETA: 0s - loss: 0.6984 - accuracy: 0.33 - ETA: 0s - loss: 0.6935 - accuracy: 0.31 - ETA: 0s - loss: 0.6924 - accuracy: 0.37 - ETA: 0s - loss: 0.6918 - accuracy: 0.42 - 0s 5ms/step - loss: 0.6930 - accuracy: 0.4453 - val_loss: 0.6873 - val_accuracy: 0.6955\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.68 - ETA: 0s - loss: 0.7012 - accuracy: 0.68 - ETA: 0s - loss: 0.7021 - accuracy: 0.61 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.7184 - accuracy: 0.53 - ETA: 0s - loss: 0.7157 - accuracy: 0.50 - ETA: 0s - loss: 0.7102 - accuracy: 0.47 - ETA: 0s - loss: 0.7073 - accuracy: 0.47 - 0s 6ms/step - loss: 0.7073 - accuracy: 0.4733 - val_loss: 0.6906 - val_accuracy: 0.6955\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7640 - accuracy: 0.59 - ETA: 0s - loss: 0.7051 - accuracy: 0.67 - ETA: 0s - loss: 0.6938 - accuracy: 0.69 - ETA: 0s - loss: 0.6866 - accuracy: 0.70 - ETA: 0s - loss: 0.6929 - accuracy: 0.70 - ETA: 0s - loss: 0.6935 - accuracy: 0.70 - ETA: 0s - loss: 0.6953 - accuracy: 0.69 - ETA: 0s - loss: 0.6941 - accuracy: 0.64 - 0s 5ms/step - loss: 0.6932 - accuracy: 0.6170 - val_loss: 0.6954 - val_accuracy: 0.3045\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6806 - accuracy: 0.28 - ETA: 0s - loss: 0.6992 - accuracy: 0.41 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.57 - ETA: 0s - loss: 0.6880 - accuracy: 0.60 - ETA: 0s - loss: 0.6908 - accuracy: 0.62 - ETA: 0s - loss: 0.6899 - accuracy: 0.63 - ETA: 0s - loss: 0.6942 - accuracy: 0.64 - 0s 5ms/step - loss: 0.6925 - accuracy: 0.6163 - val_loss: 0.6959 - val_accuracy: 0.3045\n",
      "Epoch 24/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.8006 - accuracy: 0.46 - ETA: 0s - loss: 0.6991 - accuracy: 0.30 - ETA: 0s - loss: 0.6973 - accuracy: 0.30 - ETA: 0s - loss: 0.6908 - accuracy: 0.29 - ETA: 0s - loss: 0.6888 - accuracy: 0.37 - ETA: 0s - loss: 0.6906 - accuracy: 0.43 - ETA: 0s - loss: 0.6889 - accuracy: 0.48 - ETA: 0s - loss: 0.6942 - accuracy: 0.5020Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.4860 - val_loss: 0.6960 - val_accuracy: 0.3045\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 54c138be49cae5f398ac8abf3cd695c8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7477612098058065</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.735583956970301</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.50 - ETA: 0s - loss: 1.7359 - accuracy: 0.64 - ETA: 0s - loss: 1.2610 - accuracy: 0.63 - ETA: 0s - loss: 1.0510 - accuracy: 0.65 - ETA: 0s - loss: 0.9413 - accuracy: 0.66 - 0s 5ms/step - loss: 0.9125 - accuracy: 0.6708 - val_loss: 0.5832 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.68 - ETA: 0s - loss: 0.5744 - accuracy: 0.73 - ETA: 0s - loss: 0.5638 - accuracy: 0.72 - ETA: 0s - loss: 0.5603 - accuracy: 0.73 - 0s 3ms/step - loss: 0.5531 - accuracy: 0.7406 - val_loss: 0.5893 - val_accuracy: 0.7224\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.65 - ETA: 0s - loss: 0.4764 - accuracy: 0.78 - ETA: 0s - loss: 0.4626 - accuracy: 0.79 - ETA: 0s - loss: 0.4816 - accuracy: 0.78 - 0s 3ms/step - loss: 0.4931 - accuracy: 0.7839 - val_loss: 0.6037 - val_accuracy: 0.7060\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3320 - accuracy: 0.87 - ETA: 0s - loss: 0.4406 - accuracy: 0.81 - ETA: 0s - loss: 0.4296 - accuracy: 0.80 - ETA: 0s - loss: 0.4385 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4493 - accuracy: 0.8010 - val_loss: 0.6448 - val_accuracy: 0.6866\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.84 - ETA: 0s - loss: 0.4206 - accuracy: 0.79 - ETA: 0s - loss: 0.4029 - accuracy: 0.81 - ETA: 0s - loss: 0.4125 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4205 - accuracy: 0.8137 - val_loss: 0.6993 - val_accuracy: 0.6866\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6603 - accuracy: 0.78 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3716 - accuracy: 0.83 - ETA: 0s - loss: 0.3858 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8354 - val_loss: 1.1576 - val_accuracy: 0.6433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.84 - ETA: 0s - loss: 0.3864 - accuracy: 0.83 - ETA: 0s - loss: 0.3695 - accuracy: 0.83 - ETA: 0s - loss: 0.3695 - accuracy: 0.82 - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8320 - val_loss: 1.2437 - val_accuracy: 0.6448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.81 - ETA: 0s - loss: 0.3206 - accuracy: 0.84 - ETA: 0s - loss: 0.2979 - accuracy: 0.85 - ETA: 0s - loss: 0.3309 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8466 - val_loss: 1.0678 - val_accuracy: 0.6881\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.90 - ETA: 0s - loss: 0.3888 - accuracy: 0.83 - ETA: 0s - loss: 0.3639 - accuracy: 0.84 - ETA: 0s - loss: 0.3660 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8376 - val_loss: 0.6502 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.84 - ETA: 0s - loss: 0.3738 - accuracy: 0.86 - ETA: 0s - loss: 0.3744 - accuracy: 0.85 - ETA: 0s - loss: 0.3468 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8555 - val_loss: 1.6549 - val_accuracy: 0.6537\n",
      "Epoch 11/50\n",
      "64/84 [=====================>........] - ETA: 0s - loss: 0.2117 - accuracy: 0.93 - ETA: 0s - loss: 0.2768 - accuracy: 0.88 - ETA: 0s - loss: 0.3044 - accuracy: 0.88 - ETA: 0s - loss: 0.3006 - accuracy: 0.8857Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8761 - val_loss: 1.7699 - val_accuracy: 0.6701\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.75 - ETA: 0s - loss: 1.1440 - accuracy: 0.63 - ETA: 0s - loss: 0.9730 - accuracy: 0.63 - ETA: 0s - loss: 0.8841 - accuracy: 0.63 - ETA: 0s - loss: 0.8195 - accuracy: 0.65 - 0s 4ms/step - loss: 0.8058 - accuracy: 0.6599 - val_loss: 0.6180 - val_accuracy: 0.6836\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.78 - ETA: 0s - loss: 0.5756 - accuracy: 0.72 - ETA: 0s - loss: 0.5555 - accuracy: 0.72 - ETA: 0s - loss: 0.5569 - accuracy: 0.71 - ETA: 0s - loss: 0.5561 - accuracy: 0.72 - 0s 3ms/step - loss: 0.5615 - accuracy: 0.7253 - val_loss: 0.5999 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.65 - ETA: 0s - loss: 0.5339 - accuracy: 0.76 - ETA: 0s - loss: 0.5111 - accuracy: 0.78 - ETA: 0s - loss: 0.5198 - accuracy: 0.77 - ETA: 0s - loss: 0.5209 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5217 - accuracy: 0.7667 - val_loss: 0.5718 - val_accuracy: 0.6925\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.81 - ETA: 0s - loss: 0.4469 - accuracy: 0.79 - ETA: 0s - loss: 0.4550 - accuracy: 0.80 - ETA: 0s - loss: 0.4557 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4606 - accuracy: 0.8033 - val_loss: 0.6714 - val_accuracy: 0.6761\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.84 - ETA: 0s - loss: 0.4046 - accuracy: 0.83 - ETA: 0s - loss: 0.3891 - accuracy: 0.84 - ETA: 0s - loss: 0.4009 - accuracy: 0.83 - ETA: 0s - loss: 0.4121 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8163 - val_loss: 0.6195 - val_accuracy: 0.6910\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.71 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3727 - accuracy: 0.83 - ETA: 0s - loss: 0.3839 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8331 - val_loss: 0.7200 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.90 - ETA: 0s - loss: 0.3495 - accuracy: 0.86 - ETA: 0s - loss: 0.3852 - accuracy: 0.84 - ETA: 0s - loss: 0.3984 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8373 - val_loss: 0.7543 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.87 - ETA: 0s - loss: 0.3202 - accuracy: 0.85 - ETA: 0s - loss: 0.3579 - accuracy: 0.83 - ETA: 0s - loss: 0.3565 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8447 - val_loss: 0.8112 - val_accuracy: 0.6851\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.87 - ETA: 0s - loss: 0.2774 - accuracy: 0.86 - ETA: 0s - loss: 0.3117 - accuracy: 0.86 - ETA: 0s - loss: 0.3147 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8630 - val_loss: 1.1255 - val_accuracy: 0.6687\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.78 - ETA: 0s - loss: 0.2460 - accuracy: 0.90 - ETA: 0s - loss: 0.3028 - accuracy: 0.87 - ETA: 0s - loss: 0.3211 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8630 - val_loss: 0.9013 - val_accuracy: 0.7015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.93 - ETA: 0s - loss: 0.2356 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.90 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - 0s 3ms/step - loss: 0.2954 - accuracy: 0.8820 - val_loss: 0.7587 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.90 - ETA: 0s - loss: 0.2909 - accuracy: 0.89 - ETA: 0s - loss: 0.3236 - accuracy: 0.87 - ETA: 0s - loss: 0.3254 - accuracy: 0.87 - ETA: 0s - loss: 0.3281 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8694 - val_loss: 0.9873 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.81 - ETA: 0s - loss: 0.2390 - accuracy: 0.91 - ETA: 0s - loss: 0.2597 - accuracy: 0.90 - ETA: 0s - loss: 0.2731 - accuracy: 0.90 - ETA: 0s - loss: 0.2621 - accuracy: 0.90 - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9026 - val_loss: 1.5643 - val_accuracy: 0.6716\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.93 - ETA: 0s - loss: 0.3700 - accuracy: 0.88 - ETA: 0s - loss: 0.3341 - accuracy: 0.89 - ETA: 0s - loss: 0.2883 - accuracy: 0.90 - 0s 3ms/step - loss: 0.2800 - accuracy: 0.9037 - val_loss: 1.2383 - val_accuracy: 0.7313\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.90 - ETA: 0s - loss: 0.3418 - accuracy: 0.88 - ETA: 0s - loss: 0.3936 - accuracy: 0.86 - ETA: 0s - loss: 0.3697 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8727 - val_loss: 1.4960 - val_accuracy: 0.6851\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.84 - ETA: 0s - loss: 0.3382 - accuracy: 0.87 - ETA: 0s - loss: 0.3275 - accuracy: 0.88 - ETA: 0s - loss: 0.3178 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3315 - accuracy: 0.8903 - val_loss: 1.0092 - val_accuracy: 0.7119\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.81 - ETA: 0s - loss: 0.2802 - accuracy: 0.90 - ETA: 0s - loss: 0.3810 - accuracy: 0.88 - ETA: 0s - loss: 0.3907 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8772 - val_loss: 2.5585 - val_accuracy: 0.6567\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.87 - ETA: 0s - loss: 0.4568 - accuracy: 0.86 - ETA: 0s - loss: 0.3926 - accuracy: 0.87 - ETA: 0s - loss: 0.3500 - accuracy: 0.88 - ETA: 0s - loss: 0.3321 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8895 - val_loss: 1.3095 - val_accuracy: 0.7060\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.90 - ETA: 0s - loss: 0.4419 - accuracy: 0.89 - ETA: 0s - loss: 0.4165 - accuracy: 0.87 - ETA: 0s - loss: 0.4013 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8675 - val_loss: 0.9890 - val_accuracy: 0.6716\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.90 - ETA: 0s - loss: 0.2760 - accuracy: 0.89 - ETA: 0s - loss: 0.2997 - accuracy: 0.88 - ETA: 0s - loss: 0.2915 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8843 - val_loss: 0.9436 - val_accuracy: 0.7104\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.96 - ETA: 0s - loss: 0.3082 - accuracy: 0.88 - ETA: 0s - loss: 0.2883 - accuracy: 0.89 - ETA: 0s - loss: 0.2919 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8791 - val_loss: 1.9277 - val_accuracy: 0.7224\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.87 - ETA: 0s - loss: 0.6528 - accuracy: 0.85 - ETA: 0s - loss: 0.5584 - accuracy: 0.86 - ETA: 0s - loss: 0.5672 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5558 - accuracy: 0.8429 - val_loss: 1.4121 - val_accuracy: 0.6896\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.90 - ETA: 0s - loss: 0.4574 - accuracy: 0.86 - ETA: 0s - loss: 0.3917 - accuracy: 0.87 - ETA: 0s - loss: 0.4133 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8611 - val_loss: 2.2717 - val_accuracy: 0.7403\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.90 - ETA: 0s - loss: 0.3491 - accuracy: 0.87 - ETA: 0s - loss: 0.3093 - accuracy: 0.89 - ETA: 0s - loss: 0.3595 - accuracy: 0.89 - ETA: 0s - loss: 0.4310 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8750 - val_loss: 0.8274 - val_accuracy: 0.7493\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.87 - ETA: 0s - loss: 0.5143 - accuracy: 0.82 - ETA: 0s - loss: 0.4681 - accuracy: 0.83 - ETA: 0s - loss: 0.4620 - accuracy: 0.84 - ETA: 0s - loss: 0.4529 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4529 - accuracy: 0.8496 - val_loss: 1.2282 - val_accuracy: 0.7373\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.81 - ETA: 0s - loss: 0.3847 - accuracy: 0.87 - ETA: 0s - loss: 0.3896 - accuracy: 0.87 - ETA: 0s - loss: 0.4315 - accuracy: 0.87 - ETA: 0s - loss: 0.4043 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4018 - accuracy: 0.8828 - val_loss: 2.3963 - val_accuracy: 0.7254\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.87 - ETA: 0s - loss: 0.3309 - accuracy: 0.89 - ETA: 0s - loss: 0.3451 - accuracy: 0.89 - ETA: 0s - loss: 0.3890 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8914 - val_loss: 1.4215 - val_accuracy: 0.7388\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.78 - ETA: 0s - loss: 0.3305 - accuracy: 0.90 - ETA: 0s - loss: 0.3185 - accuracy: 0.90 - ETA: 0s - loss: 0.3296 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3505 - accuracy: 0.9003 - val_loss: 1.4425 - val_accuracy: 0.7433\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.93 - ETA: 0s - loss: 0.3233 - accuracy: 0.89 - ETA: 0s - loss: 0.3006 - accuracy: 0.90 - ETA: 0s - loss: 0.3134 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3373 - accuracy: 0.9059 - val_loss: 2.4932 - val_accuracy: 0.7179\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.81 - ETA: 0s - loss: 0.3275 - accuracy: 0.88 - ETA: 0s - loss: 0.3415 - accuracy: 0.88 - ETA: 0s - loss: 0.3168 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8966 - val_loss: 1.4198 - val_accuracy: 0.7299\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.90 - ETA: 0s - loss: 0.2335 - accuracy: 0.93 - ETA: 0s - loss: 0.2532 - accuracy: 0.92 - ETA: 0s - loss: 0.2552 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2628 - accuracy: 0.9253 - val_loss: 1.7262 - val_accuracy: 0.7075\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.96 - ETA: 0s - loss: 0.2069 - accuracy: 0.94 - ETA: 0s - loss: 0.2171 - accuracy: 0.94 - ETA: 0s - loss: 0.2347 - accuracy: 0.93 - 0s 3ms/step - loss: 0.2765 - accuracy: 0.9309 - val_loss: 1.6424 - val_accuracy: 0.7119\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.93 - ETA: 0s - loss: 0.2721 - accuracy: 0.91 - ETA: 0s - loss: 0.2608 - accuracy: 0.92 - ETA: 0s - loss: 0.2740 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3155 - accuracy: 0.9100 - val_loss: 2.2288 - val_accuracy: 0.7299\n",
      "Epoch 34/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.3417 - accuracy: 0.90 - ETA: 0s - loss: 0.2938 - accuracy: 0.90 - ETA: 0s - loss: 0.2688 - accuracy: 0.91 - ETA: 0s - loss: 0.2935 - accuracy: 0.9072Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.9089 - val_loss: 2.2805 - val_accuracy: 0.7149\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.78 - ETA: 0s - loss: 1.3276 - accuracy: 0.60 - ETA: 0s - loss: 1.2142 - accuracy: 0.60 - ETA: 0s - loss: 1.0615 - accuracy: 0.62 - ETA: 0s - loss: 0.9393 - accuracy: 0.64 - 0s 4ms/step - loss: 0.8900 - accuracy: 0.6596 - val_loss: 0.5877 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.71 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5555 - accuracy: 0.72 - ETA: 0s - loss: 0.5516 - accuracy: 0.72 - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7309 - val_loss: 0.5738 - val_accuracy: 0.6881\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.81 - ETA: 0s - loss: 0.4702 - accuracy: 0.78 - ETA: 0s - loss: 0.4800 - accuracy: 0.78 - ETA: 0s - loss: 0.4939 - accuracy: 0.77 - ETA: 0s - loss: 0.4935 - accuracy: 0.77 - 0s 3ms/step - loss: 0.4935 - accuracy: 0.7760 - val_loss: 0.5997 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.68 - ETA: 0s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.4331 - accuracy: 0.79 - ETA: 0s - loss: 0.4595 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4632 - accuracy: 0.7910 - val_loss: 0.6124 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 0.84 - ETA: 0s - loss: 0.4721 - accuracy: 0.78 - ETA: 0s - loss: 0.4600 - accuracy: 0.77 - ETA: 0s - loss: 0.4403 - accuracy: 0.79 - ETA: 0s - loss: 0.4483 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4446 - accuracy: 0.7988 - val_loss: 0.6345 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.90 - ETA: 0s - loss: 0.3840 - accuracy: 0.83 - ETA: 0s - loss: 0.3910 - accuracy: 0.83 - ETA: 0s - loss: 0.3967 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8287 - val_loss: 0.8405 - val_accuracy: 0.6776\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.96 - ETA: 0s - loss: 0.3874 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.84 - ETA: 0s - loss: 0.3788 - accuracy: 0.84 - ETA: 0s - loss: 0.3947 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8346 - val_loss: 0.6467 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7587 - accuracy: 0.59 - ETA: 0s - loss: 0.4254 - accuracy: 0.80 - ETA: 0s - loss: 0.4237 - accuracy: 0.80 - ETA: 0s - loss: 0.4250 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8145 - val_loss: 0.9838 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.90 - ETA: 0s - loss: 0.5068 - accuracy: 0.81 - ETA: 0s - loss: 0.4319 - accuracy: 0.83 - ETA: 0s - loss: 0.4132 - accuracy: 0.83 - ETA: 0s - loss: 0.4099 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8358 - val_loss: 0.9889 - val_accuracy: 0.6716\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.93 - ETA: 0s - loss: 0.3483 - accuracy: 0.86 - ETA: 0s - loss: 0.3619 - accuracy: 0.85 - ETA: 0s - loss: 0.3700 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8402 - val_loss: 0.5834 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5867 - accuracy: 0.75 - ETA: 0s - loss: 0.4284 - accuracy: 0.84 - ETA: 0s - loss: 0.4099 - accuracy: 0.85 - ETA: 0s - loss: 0.3809 - accuracy: 0.85 - ETA: 0s - loss: 0.3811 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8488 - val_loss: 1.1917 - val_accuracy: 0.6537\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.87 - ETA: 0s - loss: 0.2890 - accuracy: 0.88 - ETA: 0s - loss: 0.3400 - accuracy: 0.87 - ETA: 0s - loss: 0.3642 - accuracy: 0.86 - ETA: 0s - loss: 0.3762 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8578 - val_loss: 1.0721 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.81 - ETA: 0s - loss: 0.3828 - accuracy: 0.84 - ETA: 0s - loss: 0.4043 - accuracy: 0.85 - ETA: 0s - loss: 0.4046 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8402 - val_loss: 1.1628 - val_accuracy: 0.7119\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7402 - accuracy: 0.71 - ETA: 0s - loss: 0.6477 - accuracy: 0.83 - ETA: 0s - loss: 0.5330 - accuracy: 0.83 - ETA: 0s - loss: 0.5135 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4981 - accuracy: 0.8324 - val_loss: 1.3440 - val_accuracy: 0.6687\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.78 - ETA: 0s - loss: 0.5172 - accuracy: 0.83 - ETA: 0s - loss: 0.4492 - accuracy: 0.84 - ETA: 0s - loss: 0.4243 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8514 - val_loss: 1.2459 - val_accuracy: 0.6761\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.90 - ETA: 0s - loss: 0.3006 - accuracy: 0.89 - ETA: 0s - loss: 0.2985 - accuracy: 0.88 - ETA: 0s - loss: 0.3132 - accuracy: 0.87 - ETA: 0s - loss: 0.3623 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8783 - val_loss: 1.2993 - val_accuracy: 0.7045\n",
      "Epoch 17/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2224 - accuracy: 0.90 - ETA: 0s - loss: 0.3071 - accuracy: 0.90 - ETA: 0s - loss: 0.3431 - accuracy: 0.90 - ETA: 0s - loss: 0.3622 - accuracy: 0.88 - ETA: 0s - loss: 0.3664 - accuracy: 0.8812Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8806 - val_loss: 2.0259 - val_accuracy: 0.6925\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f217591a87367770dfd5eede40b68893</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7363184094429016</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.36792783225873604</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 95</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1686 - accuracy: 0.46 - ETA: 0s - loss: 0.9264 - accuracy: 0.64 - ETA: 0s - loss: 0.7851 - accuracy: 0.68 - ETA: 0s - loss: 0.7326 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7230 - accuracy: 0.6909 - val_loss: 0.6003 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.68 - ETA: 0s - loss: 0.5896 - accuracy: 0.75 - ETA: 0s - loss: 0.5810 - accuracy: 0.75 - ETA: 0s - loss: 0.5862 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5853 - accuracy: 0.7507 - val_loss: 0.6031 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.81 - ETA: 0s - loss: 0.5568 - accuracy: 0.77 - ETA: 0s - loss: 0.5727 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5810 - accuracy: 0.7600 - val_loss: 0.6179 - val_accuracy: 0.6776\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.81 - ETA: 0s - loss: 0.5221 - accuracy: 0.77 - ETA: 0s - loss: 0.5731 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5659 - accuracy: 0.7790 - val_loss: 0.6038 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4573 - accuracy: 0.84 - ETA: 0s - loss: 0.5048 - accuracy: 0.81 - ETA: 0s - loss: 0.5021 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5138 - accuracy: 0.8044 - val_loss: 0.5763 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.90 - ETA: 0s - loss: 0.5169 - accuracy: 0.81 - ETA: 0s - loss: 0.4878 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5122 - accuracy: 0.8104 - val_loss: 0.7827 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.90 - ETA: 0s - loss: 0.6194 - accuracy: 0.83 - ETA: 0s - loss: 0.5709 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5701 - accuracy: 0.8122 - val_loss: 0.7203 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.75 - ETA: 0s - loss: 0.4774 - accuracy: 0.81 - ETA: 0s - loss: 0.4621 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8305 - val_loss: 0.7752 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.84 - ETA: 0s - loss: 0.5378 - accuracy: 0.83 - ETA: 0s - loss: 0.6105 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5602 - accuracy: 0.8190 - val_loss: 0.6836 - val_accuracy: 0.7030\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.84 - ETA: 0s - loss: 0.4374 - accuracy: 0.84 - ETA: 0s - loss: 0.4410 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8499 - val_loss: 1.3197 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.81 - ETA: 0s - loss: 0.5527 - accuracy: 0.82 - ETA: 0s - loss: 0.5098 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8361 - val_loss: 0.7127 - val_accuracy: 0.6940\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.81 - ETA: 0s - loss: 0.3645 - accuracy: 0.87 - ETA: 0s - loss: 0.4087 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8582 - val_loss: 0.6213 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.87 - ETA: 0s - loss: 0.4227 - accuracy: 0.86 - ETA: 0s - loss: 0.3838 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8660 - val_loss: 0.5963 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.87 - ETA: 0s - loss: 0.3816 - accuracy: 0.86 - ETA: 0s - loss: 0.3704 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4449 - accuracy: 0.8563 - val_loss: 0.6681 - val_accuracy: 0.7119\n",
      "Epoch 15/50\n",
      "59/84 [====================>.........] - ETA: 0s - loss: 0.5250 - accuracy: 0.81 - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.4036 - accuracy: 0.8686Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8611 - val_loss: 0.7258 - val_accuracy: 0.7090\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.59 - ETA: 0s - loss: 0.9872 - accuracy: 0.66 - ETA: 0s - loss: 0.8185 - accuracy: 0.69 - ETA: 0s - loss: 0.7568 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7449 - accuracy: 0.7021 - val_loss: 0.5543 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.78 - ETA: 0s - loss: 0.5756 - accuracy: 0.72 - ETA: 0s - loss: 0.5838 - accuracy: 0.72 - ETA: 0s - loss: 0.5798 - accuracy: 0.73 - 0s 2ms/step - loss: 0.5798 - accuracy: 0.7353 - val_loss: 0.5600 - val_accuracy: 0.7433\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.78 - ETA: 0s - loss: 0.5319 - accuracy: 0.79 - ETA: 0s - loss: 0.5401 - accuracy: 0.79 - ETA: 0s - loss: 0.5490 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7824 - val_loss: 0.5765 - val_accuracy: 0.7149\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.87 - ETA: 0s - loss: 0.4678 - accuracy: 0.83 - ETA: 0s - loss: 0.5235 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7910 - val_loss: 0.6442 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.78 - ETA: 0s - loss: 0.5496 - accuracy: 0.79 - ETA: 0s - loss: 0.5381 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7828 - val_loss: 0.5691 - val_accuracy: 0.7328\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.81 - ETA: 0s - loss: 0.5816 - accuracy: 0.78 - ETA: 0s - loss: 0.5988 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7947 - val_loss: 0.8529 - val_accuracy: 0.6896\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.78 - ETA: 0s - loss: 0.4751 - accuracy: 0.81 - ETA: 0s - loss: 0.4804 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4927 - accuracy: 0.8201 - val_loss: 0.9434 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.68 - ETA: 0s - loss: 0.5147 - accuracy: 0.81 - ETA: 0s - loss: 0.5158 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5300 - accuracy: 0.8111 - val_loss: 1.0745 - val_accuracy: 0.7463\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.78 - ETA: 0s - loss: 0.5343 - accuracy: 0.81 - ETA: 0s - loss: 0.5339 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5600 - accuracy: 0.7925 - val_loss: 0.7807 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.84 - ETA: 0s - loss: 0.6631 - accuracy: 0.79 - ETA: 0s - loss: 0.7076 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6761 - accuracy: 0.7783 - val_loss: 0.8657 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6156 - accuracy: 0.78 - ETA: 0s - loss: 0.5926 - accuracy: 0.79 - ETA: 0s - loss: 0.6106 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5952 - accuracy: 0.8052 - val_loss: 0.8058 - val_accuracy: 0.7433\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.90 - ETA: 0s - loss: 0.5268 - accuracy: 0.81 - ETA: 0s - loss: 0.5183 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5268 - accuracy: 0.8156 - val_loss: 1.2987 - val_accuracy: 0.7627\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5207 - accuracy: 0.81 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - ETA: 0s - loss: 0.4766 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4683 - accuracy: 0.8346 - val_loss: 1.2794 - val_accuracy: 0.7224\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.78 - ETA: 0s - loss: 0.4524 - accuracy: 0.83 - ETA: 0s - loss: 0.4407 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4738 - accuracy: 0.8373 - val_loss: 0.7064 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.84 - ETA: 0s - loss: 0.4628 - accuracy: 0.84 - ETA: 0s - loss: 0.4378 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4370 - accuracy: 0.8544 - val_loss: 0.8823 - val_accuracy: 0.6866\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.84 - ETA: 0s - loss: 0.4041 - accuracy: 0.87 - ETA: 0s - loss: 0.4087 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4019 - accuracy: 0.8761 - val_loss: 1.2037 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.84 - ETA: 0s - loss: 0.4239 - accuracy: 0.87 - ETA: 0s - loss: 0.4205 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8705 - val_loss: 0.7467 - val_accuracy: 0.7448\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.84 - ETA: 0s - loss: 0.3934 - accuracy: 0.87 - ETA: 0s - loss: 0.3919 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8783 - val_loss: 0.6767 - val_accuracy: 0.7299\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.90 - ETA: 0s - loss: 0.3256 - accuracy: 0.91 - ETA: 0s - loss: 0.3717 - accuracy: 0.89 - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8817 - val_loss: 0.5502 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.84 - ETA: 0s - loss: 0.4084 - accuracy: 0.89 - ETA: 0s - loss: 0.4079 - accuracy: 0.88 - ETA: 0s - loss: 0.4036 - accuracy: 0.88 - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8850 - val_loss: 0.6745 - val_accuracy: 0.7388\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.90 - ETA: 0s - loss: 0.3608 - accuracy: 0.89 - ETA: 0s - loss: 0.3787 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8921 - val_loss: 0.7641 - val_accuracy: 0.7134\n",
      "Epoch 22/50\n",
      "59/84 [====================>.........] - ETA: 0s - loss: 0.2022 - accuracy: 0.93 - ETA: 0s - loss: 0.3402 - accuracy: 0.90 - ETA: 0s - loss: 0.3441 - accuracy: 0.9036Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.9015 - val_loss: 1.0087 - val_accuracy: 0.7104\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6440 - accuracy: 0.71 - ETA: 0s - loss: 0.8669 - accuracy: 0.66 - ETA: 0s - loss: 0.7573 - accuracy: 0.67 - ETA: 0s - loss: 0.7177 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7040 - accuracy: 0.6984 - val_loss: 0.7051 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.84 - ETA: 0s - loss: 0.6001 - accuracy: 0.76 - ETA: 0s - loss: 0.5805 - accuracy: 0.76 - ETA: 0s - loss: 0.5829 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5829 - accuracy: 0.7577 - val_loss: 0.6550 - val_accuracy: 0.6821\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.75 - ETA: 0s - loss: 0.6014 - accuracy: 0.75 - ETA: 0s - loss: 0.5685 - accuracy: 0.76 - ETA: 0s - loss: 0.5521 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5521 - accuracy: 0.7719 - val_loss: 0.6218 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.71 - ETA: 0s - loss: 0.5010 - accuracy: 0.83 - ETA: 0s - loss: 0.5128 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5208 - accuracy: 0.8137 - val_loss: 0.5357 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.87 - ETA: 0s - loss: 0.4958 - accuracy: 0.83 - ETA: 0s - loss: 0.4885 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5031 - accuracy: 0.8111 - val_loss: 0.6692 - val_accuracy: 0.6866\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.75 - ETA: 0s - loss: 0.5365 - accuracy: 0.78 - ETA: 0s - loss: 0.5325 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5222 - accuracy: 0.7869 - val_loss: 0.6580 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.87 - ETA: 0s - loss: 0.3879 - accuracy: 0.85 - ETA: 0s - loss: 0.4342 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5247 - accuracy: 0.8145 - val_loss: 0.5498 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.78 - ETA: 0s - loss: 0.4795 - accuracy: 0.79 - ETA: 0s - loss: 0.5526 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7928 - val_loss: 0.6651 - val_accuracy: 0.6910\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.71 - ETA: 0s - loss: 0.4150 - accuracy: 0.83 - ETA: 0s - loss: 0.4220 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8246 - val_loss: 0.5318 - val_accuracy: 0.7403\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.90 - ETA: 0s - loss: 0.3659 - accuracy: 0.87 - ETA: 0s - loss: 0.3892 - accuracy: 0.85 - ETA: 0s - loss: 0.4052 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8477 - val_loss: 0.6157 - val_accuracy: 0.7328\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.93 - ETA: 0s - loss: 0.3530 - accuracy: 0.88 - ETA: 0s - loss: 0.3814 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3637 - accuracy: 0.8712 - val_loss: 0.6539 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.87 - ETA: 0s - loss: 0.3677 - accuracy: 0.84 - ETA: 0s - loss: 0.3937 - accuracy: 0.84 - ETA: 0s - loss: 0.4024 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8473 - val_loss: 0.5976 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.93 - ETA: 0s - loss: 0.3814 - accuracy: 0.87 - ETA: 0s - loss: 0.3579 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8847 - val_loss: 0.7637 - val_accuracy: 0.7209\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.84 - ETA: 0s - loss: 0.3394 - accuracy: 0.89 - ETA: 0s - loss: 0.3664 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8858 - val_loss: 0.7215 - val_accuracy: 0.7284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2814 - accuracy: 0.96 - ETA: 0s - loss: 0.3506 - accuracy: 0.89 - ETA: 0s - loss: 0.3312 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8977 - val_loss: 0.7612 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.87 - ETA: 0s - loss: 0.2737 - accuracy: 0.91 - ETA: 0s - loss: 0.3054 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3251 - accuracy: 0.9018 - val_loss: 0.7980 - val_accuracy: 0.7239\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.87 - ETA: 0s - loss: 0.3157 - accuracy: 0.90 - ETA: 0s - loss: 0.3078 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3175 - accuracy: 0.9059 - val_loss: 0.8111 - val_accuracy: 0.7104\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.84 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - ETA: 0s - loss: 0.2786 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9119 - val_loss: 0.8624 - val_accuracy: 0.7075\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.93 - ETA: 0s - loss: 0.2267 - accuracy: 0.93 - ETA: 0s - loss: 0.2297 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2435 - accuracy: 0.9339 - val_loss: 1.0539 - val_accuracy: 0.7119\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.93 - ETA: 0s - loss: 0.2507 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2461 - accuracy: 0.9336 - val_loss: 0.9784 - val_accuracy: 0.7119\n",
      "Epoch 21/50\n",
      "59/84 [====================>.........] - ETA: 0s - loss: 0.2077 - accuracy: 0.96 - ETA: 0s - loss: 0.2404 - accuracy: 0.93 - ETA: 0s - loss: 0.2360 - accuracy: 0.9343Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9261 - val_loss: 0.9108 - val_accuracy: 0.7045\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 324325dc68ee83beef64a9b7f8645f6d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7492537299791971</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6801011408170626</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 240</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9011 - accuracy: 0.40 - ETA: 0s - loss: 2.1272 - accuracy: 0.60 - ETA: 0s - loss: 1.4027 - accuracy: 0.63 - ETA: 0s - loss: 1.1161 - accuracy: 0.66 - 0s 4ms/step - loss: 1.0463 - accuracy: 0.6689 - val_loss: 0.5605 - val_accuracy: 0.7522\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8482 - accuracy: 0.62 - ETA: 0s - loss: 0.6463 - accuracy: 0.74 - ETA: 0s - loss: 0.6206 - accuracy: 0.75 - ETA: 0s - loss: 0.6144 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6184 - accuracy: 0.7574 - val_loss: 0.5769 - val_accuracy: 0.7373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7220 - accuracy: 0.84 - ETA: 0s - loss: 0.5559 - accuracy: 0.77 - ETA: 0s - loss: 0.5760 - accuracy: 0.76 - ETA: 0s - loss: 0.5890 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5911 - accuracy: 0.7633 - val_loss: 0.5718 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.87 - ETA: 0s - loss: 0.5438 - accuracy: 0.79 - ETA: 0s - loss: 0.5416 - accuracy: 0.79 - ETA: 0s - loss: 0.5432 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5576 - accuracy: 0.7887 - val_loss: 0.7081 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0275 - accuracy: 0.62 - ETA: 0s - loss: 0.6833 - accuracy: 0.75 - ETA: 0s - loss: 0.6486 - accuracy: 0.76 - ETA: 0s - loss: 0.6435 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6518 - accuracy: 0.7689 - val_loss: 0.6138 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.78 - ETA: 0s - loss: 0.5941 - accuracy: 0.77 - ETA: 0s - loss: 0.6071 - accuracy: 0.76 - ETA: 0s - loss: 0.6156 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6156 - accuracy: 0.7600 - val_loss: 0.6008 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.90 - ETA: 0s - loss: 0.5998 - accuracy: 0.79 - ETA: 0s - loss: 0.6122 - accuracy: 0.78 - ETA: 0s - loss: 0.6232 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6267 - accuracy: 0.7813 - val_loss: 0.5886 - val_accuracy: 0.7463\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.78 - ETA: 0s - loss: 0.6386 - accuracy: 0.79 - ETA: 0s - loss: 0.6479 - accuracy: 0.78 - ETA: 0s - loss: 0.6287 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6341 - accuracy: 0.7772 - val_loss: 0.6041 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.75 - ETA: 0s - loss: 0.6236 - accuracy: 0.77 - ETA: 0s - loss: 0.6348 - accuracy: 0.78 - ETA: 0s - loss: 0.6399 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6386 - accuracy: 0.7745 - val_loss: 0.5976 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.87 - ETA: 0s - loss: 0.7379 - accuracy: 0.74 - ETA: 0s - loss: 0.6994 - accuracy: 0.75 - ETA: 0s - loss: 0.6830 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6759 - accuracy: 0.7499 - val_loss: 0.7974 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.6589 - accuracy: 0.75 - ETA: 0s - loss: 0.7809 - accuracy: 0.75 - ETA: 0s - loss: 0.7324 - accuracy: 0.76 - ETA: 0s - loss: 0.7167 - accuracy: 0.7606Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7091 - accuracy: 0.7604 - val_loss: 0.6317 - val_accuracy: 0.7373\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7481 - accuracy: 0.68 - ETA: 0s - loss: 2.1813 - accuracy: 0.59 - ETA: 0s - loss: 1.5018 - accuracy: 0.63 - ETA: 0s - loss: 1.1951 - accuracy: 0.66 - 0s 4ms/step - loss: 1.1117 - accuracy: 0.6685 - val_loss: 0.6049 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6222 - accuracy: 0.74 - ETA: 0s - loss: 0.6068 - accuracy: 0.74 - ETA: 0s - loss: 0.6051 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6054 - accuracy: 0.7443 - val_loss: 0.6140 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.78 - ETA: 0s - loss: 0.5634 - accuracy: 0.79 - ETA: 0s - loss: 0.5768 - accuracy: 0.78 - ETA: 0s - loss: 0.5758 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7779 - val_loss: 0.5750 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9760 - accuracy: 0.65 - ETA: 0s - loss: 0.5824 - accuracy: 0.78 - ETA: 0s - loss: 0.5702 - accuracy: 0.79 - ETA: 0s - loss: 0.5791 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5727 - accuracy: 0.7895 - val_loss: 0.5856 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.81 - ETA: 0s - loss: 0.5634 - accuracy: 0.78 - ETA: 0s - loss: 0.5899 - accuracy: 0.76 - ETA: 0s - loss: 0.6036 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6070 - accuracy: 0.7622 - val_loss: 0.5965 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.81 - ETA: 0s - loss: 0.6424 - accuracy: 0.76 - ETA: 0s - loss: 0.6371 - accuracy: 0.75 - ETA: 0s - loss: 0.6347 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6410 - accuracy: 0.7615 - val_loss: 0.6299 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8180 - accuracy: 0.62 - ETA: 0s - loss: 0.6245 - accuracy: 0.74 - ETA: 0s - loss: 0.6526 - accuracy: 0.73 - ETA: 0s - loss: 0.6567 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6515 - accuracy: 0.7514 - val_loss: 0.6770 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.81 - ETA: 0s - loss: 0.6780 - accuracy: 0.76 - ETA: 0s - loss: 0.7095 - accuracy: 0.76 - ETA: 0s - loss: 0.6977 - accuracy: 0.74 - 0s 2ms/step - loss: 0.7010 - accuracy: 0.7297 - val_loss: 0.6115 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6575 - accuracy: 0.68 - ETA: 0s - loss: 0.6695 - accuracy: 0.71 - ETA: 0s - loss: 0.7241 - accuracy: 0.71 - 0s 2ms/step - loss: 0.7312 - accuracy: 0.7234 - val_loss: 0.6390 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3105 - accuracy: 0.71 - ETA: 0s - loss: 0.8099 - accuracy: 0.72 - ETA: 0s - loss: 0.7826 - accuracy: 0.66 - ETA: 0s - loss: 0.7514 - accuracy: 0.66 - 0s 2ms/step - loss: 0.7677 - accuracy: 0.6663 - val_loss: 0.6362 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.71 - ETA: 0s - loss: 0.6633 - accuracy: 0.64 - ETA: 0s - loss: 0.6996 - accuracy: 0.66 - ETA: 0s - loss: 0.6918 - accuracy: 0.64 - 0s 2ms/step - loss: 0.6869 - accuracy: 0.6353 - val_loss: 0.8065 - val_accuracy: 0.7269\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.71 - ETA: 0s - loss: 0.6706 - accuracy: 0.66 - ETA: 0s - loss: 0.6804 - accuracy: 0.66 - ETA: 0s - loss: 0.6676 - accuracy: 0.66 - 0s 2ms/step - loss: 0.6638 - accuracy: 0.6674 - val_loss: 0.7987 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.5560 - accuracy: 0.81 - ETA: 0s - loss: 0.6489 - accuracy: 0.70 - ETA: 0s - loss: 0.6581 - accuracy: 0.69 - ETA: 0s - loss: 0.6477 - accuracy: 0.7104Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.7047 - val_loss: 0.7105 - val_accuracy: 0.7030\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7401 - accuracy: 0.59 - ETA: 0s - loss: 2.3708 - accuracy: 0.57 - ETA: 0s - loss: 1.5310 - accuracy: 0.61 - ETA: 0s - loss: 1.2114 - accuracy: 0.65 - 0s 4ms/step - loss: 1.0986 - accuracy: 0.6697 - val_loss: 0.6023 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.71 - ETA: 0s - loss: 0.5908 - accuracy: 0.74 - ETA: 0s - loss: 0.5930 - accuracy: 0.75 - ETA: 0s - loss: 0.6020 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5997 - accuracy: 0.7533 - val_loss: 0.5716 - val_accuracy: 0.7567\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.90 - ETA: 0s - loss: 0.5779 - accuracy: 0.76 - ETA: 0s - loss: 0.5887 - accuracy: 0.76 - ETA: 0s - loss: 0.5813 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7816 - val_loss: 0.5640 - val_accuracy: 0.7567\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5402 - accuracy: 0.78 - ETA: 0s - loss: 0.5542 - accuracy: 0.79 - ETA: 0s - loss: 0.5736 - accuracy: 0.79 - ETA: 0s - loss: 0.5778 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5790 - accuracy: 0.7850 - val_loss: 0.5837 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.78 - ETA: 0s - loss: 0.5237 - accuracy: 0.81 - ETA: 0s - loss: 0.5498 - accuracy: 0.79 - ETA: 0s - loss: 0.5489 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7947 - val_loss: 0.5619 - val_accuracy: 0.7552\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.81 - ETA: 0s - loss: 0.5515 - accuracy: 0.79 - ETA: 0s - loss: 0.5788 - accuracy: 0.79 - ETA: 0s - loss: 0.6016 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6098 - accuracy: 0.7872 - val_loss: 0.7377 - val_accuracy: 0.7582\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5029 - accuracy: 0.84 - ETA: 0s - loss: 0.6464 - accuracy: 0.77 - ETA: 0s - loss: 0.6336 - accuracy: 0.77 - ETA: 0s - loss: 0.6518 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6484 - accuracy: 0.7604 - val_loss: 0.5746 - val_accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.78 - ETA: 0s - loss: 0.6409 - accuracy: 0.74 - ETA: 0s - loss: 0.6279 - accuracy: 0.76 - ETA: 0s - loss: 0.6426 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6287 - accuracy: 0.7745 - val_loss: 0.5812 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.75 - ETA: 0s - loss: 0.6186 - accuracy: 0.77 - ETA: 0s - loss: 0.6153 - accuracy: 0.78 - ETA: 0s - loss: 0.6319 - accuracy: 0.78 - ETA: 0s - loss: 0.6542 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6499 - accuracy: 0.7738 - val_loss: 0.5895 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7173 - accuracy: 0.75 - ETA: 0s - loss: 0.8121 - accuracy: 0.77 - ETA: 0s - loss: 0.7927 - accuracy: 0.76 - ETA: 0s - loss: 0.7582 - accuracy: 0.74 - ETA: 0s - loss: 0.7538 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7504 - accuracy: 0.7339 - val_loss: 0.5918 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7516 - accuracy: 0.78 - ETA: 0s - loss: 0.7603 - accuracy: 0.72 - ETA: 0s - loss: 0.7363 - accuracy: 0.72 - ETA: 0s - loss: 0.7322 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7503 - accuracy: 0.7342 - val_loss: 0.6242 - val_accuracy: 0.7075\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8575 - accuracy: 0.75 - ETA: 0s - loss: 0.7039 - accuracy: 0.72 - ETA: 0s - loss: 0.7035 - accuracy: 0.70 - ETA: 0s - loss: 0.6777 - accuracy: 0.69 - 0s 3ms/step - loss: 0.6807 - accuracy: 0.7066 - val_loss: 0.6264 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.81 - ETA: 0s - loss: 0.6319 - accuracy: 0.74 - ETA: 0s - loss: 0.6356 - accuracy: 0.74 - ETA: 0s - loss: 0.6553 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6574 - accuracy: 0.7279 - val_loss: 0.6277 - val_accuracy: 0.7149\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7566 - accuracy: 0.59 - ETA: 0s - loss: 0.6949 - accuracy: 0.63 - ETA: 0s - loss: 0.6938 - accuracy: 0.69 - ETA: 0s - loss: 0.6776 - accuracy: 0.69 - 0s 3ms/step - loss: 0.6784 - accuracy: 0.6883 - val_loss: 0.6153 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.68 - ETA: 0s - loss: 0.6522 - accuracy: 0.75 - ETA: 0s - loss: 0.6496 - accuracy: 0.75 - ETA: 0s - loss: 0.6918 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6905 - accuracy: 0.7342 - val_loss: 0.6222 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.5702 - accuracy: 0.78 - ETA: 0s - loss: 0.6433 - accuracy: 0.69 - ETA: 0s - loss: 0.6321 - accuracy: 0.70 - ETA: 0s - loss: 0.6404 - accuracy: 0.7179Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6530 - accuracy: 0.7182 - val_loss: 0.6222 - val_accuracy: 0.7194\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: bbdba9c786d014eb231dc7e5b7ee0c7f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7507462700208029</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8744187457912067</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7457 - accuracy: 0.78 - ETA: 0s - loss: 1.9922 - accuracy: 0.62 - ETA: 0s - loss: 1.4846 - accuracy: 0.61 - 0s 4ms/step - loss: 1.2196 - accuracy: 0.6346 - val_loss: 0.5831 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.71 - ETA: 0s - loss: 0.6217 - accuracy: 0.71 - ETA: 0s - loss: 0.6172 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6232 - accuracy: 0.7264 - val_loss: 0.5773 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6427 - accuracy: 0.71 - ETA: 0s - loss: 0.6735 - accuracy: 0.73 - ETA: 0s - loss: 0.6356 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6247 - accuracy: 0.7436 - val_loss: 0.5895 - val_accuracy: 0.7403\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.78 - ETA: 0s - loss: 0.5440 - accuracy: 0.77 - ETA: 0s - loss: 0.5445 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5555 - accuracy: 0.7712 - val_loss: 0.5776 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.87 - ETA: 0s - loss: 0.5157 - accuracy: 0.80 - ETA: 0s - loss: 0.5106 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5193 - accuracy: 0.8022 - val_loss: 0.5631 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.78 - ETA: 0s - loss: 0.5208 - accuracy: 0.79 - ETA: 0s - loss: 0.5231 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7992 - val_loss: 0.5533 - val_accuracy: 0.7493\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4086 - accuracy: 0.87 - ETA: 0s - loss: 0.5196 - accuracy: 0.81 - ETA: 0s - loss: 0.5186 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5048 - accuracy: 0.8264 - val_loss: 0.5799 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.71 - ETA: 0s - loss: 0.4937 - accuracy: 0.82 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4842 - accuracy: 0.8313 - val_loss: 0.5699 - val_accuracy: 0.7239\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.81 - ETA: 0s - loss: 0.4754 - accuracy: 0.85 - ETA: 0s - loss: 0.4636 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4599 - accuracy: 0.8499 - val_loss: 0.7142 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.90 - ETA: 0s - loss: 0.4416 - accuracy: 0.84 - ETA: 0s - loss: 0.4591 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4614 - accuracy: 0.8466 - val_loss: 0.6105 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.81 - ETA: 0s - loss: 0.4403 - accuracy: 0.86 - ETA: 0s - loss: 0.4887 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8417 - val_loss: 0.7906 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.71 - ETA: 0s - loss: 0.5415 - accuracy: 0.83 - ETA: 0s - loss: 0.5520 - accuracy: 0.82 - 0s 2ms/step - loss: 0.6314 - accuracy: 0.8197 - val_loss: 0.5832 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.75 - ETA: 0s - loss: 0.5072 - accuracy: 0.83 - ETA: 0s - loss: 0.5025 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5080 - accuracy: 0.8358 - val_loss: 0.6477 - val_accuracy: 0.6806\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.78 - ETA: 0s - loss: 0.4804 - accuracy: 0.83 - ETA: 0s - loss: 0.4736 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8380 - val_loss: 0.6186 - val_accuracy: 0.7478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - ETA: 0s - loss: 0.4666 - accuracy: 0.84 - ETA: 0s - loss: 0.4640 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4616 - accuracy: 0.8477 - val_loss: 0.7827 - val_accuracy: 0.7209\n",
      "Epoch 16/50\n",
      "64/84 [=====================>........] - ETA: 0s - loss: 0.2587 - accuracy: 0.96 - ETA: 0s - loss: 0.4404 - accuracy: 0.87 - ETA: 0s - loss: 0.4821 - accuracy: 0.8545Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.8555 - val_loss: 0.6958 - val_accuracy: 0.7224\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.62 - ETA: 0s - loss: 1.7516 - accuracy: 0.60 - ETA: 0s - loss: 1.4013 - accuracy: 0.60 - 0s 3ms/step - loss: 1.1763 - accuracy: 0.6301 - val_loss: 0.5571 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.56 - ETA: 0s - loss: 0.6497 - accuracy: 0.71 - ETA: 0s - loss: 0.6494 - accuracy: 0.71 - ETA: 0s - loss: 0.6360 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6375 - accuracy: 0.7234 - val_loss: 0.5934 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.78 - ETA: 0s - loss: 0.5776 - accuracy: 0.75 - ETA: 0s - loss: 0.5816 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7492 - val_loss: 0.5814 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.84 - ETA: 0s - loss: 0.5343 - accuracy: 0.78 - ETA: 0s - loss: 0.5391 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7854 - val_loss: 0.5650 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0064 - accuracy: 0.78 - ETA: 0s - loss: 0.5261 - accuracy: 0.81 - ETA: 0s - loss: 0.5263 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5271 - accuracy: 0.8014 - val_loss: 0.5437 - val_accuracy: 0.7612\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.90 - ETA: 0s - loss: 0.5752 - accuracy: 0.80 - ETA: 0s - loss: 0.5681 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5604 - accuracy: 0.8040 - val_loss: 0.5680 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.78 - ETA: 0s - loss: 0.6030 - accuracy: 0.76 - ETA: 0s - loss: 0.5571 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5478 - accuracy: 0.8029 - val_loss: 0.5858 - val_accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.84 - ETA: 0s - loss: 0.5180 - accuracy: 0.83 - ETA: 0s - loss: 0.5144 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5129 - accuracy: 0.8249 - val_loss: 0.6025 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.81 - ETA: 0s - loss: 0.5219 - accuracy: 0.81 - ETA: 0s - loss: 0.4931 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8354 - val_loss: 0.5931 - val_accuracy: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.93 - ETA: 0s - loss: 0.4336 - accuracy: 0.86 - ETA: 0s - loss: 0.4594 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4546 - accuracy: 0.8447 - val_loss: 0.6194 - val_accuracy: 0.7104\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4260 - accuracy: 0.84 - ETA: 0s - loss: 0.4396 - accuracy: 0.84 - ETA: 0s - loss: 0.4542 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8537 - val_loss: 0.6117 - val_accuracy: 0.7119\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6050 - accuracy: 0.75 - ETA: 0s - loss: 0.4287 - accuracy: 0.85 - ETA: 0s - loss: 0.4243 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8522 - val_loss: 0.5765 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.87 - ETA: 0s - loss: 0.4051 - accuracy: 0.86 - ETA: 0s - loss: 0.4863 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4906 - accuracy: 0.8563 - val_loss: 0.5622 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.81 - ETA: 0s - loss: 0.4271 - accuracy: 0.87 - ETA: 0s - loss: 0.4326 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4456 - accuracy: 0.8630 - val_loss: 0.7622 - val_accuracy: 0.7209\n",
      "Epoch 15/50\n",
      "66/84 [======================>.......] - ETA: 0s - loss: 0.4097 - accuracy: 0.81 - ETA: 0s - loss: 0.3860 - accuracy: 0.87 - ETA: 0s - loss: 0.4416 - accuracy: 0.8670Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8596 - val_loss: 0.6579 - val_accuracy: 0.7433\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4502 - accuracy: 0.59 - ETA: 0s - loss: 1.6812 - accuracy: 0.60 - ETA: 0s - loss: 1.2547 - accuracy: 0.63 - ETA: 0s - loss: 1.0640 - accuracy: 0.66 - 0s 4ms/step - loss: 1.0640 - accuracy: 0.6626 - val_loss: 0.6066 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.68 - ETA: 0s - loss: 0.6074 - accuracy: 0.75 - ETA: 0s - loss: 0.6248 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6219 - accuracy: 0.7469 - val_loss: 0.6013 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.81 - ETA: 0s - loss: 0.6054 - accuracy: 0.74 - ETA: 0s - loss: 0.6093 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6077 - accuracy: 0.7566 - val_loss: 0.5760 - val_accuracy: 0.7522\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.87 - ETA: 0s - loss: 0.5701 - accuracy: 0.79 - ETA: 0s - loss: 0.5745 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7880 - val_loss: 0.5815 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.81 - ETA: 0s - loss: 0.5316 - accuracy: 0.81 - ETA: 0s - loss: 0.5436 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7977 - val_loss: 0.5600 - val_accuracy: 0.7552\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.78 - ETA: 0s - loss: 0.5308 - accuracy: 0.81 - ETA: 0s - loss: 0.5252 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5275 - accuracy: 0.8167 - val_loss: 0.5653 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5994 - accuracy: 0.75 - ETA: 0s - loss: 0.5016 - accuracy: 0.82 - ETA: 0s - loss: 0.5053 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5079 - accuracy: 0.8275 - val_loss: 0.5724 - val_accuracy: 0.7448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.84 - ETA: 0s - loss: 0.5387 - accuracy: 0.83 - ETA: 0s - loss: 0.5369 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5336 - accuracy: 0.8253 - val_loss: 0.5557 - val_accuracy: 0.7522\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.84 - ETA: 0s - loss: 0.5516 - accuracy: 0.81 - ETA: 0s - loss: 0.5212 - accuracy: 0.82 - ETA: 0s - loss: 0.5270 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5229 - accuracy: 0.8275 - val_loss: 0.5895 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.93 - ETA: 0s - loss: 0.4609 - accuracy: 0.84 - ETA: 0s - loss: 0.4982 - accuracy: 0.83 - ETA: 0s - loss: 0.5099 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5103 - accuracy: 0.8305 - val_loss: 0.7242 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4910 - accuracy: 0.71 - ETA: 0s - loss: 0.5047 - accuracy: 0.82 - ETA: 0s - loss: 0.4913 - accuracy: 0.82 - ETA: 0s - loss: 0.4958 - accuracy: 0.83 - ETA: 0s - loss: 0.4906 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4854 - accuracy: 0.8387 - val_loss: 0.6337 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.87 - ETA: 0s - loss: 0.4612 - accuracy: 0.84 - ETA: 0s - loss: 0.4576 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4592 - accuracy: 0.8522 - val_loss: 0.5857 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - ETA: 0s - loss: 0.4838 - accuracy: 0.84 - ETA: 0s - loss: 0.4965 - accuracy: 0.83 - ETA: 0s - loss: 0.4835 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4749 - accuracy: 0.8485 - val_loss: 0.7303 - val_accuracy: 0.7194\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.84 - ETA: 0s - loss: 0.4270 - accuracy: 0.87 - ETA: 0s - loss: 0.4214 - accuracy: 0.87 - ETA: 0s - loss: 0.4312 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8626 - val_loss: 0.6164 - val_accuracy: 0.7254\n",
      "Epoch 15/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.3570 - accuracy: 0.93 - ETA: 0s - loss: 0.4502 - accuracy: 0.86 - ETA: 0s - loss: 0.4618 - accuracy: 0.85 - ETA: 0s - loss: 0.4504 - accuracy: 0.8577Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.8623 - val_loss: 0.5956 - val_accuracy: 0.7388\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: bf4ed4904be573cc84c53ef5c1be4adc</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7552238901456197</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7030774800391972</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 95</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.75 - ETA: 0s - loss: 1.0285 - accuracy: 0.66 - ETA: 0s - loss: 0.8597 - accuracy: 0.66 - 0s 4ms/step - loss: 0.7779 - accuracy: 0.6760 - val_loss: 0.5427 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.84 - ETA: 0s - loss: 0.5444 - accuracy: 0.77 - ETA: 0s - loss: 0.5484 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7439 - val_loss: 0.5673 - val_accuracy: 0.7373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.81 - ETA: 0s - loss: 0.4531 - accuracy: 0.79 - ETA: 0s - loss: 0.4832 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7712 - val_loss: 0.5865 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.84 - ETA: 0s - loss: 0.5264 - accuracy: 0.78 - ETA: 0s - loss: 0.5154 - accuracy: 0.76 - ETA: 0s - loss: 0.5066 - accuracy: 0.78 - ETA: 0s - loss: 0.5069 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7846 - val_loss: 0.9974 - val_accuracy: 0.7179\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.5580 - accuracy: 0.81 - ETA: 0s - loss: 0.4912 - accuracy: 0.80 - ETA: 0s - loss: 0.5258 - accuracy: 0.78 - ETA: 0s - loss: 0.5250 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5456 - accuracy: 0.7828 - val_loss: 0.9404 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.84 - ETA: 0s - loss: 0.5872 - accuracy: 0.73 - ETA: 0s - loss: 0.5619 - accuracy: 0.76 - ETA: 0s - loss: 0.5851 - accuracy: 0.76 - ETA: 0s - loss: 0.5688 - accuracy: 0.77 - ETA: 0s - loss: 0.5474 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5472 - accuracy: 0.7801 - val_loss: 1.3644 - val_accuracy: 0.6746\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.87 - ETA: 0s - loss: 0.4650 - accuracy: 0.81 - ETA: 0s - loss: 0.4635 - accuracy: 0.80 - ETA: 0s - loss: 0.4403 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8178 - val_loss: 1.8488 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.81 - ETA: 0s - loss: 0.6775 - accuracy: 0.80 - ETA: 0s - loss: 0.6159 - accuracy: 0.77 - ETA: 0s - loss: 0.5974 - accuracy: 0.78 - ETA: 0s - loss: 0.5840 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5796 - accuracy: 0.7697 - val_loss: 2.1700 - val_accuracy: 0.7164\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4692 - accuracy: 0.84 - ETA: 0s - loss: 0.4707 - accuracy: 0.82 - ETA: 0s - loss: 0.4984 - accuracy: 0.78 - ETA: 0s - loss: 0.5232 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5245 - accuracy: 0.7831 - val_loss: 0.6661 - val_accuracy: 0.6821\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.90 - ETA: 0s - loss: 0.4606 - accuracy: 0.81 - ETA: 0s - loss: 0.5489 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7921 - val_loss: 1.2806 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4628 - accuracy: 0.81 - ETA: 0s - loss: 0.4636 - accuracy: 0.81 - ETA: 0s - loss: 0.5136 - accuracy: 0.79 - ETA: 0s - loss: 0.6344 - accuracy: 0.7849Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.7835 - val_loss: 0.5928 - val_accuracy: 0.7194\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7703 - accuracy: 0.75 - ETA: 0s - loss: 0.8828 - accuracy: 0.64 - ETA: 0s - loss: 0.7505 - accuracy: 0.68 - ETA: 0s - loss: 0.7056 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6963 - accuracy: 0.6999 - val_loss: 0.5300 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.84 - ETA: 0s - loss: 0.5977 - accuracy: 0.77 - ETA: 0s - loss: 0.5892 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5867 - accuracy: 0.7630 - val_loss: 0.6314 - val_accuracy: 0.6612\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.75 - ETA: 0s - loss: 0.5725 - accuracy: 0.76 - ETA: 0s - loss: 0.5555 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7716 - val_loss: 0.5836 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.71 - ETA: 0s - loss: 0.4753 - accuracy: 0.82 - ETA: 0s - loss: 0.5009 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7992 - val_loss: 0.5679 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.87 - ETA: 0s - loss: 0.4774 - accuracy: 0.82 - ETA: 0s - loss: 0.4651 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4623 - accuracy: 0.8257 - val_loss: 1.0042 - val_accuracy: 0.7463\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.75 - ETA: 0s - loss: 0.4664 - accuracy: 0.82 - ETA: 0s - loss: 0.4647 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4700 - accuracy: 0.8234 - val_loss: 0.5728 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.87 - ETA: 0s - loss: 0.4905 - accuracy: 0.82 - ETA: 0s - loss: 0.4672 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8391 - val_loss: 0.6740 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.78 - ETA: 0s - loss: 0.4729 - accuracy: 0.85 - ETA: 0s - loss: 0.4609 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4572 - accuracy: 0.8526 - val_loss: 0.5587 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.90 - ETA: 0s - loss: 0.4152 - accuracy: 0.85 - ETA: 0s - loss: 0.4092 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8544 - val_loss: 0.6690 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.81 - ETA: 0s - loss: 0.4276 - accuracy: 0.86 - ETA: 0s - loss: 0.4332 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4522 - accuracy: 0.8451 - val_loss: 0.6064 - val_accuracy: 0.6925\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.4040 - accuracy: 0.86 - ETA: 0s - loss: 0.4090 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8645 - val_loss: 0.6251 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.81 - ETA: 0s - loss: 0.3998 - accuracy: 0.86 - ETA: 0s - loss: 0.3860 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8690 - val_loss: 0.6523 - val_accuracy: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.96 - ETA: 0s - loss: 0.3514 - accuracy: 0.89 - ETA: 0s - loss: 0.3564 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8820 - val_loss: 0.8444 - val_accuracy: 0.7075\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.75 - ETA: 0s - loss: 0.3655 - accuracy: 0.87 - ETA: 0s - loss: 0.3695 - accuracy: 0.88 - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8854 - val_loss: 0.9887 - val_accuracy: 0.7179\n",
      "Epoch 15/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.3223 - accuracy: 0.93 - ETA: 0s - loss: 0.3415 - accuracy: 0.89 - ETA: 0s - loss: 0.4006 - accuracy: 0.8805Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8847 - val_loss: 0.8668 - val_accuracy: 0.7194\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7582 - accuracy: 0.68 - ETA: 0s - loss: 1.0445 - accuracy: 0.62 - ETA: 0s - loss: 0.8824 - accuracy: 0.66 - ETA: 0s - loss: 0.8155 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7855 - accuracy: 0.6809 - val_loss: 0.5996 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.71 - ETA: 0s - loss: 0.5789 - accuracy: 0.74 - ETA: 0s - loss: 0.5783 - accuracy: 0.74 - ETA: 0s - loss: 0.5756 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5759 - accuracy: 0.7518 - val_loss: 0.5610 - val_accuracy: 0.7433\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.81 - ETA: 0s - loss: 0.5288 - accuracy: 0.78 - ETA: 0s - loss: 0.5217 - accuracy: 0.79 - ETA: 0s - loss: 0.5250 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7898 - val_loss: 0.5608 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.87 - ETA: 0s - loss: 0.5101 - accuracy: 0.79 - ETA: 0s - loss: 0.4653 - accuracy: 0.81 - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8141 - val_loss: 0.7856 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.75 - ETA: 0s - loss: 0.4396 - accuracy: 0.83 - ETA: 0s - loss: 0.4550 - accuracy: 0.83 - ETA: 0s - loss: 0.4810 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4843 - accuracy: 0.8227 - val_loss: 0.6328 - val_accuracy: 0.7149\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.84 - ETA: 0s - loss: 0.4576 - accuracy: 0.81 - ETA: 0s - loss: 0.4592 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4668 - accuracy: 0.8156 - val_loss: 0.6150 - val_accuracy: 0.7119\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.87 - ETA: 0s - loss: 0.4320 - accuracy: 0.84 - ETA: 0s - loss: 0.4256 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8350 - val_loss: 0.8844 - val_accuracy: 0.6343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4503 - accuracy: 0.90 - ETA: 0s - loss: 0.5628 - accuracy: 0.82 - ETA: 0s - loss: 0.5121 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4922 - accuracy: 0.8365 - val_loss: 0.8008 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.84 - ETA: 0s - loss: 0.4728 - accuracy: 0.86 - ETA: 0s - loss: 0.4906 - accuracy: 0.84 - ETA: 0s - loss: 0.4684 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4673 - accuracy: 0.8440 - val_loss: 0.6659 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.96 - ETA: 0s - loss: 0.4038 - accuracy: 0.83 - ETA: 0s - loss: 0.5297 - accuracy: 0.83 - ETA: 0s - loss: 0.5087 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5139 - accuracy: 0.8369 - val_loss: 0.8902 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.87 - ETA: 0s - loss: 0.6389 - accuracy: 0.84 - ETA: 0s - loss: 0.5318 - accuracy: 0.85 - ETA: 0s - loss: 0.5201 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5188 - accuracy: 0.8492 - val_loss: 0.6327 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "56/84 [===================>..........] - ETA: 0s - loss: 0.5719 - accuracy: 0.78 - ETA: 0s - loss: 0.3974 - accuracy: 0.86 - ETA: 0s - loss: 0.4106 - accuracy: 0.8650Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8679 - val_loss: 0.6688 - val_accuracy: 0.7418\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 5d9e469924f74a7ccdccc94cf89180b9</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7432835698127747</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.4875202321294636</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.62 - ETA: 0s - loss: 4.2591 - accuracy: 0.62 - ETA: 0s - loss: 2.9433 - accuracy: 0.62 - ETA: 0s - loss: 2.3752 - accuracy: 0.62 - ETA: 0s - loss: 1.9514 - accuracy: 0.63 - ETA: 0s - loss: 1.7109 - accuracy: 0.66 - ETA: 0s - loss: 1.5488 - accuracy: 0.66 - 1s 6ms/step - loss: 1.5096 - accuracy: 0.6614 - val_loss: 0.5657 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5814 - accuracy: 0.68 - ETA: 0s - loss: 0.6984 - accuracy: 0.65 - ETA: 0s - loss: 0.6549 - accuracy: 0.69 - ETA: 0s - loss: 0.6290 - accuracy: 0.71 - ETA: 0s - loss: 0.5983 - accuracy: 0.72 - ETA: 0s - loss: 0.6140 - accuracy: 0.72 - ETA: 0s - loss: 0.6184 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6174 - accuracy: 0.7174 - val_loss: 0.5873 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.90 - ETA: 0s - loss: 0.6168 - accuracy: 0.72 - ETA: 0s - loss: 0.5700 - accuracy: 0.75 - ETA: 0s - loss: 0.5504 - accuracy: 0.76 - ETA: 0s - loss: 0.5493 - accuracy: 0.77 - ETA: 0s - loss: 0.5513 - accuracy: 0.77 - ETA: 0s - loss: 0.5530 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5567 - accuracy: 0.7708 - val_loss: 0.5763 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.65 - ETA: 0s - loss: 0.4437 - accuracy: 0.78 - ETA: 0s - loss: 0.5075 - accuracy: 0.77 - ETA: 0s - loss: 0.5060 - accuracy: 0.76 - ETA: 0s - loss: 0.5131 - accuracy: 0.76 - ETA: 0s - loss: 0.5135 - accuracy: 0.77 - ETA: 0s - loss: 0.5181 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5211 - accuracy: 0.7783 - val_loss: 0.5635 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.93 - ETA: 0s - loss: 0.5072 - accuracy: 0.80 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - ETA: 0s - loss: 0.4706 - accuracy: 0.80 - ETA: 0s - loss: 0.5027 - accuracy: 0.78 - ETA: 0s - loss: 0.5127 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5156 - accuracy: 0.7884 - val_loss: 0.6225 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.84 - ETA: 0s - loss: 0.5563 - accuracy: 0.77 - ETA: 0s - loss: 0.5085 - accuracy: 0.79 - ETA: 0s - loss: 0.5041 - accuracy: 0.80 - ETA: 0s - loss: 0.4996 - accuracy: 0.80 - ETA: 0s - loss: 0.5046 - accuracy: 0.79 - ETA: 0s - loss: 0.5024 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5006 - accuracy: 0.8022 - val_loss: 0.9747 - val_accuracy: 0.6940\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8975 - accuracy: 0.87 - ETA: 0s - loss: 0.6057 - accuracy: 0.79 - ETA: 0s - loss: 0.5760 - accuracy: 0.80 - ETA: 0s - loss: 0.5443 - accuracy: 0.80 - ETA: 0s - loss: 0.5299 - accuracy: 0.80 - ETA: 0s - loss: 0.5240 - accuracy: 0.80 - ETA: 0s - loss: 0.5239 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5259 - accuracy: 0.7913 - val_loss: 0.5585 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.90 - ETA: 0s - loss: 0.4636 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4403 - accuracy: 0.80 - ETA: 0s - loss: 0.4511 - accuracy: 0.80 - ETA: 0s - loss: 0.4622 - accuracy: 0.80 - ETA: 0s - loss: 0.4732 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4714 - accuracy: 0.8081 - val_loss: 0.6477 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.87 - ETA: 0s - loss: 0.4092 - accuracy: 0.84 - ETA: 0s - loss: 0.3930 - accuracy: 0.84 - ETA: 0s - loss: 0.3861 - accuracy: 0.84 - ETA: 0s - loss: 0.3991 - accuracy: 0.84 - ETA: 0s - loss: 0.4714 - accuracy: 0.82 - ETA: 0s - loss: 0.4712 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4712 - accuracy: 0.8290 - val_loss: 0.7446 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.96 - ETA: 0s - loss: 0.4022 - accuracy: 0.86 - ETA: 0s - loss: 0.4775 - accuracy: 0.83 - ETA: 0s - loss: 0.4918 - accuracy: 0.83 - ETA: 0s - loss: 0.4794 - accuracy: 0.82 - ETA: 0s - loss: 0.4751 - accuracy: 0.82 - ETA: 0s - loss: 0.4780 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4837 - accuracy: 0.8130 - val_loss: 0.7827 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.93 - ETA: 0s - loss: 0.4396 - accuracy: 0.83 - ETA: 0s - loss: 0.4172 - accuracy: 0.84 - ETA: 0s - loss: 0.4218 - accuracy: 0.82 - ETA: 0s - loss: 0.4140 - accuracy: 0.83 - ETA: 0s - loss: 0.4087 - accuracy: 0.83 - ETA: 0s - loss: 0.4153 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4163 - accuracy: 0.8335 - val_loss: 0.8377 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.93 - ETA: 0s - loss: 0.4896 - accuracy: 0.82 - ETA: 0s - loss: 0.4589 - accuracy: 0.82 - ETA: 0s - loss: 0.4324 - accuracy: 0.82 - ETA: 0s - loss: 0.4397 - accuracy: 0.82 - ETA: 0s - loss: 0.4510 - accuracy: 0.82 - ETA: 0s - loss: 0.4666 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4666 - accuracy: 0.8208 - val_loss: 0.7544 - val_accuracy: 0.7015\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.75 - ETA: 0s - loss: 0.5757 - accuracy: 0.82 - ETA: 0s - loss: 0.5314 - accuracy: 0.82 - ETA: 0s - loss: 0.4882 - accuracy: 0.84 - ETA: 0s - loss: 0.4677 - accuracy: 0.84 - ETA: 0s - loss: 0.4562 - accuracy: 0.84 - ETA: 0s - loss: 0.4510 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4549 - accuracy: 0.8387 - val_loss: 0.9329 - val_accuracy: 0.7179\n",
      "Epoch 14/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.1307 - accuracy: 0.96 - ETA: 0s - loss: 0.4628 - accuracy: 0.81 - ETA: 0s - loss: 0.4472 - accuracy: 0.82 - ETA: 0s - loss: 0.5490 - accuracy: 0.82 - ETA: 0s - loss: 0.5552 - accuracy: 0.82 - ETA: 0s - loss: 0.5367 - accuracy: 0.81 - ETA: 0s - loss: 0.5491 - accuracy: 0.8141Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5707 - accuracy: 0.8089 - val_loss: 0.9435 - val_accuracy: 0.7388\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0504 - accuracy: 0.43 - ETA: 0s - loss: 2.9511 - accuracy: 0.57 - ETA: 0s - loss: 2.3083 - accuracy: 0.58 - ETA: 0s - loss: 1.7707 - accuracy: 0.61 - ETA: 0s - loss: 1.5170 - accuracy: 0.62 - ETA: 0s - loss: 1.3419 - accuracy: 0.64 - 0s 5ms/step - loss: 1.2571 - accuracy: 0.6402 - val_loss: 0.5917 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.71 - ETA: 0s - loss: 0.5398 - accuracy: 0.73 - ETA: 0s - loss: 0.6132 - accuracy: 0.72 - ETA: 0s - loss: 0.6173 - accuracy: 0.72 - ETA: 0s - loss: 0.6133 - accuracy: 0.72 - ETA: 0s - loss: 0.6280 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6306 - accuracy: 0.7152 - val_loss: 0.6089 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.71 - ETA: 0s - loss: 0.5436 - accuracy: 0.74 - ETA: 0s - loss: 0.5368 - accuracy: 0.75 - ETA: 0s - loss: 0.5484 - accuracy: 0.76 - ETA: 0s - loss: 0.5529 - accuracy: 0.76 - ETA: 0s - loss: 0.5383 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5338 - accuracy: 0.7667 - val_loss: 0.5844 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.75 - ETA: 0s - loss: 0.5252 - accuracy: 0.76 - ETA: 0s - loss: 0.5735 - accuracy: 0.74 - ETA: 0s - loss: 0.5626 - accuracy: 0.76 - ETA: 0s - loss: 0.5807 - accuracy: 0.76 - ETA: 0s - loss: 0.5794 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5712 - accuracy: 0.7622 - val_loss: 0.5734 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.81 - ETA: 0s - loss: 0.5581 - accuracy: 0.79 - ETA: 0s - loss: 0.5185 - accuracy: 0.80 - ETA: 0s - loss: 0.5153 - accuracy: 0.80 - ETA: 0s - loss: 0.5265 - accuracy: 0.79 - ETA: 0s - loss: 0.5215 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5711 - accuracy: 0.7876 - val_loss: 1.0948 - val_accuracy: 0.6179\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.78 - ETA: 0s - loss: 0.6662 - accuracy: 0.77 - ETA: 0s - loss: 0.6199 - accuracy: 0.78 - ETA: 0s - loss: 0.6063 - accuracy: 0.77 - ETA: 0s - loss: 0.6620 - accuracy: 0.76 - ETA: 0s - loss: 0.6409 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6315 - accuracy: 0.7686 - val_loss: 0.7104 - val_accuracy: 0.6940\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.68 - ETA: 0s - loss: 0.5656 - accuracy: 0.78 - ETA: 0s - loss: 0.5658 - accuracy: 0.79 - ETA: 0s - loss: 0.5698 - accuracy: 0.79 - ETA: 0s - loss: 0.5696 - accuracy: 0.79 - ETA: 0s - loss: 0.5637 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5649 - accuracy: 0.8022 - val_loss: 0.6825 - val_accuracy: 0.7224\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.75 - ETA: 0s - loss: 0.4581 - accuracy: 0.80 - ETA: 0s - loss: 0.5096 - accuracy: 0.80 - ETA: 0s - loss: 0.6091 - accuracy: 0.80 - ETA: 0s - loss: 0.5935 - accuracy: 0.80 - ETA: 0s - loss: 0.5823 - accuracy: 0.81 - ETA: 0s - loss: 0.5653 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5624 - accuracy: 0.8100 - val_loss: 0.9209 - val_accuracy: 0.6985\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.78 - ETA: 0s - loss: 0.5451 - accuracy: 0.81 - ETA: 0s - loss: 0.5106 - accuracy: 0.81 - ETA: 0s - loss: 0.5010 - accuracy: 0.81 - ETA: 0s - loss: 0.5068 - accuracy: 0.81 - ETA: 0s - loss: 0.5042 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4975 - accuracy: 0.8163 - val_loss: 0.7819 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.81 - ETA: 0s - loss: 0.4433 - accuracy: 0.83 - ETA: 0s - loss: 0.4146 - accuracy: 0.85 - ETA: 0s - loss: 0.4415 - accuracy: 0.83 - ETA: 0s - loss: 0.4927 - accuracy: 0.83 - ETA: 0s - loss: 0.5113 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5282 - accuracy: 0.8324 - val_loss: 0.9690 - val_accuracy: 0.6806\n",
      "Epoch 11/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.4883 - accuracy: 0.78 - ETA: 0s - loss: 0.8483 - accuracy: 0.77 - ETA: 0s - loss: 0.8423 - accuracy: 0.78 - ETA: 0s - loss: 0.7476 - accuracy: 0.79 - ETA: 0s - loss: 0.6914 - accuracy: 0.80 - ETA: 0s - loss: 0.6638 - accuracy: 0.8099Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6592 - accuracy: 0.8014 - val_loss: 0.6055 - val_accuracy: 0.7284\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.6269 - accuracy: 0.53 - ETA: 0s - loss: 2.9802 - accuracy: 0.59 - ETA: 0s - loss: 2.2216 - accuracy: 0.61 - ETA: 0s - loss: 1.8853 - accuracy: 0.61 - ETA: 0s - loss: 1.6587 - accuracy: 0.62 - ETA: 0s - loss: 1.4522 - accuracy: 0.62 - 0s 5ms/step - loss: 1.3469 - accuracy: 0.6361 - val_loss: 0.5517 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7353 - accuracy: 0.71 - ETA: 0s - loss: 0.6455 - accuracy: 0.70 - ETA: 0s - loss: 0.6689 - accuracy: 0.70 - ETA: 0s - loss: 0.6399 - accuracy: 0.70 - ETA: 0s - loss: 0.6198 - accuracy: 0.71 - ETA: 0s - loss: 0.6193 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6144 - accuracy: 0.7122 - val_loss: 0.5812 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.65 - ETA: 0s - loss: 0.6806 - accuracy: 0.69 - ETA: 0s - loss: 0.6220 - accuracy: 0.71 - ETA: 0s - loss: 0.5967 - accuracy: 0.71 - ETA: 0s - loss: 0.5829 - accuracy: 0.73 - ETA: 0s - loss: 0.5749 - accuracy: 0.74 - ETA: 0s - loss: 0.5709 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5683 - accuracy: 0.7480 - val_loss: 0.5549 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.78 - ETA: 0s - loss: 0.4786 - accuracy: 0.78 - ETA: 0s - loss: 0.4905 - accuracy: 0.78 - ETA: 0s - loss: 0.4863 - accuracy: 0.78 - ETA: 0s - loss: 0.4910 - accuracy: 0.79 - ETA: 0s - loss: 0.4988 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5050 - accuracy: 0.7891 - val_loss: 0.5550 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.65 - ETA: 0s - loss: 0.4695 - accuracy: 0.82 - ETA: 0s - loss: 0.5455 - accuracy: 0.80 - ETA: 0s - loss: 0.5162 - accuracy: 0.81 - ETA: 0s - loss: 0.5124 - accuracy: 0.81 - ETA: 0s - loss: 0.5438 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5489 - accuracy: 0.7981 - val_loss: 0.5660 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.75 - ETA: 0s - loss: 0.5122 - accuracy: 0.80 - ETA: 0s - loss: 0.4940 - accuracy: 0.80 - ETA: 0s - loss: 0.4751 - accuracy: 0.81 - ETA: 0s - loss: 0.4684 - accuracy: 0.81 - ETA: 0s - loss: 0.4832 - accuracy: 0.80 - ETA: 0s - loss: 0.4805 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4805 - accuracy: 0.8048 - val_loss: 0.6801 - val_accuracy: 0.6851\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.75 - ETA: 0s - loss: 0.4664 - accuracy: 0.80 - ETA: 0s - loss: 0.4515 - accuracy: 0.81 - ETA: 0s - loss: 0.4397 - accuracy: 0.82 - ETA: 0s - loss: 0.4318 - accuracy: 0.82 - ETA: 0s - loss: 0.4372 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8294 - val_loss: 0.7250 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4962 - accuracy: 0.84 - ETA: 0s - loss: 0.4434 - accuracy: 0.83 - ETA: 0s - loss: 0.4113 - accuracy: 0.84 - ETA: 0s - loss: 0.3891 - accuracy: 0.85 - ETA: 0s - loss: 0.3911 - accuracy: 0.85 - ETA: 0s - loss: 0.4117 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4154 - accuracy: 0.8447 - val_loss: 0.6460 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.87 - ETA: 0s - loss: 0.4327 - accuracy: 0.83 - ETA: 0s - loss: 0.4077 - accuracy: 0.84 - ETA: 0s - loss: 0.4118 - accuracy: 0.84 - ETA: 0s - loss: 0.4077 - accuracy: 0.84 - ETA: 0s - loss: 0.4063 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4041 - accuracy: 0.8473 - val_loss: 0.6633 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.90 - ETA: 0s - loss: 0.3340 - accuracy: 0.87 - ETA: 0s - loss: 0.3418 - accuracy: 0.86 - ETA: 0s - loss: 0.3405 - accuracy: 0.87 - ETA: 0s - loss: 0.3530 - accuracy: 0.86 - ETA: 0s - loss: 0.3864 - accuracy: 0.85 - ETA: 0s - loss: 0.3921 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3955 - accuracy: 0.8425 - val_loss: 0.7196 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.78 - ETA: 0s - loss: 0.3702 - accuracy: 0.84 - ETA: 0s - loss: 0.3628 - accuracy: 0.85 - ETA: 0s - loss: 0.3666 - accuracy: 0.85 - ETA: 0s - loss: 0.3677 - accuracy: 0.85 - ETA: 0s - loss: 0.3596 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8626 - val_loss: 0.7595 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.90 - ETA: 0s - loss: 0.3469 - accuracy: 0.87 - ETA: 0s - loss: 0.3275 - accuracy: 0.88 - ETA: 0s - loss: 0.3281 - accuracy: 0.88 - ETA: 0s - loss: 0.3359 - accuracy: 0.87 - ETA: 0s - loss: 0.3373 - accuracy: 0.87 - ETA: 0s - loss: 0.3394 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3394 - accuracy: 0.8750 - val_loss: 1.0628 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.93 - ETA: 0s - loss: 0.2875 - accuracy: 0.89 - ETA: 0s - loss: 0.3111 - accuracy: 0.88 - ETA: 0s - loss: 0.3203 - accuracy: 0.88 - ETA: 0s - loss: 0.3158 - accuracy: 0.89 - ETA: 0s - loss: 0.3214 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3222 - accuracy: 0.8880 - val_loss: 0.9394 - val_accuracy: 0.7030\n",
      "Epoch 14/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2770 - accuracy: 0.90 - ETA: 0s - loss: 0.2918 - accuracy: 0.90 - ETA: 0s - loss: 0.2627 - accuracy: 0.90 - ETA: 0s - loss: 0.3052 - accuracy: 0.90 - ETA: 0s - loss: 0.3172 - accuracy: 0.88 - ETA: 0s - loss: 0.3175 - accuracy: 0.88 - ETA: 0s - loss: 0.3158 - accuracy: 0.8852Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3128 - accuracy: 0.8865 - val_loss: 1.4722 - val_accuracy: 0.7060\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 4697a61359c579fb5a258331107899c1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7378109296162924</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6262156964552154</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8767 - accuracy: 0.65 - ETA: 0s - loss: 1.6752 - accuracy: 0.59 - ETA: 0s - loss: 1.1610 - accuracy: 0.62 - ETA: 0s - loss: 0.9650 - accuracy: 0.65 - ETA: 0s - loss: 0.8703 - accuracy: 0.67 - ETA: 0s - loss: 0.8070 - accuracy: 0.68 - ETA: 0s - loss: 0.7756 - accuracy: 0.68 - 1s 6ms/step - loss: 0.7607 - accuracy: 0.6887 - val_loss: 0.6230 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8458 - accuracy: 0.56 - ETA: 0s - loss: 0.6068 - accuracy: 0.73 - ETA: 0s - loss: 0.5505 - accuracy: 0.77 - ETA: 0s - loss: 0.5332 - accuracy: 0.77 - ETA: 0s - loss: 0.5279 - accuracy: 0.77 - ETA: 0s - loss: 0.5208 - accuracy: 0.77 - ETA: 0s - loss: 0.5183 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5168 - accuracy: 0.7786 - val_loss: 0.5772 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.81 - ETA: 0s - loss: 0.4795 - accuracy: 0.76 - ETA: 0s - loss: 0.4653 - accuracy: 0.77 - ETA: 0s - loss: 0.4681 - accuracy: 0.78 - ETA: 0s - loss: 0.4710 - accuracy: 0.78 - ETA: 0s - loss: 0.4787 - accuracy: 0.79 - ETA: 0s - loss: 0.4761 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4752 - accuracy: 0.7902 - val_loss: 0.6724 - val_accuracy: 0.6687\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.65 - ETA: 0s - loss: 0.4325 - accuracy: 0.82 - ETA: 0s - loss: 0.4360 - accuracy: 0.82 - ETA: 0s - loss: 0.4588 - accuracy: 0.80 - ETA: 0s - loss: 0.4422 - accuracy: 0.80 - ETA: 0s - loss: 0.4413 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4384 - accuracy: 0.8048 - val_loss: 0.6857 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.90 - ETA: 0s - loss: 0.4275 - accuracy: 0.82 - ETA: 0s - loss: 0.4125 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3803 - accuracy: 0.82 - ETA: 0s - loss: 0.3817 - accuracy: 0.82 - ETA: 0s - loss: 0.3759 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3784 - accuracy: 0.8264 - val_loss: 0.8020 - val_accuracy: 0.6687\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.90 - ETA: 0s - loss: 0.2990 - accuracy: 0.86 - ETA: 0s - loss: 0.3110 - accuracy: 0.86 - ETA: 0s - loss: 0.3153 - accuracy: 0.85 - ETA: 0s - loss: 0.3012 - accuracy: 0.85 - ETA: 0s - loss: 0.3203 - accuracy: 0.84 - ETA: 0s - loss: 0.3373 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8391 - val_loss: 0.9408 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.90 - ETA: 0s - loss: 0.2262 - accuracy: 0.88 - ETA: 0s - loss: 0.3726 - accuracy: 0.85 - ETA: 0s - loss: 0.3955 - accuracy: 0.84 - ETA: 0s - loss: 0.4152 - accuracy: 0.83 - ETA: 0s - loss: 0.4118 - accuracy: 0.83 - ETA: 0s - loss: 0.4053 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4053 - accuracy: 0.8317 - val_loss: 0.7184 - val_accuracy: 0.7149\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.84 - ETA: 0s - loss: 0.2960 - accuracy: 0.85 - ETA: 0s - loss: 0.2936 - accuracy: 0.86 - ETA: 0s - loss: 0.4369 - accuracy: 0.85 - ETA: 0s - loss: 0.4550 - accuracy: 0.83 - ETA: 0s - loss: 0.4557 - accuracy: 0.82 - ETA: 0s - loss: 0.4453 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4524 - accuracy: 0.8264 - val_loss: 1.1020 - val_accuracy: 0.6791\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.71 - ETA: 0s - loss: 0.5007 - accuracy: 0.78 - ETA: 0s - loss: 0.4763 - accuracy: 0.79 - ETA: 0s - loss: 0.4533 - accuracy: 0.79 - ETA: 0s - loss: 0.4316 - accuracy: 0.79 - ETA: 0s - loss: 0.4154 - accuracy: 0.80 - ETA: 0s - loss: 0.4178 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4133 - accuracy: 0.8096 - val_loss: 0.9427 - val_accuracy: 0.6701\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.84 - ETA: 0s - loss: 0.3565 - accuracy: 0.85 - ETA: 0s - loss: 0.3521 - accuracy: 0.86 - ETA: 0s - loss: 0.3270 - accuracy: 0.86 - ETA: 0s - loss: 0.3139 - accuracy: 0.86 - ETA: 0s - loss: 0.3085 - accuracy: 0.86 - ETA: 0s - loss: 0.3028 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3069 - accuracy: 0.8671 - val_loss: 0.8707 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.2833 - accuracy: 0.87 - ETA: 0s - loss: 0.2484 - accuracy: 0.88 - ETA: 0s - loss: 0.2393 - accuracy: 0.89 - ETA: 0s - loss: 0.2415 - accuracy: 0.88 - ETA: 0s - loss: 0.2454 - accuracy: 0.88 - ETA: 0s - loss: 0.2516 - accuracy: 0.88 - ETA: 0s - loss: 0.2506 - accuracy: 0.8856Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2515 - accuracy: 0.8824 - val_loss: 1.0668 - val_accuracy: 0.6910\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7097 - accuracy: 0.75 - ETA: 0s - loss: 3.3740 - accuracy: 0.56 - ETA: 0s - loss: 2.0329 - accuracy: 0.61 - ETA: 0s - loss: 1.5369 - accuracy: 0.64 - ETA: 0s - loss: 1.2955 - accuracy: 0.65 - ETA: 0s - loss: 1.1562 - accuracy: 0.67 - ETA: 0s - loss: 1.0603 - accuracy: 0.68 - 1s 6ms/step - loss: 1.0113 - accuracy: 0.6913 - val_loss: 0.6258 - val_accuracy: 0.6075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5373 - accuracy: 0.68 - ETA: 0s - loss: 0.5098 - accuracy: 0.74 - ETA: 0s - loss: 0.5808 - accuracy: 0.73 - ETA: 0s - loss: 0.5686 - accuracy: 0.75 - ETA: 0s - loss: 0.5755 - accuracy: 0.75 - ETA: 0s - loss: 0.5664 - accuracy: 0.75 - ETA: 0s - loss: 0.5755 - accuracy: 0.74 - 0s 5ms/step - loss: 0.5728 - accuracy: 0.7499 - val_loss: 0.5718 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.81 - ETA: 0s - loss: 0.4790 - accuracy: 0.79 - ETA: 0s - loss: 0.4874 - accuracy: 0.79 - ETA: 0s - loss: 0.4891 - accuracy: 0.79 - ETA: 0s - loss: 0.4977 - accuracy: 0.78 - ETA: 0s - loss: 0.4952 - accuracy: 0.79 - ETA: 0s - loss: 0.4982 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4987 - accuracy: 0.7973 - val_loss: 0.6873 - val_accuracy: 0.6821\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3663 - accuracy: 0.93 - ETA: 0s - loss: 0.4378 - accuracy: 0.85 - ETA: 0s - loss: 0.4713 - accuracy: 0.83 - ETA: 0s - loss: 0.4799 - accuracy: 0.82 - ETA: 0s - loss: 0.4949 - accuracy: 0.81 - ETA: 0s - loss: 0.5005 - accuracy: 0.81 - ETA: 0s - loss: 0.4969 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4935 - accuracy: 0.8115 - val_loss: 0.5953 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.84 - ETA: 0s - loss: 0.4385 - accuracy: 0.85 - ETA: 0s - loss: 0.4337 - accuracy: 0.85 - ETA: 0s - loss: 0.4605 - accuracy: 0.83 - ETA: 0s - loss: 0.4610 - accuracy: 0.83 - ETA: 0s - loss: 0.4573 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4612 - accuracy: 0.8305 - val_loss: 1.0187 - val_accuracy: 0.6657\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.84 - ETA: 0s - loss: 0.3608 - accuracy: 0.86 - ETA: 0s - loss: 0.3541 - accuracy: 0.87 - ETA: 0s - loss: 0.3951 - accuracy: 0.85 - ETA: 0s - loss: 0.3918 - accuracy: 0.85 - ETA: 0s - loss: 0.4097 - accuracy: 0.84 - ETA: 0s - loss: 0.4294 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8455 - val_loss: 0.6111 - val_accuracy: 0.6910\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.75 - ETA: 0s - loss: 0.5530 - accuracy: 0.78 - ETA: 0s - loss: 0.4807 - accuracy: 0.82 - ETA: 0s - loss: 0.4549 - accuracy: 0.82 - ETA: 0s - loss: 0.4599 - accuracy: 0.82 - ETA: 0s - loss: 0.4484 - accuracy: 0.83 - ETA: 0s - loss: 0.4459 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4433 - accuracy: 0.8328 - val_loss: 0.7800 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.96 - ETA: 0s - loss: 0.3423 - accuracy: 0.87 - ETA: 0s - loss: 0.3295 - accuracy: 0.87 - ETA: 0s - loss: 0.3352 - accuracy: 0.87 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.3583 - accuracy: 0.86 - ETA: 0s - loss: 0.3660 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3754 - accuracy: 0.8600 - val_loss: 1.0104 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.87 - ETA: 0s - loss: 0.3296 - accuracy: 0.84 - ETA: 0s - loss: 0.3375 - accuracy: 0.85 - ETA: 0s - loss: 0.3730 - accuracy: 0.85 - ETA: 0s - loss: 0.3700 - accuracy: 0.85 - ETA: 0s - loss: 0.3625 - accuracy: 0.85 - ETA: 0s - loss: 0.3606 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3933 - accuracy: 0.8533 - val_loss: 0.6260 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.90 - ETA: 0s - loss: 0.6065 - accuracy: 0.88 - ETA: 0s - loss: 0.6137 - accuracy: 0.86 - ETA: 0s - loss: 0.5492 - accuracy: 0.86 - ETA: 0s - loss: 0.5223 - accuracy: 0.85 - ETA: 0s - loss: 0.5153 - accuracy: 0.85 - ETA: 0s - loss: 0.5078 - accuracy: 0.85 - 0s 4ms/step - loss: 0.5117 - accuracy: 0.8518 - val_loss: 1.0845 - val_accuracy: 0.6896\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.65 - ETA: 0s - loss: 0.6096 - accuracy: 0.79 - ETA: 0s - loss: 0.6382 - accuracy: 0.81 - ETA: 0s - loss: 0.5711 - accuracy: 0.83 - ETA: 0s - loss: 0.6686 - accuracy: 0.82 - ETA: 0s - loss: 0.6363 - accuracy: 0.82 - ETA: 0s - loss: 0.6096 - accuracy: 0.82 - 0s 5ms/step - loss: 0.6075 - accuracy: 0.8320 - val_loss: 0.8421 - val_accuracy: 0.6896\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9028 - accuracy: 0.75 - ETA: 0s - loss: 0.4893 - accuracy: 0.81 - ETA: 0s - loss: 0.4790 - accuracy: 0.82 - ETA: 0s - loss: 0.5503 - accuracy: 0.81 - ETA: 0s - loss: 0.5511 - accuracy: 0.80 - ETA: 0s - loss: 0.8513 - accuracy: 0.81 - ETA: 0s - loss: 0.8312 - accuracy: 0.80 - 0s 5ms/step - loss: 0.7973 - accuracy: 0.8029 - val_loss: 0.6023 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.78 - ETA: 0s - loss: 0.5678 - accuracy: 0.80 - ETA: 0s - loss: 0.5529 - accuracy: 0.82 - ETA: 0s - loss: 0.5508 - accuracy: 0.81 - ETA: 0s - loss: 0.5559 - accuracy: 0.81 - ETA: 0s - loss: 0.5424 - accuracy: 0.81 - ETA: 0s - loss: 0.5338 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5365 - accuracy: 0.8145 - val_loss: 1.0701 - val_accuracy: 0.7358\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.78 - ETA: 0s - loss: 0.5493 - accuracy: 0.83 - ETA: 0s - loss: 0.5335 - accuracy: 0.83 - ETA: 0s - loss: 0.5159 - accuracy: 0.84 - ETA: 0s - loss: 0.5286 - accuracy: 0.83 - ETA: 0s - loss: 0.5215 - accuracy: 0.83 - ETA: 0s - loss: 0.5112 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5161 - accuracy: 0.8365 - val_loss: 0.6195 - val_accuracy: 0.7478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.81 - ETA: 0s - loss: 0.4881 - accuracy: 0.85 - ETA: 0s - loss: 0.4938 - accuracy: 0.84 - ETA: 0s - loss: 0.5041 - accuracy: 0.85 - ETA: 0s - loss: 0.5654 - accuracy: 0.84 - ETA: 0s - loss: 0.5743 - accuracy: 0.84 - ETA: 0s - loss: 0.5614 - accuracy: 0.84 - ETA: 0s - loss: 0.5516 - accuracy: 0.84 - 0s 6ms/step - loss: 0.5516 - accuracy: 0.8458 - val_loss: 0.8082 - val_accuracy: 0.7612\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.84 - ETA: 0s - loss: 0.5115 - accuracy: 0.83 - ETA: 0s - loss: 0.4859 - accuracy: 0.84 - ETA: 0s - loss: 0.4617 - accuracy: 0.85 - ETA: 0s - loss: 0.4604 - accuracy: 0.85 - ETA: 0s - loss: 0.4620 - accuracy: 0.85 - ETA: 0s - loss: 0.4643 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4656 - accuracy: 0.8570 - val_loss: 0.7034 - val_accuracy: 0.7254\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.96 - ETA: 0s - loss: 0.3891 - accuracy: 0.88 - ETA: 0s - loss: 0.3927 - accuracy: 0.88 - ETA: 0s - loss: 0.4165 - accuracy: 0.87 - ETA: 0s - loss: 0.4366 - accuracy: 0.87 - ETA: 0s - loss: 0.4388 - accuracy: 0.87 - ETA: 0s - loss: 0.4323 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8727 - val_loss: 0.7829 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.84 - ETA: 0s - loss: 0.3837 - accuracy: 0.86 - ETA: 0s - loss: 0.3740 - accuracy: 0.87 - ETA: 0s - loss: 0.3764 - accuracy: 0.88 - ETA: 0s - loss: 0.3851 - accuracy: 0.88 - ETA: 0s - loss: 0.3822 - accuracy: 0.87 - ETA: 0s - loss: 0.3775 - accuracy: 0.88 - ETA: 0s - loss: 0.3815 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8806 - val_loss: 0.6589 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.78 - ETA: 0s - loss: 0.3876 - accuracy: 0.89 - ETA: 0s - loss: 0.3622 - accuracy: 0.90 - ETA: 0s - loss: 0.3841 - accuracy: 0.89 - ETA: 0s - loss: 0.3831 - accuracy: 0.88 - ETA: 0s - loss: 0.3844 - accuracy: 0.88 - ETA: 0s - loss: 0.3693 - accuracy: 0.89 - ETA: 0s - loss: 0.3679 - accuracy: 0.89 - ETA: 0s - loss: 0.3728 - accuracy: 0.89 - 0s 6ms/step - loss: 0.3710 - accuracy: 0.8921 - val_loss: 0.8415 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.81 - ETA: 0s - loss: 0.2887 - accuracy: 0.90 - ETA: 0s - loss: 0.2924 - accuracy: 0.90 - ETA: 0s - loss: 0.2956 - accuracy: 0.90 - ETA: 0s - loss: 0.3146 - accuracy: 0.90 - ETA: 0s - loss: 0.3128 - accuracy: 0.90 - ETA: 0s - loss: 0.3298 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3297 - accuracy: 0.9007 - val_loss: 1.2977 - val_accuracy: 0.7104\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.93 - ETA: 0s - loss: 0.3763 - accuracy: 0.90 - ETA: 0s - loss: 0.3746 - accuracy: 0.90 - ETA: 0s - loss: 0.3530 - accuracy: 0.90 - ETA: 0s - loss: 0.3368 - accuracy: 0.91 - ETA: 0s - loss: 0.3301 - accuracy: 0.91 - ETA: 0s - loss: 0.3441 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3416 - accuracy: 0.9074 - val_loss: 1.0324 - val_accuracy: 0.6925\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.93 - ETA: 0s - loss: 0.2738 - accuracy: 0.92 - ETA: 0s - loss: 0.3011 - accuracy: 0.92 - ETA: 0s - loss: 0.3166 - accuracy: 0.92 - ETA: 0s - loss: 0.3269 - accuracy: 0.91 - ETA: 0s - loss: 0.3282 - accuracy: 0.91 - ETA: 0s - loss: 0.3783 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3786 - accuracy: 0.9078 - val_loss: 1.3364 - val_accuracy: 0.6507\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3427 - accuracy: 0.90 - ETA: 0s - loss: 0.3622 - accuracy: 0.89 - ETA: 0s - loss: 0.3955 - accuracy: 0.87 - ETA: 0s - loss: 0.3998 - accuracy: 0.87 - ETA: 0s - loss: 0.3892 - accuracy: 0.88 - ETA: 0s - loss: 0.3975 - accuracy: 0.87 - ETA: 0s - loss: 0.3831 - accuracy: 0.88 - 0s 6ms/step - loss: 0.3828 - accuracy: 0.8832 - val_loss: 0.9885 - val_accuracy: 0.7194\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.96 - ETA: 0s - loss: 0.3437 - accuracy: 0.90 - ETA: 0s - loss: 0.3498 - accuracy: 0.89 - ETA: 0s - loss: 0.3474 - accuracy: 0.89 - ETA: 0s - loss: 0.4110 - accuracy: 0.89 - ETA: 0s - loss: 0.4082 - accuracy: 0.89 - ETA: 0s - loss: 0.8210 - accuracy: 0.89 - 0s 4ms/step - loss: 0.8132 - accuracy: 0.8914 - val_loss: 0.7404 - val_accuracy: 0.6955\n",
      "Epoch 25/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.5912 - accuracy: 0.59 - ETA: 0s - loss: 1.1090 - accuracy: 0.82 - ETA: 0s - loss: 0.8728 - accuracy: 0.82 - ETA: 0s - loss: 0.7271 - accuracy: 0.84 - ETA: 0s - loss: 0.7120 - accuracy: 0.82 - ETA: 0s - loss: 0.6900 - accuracy: 0.82 - ETA: 0s - loss: 0.6460 - accuracy: 0.8328Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6418 - accuracy: 0.8328 - val_loss: 1.1720 - val_accuracy: 0.7313\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.65 - ETA: 0s - loss: 1.9406 - accuracy: 0.64 - ETA: 0s - loss: 1.4353 - accuracy: 0.66 - ETA: 0s - loss: 1.1804 - accuracy: 0.68 - ETA: 0s - loss: 1.0489 - accuracy: 0.69 - ETA: 0s - loss: 0.9445 - accuracy: 0.71 - ETA: 0s - loss: 0.9043 - accuracy: 0.70 - 0s 6ms/step - loss: 0.8851 - accuracy: 0.7006 - val_loss: 0.5957 - val_accuracy: 0.6687\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.71 - ETA: 0s - loss: 0.6818 - accuracy: 0.71 - ETA: 0s - loss: 0.6191 - accuracy: 0.72 - ETA: 0s - loss: 0.6235 - accuracy: 0.71 - ETA: 0s - loss: 0.6084 - accuracy: 0.72 - ETA: 0s - loss: 0.6148 - accuracy: 0.73 - ETA: 0s - loss: 0.6154 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6147 - accuracy: 0.7342 - val_loss: 0.5757 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.81 - ETA: 0s - loss: 0.5026 - accuracy: 0.78 - ETA: 0s - loss: 0.5081 - accuracy: 0.78 - ETA: 0s - loss: 0.5026 - accuracy: 0.79 - ETA: 0s - loss: 0.5128 - accuracy: 0.79 - ETA: 0s - loss: 0.5135 - accuracy: 0.78 - ETA: 0s - loss: 0.5246 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5220 - accuracy: 0.7779 - val_loss: 0.5523 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.78 - ETA: 0s - loss: 0.4692 - accuracy: 0.78 - ETA: 0s - loss: 0.4937 - accuracy: 0.78 - ETA: 0s - loss: 0.4952 - accuracy: 0.79 - ETA: 0s - loss: 0.4911 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4920 - accuracy: 0.8014 - val_loss: 0.5715 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.81 - ETA: 0s - loss: 0.3657 - accuracy: 0.85 - ETA: 0s - loss: 0.4023 - accuracy: 0.84 - ETA: 0s - loss: 0.4218 - accuracy: 0.83 - ETA: 0s - loss: 0.4267 - accuracy: 0.83 - ETA: 0s - loss: 0.4236 - accuracy: 0.82 - ETA: 0s - loss: 0.4315 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8219 - val_loss: 0.6036 - val_accuracy: 0.6940\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.93 - ETA: 0s - loss: 0.3787 - accuracy: 0.84 - ETA: 0s - loss: 0.3828 - accuracy: 0.84 - ETA: 0s - loss: 0.3840 - accuracy: 0.84 - ETA: 0s - loss: 0.3818 - accuracy: 0.84 - ETA: 0s - loss: 0.4536 - accuracy: 0.83 - ETA: 0s - loss: 0.4749 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4813 - accuracy: 0.8167 - val_loss: 0.5750 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.75 - ETA: 0s - loss: 0.3993 - accuracy: 0.83 - ETA: 0s - loss: 0.3967 - accuracy: 0.81 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4276 - accuracy: 0.80 - ETA: 0s - loss: 0.4351 - accuracy: 0.80 - ETA: 0s - loss: 0.4360 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4332 - accuracy: 0.8115 - val_loss: 0.9549 - val_accuracy: 0.7075\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.87 - ETA: 0s - loss: 0.4356 - accuracy: 0.85 - ETA: 0s - loss: 0.4550 - accuracy: 0.85 - ETA: 0s - loss: 0.4506 - accuracy: 0.84 - ETA: 0s - loss: 0.4731 - accuracy: 0.83 - ETA: 0s - loss: 0.4662 - accuracy: 0.82 - ETA: 0s - loss: 0.4551 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8287 - val_loss: 1.1923 - val_accuracy: 0.6836\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.78 - ETA: 0s - loss: 0.3666 - accuracy: 0.87 - ETA: 0s - loss: 0.3961 - accuracy: 0.84 - ETA: 0s - loss: 0.3969 - accuracy: 0.84 - ETA: 0s - loss: 0.4033 - accuracy: 0.84 - ETA: 0s - loss: 0.4048 - accuracy: 0.84 - ETA: 0s - loss: 0.4217 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8443 - val_loss: 0.5949 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.84 - ETA: 0s - loss: 0.3544 - accuracy: 0.85 - ETA: 0s - loss: 0.3705 - accuracy: 0.84 - ETA: 0s - loss: 0.3764 - accuracy: 0.83 - ETA: 0s - loss: 0.3985 - accuracy: 0.83 - ETA: 0s - loss: 0.4028 - accuracy: 0.83 - ETA: 0s - loss: 0.3940 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3948 - accuracy: 0.8410 - val_loss: 1.0116 - val_accuracy: 0.6597\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.84 - ETA: 0s - loss: 0.4057 - accuracy: 0.81 - ETA: 0s - loss: 0.3626 - accuracy: 0.83 - ETA: 0s - loss: 0.3518 - accuracy: 0.85 - ETA: 0s - loss: 0.3292 - accuracy: 0.85 - ETA: 0s - loss: 0.3459 - accuracy: 0.85 - ETA: 0s - loss: 0.3420 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8518 - val_loss: 0.7312 - val_accuracy: 0.7075\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.84 - ETA: 0s - loss: 0.3173 - accuracy: 0.86 - ETA: 0s - loss: 0.3105 - accuracy: 0.86 - ETA: 0s - loss: 0.3564 - accuracy: 0.85 - ETA: 0s - loss: 0.3740 - accuracy: 0.85 - ETA: 0s - loss: 0.5961 - accuracy: 0.83 - ETA: 0s - loss: 0.6117 - accuracy: 0.82 - 0s 4ms/step - loss: 0.6086 - accuracy: 0.8175 - val_loss: 0.7705 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7431 - accuracy: 0.59 - ETA: 0s - loss: 0.5078 - accuracy: 0.77 - ETA: 0s - loss: 0.5514 - accuracy: 0.79 - ETA: 0s - loss: 0.5458 - accuracy: 0.81 - ETA: 0s - loss: 0.5446 - accuracy: 0.80 - ETA: 0s - loss: 0.5330 - accuracy: 0.80 - ETA: 0s - loss: 0.5240 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5096 - accuracy: 0.8126 - val_loss: 1.0099 - val_accuracy: 0.6373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.71 - ETA: 0s - loss: 0.4400 - accuracy: 0.82 - ETA: 0s - loss: 0.4617 - accuracy: 0.83 - ETA: 0s - loss: 0.4658 - accuracy: 0.83 - ETA: 0s - loss: 0.4711 - accuracy: 0.83 - ETA: 0s - loss: 0.4660 - accuracy: 0.84 - ETA: 0s - loss: 0.4700 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.8421Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8421 - val_loss: 0.6920 - val_accuracy: 0.7149\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 39d3ec2957d2cf6b367047b8a73911d6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.744278609752655</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.13754091372564337</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 145</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8194 - accuracy: 0.62 - ETA: 0s - loss: 4.8499 - accuracy: 0.63 - ETA: 0s - loss: 2.7498 - accuracy: 0.66 - ETA: 0s - loss: 2.0373 - accuracy: 0.68 - ETA: 0s - loss: 1.6778 - accuracy: 0.69 - ETA: 0s - loss: 1.4859 - accuracy: 0.70 - ETA: 0s - loss: 1.3373 - accuracy: 0.70 - ETA: 0s - loss: 1.2289 - accuracy: 0.70 - 1s 7ms/step - loss: 1.1678 - accuracy: 0.7036 - val_loss: 0.6017 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.84 - ETA: 0s - loss: 0.5414 - accuracy: 0.74 - ETA: 0s - loss: 0.5626 - accuracy: 0.72 - ETA: 0s - loss: 0.5760 - accuracy: 0.72 - ETA: 0s - loss: 0.5710 - accuracy: 0.72 - ETA: 0s - loss: 0.5813 - accuracy: 0.72 - ETA: 0s - loss: 0.5805 - accuracy: 0.72 - ETA: 0s - loss: 0.5770 - accuracy: 0.72 - 0s 6ms/step - loss: 0.5808 - accuracy: 0.7286 - val_loss: 0.5949 - val_accuracy: 0.7373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8507 - accuracy: 0.68 - ETA: 0s - loss: 0.5828 - accuracy: 0.78 - ETA: 0s - loss: 0.5699 - accuracy: 0.77 - ETA: 0s - loss: 0.5707 - accuracy: 0.76 - ETA: 0s - loss: 0.6144 - accuracy: 0.76 - ETA: 0s - loss: 0.6083 - accuracy: 0.76 - ETA: 0s - loss: 0.5978 - accuracy: 0.76 - ETA: 0s - loss: 0.5970 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5970 - accuracy: 0.7686 - val_loss: 0.5660 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.75 - ETA: 0s - loss: 0.5285 - accuracy: 0.78 - ETA: 0s - loss: 0.5160 - accuracy: 0.78 - ETA: 0s - loss: 0.5291 - accuracy: 0.77 - ETA: 0s - loss: 0.5303 - accuracy: 0.78 - ETA: 0s - loss: 0.5210 - accuracy: 0.78 - ETA: 0s - loss: 0.5234 - accuracy: 0.78 - ETA: 0s - loss: 0.5247 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5247 - accuracy: 0.7876 - val_loss: 0.6615 - val_accuracy: 0.6254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.68 - ETA: 0s - loss: 0.4723 - accuracy: 0.82 - ETA: 0s - loss: 0.4780 - accuracy: 0.81 - ETA: 0s - loss: 0.4929 - accuracy: 0.82 - ETA: 0s - loss: 0.4817 - accuracy: 0.81 - ETA: 0s - loss: 0.5026 - accuracy: 0.82 - ETA: 0s - loss: 0.4953 - accuracy: 0.82 - ETA: 0s - loss: 0.5030 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5081 - accuracy: 0.8182 - val_loss: 0.7958 - val_accuracy: 0.6806\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.90 - ETA: 0s - loss: 0.4262 - accuracy: 0.81 - ETA: 0s - loss: 0.4414 - accuracy: 0.82 - ETA: 0s - loss: 0.4481 - accuracy: 0.82 - ETA: 0s - loss: 0.4373 - accuracy: 0.83 - ETA: 0s - loss: 0.4448 - accuracy: 0.83 - ETA: 0s - loss: 0.4407 - accuracy: 0.82 - ETA: 0s - loss: 0.4476 - accuracy: 0.82 - ETA: 0s - loss: 0.4564 - accuracy: 0.82 - 0s 6ms/step - loss: 0.4586 - accuracy: 0.8238 - val_loss: 0.5951 - val_accuracy: 0.6791\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4332 - accuracy: 0.90 - ETA: 0s - loss: 0.4566 - accuracy: 0.81 - ETA: 0s - loss: 0.4844 - accuracy: 0.82 - ETA: 0s - loss: 0.4561 - accuracy: 0.84 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - ETA: 0s - loss: 0.4550 - accuracy: 0.84 - ETA: 0s - loss: 0.4564 - accuracy: 0.84 - ETA: 0s - loss: 0.4458 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4532 - accuracy: 0.8496 - val_loss: 1.0045 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.81 - ETA: 0s - loss: 0.4229 - accuracy: 0.85 - ETA: 0s - loss: 0.3976 - accuracy: 0.85 - ETA: 0s - loss: 0.3911 - accuracy: 0.85 - ETA: 0s - loss: 0.3842 - accuracy: 0.85 - ETA: 0s - loss: 0.3834 - accuracy: 0.85 - ETA: 0s - loss: 0.3900 - accuracy: 0.85 - ETA: 0s - loss: 0.3918 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3844 - accuracy: 0.8623 - val_loss: 0.7186 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.75 - ETA: 0s - loss: 0.3321 - accuracy: 0.90 - ETA: 0s - loss: 0.3469 - accuracy: 0.89 - ETA: 0s - loss: 0.3489 - accuracy: 0.88 - ETA: 0s - loss: 0.3536 - accuracy: 0.88 - ETA: 0s - loss: 0.3502 - accuracy: 0.87 - ETA: 0s - loss: 0.3517 - accuracy: 0.87 - ETA: 0s - loss: 0.3678 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3713 - accuracy: 0.8705 - val_loss: 0.8223 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.90 - ETA: 0s - loss: 0.3177 - accuracy: 0.90 - ETA: 0s - loss: 0.3074 - accuracy: 0.91 - ETA: 0s - loss: 0.3038 - accuracy: 0.91 - ETA: 0s - loss: 0.3173 - accuracy: 0.90 - ETA: 0s - loss: 0.3216 - accuracy: 0.89 - ETA: 0s - loss: 0.3219 - accuracy: 0.89 - ETA: 0s - loss: 0.3304 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3343 - accuracy: 0.8936 - val_loss: 0.9337 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.93 - ETA: 0s - loss: 0.3350 - accuracy: 0.89 - ETA: 0s - loss: 0.3639 - accuracy: 0.88 - ETA: 0s - loss: 0.3903 - accuracy: 0.88 - ETA: 0s - loss: 0.3903 - accuracy: 0.87 - ETA: 0s - loss: 0.3785 - accuracy: 0.88 - ETA: 0s - loss: 0.3720 - accuracy: 0.88 - ETA: 0s - loss: 0.3649 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3832 - accuracy: 0.8832 - val_loss: 1.1843 - val_accuracy: 0.6910\n",
      "Epoch 12/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3971 - accuracy: 0.90 - ETA: 0s - loss: 1.1134 - accuracy: 0.87 - ETA: 0s - loss: 0.7624 - accuracy: 0.86 - ETA: 0s - loss: 0.6482 - accuracy: 0.86 - ETA: 0s - loss: 0.5866 - accuracy: 0.86 - ETA: 0s - loss: 0.5503 - accuracy: 0.86 - ETA: 0s - loss: 0.5357 - accuracy: 0.86 - ETA: 0s - loss: 0.5278 - accuracy: 0.8569Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5261 - accuracy: 0.8559 - val_loss: 0.6657 - val_accuracy: 0.7224\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1413 - accuracy: 0.53 - ETA: 0s - loss: 3.4207 - accuracy: 0.58 - ETA: 0s - loss: 2.1252 - accuracy: 0.65 - ETA: 0s - loss: 1.6235 - accuracy: 0.68 - ETA: 0s - loss: 1.3696 - accuracy: 0.69 - ETA: 0s - loss: 1.2236 - accuracy: 0.68 - ETA: 0s - loss: 1.1223 - accuracy: 0.69 - ETA: 0s - loss: 1.0604 - accuracy: 0.69 - 1s 7ms/step - loss: 1.0226 - accuracy: 0.6913 - val_loss: 0.6905 - val_accuracy: 0.6910\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.75 - ETA: 0s - loss: 0.5494 - accuracy: 0.75 - ETA: 0s - loss: 0.6226 - accuracy: 0.73 - ETA: 0s - loss: 0.6200 - accuracy: 0.72 - ETA: 0s - loss: 0.6072 - accuracy: 0.74 - ETA: 0s - loss: 0.5894 - accuracy: 0.75 - ETA: 0s - loss: 0.5898 - accuracy: 0.75 - ETA: 0s - loss: 0.5912 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5922 - accuracy: 0.7559 - val_loss: 0.6785 - val_accuracy: 0.7045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.81 - ETA: 0s - loss: 0.6205 - accuracy: 0.79 - ETA: 0s - loss: 0.5844 - accuracy: 0.76 - ETA: 0s - loss: 0.5726 - accuracy: 0.77 - ETA: 0s - loss: 0.5570 - accuracy: 0.77 - ETA: 0s - loss: 0.5546 - accuracy: 0.78 - ETA: 0s - loss: 0.5595 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5586 - accuracy: 0.7824 - val_loss: 0.6719 - val_accuracy: 0.6388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.84 - ETA: 0s - loss: 0.4671 - accuracy: 0.80 - ETA: 0s - loss: 0.4755 - accuracy: 0.82 - ETA: 0s - loss: 0.4794 - accuracy: 0.82 - ETA: 0s - loss: 0.4839 - accuracy: 0.81 - ETA: 0s - loss: 0.4969 - accuracy: 0.80 - ETA: 0s - loss: 0.4995 - accuracy: 0.80 - ETA: 0s - loss: 0.4964 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5013 - accuracy: 0.8066 - val_loss: 0.7978 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.87 - ETA: 0s - loss: 0.4600 - accuracy: 0.84 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4535 - accuracy: 0.84 - ETA: 0s - loss: 0.4546 - accuracy: 0.83 - ETA: 0s - loss: 0.4740 - accuracy: 0.81 - ETA: 0s - loss: 0.4927 - accuracy: 0.81 - ETA: 0s - loss: 0.5054 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5153 - accuracy: 0.7996 - val_loss: 1.1015 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.68 - ETA: 0s - loss: 0.9056 - accuracy: 0.81 - ETA: 0s - loss: 0.8212 - accuracy: 0.77 - ETA: 0s - loss: 0.7217 - accuracy: 0.77 - ETA: 0s - loss: 0.6720 - accuracy: 0.78 - ETA: 0s - loss: 0.6523 - accuracy: 0.78 - ETA: 0s - loss: 0.6942 - accuracy: 0.78 - ETA: 0s - loss: 0.6823 - accuracy: 0.78 - 0s 6ms/step - loss: 0.6823 - accuracy: 0.7831 - val_loss: 0.7838 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3982 - accuracy: 0.93 - ETA: 0s - loss: 0.4895 - accuracy: 0.85 - ETA: 0s - loss: 0.4937 - accuracy: 0.84 - ETA: 0s - loss: 0.5125 - accuracy: 0.83 - ETA: 0s - loss: 0.5224 - accuracy: 0.82 - ETA: 0s - loss: 0.5171 - accuracy: 0.81 - ETA: 0s - loss: 0.5212 - accuracy: 0.81 - ETA: 0s - loss: 0.5161 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5142 - accuracy: 0.8111 - val_loss: 0.5793 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.90 - ETA: 0s - loss: 0.4103 - accuracy: 0.84 - ETA: 0s - loss: 0.4166 - accuracy: 0.84 - ETA: 0s - loss: 0.4180 - accuracy: 0.84 - ETA: 0s - loss: 0.4405 - accuracy: 0.83 - ETA: 0s - loss: 0.4479 - accuracy: 0.83 - ETA: 0s - loss: 0.4561 - accuracy: 0.82 - ETA: 0s - loss: 0.4653 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8264 - val_loss: 0.6970 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.93 - ETA: 0s - loss: 0.5080 - accuracy: 0.83 - ETA: 0s - loss: 0.4519 - accuracy: 0.83 - ETA: 0s - loss: 0.4970 - accuracy: 0.82 - ETA: 0s - loss: 0.4776 - accuracy: 0.83 - ETA: 0s - loss: 0.4588 - accuracy: 0.84 - ETA: 0s - loss: 0.4548 - accuracy: 0.84 - ETA: 0s - loss: 0.5112 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5106 - accuracy: 0.8414 - val_loss: 0.5754 - val_accuracy: 0.7343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.87 - ETA: 0s - loss: 0.4705 - accuracy: 0.85 - ETA: 0s - loss: 0.4941 - accuracy: 0.83 - ETA: 0s - loss: 0.4686 - accuracy: 0.84 - ETA: 0s - loss: 0.5541 - accuracy: 0.84 - ETA: 0s - loss: 0.5389 - accuracy: 0.83 - ETA: 0s - loss: 0.5244 - accuracy: 0.83 - ETA: 0s - loss: 0.5763 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5763 - accuracy: 0.8279 - val_loss: 0.5867 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.65 - ETA: 0s - loss: 0.6650 - accuracy: 0.76 - ETA: 0s - loss: 0.6051 - accuracy: 0.78 - ETA: 0s - loss: 0.5612 - accuracy: 0.80 - ETA: 0s - loss: 0.5390 - accuracy: 0.80 - ETA: 0s - loss: 0.5173 - accuracy: 0.81 - ETA: 0s - loss: 0.5015 - accuracy: 0.82 - ETA: 0s - loss: 0.4923 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4933 - accuracy: 0.8272 - val_loss: 0.6112 - val_accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4382 - accuracy: 0.84 - ETA: 0s - loss: 0.5140 - accuracy: 0.86 - ETA: 0s - loss: 0.4825 - accuracy: 0.85 - ETA: 0s - loss: 0.4634 - accuracy: 0.85 - ETA: 0s - loss: 0.4560 - accuracy: 0.85 - ETA: 0s - loss: 0.4526 - accuracy: 0.85 - ETA: 0s - loss: 0.5090 - accuracy: 0.84 - ETA: 0s - loss: 0.5146 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5114 - accuracy: 0.8425 - val_loss: 0.6303 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.84 - ETA: 0s - loss: 0.4769 - accuracy: 0.83 - ETA: 0s - loss: 0.4631 - accuracy: 0.85 - ETA: 0s - loss: 0.4699 - accuracy: 0.84 - ETA: 0s - loss: 0.5012 - accuracy: 0.84 - ETA: 0s - loss: 0.5031 - accuracy: 0.84 - ETA: 0s - loss: 0.5068 - accuracy: 0.84 - ETA: 0s - loss: 0.5114 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5105 - accuracy: 0.8466 - val_loss: 1.7050 - val_accuracy: 0.7060\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7232 - accuracy: 0.78 - ETA: 0s - loss: 0.5624 - accuracy: 0.82 - ETA: 0s - loss: 0.5918 - accuracy: 0.80 - ETA: 0s - loss: 0.5610 - accuracy: 0.81 - ETA: 0s - loss: 0.5486 - accuracy: 0.82 - ETA: 0s - loss: 0.5406 - accuracy: 0.82 - ETA: 0s - loss: 0.5440 - accuracy: 0.82 - ETA: 0s - loss: 0.5361 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5405 - accuracy: 0.8268 - val_loss: 1.1654 - val_accuracy: 0.7075\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.90 - ETA: 0s - loss: 0.5123 - accuracy: 0.81 - ETA: 0s - loss: 0.6129 - accuracy: 0.81 - ETA: 0s - loss: 0.5800 - accuracy: 0.81 - ETA: 0s - loss: 0.6413 - accuracy: 0.82 - ETA: 0s - loss: 0.8036 - accuracy: 0.81 - ETA: 0s - loss: 0.7813 - accuracy: 0.80 - ETA: 0s - loss: 0.7909 - accuracy: 0.79 - 0s 5ms/step - loss: 0.7909 - accuracy: 0.7999 - val_loss: 0.9927 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5467 - accuracy: 0.81 - ETA: 0s - loss: 0.6360 - accuracy: 0.78 - ETA: 0s - loss: 0.6270 - accuracy: 0.78 - ETA: 0s - loss: 0.6172 - accuracy: 0.78 - ETA: 0s - loss: 0.6219 - accuracy: 0.78 - ETA: 0s - loss: 0.6241 - accuracy: 0.78 - ETA: 0s - loss: 0.6224 - accuracy: 0.78 - ETA: 0s - loss: 0.6265 - accuracy: 0.7782Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6252 - accuracy: 0.7798 - val_loss: 0.8888 - val_accuracy: 0.7358\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.68 - ETA: 0s - loss: 1.6397 - accuracy: 0.60 - ETA: 0s - loss: 1.1903 - accuracy: 0.66 - ETA: 0s - loss: 1.0153 - accuracy: 0.67 - ETA: 0s - loss: 0.9298 - accuracy: 0.68 - ETA: 0s - loss: 0.8701 - accuracy: 0.69 - ETA: 0s - loss: 0.8385 - accuracy: 0.69 - ETA: 0s - loss: 0.8054 - accuracy: 0.69 - 1s 6ms/step - loss: 0.7958 - accuracy: 0.6969 - val_loss: 0.5632 - val_accuracy: 0.7104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.78 - ETA: 0s - loss: 0.5820 - accuracy: 0.74 - ETA: 0s - loss: 0.5790 - accuracy: 0.74 - ETA: 0s - loss: 0.5809 - accuracy: 0.74 - ETA: 0s - loss: 0.5712 - accuracy: 0.75 - ETA: 0s - loss: 0.5798 - accuracy: 0.75 - ETA: 0s - loss: 0.5730 - accuracy: 0.75 - ETA: 0s - loss: 0.5722 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5722 - accuracy: 0.7551 - val_loss: 0.5980 - val_accuracy: 0.7284\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.78 - ETA: 0s - loss: 0.5249 - accuracy: 0.77 - ETA: 0s - loss: 0.5353 - accuracy: 0.77 - ETA: 0s - loss: 0.5263 - accuracy: 0.78 - ETA: 0s - loss: 0.5424 - accuracy: 0.78 - ETA: 0s - loss: 0.5485 - accuracy: 0.78 - ETA: 0s - loss: 0.5501 - accuracy: 0.78 - ETA: 0s - loss: 0.5475 - accuracy: 0.78 - 0s 6ms/step - loss: 0.5504 - accuracy: 0.7809 - val_loss: 0.5755 - val_accuracy: 0.7478\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.90 - ETA: 0s - loss: 0.4661 - accuracy: 0.81 - ETA: 0s - loss: 0.4956 - accuracy: 0.81 - ETA: 0s - loss: 0.5049 - accuracy: 0.80 - ETA: 0s - loss: 0.5163 - accuracy: 0.80 - ETA: 0s - loss: 0.5394 - accuracy: 0.80 - ETA: 0s - loss: 0.5385 - accuracy: 0.80 - ETA: 0s - loss: 0.5335 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5335 - accuracy: 0.8033 - val_loss: 0.5695 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.87 - ETA: 0s - loss: 0.4834 - accuracy: 0.81 - ETA: 0s - loss: 0.4425 - accuracy: 0.83 - ETA: 0s - loss: 0.4685 - accuracy: 0.82 - ETA: 0s - loss: 0.4779 - accuracy: 0.82 - ETA: 0s - loss: 0.4842 - accuracy: 0.82 - ETA: 0s - loss: 0.4963 - accuracy: 0.81 - ETA: 0s - loss: 0.5281 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5275 - accuracy: 0.8096 - val_loss: 0.7815 - val_accuracy: 0.7119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.78 - ETA: 0s - loss: 0.5987 - accuracy: 0.78 - ETA: 0s - loss: 0.5288 - accuracy: 0.80 - ETA: 0s - loss: 0.5062 - accuracy: 0.81 - ETA: 0s - loss: 0.5016 - accuracy: 0.81 - ETA: 0s - loss: 0.5830 - accuracy: 0.81 - ETA: 0s - loss: 0.5809 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5827 - accuracy: 0.8096 - val_loss: 0.6629 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.81 - ETA: 0s - loss: 0.5449 - accuracy: 0.81 - ETA: 0s - loss: 0.5177 - accuracy: 0.83 - ETA: 0s - loss: 0.5080 - accuracy: 0.84 - ETA: 0s - loss: 0.5013 - accuracy: 0.84 - ETA: 0s - loss: 0.5044 - accuracy: 0.84 - ETA: 0s - loss: 0.5090 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5055 - accuracy: 0.8343 - val_loss: 0.6423 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.96 - ETA: 0s - loss: 0.4708 - accuracy: 0.85 - ETA: 0s - loss: 0.4649 - accuracy: 0.85 - ETA: 0s - loss: 0.4714 - accuracy: 0.84 - ETA: 0s - loss: 0.4639 - accuracy: 0.84 - ETA: 0s - loss: 0.4609 - accuracy: 0.84 - ETA: 0s - loss: 0.4711 - accuracy: 0.84 - ETA: 0s - loss: 0.4707 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4707 - accuracy: 0.8436 - val_loss: 0.8323 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.93 - ETA: 0s - loss: 0.4158 - accuracy: 0.84 - ETA: 0s - loss: 0.4404 - accuracy: 0.84 - ETA: 0s - loss: 0.5207 - accuracy: 0.83 - ETA: 0s - loss: 0.5009 - accuracy: 0.84 - ETA: 0s - loss: 0.4978 - accuracy: 0.84 - ETA: 0s - loss: 0.5281 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5200 - accuracy: 0.8410 - val_loss: 0.6776 - val_accuracy: 0.7403\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.75 - ETA: 0s - loss: 0.4725 - accuracy: 0.85 - ETA: 0s - loss: 0.4413 - accuracy: 0.85 - ETA: 0s - loss: 0.4378 - accuracy: 0.85 - ETA: 0s - loss: 0.4497 - accuracy: 0.85 - ETA: 0s - loss: 0.4498 - accuracy: 0.84 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4527 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4527 - accuracy: 0.8455 - val_loss: 0.6404 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.81 - ETA: 0s - loss: 0.4343 - accuracy: 0.86 - ETA: 0s - loss: 0.4176 - accuracy: 0.86 - ETA: 0s - loss: 0.4163 - accuracy: 0.86 - ETA: 0s - loss: 0.4246 - accuracy: 0.86 - ETA: 0s - loss: 0.4530 - accuracy: 0.86 - ETA: 0s - loss: 0.4557 - accuracy: 0.86 - ETA: 0s - loss: 0.4460 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4480 - accuracy: 0.8604 - val_loss: 0.6788 - val_accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3932 - accuracy: 0.90 - ETA: 0s - loss: 0.4048 - accuracy: 0.86 - ETA: 0s - loss: 0.3984 - accuracy: 0.87 - ETA: 0s - loss: 0.4079 - accuracy: 0.87 - ETA: 0s - loss: 0.4298 - accuracy: 0.86 - ETA: 0s - loss: 0.4291 - accuracy: 0.86 - ETA: 0s - loss: 0.4253 - accuracy: 0.86 - ETA: 0s - loss: 0.4155 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4159 - accuracy: 0.8694 - val_loss: 0.8340 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.90 - ETA: 0s - loss: 0.3976 - accuracy: 0.88 - ETA: 0s - loss: 0.3795 - accuracy: 0.88 - ETA: 0s - loss: 0.3731 - accuracy: 0.89 - ETA: 0s - loss: 0.3810 - accuracy: 0.88 - ETA: 0s - loss: 0.3926 - accuracy: 0.88 - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - ETA: 0s - loss: 0.4176 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4207 - accuracy: 0.8746 - val_loss: 0.5639 - val_accuracy: 0.7537\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.96 - ETA: 0s - loss: 0.4200 - accuracy: 0.87 - ETA: 0s - loss: 0.4118 - accuracy: 0.87 - ETA: 0s - loss: 0.3891 - accuracy: 0.88 - ETA: 0s - loss: 0.4070 - accuracy: 0.87 - ETA: 0s - loss: 0.4190 - accuracy: 0.87 - ETA: 0s - loss: 0.4250 - accuracy: 0.87 - ETA: 0s - loss: 0.4246 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4234 - accuracy: 0.8705 - val_loss: 0.9067 - val_accuracy: 0.7194\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.87 - ETA: 0s - loss: 0.3559 - accuracy: 0.89 - ETA: 0s - loss: 0.3773 - accuracy: 0.88 - ETA: 0s - loss: 0.3729 - accuracy: 0.88 - ETA: 0s - loss: 0.3859 - accuracy: 0.88 - ETA: 0s - loss: 0.3968 - accuracy: 0.88 - ETA: 0s - loss: 0.4074 - accuracy: 0.87 - ETA: 0s - loss: 0.4349 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4433 - accuracy: 0.8694 - val_loss: 2.0636 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1809 - accuracy: 0.84 - ETA: 0s - loss: 0.5631 - accuracy: 0.84 - ETA: 0s - loss: 0.4920 - accuracy: 0.85 - ETA: 0s - loss: 0.4871 - accuracy: 0.84 - ETA: 0s - loss: 0.4731 - accuracy: 0.85 - ETA: 0s - loss: 0.4612 - accuracy: 0.85 - ETA: 0s - loss: 0.4568 - accuracy: 0.86 - ETA: 0s - loss: 0.4460 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4417 - accuracy: 0.8697 - val_loss: 1.1174 - val_accuracy: 0.7239\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.93 - ETA: 0s - loss: 2.2162 - accuracy: 0.89 - ETA: 0s - loss: 1.7872 - accuracy: 0.88 - ETA: 0s - loss: 1.3153 - accuracy: 0.87 - ETA: 0s - loss: 1.0706 - accuracy: 0.87 - ETA: 0s - loss: 0.9634 - accuracy: 0.87 - ETA: 0s - loss: 0.9060 - accuracy: 0.86 - ETA: 0s - loss: 0.8604 - accuracy: 0.86 - 0s 5ms/step - loss: 0.8707 - accuracy: 0.8645 - val_loss: 2.0105 - val_accuracy: 0.6418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.5247 - accuracy: 0.78 - ETA: 0s - loss: 0.8144 - accuracy: 0.78 - ETA: 0s - loss: 1.1422 - accuracy: 0.78 - ETA: 0s - loss: 0.9523 - accuracy: 0.80 - ETA: 0s - loss: 0.8940 - accuracy: 0.80 - ETA: 0s - loss: 0.8702 - accuracy: 0.79 - ETA: 0s - loss: 0.8220 - accuracy: 0.79 - 0s 5ms/step - loss: 0.7871 - accuracy: 0.8033 - val_loss: 0.8546 - val_accuracy: 0.7269\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.75 - ETA: 0s - loss: 0.6600 - accuracy: 0.76 - ETA: 0s - loss: 0.6740 - accuracy: 0.77 - ETA: 0s - loss: 0.6282 - accuracy: 0.78 - ETA: 0s - loss: 0.6065 - accuracy: 0.79 - ETA: 0s - loss: 0.6312 - accuracy: 0.80 - ETA: 0s - loss: 0.6258 - accuracy: 0.80 - ETA: 0s - loss: 0.6119 - accuracy: 0.80 - ETA: 0s - loss: 0.5994 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5994 - accuracy: 0.8137 - val_loss: 1.1562 - val_accuracy: 0.7522\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7941 - accuracy: 0.68 - ETA: 0s - loss: 0.5388 - accuracy: 0.81 - ETA: 0s - loss: 0.6002 - accuracy: 0.82 - ETA: 0s - loss: 0.5575 - accuracy: 0.83 - ETA: 0s - loss: 0.5366 - accuracy: 0.83 - ETA: 0s - loss: 0.5209 - accuracy: 0.84 - ETA: 0s - loss: 0.5076 - accuracy: 0.85 - ETA: 0s - loss: 0.5078 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5129 - accuracy: 0.8473 - val_loss: 0.6781 - val_accuracy: 0.7030\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.90 - ETA: 0s - loss: 0.5245 - accuracy: 0.82 - ETA: 0s - loss: 0.4756 - accuracy: 0.85 - ETA: 0s - loss: 0.4516 - accuracy: 0.86 - ETA: 0s - loss: 0.4575 - accuracy: 0.86 - ETA: 0s - loss: 0.5137 - accuracy: 0.86 - ETA: 0s - loss: 0.5252 - accuracy: 0.85 - ETA: 0s - loss: 0.5312 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5312 - accuracy: 0.8473 - val_loss: 0.6287 - val_accuracy: 0.7299\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.78 - ETA: 0s - loss: 0.4970 - accuracy: 0.83 - ETA: 0s - loss: 0.5272 - accuracy: 0.82 - ETA: 0s - loss: 0.5139 - accuracy: 0.83 - ETA: 0s - loss: 0.5224 - accuracy: 0.82 - ETA: 0s - loss: 0.5119 - accuracy: 0.82 - ETA: 0s - loss: 0.5038 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4960 - accuracy: 0.8387 - val_loss: 0.9197 - val_accuracy: 0.7239\n",
      "Epoch 23/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4379 - accuracy: 0.84 - ETA: 0s - loss: 0.4279 - accuracy: 0.85 - ETA: 0s - loss: 0.4178 - accuracy: 0.86 - ETA: 0s - loss: 0.4254 - accuracy: 0.86 - ETA: 0s - loss: 0.4242 - accuracy: 0.86 - ETA: 0s - loss: 0.4190 - accuracy: 0.87 - ETA: 0s - loss: 0.4291 - accuracy: 0.87 - ETA: 0s - loss: 0.4387 - accuracy: 0.8653Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4426 - accuracy: 0.8619 - val_loss: 1.4055 - val_accuracy: 0.7000\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 6ee9a03d76e4d9f6154c101aaadeb906</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7457711497942606</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5313672117815393</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 185</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8045 - accuracy: 0.65 - ETA: 0s - loss: 3.1360 - accuracy: 0.58 - ETA: 0s - loss: 1.9392 - accuracy: 0.64 - ETA: 0s - loss: 1.4903 - accuracy: 0.69 - ETA: 0s - loss: 1.2660 - accuracy: 0.70 - ETA: 0s - loss: 1.1340 - accuracy: 0.71 - ETA: 0s - loss: 1.0470 - accuracy: 0.71 - ETA: 0s - loss: 0.9864 - accuracy: 0.71 - 1s 7ms/step - loss: 0.9631 - accuracy: 0.7088 - val_loss: 0.5788 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.81 - ETA: 0s - loss: 0.5679 - accuracy: 0.74 - ETA: 0s - loss: 0.5366 - accuracy: 0.75 - ETA: 0s - loss: 0.5371 - accuracy: 0.76 - ETA: 0s - loss: 0.5357 - accuracy: 0.77 - ETA: 0s - loss: 0.5388 - accuracy: 0.76 - ETA: 0s - loss: 0.5357 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5363 - accuracy: 0.7764 - val_loss: 0.5756 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.87 - ETA: 0s - loss: 0.4903 - accuracy: 0.82 - ETA: 0s - loss: 0.4911 - accuracy: 0.81 - ETA: 0s - loss: 0.4815 - accuracy: 0.80 - ETA: 0s - loss: 0.4979 - accuracy: 0.80 - ETA: 0s - loss: 0.4969 - accuracy: 0.81 - ETA: 0s - loss: 0.5037 - accuracy: 0.80 - ETA: 0s - loss: 0.5089 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5099 - accuracy: 0.8018 - val_loss: 0.6642 - val_accuracy: 0.6687\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.78 - ETA: 0s - loss: 0.4758 - accuracy: 0.79 - ETA: 0s - loss: 0.4797 - accuracy: 0.79 - ETA: 0s - loss: 0.4974 - accuracy: 0.79 - ETA: 0s - loss: 0.5442 - accuracy: 0.79 - ETA: 0s - loss: 0.6273 - accuracy: 0.78 - ETA: 0s - loss: 0.6082 - accuracy: 0.78 - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6073 - accuracy: 0.7880 - val_loss: 0.6838 - val_accuracy: 0.6821\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.78 - ETA: 0s - loss: 0.4768 - accuracy: 0.83 - ETA: 0s - loss: 0.4766 - accuracy: 0.81 - ETA: 0s - loss: 0.5524 - accuracy: 0.81 - ETA: 0s - loss: 0.5454 - accuracy: 0.81 - ETA: 0s - loss: 0.5297 - accuracy: 0.81 - ETA: 0s - loss: 0.5210 - accuracy: 0.80 - ETA: 0s - loss: 0.5195 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5198 - accuracy: 0.8059 - val_loss: 0.5940 - val_accuracy: 0.7119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.84 - ETA: 0s - loss: 0.4208 - accuracy: 0.85 - ETA: 0s - loss: 0.4272 - accuracy: 0.83 - ETA: 0s - loss: 0.4562 - accuracy: 0.82 - ETA: 0s - loss: 0.4460 - accuracy: 0.83 - ETA: 0s - loss: 0.4530 - accuracy: 0.83 - ETA: 0s - loss: 0.4604 - accuracy: 0.82 - ETA: 0s - loss: 0.4673 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4698 - accuracy: 0.8234 - val_loss: 0.6179 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.84 - ETA: 0s - loss: 0.4403 - accuracy: 0.85 - ETA: 0s - loss: 0.4308 - accuracy: 0.85 - ETA: 0s - loss: 0.4234 - accuracy: 0.85 - ETA: 0s - loss: 0.4255 - accuracy: 0.84 - ETA: 0s - loss: 0.4430 - accuracy: 0.83 - ETA: 0s - loss: 0.4476 - accuracy: 0.83 - ETA: 0s - loss: 0.4600 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4640 - accuracy: 0.8279 - val_loss: 0.5477 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.84 - ETA: 0s - loss: 0.5175 - accuracy: 0.82 - ETA: 0s - loss: 0.4852 - accuracy: 0.84 - ETA: 0s - loss: 0.4519 - accuracy: 0.85 - ETA: 0s - loss: 0.4449 - accuracy: 0.85 - ETA: 0s - loss: 0.4540 - accuracy: 0.84 - ETA: 0s - loss: 0.4631 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4739 - accuracy: 0.8350 - val_loss: 0.5486 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - ETA: 0s - loss: 0.3951 - accuracy: 0.87 - ETA: 0s - loss: 0.4080 - accuracy: 0.85 - ETA: 0s - loss: 0.4267 - accuracy: 0.85 - ETA: 0s - loss: 0.4320 - accuracy: 0.84 - ETA: 0s - loss: 0.4356 - accuracy: 0.84 - ETA: 0s - loss: 0.4371 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4638 - accuracy: 0.8443 - val_loss: 0.7313 - val_accuracy: 0.6925\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.84 - ETA: 0s - loss: 0.3988 - accuracy: 0.83 - ETA: 0s - loss: 0.3927 - accuracy: 0.84 - ETA: 0s - loss: 0.4259 - accuracy: 0.84 - ETA: 0s - loss: 0.4176 - accuracy: 0.85 - ETA: 0s - loss: 0.4228 - accuracy: 0.85 - ETA: 0s - loss: 0.4345 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8563 - val_loss: 9.2792 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 6.3890 - accuracy: 0.90 - ETA: 0s - loss: 3.5258 - accuracy: 0.70 - ETA: 0s - loss: 2.3048 - accuracy: 0.68 - ETA: 0s - loss: 1.8143 - accuracy: 0.69 - ETA: 0s - loss: 1.5427 - accuracy: 0.70 - ETA: 0s - loss: 1.3799 - accuracy: 0.69 - ETA: 0s - loss: 1.2707 - accuracy: 0.69 - ETA: 0s - loss: 1.1877 - accuracy: 0.69 - 0s 5ms/step - loss: 1.1877 - accuracy: 0.6954 - val_loss: 0.6141 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.65 - ETA: 0s - loss: 0.6412 - accuracy: 0.69 - ETA: 0s - loss: 0.6354 - accuracy: 0.68 - ETA: 0s - loss: 0.6442 - accuracy: 0.65 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.61 - ETA: 0s - loss: 0.6456 - accuracy: 0.61 - ETA: 0s - loss: 0.6519 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6534 - accuracy: 0.6073 - val_loss: 0.6862 - val_accuracy: 0.4866\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.56 - ETA: 0s - loss: 0.6179 - accuracy: 0.51 - ETA: 0s - loss: 0.6484 - accuracy: 0.53 - ETA: 0s - loss: 0.6463 - accuracy: 0.53 - ETA: 0s - loss: 0.6484 - accuracy: 0.53 - ETA: 0s - loss: 0.6540 - accuracy: 0.53 - ETA: 0s - loss: 0.6540 - accuracy: 0.53 - ETA: 0s - loss: 0.6515 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6515 - accuracy: 0.5375 - val_loss: 0.6868 - val_accuracy: 0.4896\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.56 - ETA: 0s - loss: 0.6621 - accuracy: 0.56 - ETA: 0s - loss: 0.6405 - accuracy: 0.55 - ETA: 0s - loss: 0.6423 - accuracy: 0.56 - ETA: 0s - loss: 0.6454 - accuracy: 0.56 - ETA: 0s - loss: 0.6561 - accuracy: 0.54 - ETA: 0s - loss: 0.6592 - accuracy: 0.51 - ETA: 0s - loss: 0.6639 - accuracy: 0.50 - 0s 5ms/step - loss: 0.6644 - accuracy: 0.4942 - val_loss: 0.7385 - val_accuracy: 0.3478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.46 - ETA: 0s - loss: 0.6502 - accuracy: 0.41 - ETA: 0s - loss: 0.6490 - accuracy: 0.40 - ETA: 0s - loss: 0.6598 - accuracy: 0.39 - ETA: 0s - loss: 0.6667 - accuracy: 0.40 - ETA: 0s - loss: 0.6721 - accuracy: 0.40 - ETA: 0s - loss: 0.6803 - accuracy: 0.40 - ETA: 0s - loss: 0.6806 - accuracy: 0.40 - 0s 5ms/step - loss: 0.6794 - accuracy: 0.4005 - val_loss: 0.7141 - val_accuracy: 0.3627\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7815 - accuracy: 0.31 - ETA: 0s - loss: 0.6950 - accuracy: 0.40 - ETA: 0s - loss: 0.7001 - accuracy: 0.42 - ETA: 0s - loss: 0.6874 - accuracy: 0.42 - ETA: 0s - loss: 0.6923 - accuracy: 0.41 - ETA: 0s - loss: 0.6866 - accuracy: 0.40 - ETA: 0s - loss: 0.6877 - accuracy: 0.40 - ETA: 0s - loss: 0.6858 - accuracy: 0.39 - 0s 5ms/step - loss: 0.6842 - accuracy: 0.3983 - val_loss: 0.6939 - val_accuracy: 0.3627\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.37 - ETA: 0s - loss: 0.7027 - accuracy: 0.42 - ETA: 0s - loss: 0.6902 - accuracy: 0.38 - ETA: 0s - loss: 0.6896 - accuracy: 0.38 - ETA: 0s - loss: 0.6829 - accuracy: 0.38 - ETA: 0s - loss: 0.6844 - accuracy: 0.39 - ETA: 0s - loss: 0.6820 - accuracy: 0.39 - ETA: 0s - loss: 0.6848 - accuracy: 0.39 - 0s 5ms/step - loss: 0.6845 - accuracy: 0.3983 - val_loss: 0.6992 - val_accuracy: 0.3627\n",
      "Epoch 18/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6776 - accuracy: 0.37 - ETA: 0s - loss: 0.6829 - accuracy: 0.36 - ETA: 0s - loss: 0.6800 - accuracy: 0.38 - ETA: 0s - loss: 0.6769 - accuracy: 0.38 - ETA: 0s - loss: 0.6805 - accuracy: 0.38 - ETA: 0s - loss: 0.6862 - accuracy: 0.39 - ETA: 0s - loss: 0.6840 - accuracy: 0.39 - ETA: 0s - loss: 0.6845 - accuracy: 0.3978Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.3998 - val_loss: 0.7032 - val_accuracy: 0.3627\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8687 - accuracy: 0.65 - ETA: 0s - loss: 2.6912 - accuracy: 0.58 - ETA: 0s - loss: 1.6341 - accuracy: 0.64 - ETA: 0s - loss: 1.2907 - accuracy: 0.65 - ETA: 0s - loss: 1.1348 - accuracy: 0.67 - ETA: 0s - loss: 1.0273 - accuracy: 0.68 - ETA: 0s - loss: 0.9594 - accuracy: 0.69 - ETA: 0s - loss: 0.9013 - accuracy: 0.70 - 1s 6ms/step - loss: 0.8981 - accuracy: 0.7032 - val_loss: 0.5455 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.59 - ETA: 0s - loss: 0.5060 - accuracy: 0.77 - ETA: 0s - loss: 0.5029 - accuracy: 0.76 - ETA: 0s - loss: 0.5137 - accuracy: 0.77 - ETA: 0s - loss: 0.5137 - accuracy: 0.76 - ETA: 0s - loss: 0.5148 - accuracy: 0.77 - ETA: 0s - loss: 0.5168 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5230 - accuracy: 0.7671 - val_loss: 0.6051 - val_accuracy: 0.6940\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.81 - ETA: 0s - loss: 0.4556 - accuracy: 0.78 - ETA: 0s - loss: 0.4902 - accuracy: 0.79 - ETA: 0s - loss: 0.5021 - accuracy: 0.78 - ETA: 0s - loss: 0.4933 - accuracy: 0.79 - ETA: 0s - loss: 0.5020 - accuracy: 0.78 - ETA: 0s - loss: 0.4903 - accuracy: 0.79 - 0s 5ms/step - loss: 0.4802 - accuracy: 0.7954 - val_loss: 0.6060 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.96 - ETA: 0s - loss: 0.3973 - accuracy: 0.84 - ETA: 0s - loss: 0.3997 - accuracy: 0.84 - ETA: 0s - loss: 0.4044 - accuracy: 0.83 - ETA: 0s - loss: 0.3994 - accuracy: 0.84 - ETA: 0s - loss: 0.4197 - accuracy: 0.82 - ETA: 0s - loss: 0.4218 - accuracy: 0.82 - ETA: 0s - loss: 0.4236 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8246 - val_loss: 0.7396 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.87 - ETA: 0s - loss: 0.3913 - accuracy: 0.84 - ETA: 0s - loss: 0.3571 - accuracy: 0.84 - ETA: 0s - loss: 0.3425 - accuracy: 0.85 - ETA: 0s - loss: 0.3798 - accuracy: 0.84 - ETA: 0s - loss: 0.3895 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.83 - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8365 - val_loss: 0.7439 - val_accuracy: 0.6806\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.78 - ETA: 0s - loss: 0.3790 - accuracy: 0.85 - ETA: 0s - loss: 0.3715 - accuracy: 0.85 - ETA: 0s - loss: 0.3668 - accuracy: 0.86 - ETA: 0s - loss: 0.3728 - accuracy: 0.85 - ETA: 0s - loss: 0.3755 - accuracy: 0.85 - ETA: 0s - loss: 0.3753 - accuracy: 0.84 - ETA: 0s - loss: 0.3731 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3716 - accuracy: 0.8514 - val_loss: 0.8346 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.90 - ETA: 0s - loss: 0.4034 - accuracy: 0.84 - ETA: 0s - loss: 0.3674 - accuracy: 0.84 - ETA: 0s - loss: 0.3998 - accuracy: 0.83 - ETA: 0s - loss: 0.3901 - accuracy: 0.83 - ETA: 0s - loss: 0.4008 - accuracy: 0.83 - ETA: 0s - loss: 0.4108 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8387 - val_loss: 1.2617 - val_accuracy: 0.6716\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.90 - ETA: 0s - loss: 0.3687 - accuracy: 0.82 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.4451 - accuracy: 0.83 - ETA: 0s - loss: 0.4550 - accuracy: 0.82 - ETA: 0s - loss: 0.4583 - accuracy: 0.82 - ETA: 0s - loss: 0.4530 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4518 - accuracy: 0.8208 - val_loss: 1.2605 - val_accuracy: 0.7269\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.84 - ETA: 0s - loss: 0.5481 - accuracy: 0.84 - ETA: 0s - loss: 0.5829 - accuracy: 0.84 - ETA: 0s - loss: 0.6047 - accuracy: 0.81 - ETA: 0s - loss: 0.6228 - accuracy: 0.79 - ETA: 0s - loss: 0.6047 - accuracy: 0.79 - ETA: 0s - loss: 0.5920 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5807 - accuracy: 0.7865 - val_loss: 0.6567 - val_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.87 - ETA: 0s - loss: 0.4769 - accuracy: 0.81 - ETA: 0s - loss: 0.4875 - accuracy: 0.81 - ETA: 0s - loss: 0.4767 - accuracy: 0.81 - ETA: 0s - loss: 0.4646 - accuracy: 0.81 - ETA: 0s - loss: 0.4735 - accuracy: 0.80 - ETA: 0s - loss: 0.4666 - accuracy: 0.80 - ETA: 0s - loss: 0.4632 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4635 - accuracy: 0.8096 - val_loss: 0.6264 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.71 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.6035 - accuracy: 0.82 - ETA: 0s - loss: 0.5782 - accuracy: 0.80 - ETA: 0s - loss: 0.5562 - accuracy: 0.81 - ETA: 0s - loss: 0.5402 - accuracy: 0.82 - ETA: 0s - loss: 0.5247 - accuracy: 0.82 - ETA: 0s - loss: 0.5149 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5105 - accuracy: 0.8320 - val_loss: 0.7165 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.84 - ETA: 0s - loss: 0.4042 - accuracy: 0.88 - ETA: 0s - loss: 0.4167 - accuracy: 0.86 - ETA: 0s - loss: 0.4311 - accuracy: 0.85 - ETA: 0s - loss: 0.4248 - accuracy: 0.85 - ETA: 0s - loss: 0.4344 - accuracy: 0.84 - ETA: 0s - loss: 0.4345 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8522 - val_loss: 0.8495 - val_accuracy: 0.7313\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.87 - ETA: 0s - loss: 0.3621 - accuracy: 0.88 - ETA: 0s - loss: 0.3716 - accuracy: 0.86 - ETA: 0s - loss: 0.3881 - accuracy: 0.86 - ETA: 0s - loss: 0.3886 - accuracy: 0.86 - ETA: 0s - loss: 0.3913 - accuracy: 0.86 - ETA: 0s - loss: 0.4120 - accuracy: 0.85 - ETA: 0s - loss: 0.4068 - accuracy: 0.85 - ETA: 0s - loss: 0.4146 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4095 - accuracy: 0.8518 - val_loss: 1.2142 - val_accuracy: 0.7179\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.87 - ETA: 0s - loss: 0.3942 - accuracy: 0.87 - ETA: 0s - loss: 0.4076 - accuracy: 0.86 - ETA: 0s - loss: 0.4192 - accuracy: 0.86 - ETA: 0s - loss: 0.6202 - accuracy: 0.84 - ETA: 0s - loss: 0.6392 - accuracy: 0.82 - ETA: 0s - loss: 0.6452 - accuracy: 0.81 - ETA: 0s - loss: 0.6387 - accuracy: 0.80 - 0s 6ms/step - loss: 0.6424 - accuracy: 0.8018 - val_loss: 0.7507 - val_accuracy: 0.7582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5273 - accuracy: 0.78 - ETA: 0s - loss: 0.6180 - accuracy: 0.78 - ETA: 0s - loss: 0.6032 - accuracy: 0.78 - ETA: 0s - loss: 0.6017 - accuracy: 0.78 - ETA: 0s - loss: 0.5770 - accuracy: 0.80 - ETA: 0s - loss: 0.5739 - accuracy: 0.79 - ETA: 0s - loss: 0.5617 - accuracy: 0.80 - ETA: 0s - loss: 0.5524 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5591 - accuracy: 0.8070 - val_loss: 0.6751 - val_accuracy: 0.7537\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.84 - ETA: 0s - loss: 0.5288 - accuracy: 0.81 - ETA: 0s - loss: 0.5657 - accuracy: 0.80 - ETA: 0s - loss: 0.5466 - accuracy: 0.81 - ETA: 0s - loss: 0.5510 - accuracy: 0.81 - ETA: 0s - loss: 0.5462 - accuracy: 0.81 - ETA: 0s - loss: 0.5462 - accuracy: 0.80 - ETA: 0s - loss: 0.5467 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5459 - accuracy: 0.8081 - val_loss: 0.6813 - val_accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.78 - ETA: 0s - loss: 0.4871 - accuracy: 0.82 - ETA: 0s - loss: 0.5089 - accuracy: 0.82 - ETA: 0s - loss: 0.5256 - accuracy: 0.80 - ETA: 0s - loss: 0.5368 - accuracy: 0.80 - ETA: 0s - loss: 0.5293 - accuracy: 0.80 - ETA: 0s - loss: 0.5321 - accuracy: 0.81 - ETA: 0s - loss: 0.5319 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5260 - accuracy: 0.8149 - val_loss: 0.6363 - val_accuracy: 0.7373\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.84 - ETA: 0s - loss: 0.5138 - accuracy: 0.83 - ETA: 0s - loss: 0.6523 - accuracy: 0.83 - ETA: 0s - loss: 0.6303 - accuracy: 0.82 - ETA: 0s - loss: 0.6078 - accuracy: 0.82 - ETA: 0s - loss: 0.5890 - accuracy: 0.82 - ETA: 0s - loss: 0.5731 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5601 - accuracy: 0.8350 - val_loss: 0.8867 - val_accuracy: 0.7433\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5549 - accuracy: 0.87 - ETA: 0s - loss: 0.6173 - accuracy: 0.81 - ETA: 0s - loss: 0.6628 - accuracy: 0.80 - ETA: 0s - loss: 0.6314 - accuracy: 0.79 - ETA: 0s - loss: 0.6032 - accuracy: 0.80 - ETA: 0s - loss: 0.5770 - accuracy: 0.81 - ETA: 0s - loss: 0.5655 - accuracy: 0.82 - ETA: 0s - loss: 0.5542 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5541 - accuracy: 0.8279 - val_loss: 0.8419 - val_accuracy: 0.7567\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.78 - ETA: 0s - loss: 0.5396 - accuracy: 0.81 - ETA: 0s - loss: 0.5178 - accuracy: 0.82 - ETA: 0s - loss: 0.5325 - accuracy: 0.82 - ETA: 0s - loss: 0.8469 - accuracy: 0.81 - ETA: 0s - loss: 0.8385 - accuracy: 0.79 - ETA: 0s - loss: 0.7986 - accuracy: 0.79 - 0s 5ms/step - loss: 0.7702 - accuracy: 0.7981 - val_loss: 0.8236 - val_accuracy: 0.7433\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.75 - ETA: 0s - loss: 0.6057 - accuracy: 0.76 - ETA: 0s - loss: 0.6493 - accuracy: 0.76 - ETA: 0s - loss: 0.6551 - accuracy: 0.75 - ETA: 0s - loss: 0.6453 - accuracy: 0.75 - ETA: 0s - loss: 0.6277 - accuracy: 0.76 - ETA: 0s - loss: 0.6214 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6279 - accuracy: 0.7563 - val_loss: 0.6316 - val_accuracy: 0.6955\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.62 - ETA: 0s - loss: 0.7046 - accuracy: 0.68 - ETA: 0s - loss: 0.6845 - accuracy: 0.70 - ETA: 0s - loss: 0.7026 - accuracy: 0.65 - ETA: 0s - loss: 0.6926 - accuracy: 0.60 - ETA: 0s - loss: 0.6913 - accuracy: 0.57 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - 0s 5ms/step - loss: 0.6875 - accuracy: 0.5487 - val_loss: 0.6680 - val_accuracy: 0.4552\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.53 - ETA: 0s - loss: 0.6714 - accuracy: 0.45 - ETA: 0s - loss: 0.6714 - accuracy: 0.44 - ETA: 0s - loss: 0.7121 - accuracy: 0.45 - ETA: 0s - loss: 0.7056 - accuracy: 0.44 - ETA: 0s - loss: 0.7006 - accuracy: 0.44 - ETA: 0s - loss: 0.6960 - accuracy: 0.45 - ETA: 0s - loss: 0.6955 - accuracy: 0.46 - 0s 5ms/step - loss: 0.6955 - accuracy: 0.4614 - val_loss: 0.6749 - val_accuracy: 0.4567\n",
      "Epoch 24/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.7221 - accuracy: 0.28 - ETA: 0s - loss: 0.6717 - accuracy: 0.44 - ETA: 0s - loss: 0.6655 - accuracy: 0.46 - ETA: 0s - loss: 0.6669 - accuracy: 0.45 - ETA: 0s - loss: 0.6754 - accuracy: 0.46 - ETA: 0s - loss: 0.6747 - accuracy: 0.46 - ETA: 0s - loss: 0.6747 - accuracy: 0.4666Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6764 - accuracy: 0.4647 - val_loss: 0.6939 - val_accuracy: 0.4030\n",
      "Epoch 00024: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.78 - ETA: 0s - loss: 3.7653 - accuracy: 0.58 - ETA: 0s - loss: 2.0852 - accuracy: 0.65 - ETA: 0s - loss: 1.5727 - accuracy: 0.67 - ETA: 0s - loss: 1.3058 - accuracy: 0.68 - ETA: 0s - loss: 1.1647 - accuracy: 0.69 - ETA: 0s - loss: 1.0637 - accuracy: 0.70 - ETA: 0s - loss: 0.9932 - accuracy: 0.71 - 1s 6ms/step - loss: 0.9880 - accuracy: 0.7130 - val_loss: 0.5682 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.5162 - accuracy: 0.79 - ETA: 0s - loss: 0.5592 - accuracy: 0.77 - ETA: 0s - loss: 0.5557 - accuracy: 0.77 - ETA: 0s - loss: 0.5443 - accuracy: 0.77 - ETA: 0s - loss: 0.5420 - accuracy: 0.76 - ETA: 0s - loss: 0.5283 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5298 - accuracy: 0.7719 - val_loss: 0.5979 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.84 - ETA: 0s - loss: 0.4224 - accuracy: 0.83 - ETA: 0s - loss: 0.4592 - accuracy: 0.81 - ETA: 0s - loss: 0.4562 - accuracy: 0.80 - ETA: 0s - loss: 0.4473 - accuracy: 0.80 - ETA: 0s - loss: 0.4457 - accuracy: 0.80 - ETA: 0s - loss: 0.4525 - accuracy: 0.80 - ETA: 0s - loss: 0.4578 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8048 - val_loss: 0.5808 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.87 - ETA: 0s - loss: 0.3841 - accuracy: 0.83 - ETA: 0s - loss: 0.4138 - accuracy: 0.81 - ETA: 0s - loss: 0.3950 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3977 - accuracy: 0.82 - ETA: 0s - loss: 0.4061 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4011 - accuracy: 0.8238 - val_loss: 0.8546 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.93 - ETA: 0s - loss: 0.2925 - accuracy: 0.83 - ETA: 0s - loss: 0.3181 - accuracy: 0.85 - ETA: 0s - loss: 0.4264 - accuracy: 0.83 - ETA: 0s - loss: 0.4457 - accuracy: 0.83 - ETA: 0s - loss: 0.4412 - accuracy: 0.83 - ETA: 0s - loss: 0.4478 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4507 - accuracy: 0.8242 - val_loss: 0.6357 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.87 - ETA: 0s - loss: 0.3790 - accuracy: 0.85 - ETA: 0s - loss: 0.3682 - accuracy: 0.85 - ETA: 0s - loss: 0.3749 - accuracy: 0.85 - ETA: 0s - loss: 0.3811 - accuracy: 0.84 - ETA: 0s - loss: 0.3801 - accuracy: 0.84 - ETA: 0s - loss: 0.3709 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3710 - accuracy: 0.8414 - val_loss: 0.8378 - val_accuracy: 0.6910\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.84 - ETA: 0s - loss: 0.3222 - accuracy: 0.85 - ETA: 0s - loss: 0.3773 - accuracy: 0.83 - ETA: 0s - loss: 0.3785 - accuracy: 0.83 - ETA: 0s - loss: 0.3937 - accuracy: 0.83 - ETA: 0s - loss: 0.3857 - accuracy: 0.83 - ETA: 0s - loss: 0.3753 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8481 - val_loss: 0.9206 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 1.00 - ETA: 0s - loss: 0.2553 - accuracy: 0.87 - ETA: 0s - loss: 0.3157 - accuracy: 0.86 - ETA: 0s - loss: 0.3313 - accuracy: 0.85 - ETA: 0s - loss: 0.3323 - accuracy: 0.86 - ETA: 0s - loss: 0.3384 - accuracy: 0.85 - ETA: 0s - loss: 0.3401 - accuracy: 0.85 - ETA: 0s - loss: 0.3402 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3417 - accuracy: 0.8570 - val_loss: 1.1968 - val_accuracy: 0.7104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.93 - ETA: 0s - loss: 0.3819 - accuracy: 0.86 - ETA: 0s - loss: 0.3775 - accuracy: 0.86 - ETA: 0s - loss: 0.4267 - accuracy: 0.86 - ETA: 0s - loss: 0.4218 - accuracy: 0.85 - ETA: 0s - loss: 0.4047 - accuracy: 0.86 - ETA: 0s - loss: 0.4054 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4025 - accuracy: 0.8537 - val_loss: 1.0128 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3677 - accuracy: 0.87 - ETA: 0s - loss: 0.4240 - accuracy: 0.82 - ETA: 0s - loss: 0.4414 - accuracy: 0.82 - ETA: 0s - loss: 0.4380 - accuracy: 0.84 - ETA: 0s - loss: 0.5222 - accuracy: 0.83 - ETA: 0s - loss: 0.4990 - accuracy: 0.83 - ETA: 0s - loss: 0.4793 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4736 - accuracy: 0.8264 - val_loss: 0.7023 - val_accuracy: 0.6627\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.87 - ETA: 0s - loss: 0.2782 - accuracy: 0.83 - ETA: 0s - loss: 0.3196 - accuracy: 0.84 - ETA: 0s - loss: 0.2997 - accuracy: 0.86 - ETA: 0s - loss: 0.2917 - accuracy: 0.86 - ETA: 0s - loss: 0.2956 - accuracy: 0.86 - ETA: 0s - loss: 0.2981 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3034 - accuracy: 0.8656 - val_loss: 0.6816 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.96 - ETA: 0s - loss: 0.2636 - accuracy: 0.89 - ETA: 0s - loss: 0.2826 - accuracy: 0.89 - ETA: 0s - loss: 0.2891 - accuracy: 0.89 - ETA: 0s - loss: 0.2861 - accuracy: 0.89 - ETA: 0s - loss: 0.3009 - accuracy: 0.88 - ETA: 0s - loss: 0.3028 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3009 - accuracy: 0.8854 - val_loss: 1.1570 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.84 - ETA: 0s - loss: 0.2980 - accuracy: 0.88 - ETA: 0s - loss: 0.2884 - accuracy: 0.88 - ETA: 0s - loss: 0.2824 - accuracy: 0.89 - ETA: 0s - loss: 0.2910 - accuracy: 0.88 - ETA: 0s - loss: 0.2951 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2978 - accuracy: 0.87 - 0s 5ms/step - loss: 0.2968 - accuracy: 0.8794 - val_loss: 0.7926 - val_accuracy: 0.7075\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.87 - ETA: 0s - loss: 0.2312 - accuracy: 0.87 - ETA: 0s - loss: 0.2101 - accuracy: 0.89 - ETA: 0s - loss: 0.2361 - accuracy: 0.88 - ETA: 0s - loss: 0.2424 - accuracy: 0.88 - ETA: 0s - loss: 0.2465 - accuracy: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.89 - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8858 - val_loss: 1.5093 - val_accuracy: 0.6821\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.93 - ETA: 0s - loss: 0.3075 - accuracy: 0.88 - ETA: 0s - loss: 0.2986 - accuracy: 0.86 - ETA: 0s - loss: 0.2686 - accuracy: 0.88 - ETA: 0s - loss: 0.2690 - accuracy: 0.88 - ETA: 0s - loss: 0.2573 - accuracy: 0.88 - ETA: 0s - loss: 0.2541 - accuracy: 0.89 - 0s 5ms/step - loss: 0.2624 - accuracy: 0.8899 - val_loss: 1.0334 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.84 - ETA: 0s - loss: 0.2190 - accuracy: 0.91 - ETA: 0s - loss: 0.2228 - accuracy: 0.92 - ETA: 0s - loss: 0.2698 - accuracy: 0.91 - ETA: 0s - loss: 0.2753 - accuracy: 0.91 - ETA: 0s - loss: 0.2825 - accuracy: 0.91 - ETA: 0s - loss: 0.2821 - accuracy: 0.91 - ETA: 0s - loss: 0.2852 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3290 - accuracy: 0.9097 - val_loss: 1.4636 - val_accuracy: 0.7090\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.78 - ETA: 0s - loss: 0.2931 - accuracy: 0.90 - ETA: 0s - loss: 0.3783 - accuracy: 0.89 - ETA: 0s - loss: 0.3442 - accuracy: 0.89 - ETA: 0s - loss: 0.3208 - accuracy: 0.89 - ETA: 0s - loss: 0.3220 - accuracy: 0.89 - ETA: 0s - loss: 0.3267 - accuracy: 0.88 - ETA: 0s - loss: 0.3290 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3346 - accuracy: 0.8880 - val_loss: 0.9554 - val_accuracy: 0.7224\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.87 - ETA: 0s - loss: 0.3262 - accuracy: 0.89 - ETA: 0s - loss: 0.3250 - accuracy: 0.89 - ETA: 0s - loss: 0.3233 - accuracy: 0.88 - ETA: 0s - loss: 0.3298 - accuracy: 0.89 - ETA: 0s - loss: 0.3302 - accuracy: 0.89 - ETA: 0s - loss: 0.3184 - accuracy: 0.88 - ETA: 0s - loss: 0.3141 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8876 - val_loss: 1.6023 - val_accuracy: 0.7224\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.90 - ETA: 0s - loss: 0.2127 - accuracy: 0.89 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2237 - accuracy: 0.90 - ETA: 0s - loss: 0.2393 - accuracy: 0.90 - ETA: 0s - loss: 0.2429 - accuracy: 0.90 - ETA: 0s - loss: 0.2398 - accuracy: 0.90 - ETA: 0s - loss: 0.2588 - accuracy: 0.90 - 0s 5ms/step - loss: 0.2579 - accuracy: 0.9033 - val_loss: 4.1763 - val_accuracy: 0.7254\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 1.00 - ETA: 0s - loss: 0.2831 - accuracy: 0.89 - ETA: 0s - loss: 0.3056 - accuracy: 0.88 - ETA: 0s - loss: 0.2984 - accuracy: 0.89 - ETA: 0s - loss: 0.2884 - accuracy: 0.89 - ETA: 0s - loss: 0.2897 - accuracy: 0.89 - ETA: 0s - loss: 0.3231 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8854 - val_loss: 1.7075 - val_accuracy: 0.6567\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.68 - ETA: 0s - loss: 0.4466 - accuracy: 0.80 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - ETA: 0s - loss: 0.4818 - accuracy: 0.81 - ETA: 0s - loss: 0.4614 - accuracy: 0.82 - ETA: 0s - loss: 0.4345 - accuracy: 0.82 - ETA: 0s - loss: 0.4207 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4053 - accuracy: 0.8410 - val_loss: 1.5539 - val_accuracy: 0.7358\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6349 - accuracy: 0.90 - ETA: 0s - loss: 0.3776 - accuracy: 0.85 - ETA: 0s - loss: 0.3531 - accuracy: 0.86 - ETA: 0s - loss: 0.3254 - accuracy: 0.88 - ETA: 0s - loss: 0.3260 - accuracy: 0.88 - ETA: 0s - loss: 0.3228 - accuracy: 0.87 - ETA: 0s - loss: 0.3251 - accuracy: 0.88 - ETA: 0s - loss: 0.3338 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8806 - val_loss: 4.1512 - val_accuracy: 0.7164\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.87 - ETA: 0s - loss: 0.3073 - accuracy: 0.89 - ETA: 0s - loss: 0.4082 - accuracy: 0.86 - ETA: 0s - loss: 0.3946 - accuracy: 0.84 - ETA: 0s - loss: 0.5105 - accuracy: 0.85 - ETA: 0s - loss: 0.5174 - accuracy: 0.84 - ETA: 0s - loss: 0.5895 - accuracy: 0.83 - ETA: 0s - loss: 0.6615 - accuracy: 0.83 - 0s 5ms/step - loss: 0.6827 - accuracy: 0.8324 - val_loss: 1.2093 - val_accuracy: 0.7060\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.84 - ETA: 0s - loss: 0.4158 - accuracy: 0.87 - ETA: 0s - loss: 0.4707 - accuracy: 0.85 - ETA: 0s - loss: 0.4686 - accuracy: 0.84 - ETA: 0s - loss: 0.4516 - accuracy: 0.84 - ETA: 0s - loss: 0.4541 - accuracy: 0.85 - ETA: 0s - loss: 0.4530 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4574 - accuracy: 0.8425 - val_loss: 1.7962 - val_accuracy: 0.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3768 - accuracy: 0.87 - ETA: 0s - loss: 0.4126 - accuracy: 0.83 - ETA: 0s - loss: 0.4337 - accuracy: 0.83 - ETA: 0s - loss: 0.4288 - accuracy: 0.84 - ETA: 0s - loss: 0.4192 - accuracy: 0.84 - ETA: 0s - loss: 0.4078 - accuracy: 0.85 - ETA: 0s - loss: 0.3997 - accuracy: 0.84 - ETA: 0s - loss: 0.3995 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3992 - accuracy: 0.8455 - val_loss: 2.2851 - val_accuracy: 0.7537\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.84 - ETA: 0s - loss: 0.2798 - accuracy: 0.89 - ETA: 0s - loss: 0.3065 - accuracy: 0.88 - ETA: 0s - loss: 0.3134 - accuracy: 0.87 - ETA: 0s - loss: 0.3714 - accuracy: 0.87 - ETA: 0s - loss: 0.6422 - accuracy: 0.86 - ETA: 0s - loss: 0.6635 - accuracy: 0.86 - ETA: 0s - loss: 0.6428 - accuracy: 0.85 - 0s 5ms/step - loss: 0.6427 - accuracy: 0.8548 - val_loss: 0.8250 - val_accuracy: 0.7343\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.81 - ETA: 0s - loss: 0.7455 - accuracy: 0.83 - ETA: 0s - loss: 0.6110 - accuracy: 0.84 - ETA: 0s - loss: 0.5568 - accuracy: 0.84 - ETA: 0s - loss: 0.5263 - accuracy: 0.84 - ETA: 0s - loss: 0.5073 - accuracy: 0.84 - ETA: 0s - loss: 0.4919 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4901 - accuracy: 0.8522 - val_loss: 0.6661 - val_accuracy: 0.7433\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.90 - ETA: 0s - loss: 0.3899 - accuracy: 0.89 - ETA: 0s - loss: 0.3967 - accuracy: 0.87 - ETA: 0s - loss: 0.4118 - accuracy: 0.87 - ETA: 0s - loss: 0.4107 - accuracy: 0.87 - ETA: 0s - loss: 0.4030 - accuracy: 0.86 - ETA: 0s - loss: 0.4054 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4025 - accuracy: 0.8712 - val_loss: 1.3316 - val_accuracy: 0.7030\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.96 - ETA: 0s - loss: 0.3967 - accuracy: 0.86 - ETA: 0s - loss: 0.3618 - accuracy: 0.89 - ETA: 0s - loss: 0.4121 - accuracy: 0.88 - ETA: 0s - loss: 0.4187 - accuracy: 0.87 - ETA: 0s - loss: 0.4303 - accuracy: 0.87 - ETA: 0s - loss: 0.4182 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8750 - val_loss: 0.6669 - val_accuracy: 0.7493\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.93 - ETA: 0s - loss: 0.3371 - accuracy: 0.91 - ETA: 0s - loss: 0.3266 - accuracy: 0.91 - ETA: 0s - loss: 0.3349 - accuracy: 0.91 - ETA: 0s - loss: 0.3458 - accuracy: 0.90 - ETA: 0s - loss: 0.3525 - accuracy: 0.91 - ETA: 0s - loss: 0.3499 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3419 - accuracy: 0.9104 - val_loss: 1.3755 - val_accuracy: 0.7164\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.90 - ETA: 0s - loss: 0.3448 - accuracy: 0.89 - ETA: 0s - loss: 0.3316 - accuracy: 0.90 - ETA: 0s - loss: 0.3441 - accuracy: 0.89 - ETA: 0s - loss: 0.3235 - accuracy: 0.90 - ETA: 0s - loss: 0.3216 - accuracy: 0.90 - ETA: 0s - loss: 0.3187 - accuracy: 0.90 - ETA: 0s - loss: 0.3150 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3201 - accuracy: 0.9082 - val_loss: 1.1587 - val_accuracy: 0.7433\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.90 - ETA: 0s - loss: 0.4604 - accuracy: 0.88 - ETA: 0s - loss: 0.4167 - accuracy: 0.89 - ETA: 0s - loss: 0.3993 - accuracy: 0.89 - ETA: 0s - loss: 0.3793 - accuracy: 0.90 - ETA: 0s - loss: 0.4062 - accuracy: 0.89 - ETA: 0s - loss: 0.3936 - accuracy: 0.89 - ETA: 0s - loss: 0.4139 - accuracy: 0.89 - 0s 5ms/step - loss: 0.4103 - accuracy: 0.8973 - val_loss: 1.1394 - val_accuracy: 0.7403\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.93 - ETA: 0s - loss: 0.3734 - accuracy: 0.87 - ETA: 0s - loss: 0.3482 - accuracy: 0.88 - ETA: 0s - loss: 0.3277 - accuracy: 0.90 - ETA: 0s - loss: 0.3174 - accuracy: 0.90 - ETA: 0s - loss: 0.3181 - accuracy: 0.90 - ETA: 0s - loss: 0.3106 - accuracy: 0.91 - 0s 5ms/step - loss: 0.3116 - accuracy: 0.9112 - val_loss: 0.9693 - val_accuracy: 0.7343\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.96 - ETA: 0s - loss: 0.2626 - accuracy: 0.93 - ETA: 0s - loss: 0.2620 - accuracy: 0.93 - ETA: 0s - loss: 0.2681 - accuracy: 0.93 - ETA: 0s - loss: 0.2727 - accuracy: 0.93 - ETA: 0s - loss: 0.2748 - accuracy: 0.92 - ETA: 0s - loss: 0.2755 - accuracy: 0.92 - ETA: 0s - loss: 0.3047 - accuracy: 0.92 - 0s 5ms/step - loss: 0.3066 - accuracy: 0.9231 - val_loss: 2.9742 - val_accuracy: 0.7209\n",
      "Epoch 35/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.2776 - accuracy: 0.93 - ETA: 0s - loss: 0.2259 - accuracy: 0.94 - ETA: 0s - loss: 0.2540 - accuracy: 0.94 - ETA: 0s - loss: 0.2714 - accuracy: 0.94 - ETA: 0s - loss: 0.2700 - accuracy: 0.94 - ETA: 0s - loss: 0.2688 - accuracy: 0.93 - ETA: 0s - loss: 0.2708 - accuracy: 0.9375Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2713 - accuracy: 0.9358 - val_loss: 1.5761 - val_accuracy: 0.7299\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 74acb6129b0a8e32365d6d94e5b8f5d1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7507462700208029</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.1431762603410921</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 185</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3336 - accuracy: 0.53 - ETA: 0s - loss: 4.3437 - accuracy: 0.52 - ETA: 0s - loss: 3.1348 - accuracy: 0.54 - ETA: 0s - loss: 2.7207 - accuracy: 0.56 - ETA: 0s - loss: 2.2905 - accuracy: 0.59 - ETA: 0s - loss: 2.0057 - accuracy: 0.60 - 0s 5ms/step - loss: 1.9388 - accuracy: 0.6043 - val_loss: 0.6014 - val_accuracy: 0.7418\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9867 - accuracy: 0.53 - ETA: 0s - loss: 0.8251 - accuracy: 0.65 - ETA: 0s - loss: 0.8397 - accuracy: 0.65 - ETA: 0s - loss: 0.8051 - accuracy: 0.66 - ETA: 0s - loss: 0.7613 - accuracy: 0.67 - ETA: 0s - loss: 0.7562 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7578 - accuracy: 0.6756 - val_loss: 0.6045 - val_accuracy: 0.7313\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.65 - ETA: 0s - loss: 0.6066 - accuracy: 0.71 - ETA: 0s - loss: 0.6367 - accuracy: 0.71 - ETA: 0s - loss: 0.6596 - accuracy: 0.71 - ETA: 0s - loss: 0.6539 - accuracy: 0.71 - ETA: 0s - loss: 0.6615 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6637 - accuracy: 0.7055 - val_loss: 0.6203 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5407 - accuracy: 0.78 - ETA: 0s - loss: 0.6287 - accuracy: 0.72 - ETA: 0s - loss: 0.6113 - accuracy: 0.73 - ETA: 0s - loss: 0.6193 - accuracy: 0.74 - ETA: 0s - loss: 0.6159 - accuracy: 0.73 - ETA: 0s - loss: 0.6386 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6480 - accuracy: 0.7256 - val_loss: 0.6063 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5865 - accuracy: 0.65 - ETA: 0s - loss: 0.6150 - accuracy: 0.68 - ETA: 0s - loss: 0.6528 - accuracy: 0.70 - ETA: 0s - loss: 0.6756 - accuracy: 0.73 - ETA: 0s - loss: 0.7050 - accuracy: 0.72 - ETA: 0s - loss: 0.6878 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7052 - accuracy: 0.7350 - val_loss: 0.6157 - val_accuracy: 0.6985\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.71 - ETA: 0s - loss: 1.2637 - accuracy: 0.70 - ETA: 0s - loss: 1.0722 - accuracy: 0.70 - ETA: 0s - loss: 1.0138 - accuracy: 0.69 - ETA: 0s - loss: 1.0398 - accuracy: 0.69 - ETA: 0s - loss: 0.9776 - accuracy: 0.70 - 0s 4ms/step - loss: 0.9833 - accuracy: 0.7003 - val_loss: 0.6715 - val_accuracy: 0.7075\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.71 - ETA: 0s - loss: 0.6818 - accuracy: 0.72 - ETA: 0s - loss: 0.8303 - accuracy: 0.69 - ETA: 0s - loss: 0.7959 - accuracy: 0.71 - ETA: 0s - loss: 0.7581 - accuracy: 0.70 - ETA: 0s - loss: 0.7519 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7362 - accuracy: 0.7096 - val_loss: 0.6330 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.90 - ETA: 0s - loss: 0.6429 - accuracy: 0.77 - ETA: 0s - loss: 0.5967 - accuracy: 0.74 - ETA: 0s - loss: 0.5913 - accuracy: 0.76 - ETA: 0s - loss: 0.7043 - accuracy: 0.76 - ETA: 0s - loss: 0.7477 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7497 - accuracy: 0.7469 - val_loss: 0.7428 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.78 - ETA: 0s - loss: 0.8601 - accuracy: 0.74 - ETA: 0s - loss: 0.7472 - accuracy: 0.76 - ETA: 0s - loss: 0.6934 - accuracy: 0.77 - ETA: 0s - loss: 0.6801 - accuracy: 0.77 - ETA: 0s - loss: 0.6766 - accuracy: 0.77 - ETA: 0s - loss: 0.6735 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6828 - accuracy: 0.7656 - val_loss: 0.6660 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.81 - ETA: 0s - loss: 0.8765 - accuracy: 0.75 - ETA: 0s - loss: 0.7423 - accuracy: 0.74 - ETA: 0s - loss: 0.6843 - accuracy: 0.75 - ETA: 0s - loss: 0.7932 - accuracy: 0.76 - ETA: 0s - loss: 0.7642 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7475 - accuracy: 0.7559 - val_loss: 0.6230 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.68 - ETA: 0s - loss: 0.5515 - accuracy: 0.77 - ETA: 0s - loss: 0.5351 - accuracy: 0.78 - ETA: 0s - loss: 0.5388 - accuracy: 0.79 - ETA: 0s - loss: 0.5401 - accuracy: 0.79 - ETA: 0s - loss: 0.5468 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5634 - accuracy: 0.7969 - val_loss: 0.6826 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.90 - ETA: 0s - loss: 0.5066 - accuracy: 0.80 - ETA: 0s - loss: 0.5102 - accuracy: 0.79 - ETA: 0s - loss: 0.5085 - accuracy: 0.80 - ETA: 0s - loss: 0.5114 - accuracy: 0.80 - ETA: 0s - loss: 0.5372 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5392 - accuracy: 0.8055 - val_loss: 0.6507 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3866 - accuracy: 0.87 - ETA: 0s - loss: 0.4287 - accuracy: 0.85 - ETA: 0s - loss: 0.4482 - accuracy: 0.84 - ETA: 0s - loss: 0.4676 - accuracy: 0.83 - ETA: 0s - loss: 0.4774 - accuracy: 0.82 - ETA: 0s - loss: 0.4858 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4916 - accuracy: 0.8223 - val_loss: 0.6911 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.83 - ETA: 0s - loss: 0.8011 - accuracy: 0.81 - ETA: 0s - loss: 0.7259 - accuracy: 0.81 - ETA: 0s - loss: 0.6732 - accuracy: 0.81 - ETA: 0s - loss: 0.6513 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6384 - accuracy: 0.8137 - val_loss: 0.6687 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.78 - ETA: 0s - loss: 0.6806 - accuracy: 0.77 - ETA: 0s - loss: 0.6813 - accuracy: 0.79 - ETA: 0s - loss: 0.6930 - accuracy: 0.79 - ETA: 0s - loss: 0.8852 - accuracy: 0.77 - ETA: 0s - loss: 1.2718 - accuracy: 0.77 - 0s 4ms/step - loss: 1.3591 - accuracy: 0.7749 - val_loss: 0.7468 - val_accuracy: 0.7358\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.1641 - accuracy: 0.84 - ETA: 0s - loss: 1.3950 - accuracy: 0.79 - ETA: 0s - loss: 2.9295 - accuracy: 0.76 - ETA: 0s - loss: 2.2902 - accuracy: 0.75 - ETA: 0s - loss: 1.9727 - accuracy: 0.74 - ETA: 0s - loss: 1.7117 - accuracy: 0.75 - 0s 4ms/step - loss: 1.6124 - accuracy: 0.7507 - val_loss: 0.6354 - val_accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5801 - accuracy: 0.81 - ETA: 0s - loss: 0.8953 - accuracy: 0.74 - ETA: 0s - loss: 1.2719 - accuracy: 0.74 - ETA: 0s - loss: 1.3110 - accuracy: 0.75 - ETA: 0s - loss: 1.2181 - accuracy: 0.75 - ETA: 0s - loss: 1.1320 - accuracy: 0.74 - 0s 4ms/step - loss: 1.0904 - accuracy: 0.7436 - val_loss: 0.6635 - val_accuracy: 0.7433\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.81 - ETA: 0s - loss: 0.6224 - accuracy: 0.77 - ETA: 0s - loss: 0.6212 - accuracy: 0.77 - ETA: 0s - loss: 0.6433 - accuracy: 0.77 - ETA: 0s - loss: 0.6456 - accuracy: 0.77 - ETA: 0s - loss: 0.7234 - accuracy: 0.77 - ETA: 0s - loss: 0.7914 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7895 - accuracy: 0.7772 - val_loss: 0.6844 - val_accuracy: 0.7403\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.75 - ETA: 0s - loss: 1.7428 - accuracy: 0.74 - ETA: 0s - loss: 1.5329 - accuracy: 0.77 - ETA: 0s - loss: 1.2831 - accuracy: 0.77 - ETA: 0s - loss: 1.1239 - accuracy: 0.77 - ETA: 0s - loss: 1.0400 - accuracy: 0.77 - ETA: 0s - loss: 0.9709 - accuracy: 0.78 - 0s 4ms/step - loss: 0.9709 - accuracy: 0.7813 - val_loss: 0.7061 - val_accuracy: 0.7328\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.53 - ETA: 0s - loss: 0.6731 - accuracy: 0.77 - ETA: 0s - loss: 0.9191 - accuracy: 0.77 - ETA: 0s - loss: 0.9024 - accuracy: 0.77 - ETA: 0s - loss: 0.8858 - accuracy: 0.77 - ETA: 0s - loss: 0.8897 - accuracy: 0.77 - ETA: 0s - loss: 0.8688 - accuracy: 0.77 - 0s 5ms/step - loss: 0.8626 - accuracy: 0.7712 - val_loss: 0.6066 - val_accuracy: 0.7493\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.75 - ETA: 0s - loss: 0.7732 - accuracy: 0.75 - ETA: 0s - loss: 0.7598 - accuracy: 0.75 - ETA: 0s - loss: 0.7328 - accuracy: 0.75 - ETA: 0s - loss: 0.7045 - accuracy: 0.77 - ETA: 0s - loss: 0.7327 - accuracy: 0.77 - ETA: 0s - loss: 0.8616 - accuracy: 0.77 - 0s 4ms/step - loss: 0.9338 - accuracy: 0.7753 - val_loss: 0.6290 - val_accuracy: 0.7388\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.81 - ETA: 0s - loss: 9.6871 - accuracy: 0.74 - ETA: 0s - loss: 5.4514 - accuracy: 0.75 - ETA: 0s - loss: 4.2834 - accuracy: 0.76 - ETA: 0s - loss: 3.3174 - accuracy: 0.77 - ETA: 0s - loss: 2.7822 - accuracy: 0.77 - ETA: 0s - loss: 2.7106 - accuracy: 0.76 - 0s 5ms/step - loss: 2.6436 - accuracy: 0.7704 - val_loss: 0.6147 - val_accuracy: 0.7552\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5275 - accuracy: 0.84 - ETA: 0s - loss: 1.4223 - accuracy: 0.78 - ETA: 0s - loss: 1.0874 - accuracy: 0.78 - ETA: 0s - loss: 1.0488 - accuracy: 0.78 - ETA: 0s - loss: 1.0249 - accuracy: 0.77 - ETA: 0s - loss: 0.9857 - accuracy: 0.77 - ETA: 0s - loss: 0.9386 - accuracy: 0.77 - 0s 4ms/step - loss: 0.9481 - accuracy: 0.7775 - val_loss: 0.6750 - val_accuracy: 0.7299\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.78 - ETA: 0s - loss: 0.8399 - accuracy: 0.77 - ETA: 0s - loss: 0.7921 - accuracy: 0.77 - ETA: 0s - loss: 0.7435 - accuracy: 0.76 - ETA: 0s - loss: 0.7268 - accuracy: 0.76 - ETA: 0s - loss: 0.7157 - accuracy: 0.75 - ETA: 0s - loss: 0.7043 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7056 - accuracy: 0.7648 - val_loss: 0.6332 - val_accuracy: 0.7433\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.4499 - accuracy: 0.75 - ETA: 0s - loss: 0.8170 - accuracy: 0.75 - ETA: 0s - loss: 0.7686 - accuracy: 0.76 - ETA: 0s - loss: 0.7208 - accuracy: 0.76 - ETA: 0s - loss: 0.6881 - accuracy: 0.77 - ETA: 0s - loss: 0.6871 - accuracy: 0.76 - ETA: 0s - loss: 0.7122 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7108 - accuracy: 0.7727 - val_loss: 0.6828 - val_accuracy: 0.7418\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.84 - ETA: 0s - loss: 0.7636 - accuracy: 0.74 - ETA: 0s - loss: 0.7099 - accuracy: 0.75 - ETA: 0s - loss: 0.6752 - accuracy: 0.76 - ETA: 0s - loss: 0.6924 - accuracy: 0.77 - ETA: 0s - loss: 0.6794 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6726 - accuracy: 0.7723 - val_loss: 0.6494 - val_accuracy: 0.7433\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.78 - ETA: 0s - loss: 0.5903 - accuracy: 0.79 - ETA: 0s - loss: 0.5804 - accuracy: 0.79 - ETA: 0s - loss: 0.5920 - accuracy: 0.79 - ETA: 0s - loss: 0.5885 - accuracy: 0.78 - ETA: 0s - loss: 0.6020 - accuracy: 0.78 - ETA: 0s - loss: 0.6019 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6034 - accuracy: 0.7828 - val_loss: 0.6259 - val_accuracy: 0.7478\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.87 - ETA: 0s - loss: 0.5867 - accuracy: 0.78 - ETA: 0s - loss: 0.5864 - accuracy: 0.78 - ETA: 0s - loss: 0.5807 - accuracy: 0.79 - ETA: 0s - loss: 0.5950 - accuracy: 0.78 - ETA: 0s - loss: 0.5971 - accuracy: 0.78 - ETA: 0s - loss: 0.5973 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5987 - accuracy: 0.7854 - val_loss: 0.8341 - val_accuracy: 0.7418\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.87 - ETA: 0s - loss: 0.6304 - accuracy: 0.77 - ETA: 0s - loss: 0.6053 - accuracy: 0.78 - ETA: 0s - loss: 0.5983 - accuracy: 0.78 - ETA: 0s - loss: 0.6001 - accuracy: 0.78 - ETA: 0s - loss: 0.5933 - accuracy: 0.78 - ETA: 0s - loss: 0.5941 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5941 - accuracy: 0.7880 - val_loss: 0.6449 - val_accuracy: 0.7537\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.84 - ETA: 0s - loss: 0.5586 - accuracy: 0.80 - ETA: 0s - loss: 0.5540 - accuracy: 0.80 - ETA: 0s - loss: 0.5875 - accuracy: 0.80 - ETA: 0s - loss: 0.5856 - accuracy: 0.80 - ETA: 0s - loss: 0.6008 - accuracy: 0.79 - ETA: 0s - loss: 0.5974 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5964 - accuracy: 0.7984 - val_loss: 0.7410 - val_accuracy: 0.7567\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6687 - accuracy: 0.68 - ETA: 0s - loss: 0.7603 - accuracy: 0.77 - ETA: 0s - loss: 0.6824 - accuracy: 0.77 - ETA: 0s - loss: 0.6329 - accuracy: 0.79 - ETA: 0s - loss: 0.6229 - accuracy: 0.79 - ETA: 0s - loss: 0.6167 - accuracy: 0.79 - ETA: 0s - loss: 0.6028 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6039 - accuracy: 0.7951 - val_loss: 0.6911 - val_accuracy: 0.7552\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.84 - ETA: 0s - loss: 0.5604 - accuracy: 0.80 - ETA: 0s - loss: 0.5862 - accuracy: 0.80 - ETA: 0s - loss: 0.5926 - accuracy: 0.80 - ETA: 0s - loss: 0.5777 - accuracy: 0.81 - ETA: 0s - loss: 0.6220 - accuracy: 0.80 - ETA: 0s - loss: 0.6414 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6414 - accuracy: 0.8003 - val_loss: 0.7196 - val_accuracy: 0.7478\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.81 - ETA: 0s - loss: 1.0387 - accuracy: 0.78 - ETA: 0s - loss: 0.8446 - accuracy: 0.78 - ETA: 0s - loss: 1.4656 - accuracy: 0.80 - ETA: 0s - loss: 1.2541 - accuracy: 0.79 - ETA: 0s - loss: 1.1227 - accuracy: 0.79 - 0s 4ms/step - loss: 1.0923 - accuracy: 0.7973 - val_loss: 1.0165 - val_accuracy: 0.7463\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5571 - accuracy: 0.78 - ETA: 0s - loss: 0.5643 - accuracy: 0.81 - ETA: 0s - loss: 0.6012 - accuracy: 0.80 - ETA: 0s - loss: 0.6081 - accuracy: 0.80 - ETA: 0s - loss: 0.6413 - accuracy: 0.80 - ETA: 0s - loss: 0.6638 - accuracy: 0.80 - ETA: 0s - loss: 0.6785 - accuracy: 0.79 - ETA: 0s - loss: 0.6712 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6706 - accuracy: 0.7925 - val_loss: 0.6201 - val_accuracy: 0.7328\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6062 - accuracy: 0.78 - ETA: 0s - loss: 0.6707 - accuracy: 0.79 - ETA: 0s - loss: 0.7269 - accuracy: 0.79 - ETA: 0s - loss: 0.7099 - accuracy: 0.77 - ETA: 0s - loss: 0.7373 - accuracy: 0.78 - ETA: 0s - loss: 0.7234 - accuracy: 0.78 - ETA: 0s - loss: 0.7072 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6994 - accuracy: 0.7828 - val_loss: 0.6722 - val_accuracy: 0.7612\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7200 - accuracy: 0.65 - ETA: 0s - loss: 0.6225 - accuracy: 0.77 - ETA: 0s - loss: 0.6090 - accuracy: 0.77 - ETA: 0s - loss: 0.6360 - accuracy: 0.78 - ETA: 0s - loss: 0.6565 - accuracy: 0.78 - ETA: 0s - loss: 0.6442 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6710 - accuracy: 0.7887 - val_loss: 0.6630 - val_accuracy: 0.7612\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.84 - ETA: 0s - loss: 0.6270 - accuracy: 0.74 - ETA: 0s - loss: 0.5956 - accuracy: 0.77 - ETA: 0s - loss: 0.5928 - accuracy: 0.78 - ETA: 0s - loss: 0.5991 - accuracy: 0.78 - ETA: 0s - loss: 0.6030 - accuracy: 0.78 - ETA: 0s - loss: 0.5938 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5939 - accuracy: 0.7895 - val_loss: 0.7115 - val_accuracy: 0.7597\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.65 - ETA: 0s - loss: 0.5723 - accuracy: 0.79 - ETA: 0s - loss: 1.3168 - accuracy: 0.78 - ETA: 0s - loss: 1.0964 - accuracy: 0.79 - ETA: 0s - loss: 0.9635 - accuracy: 0.79 - ETA: 0s - loss: 0.9092 - accuracy: 0.79 - 0s 4ms/step - loss: 0.9155 - accuracy: 0.7928 - val_loss: 0.9076 - val_accuracy: 0.7463\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1786 - accuracy: 0.84 - ETA: 0s - loss: 0.8930 - accuracy: 0.77 - ETA: 0s - loss: 1.6824 - accuracy: 0.77 - ETA: 0s - loss: 1.6643 - accuracy: 0.75 - ETA: 0s - loss: 1.4184 - accuracy: 0.75 - ETA: 0s - loss: 1.3301 - accuracy: 0.74 - 0s 4ms/step - loss: 1.2439 - accuracy: 0.7536 - val_loss: 0.6487 - val_accuracy: 0.7343\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.75 - ETA: 0s - loss: 0.9139 - accuracy: 0.77 - ETA: 0s - loss: 0.8156 - accuracy: 0.77 - ETA: 0s - loss: 0.7901 - accuracy: 0.75 - ETA: 0s - loss: 0.7759 - accuracy: 0.76 - ETA: 0s - loss: 0.7677 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7746 - accuracy: 0.7510 - val_loss: 0.6541 - val_accuracy: 0.7045\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1352 - accuracy: 0.81 - ETA: 0s - loss: 0.7312 - accuracy: 0.69 - ETA: 0s - loss: 0.7022 - accuracy: 0.71 - ETA: 0s - loss: 0.7603 - accuracy: 0.72 - ETA: 0s - loss: 0.7574 - accuracy: 0.73 - ETA: 0s - loss: 0.8940 - accuracy: 0.74 - 0s 4ms/step - loss: 0.9010 - accuracy: 0.7533 - val_loss: 0.6931 - val_accuracy: 0.7433\n",
      "Epoch 42/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.65 - ETA: 0s - loss: 0.6386 - accuracy: 0.73 - ETA: 0s - loss: 0.6824 - accuracy: 0.73 - ETA: 0s - loss: 0.6726 - accuracy: 0.74 - ETA: 0s - loss: 0.6844 - accuracy: 0.75 - ETA: 0s - loss: 0.6827 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6915 - accuracy: 0.7574 - val_loss: 1.0345 - val_accuracy: 0.7418\n",
      "Epoch 43/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2187 - accuracy: 0.75 - ETA: 0s - loss: 0.9766 - accuracy: 0.73 - ETA: 0s - loss: 0.8197 - accuracy: 0.75 - ETA: 0s - loss: 0.7598 - accuracy: 0.77 - ETA: 0s - loss: 0.7354 - accuracy: 0.77 - ETA: 0s - loss: 0.7154 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7080 - accuracy: 0.7772 - val_loss: 0.7039 - val_accuracy: 0.7403\n",
      "Epoch 44/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.84 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.5940 - accuracy: 0.80 - ETA: 0s - loss: 0.6669 - accuracy: 0.79 - ETA: 0s - loss: 0.6604 - accuracy: 0.79 - ETA: 0s - loss: 0.6693 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6947 - accuracy: 0.7857 - val_loss: 0.6843 - val_accuracy: 0.7224\n",
      "Epoch 45/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.5113 - accuracy: 0.84 - ETA: 0s - loss: 0.9061 - accuracy: 0.76 - ETA: 0s - loss: 0.8127 - accuracy: 0.76 - ETA: 0s - loss: 0.7635 - accuracy: 0.76 - ETA: 0s - loss: 0.7727 - accuracy: 0.77 - ETA: 0s - loss: 0.7434 - accuracy: 0.7721Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7451 - accuracy: 0.7697 - val_loss: 0.6698 - val_accuracy: 0.7269\n",
      "Epoch 00045: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.59 - ETA: 0s - loss: 4.5166 - accuracy: 0.61 - ETA: 0s - loss: 3.0169 - accuracy: 0.62 - ETA: 0s - loss: 2.4501 - accuracy: 0.60 - ETA: 0s - loss: 2.1387 - accuracy: 0.60 - ETA: 0s - loss: 1.8961 - accuracy: 0.61 - 0s 5ms/step - loss: 1.7707 - accuracy: 0.6252 - val_loss: 0.5983 - val_accuracy: 0.7284\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1645 - accuracy: 0.65 - ETA: 0s - loss: 0.9594 - accuracy: 0.67 - ETA: 0s - loss: 0.9276 - accuracy: 0.68 - ETA: 0s - loss: 0.8967 - accuracy: 0.68 - ETA: 0s - loss: 0.8670 - accuracy: 0.69 - ETA: 0s - loss: 0.8337 - accuracy: 0.69 - 0s 4ms/step - loss: 0.8304 - accuracy: 0.6943 - val_loss: 0.6529 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7628 - accuracy: 0.71 - ETA: 0s - loss: 0.6592 - accuracy: 0.69 - ETA: 0s - loss: 0.6858 - accuracy: 0.70 - ETA: 0s - loss: 0.7299 - accuracy: 0.69 - ETA: 0s - loss: 0.7193 - accuracy: 0.69 - ETA: 0s - loss: 0.7255 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7261 - accuracy: 0.6962 - val_loss: 0.6532 - val_accuracy: 0.7284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4692 - accuracy: 0.78 - ETA: 0s - loss: 0.6171 - accuracy: 0.75 - ETA: 0s - loss: 0.7389 - accuracy: 0.74 - ETA: 0s - loss: 0.7492 - accuracy: 0.73 - ETA: 0s - loss: 0.7207 - accuracy: 0.73 - ETA: 0s - loss: 0.7392 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7417 - accuracy: 0.7346 - val_loss: 0.6234 - val_accuracy: 0.7149\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.8114 - accuracy: 0.69 - ETA: 0s - loss: 0.7890 - accuracy: 0.71 - ETA: 0s - loss: 0.7856 - accuracy: 0.72 - ETA: 0s - loss: 0.7762 - accuracy: 0.72 - ETA: 0s - loss: 0.7689 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7706 - accuracy: 0.7335 - val_loss: 0.6144 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.75 - ETA: 0s - loss: 0.6824 - accuracy: 0.74 - ETA: 0s - loss: 0.6799 - accuracy: 0.75 - ETA: 0s - loss: 0.6912 - accuracy: 0.74 - ETA: 0s - loss: 0.7077 - accuracy: 0.74 - ETA: 0s - loss: 0.6894 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6902 - accuracy: 0.7529 - val_loss: 0.6153 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.84 - ETA: 0s - loss: 0.7047 - accuracy: 0.79 - ETA: 0s - loss: 0.8663 - accuracy: 0.77 - ETA: 0s - loss: 0.7708 - accuracy: 0.78 - ETA: 0s - loss: 0.7474 - accuracy: 0.77 - 0s 3ms/step - loss: 0.7381 - accuracy: 0.7734 - val_loss: 0.6463 - val_accuracy: 0.7224\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.87 - ETA: 0s - loss: 0.6421 - accuracy: 0.79 - ETA: 0s - loss: 0.6495 - accuracy: 0.78 - ETA: 0s - loss: 0.6544 - accuracy: 0.77 - ETA: 0s - loss: 0.6464 - accuracy: 0.77 - ETA: 0s - loss: 0.6675 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6719 - accuracy: 0.7786 - val_loss: 0.6261 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.75 - ETA: 0s - loss: 0.5864 - accuracy: 0.80 - ETA: 0s - loss: 0.5833 - accuracy: 0.80 - ETA: 0s - loss: 0.5962 - accuracy: 0.80 - ETA: 0s - loss: 0.6014 - accuracy: 0.79 - ETA: 0s - loss: 0.6083 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6081 - accuracy: 0.7940 - val_loss: 0.6186 - val_accuracy: 0.7522\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.87 - ETA: 0s - loss: 0.6213 - accuracy: 0.81 - ETA: 0s - loss: 0.6603 - accuracy: 0.78 - ETA: 0s - loss: 0.6833 - accuracy: 0.78 - ETA: 0s - loss: 0.6606 - accuracy: 0.79 - ETA: 0s - loss: 0.6586 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6537 - accuracy: 0.7898 - val_loss: 0.6268 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.81 - ETA: 0s - loss: 0.6894 - accuracy: 0.76 - ETA: 0s - loss: 0.6186 - accuracy: 0.79 - ETA: 0s - loss: 0.7463 - accuracy: 0.79 - ETA: 0s - loss: 0.7415 - accuracy: 0.78 - ETA: 0s - loss: 0.7289 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7170 - accuracy: 0.7913 - val_loss: 0.8720 - val_accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.90 - ETA: 0s - loss: 0.8917 - accuracy: 0.83 - ETA: 0s - loss: 0.7611 - accuracy: 0.80 - ETA: 0s - loss: 0.7336 - accuracy: 0.80 - ETA: 0s - loss: 0.7022 - accuracy: 0.80 - ETA: 0s - loss: 0.6904 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6881 - accuracy: 0.7984 - val_loss: 0.6552 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8891 - accuracy: 0.59 - ETA: 0s - loss: 0.6571 - accuracy: 0.76 - ETA: 0s - loss: 0.6503 - accuracy: 0.77 - ETA: 0s - loss: 0.6414 - accuracy: 0.77 - ETA: 0s - loss: 0.6365 - accuracy: 0.77 - ETA: 0s - loss: 0.6234 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6196 - accuracy: 0.7842 - val_loss: 0.7498 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.90 - ETA: 0s - loss: 0.6146 - accuracy: 0.82 - ETA: 0s - loss: 0.6305 - accuracy: 0.80 - ETA: 0s - loss: 0.6999 - accuracy: 0.80 - ETA: 0s - loss: 0.6892 - accuracy: 0.80 - ETA: 0s - loss: 0.6952 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6876 - accuracy: 0.8022 - val_loss: 0.8183 - val_accuracy: 0.7478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.84 - ETA: 0s - loss: 0.9177 - accuracy: 0.75 - ETA: 0s - loss: 1.0037 - accuracy: 0.74 - ETA: 0s - loss: 0.9869 - accuracy: 0.74 - ETA: 0s - loss: 0.9854 - accuracy: 0.75 - ETA: 0s - loss: 1.1152 - accuracy: 0.75 - 0s 4ms/step - loss: 1.1779 - accuracy: 0.7581 - val_loss: 0.7944 - val_accuracy: 0.7358\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.75 - ETA: 0s - loss: 2.0152 - accuracy: 0.76 - ETA: 0s - loss: 1.5033 - accuracy: 0.73 - ETA: 0s - loss: 1.3668 - accuracy: 0.74 - ETA: 0s - loss: 1.3309 - accuracy: 0.73 - ETA: 0s - loss: 1.2848 - accuracy: 0.74 - 0s 4ms/step - loss: 1.2925 - accuracy: 0.7361 - val_loss: 0.6392 - val_accuracy: 0.7328\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.75 - ETA: 0s - loss: 1.0371 - accuracy: 0.71 - ETA: 0s - loss: 0.9883 - accuracy: 0.70 - ETA: 0s - loss: 1.1830 - accuracy: 0.72 - ETA: 0s - loss: 1.3111 - accuracy: 0.72 - ETA: 0s - loss: 1.2038 - accuracy: 0.73 - 0s 4ms/step - loss: 1.2018 - accuracy: 0.7324 - val_loss: 0.6593 - val_accuracy: 0.7254\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0493 - accuracy: 0.68 - ETA: 0s - loss: 0.7881 - accuracy: 0.71 - ETA: 0s - loss: 0.8311 - accuracy: 0.74 - ETA: 0s - loss: 1.2470 - accuracy: 0.74 - ETA: 0s - loss: 1.3231 - accuracy: 0.73 - ETA: 0s - loss: 1.2309 - accuracy: 0.73 - 0s 4ms/step - loss: 1.2090 - accuracy: 0.7417 - val_loss: 0.6627 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 1.5349 - accuracy: 0.75 - ETA: 0s - loss: 2.6470 - accuracy: 0.73 - ETA: 0s - loss: 1.9067 - accuracy: 0.70 - ETA: 0s - loss: 1.6019 - accuracy: 0.70 - ETA: 0s - loss: 1.4256 - accuracy: 0.69 - ETA: 0s - loss: 1.2781 - accuracy: 0.7054Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2206 - accuracy: 0.7029 - val_loss: 0.6878 - val_accuracy: 0.6955\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7401 - accuracy: 0.59 - ETA: 0s - loss: 5.0095 - accuracy: 0.62 - ETA: 0s - loss: 3.7208 - accuracy: 0.63 - ETA: 0s - loss: 3.1163 - accuracy: 0.63 - ETA: 0s - loss: 2.6999 - accuracy: 0.62 - ETA: 0s - loss: 2.3226 - accuracy: 0.62 - 0s 5ms/step - loss: 2.1251 - accuracy: 0.6346 - val_loss: 0.6058 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.62 - ETA: 0s - loss: 0.8460 - accuracy: 0.65 - ETA: 0s - loss: 0.7660 - accuracy: 0.67 - ETA: 0s - loss: 0.7539 - accuracy: 0.68 - ETA: 0s - loss: 0.7525 - accuracy: 0.67 - ETA: 0s - loss: 0.7573 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7601 - accuracy: 0.6797 - val_loss: 0.6385 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6272 - accuracy: 0.65 - ETA: 0s - loss: 0.7345 - accuracy: 0.67 - ETA: 0s - loss: 0.6821 - accuracy: 0.68 - ETA: 0s - loss: 0.6654 - accuracy: 0.70 - ETA: 0s - loss: 0.6643 - accuracy: 0.69 - ETA: 0s - loss: 0.6516 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6497 - accuracy: 0.7096 - val_loss: 0.5868 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.87 - ETA: 0s - loss: 0.7464 - accuracy: 0.75 - ETA: 0s - loss: 0.6948 - accuracy: 0.75 - ETA: 0s - loss: 0.7055 - accuracy: 0.74 - ETA: 0s - loss: 0.7039 - accuracy: 0.73 - ETA: 0s - loss: 0.6941 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6849 - accuracy: 0.7424 - val_loss: 0.6138 - val_accuracy: 0.7194\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.81 - ETA: 0s - loss: 0.6946 - accuracy: 0.70 - ETA: 0s - loss: 0.6849 - accuracy: 0.72 - ETA: 0s - loss: 0.7073 - accuracy: 0.71 - ETA: 0s - loss: 0.7706 - accuracy: 0.71 - ETA: 0s - loss: 0.7669 - accuracy: 0.72 - 0s 4ms/step - loss: 0.7786 - accuracy: 0.7271 - val_loss: 0.6030 - val_accuracy: 0.7478\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5203 - accuracy: 0.78 - ETA: 0s - loss: 0.7489 - accuracy: 0.71 - ETA: 0s - loss: 0.7375 - accuracy: 0.72 - ETA: 0s - loss: 0.7715 - accuracy: 0.72 - ETA: 0s - loss: 0.7549 - accuracy: 0.73 - ETA: 0s - loss: 0.7586 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7739 - accuracy: 0.7402 - val_loss: 0.7076 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.68 - ETA: 0s - loss: 0.6822 - accuracy: 0.75 - ETA: 0s - loss: 0.7119 - accuracy: 0.75 - ETA: 0s - loss: 0.7221 - accuracy: 0.75 - ETA: 0s - loss: 0.7730 - accuracy: 0.75 - ETA: 0s - loss: 0.7920 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8482 - accuracy: 0.7574 - val_loss: 0.6115 - val_accuracy: 0.7448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.56 - ETA: 0s - loss: 0.7572 - accuracy: 0.75 - ETA: 0s - loss: 0.7339 - accuracy: 0.75 - ETA: 0s - loss: 0.7292 - accuracy: 0.75 - ETA: 0s - loss: 0.7264 - accuracy: 0.75 - ETA: 0s - loss: 0.7022 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7053 - accuracy: 0.7577 - val_loss: 0.7936 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5117 - accuracy: 0.81 - ETA: 0s - loss: 0.9495 - accuracy: 0.76 - ETA: 0s - loss: 0.8354 - accuracy: 0.78 - ETA: 0s - loss: 0.7565 - accuracy: 0.78 - ETA: 0s - loss: 0.7422 - accuracy: 0.78 - ETA: 0s - loss: 0.7116 - accuracy: 0.79 - 0s 4ms/step - loss: 0.7110 - accuracy: 0.7880 - val_loss: 0.5994 - val_accuracy: 0.7343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.84 - ETA: 0s - loss: 0.5706 - accuracy: 0.82 - ETA: 0s - loss: 0.5973 - accuracy: 0.81 - ETA: 0s - loss: 0.6065 - accuracy: 0.81 - ETA: 0s - loss: 0.6056 - accuracy: 0.81 - ETA: 0s - loss: 0.6126 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6098 - accuracy: 0.8033 - val_loss: 0.5852 - val_accuracy: 0.7493\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.87 - ETA: 0s - loss: 0.6329 - accuracy: 0.78 - ETA: 0s - loss: 0.7064 - accuracy: 0.80 - ETA: 0s - loss: 0.6667 - accuracy: 0.80 - ETA: 0s - loss: 0.6826 - accuracy: 0.79 - ETA: 0s - loss: 0.6750 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6982 - accuracy: 0.7954 - val_loss: 0.5911 - val_accuracy: 0.7507\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.87 - ETA: 0s - loss: 0.7098 - accuracy: 0.80 - ETA: 0s - loss: 0.8696 - accuracy: 0.79 - ETA: 0s - loss: 0.7992 - accuracy: 0.79 - ETA: 0s - loss: 0.7654 - accuracy: 0.80 - ETA: 0s - loss: 0.7614 - accuracy: 0.79 - 0s 4ms/step - loss: 0.7650 - accuracy: 0.7969 - val_loss: 0.6220 - val_accuracy: 0.7627\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.75 - ETA: 0s - loss: 0.7511 - accuracy: 0.76 - ETA: 0s - loss: 0.8596 - accuracy: 0.77 - ETA: 0s - loss: 0.7660 - accuracy: 0.79 - ETA: 0s - loss: 0.7678 - accuracy: 0.79 - ETA: 0s - loss: 0.7866 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7757 - accuracy: 0.7809 - val_loss: 0.6730 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.84 - ETA: 0s - loss: 0.6554 - accuracy: 0.79 - ETA: 0s - loss: 0.7193 - accuracy: 0.78 - ETA: 0s - loss: 0.7672 - accuracy: 0.77 - ETA: 0s - loss: 0.7812 - accuracy: 0.77 - ETA: 0s - loss: 0.7429 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7943 - accuracy: 0.7839 - val_loss: 0.6021 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.81 - ETA: 0s - loss: 0.8049 - accuracy: 0.79 - ETA: 0s - loss: 0.7951 - accuracy: 0.80 - ETA: 0s - loss: 1.0581 - accuracy: 0.78 - ETA: 0s - loss: 0.9884 - accuracy: 0.78 - ETA: 0s - loss: 0.9610 - accuracy: 0.77 - 0s 4ms/step - loss: 0.9322 - accuracy: 0.7794 - val_loss: 0.6127 - val_accuracy: 0.7493\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6187 - accuracy: 0.78 - ETA: 0s - loss: 0.5748 - accuracy: 0.79 - ETA: 0s - loss: 0.6068 - accuracy: 0.78 - ETA: 0s - loss: 0.6595 - accuracy: 0.79 - ETA: 0s - loss: 0.6741 - accuracy: 0.78 - ETA: 0s - loss: 0.6662 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6995 - accuracy: 0.7910 - val_loss: 0.6271 - val_accuracy: 0.7448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5571 - accuracy: 0.81 - ETA: 0s - loss: 0.5982 - accuracy: 0.80 - ETA: 0s - loss: 0.6305 - accuracy: 0.79 - ETA: 0s - loss: 0.6726 - accuracy: 0.79 - ETA: 0s - loss: 0.6897 - accuracy: 0.79 - ETA: 0s - loss: 0.9436 - accuracy: 0.78 - 0s 4ms/step - loss: 0.9088 - accuracy: 0.7936 - val_loss: 0.6915 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.75 - ETA: 0s - loss: 0.6928 - accuracy: 0.77 - ETA: 0s - loss: 0.7290 - accuracy: 0.76 - ETA: 0s - loss: 0.7229 - accuracy: 0.77 - ETA: 0s - loss: 0.7208 - accuracy: 0.78 - ETA: 0s - loss: 0.7107 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6999 - accuracy: 0.7865 - val_loss: 0.6272 - val_accuracy: 0.7388\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 4.4547 - accuracy: 0.71 - ETA: 0s - loss: 2.0597 - accuracy: 0.75 - ETA: 0s - loss: 1.4064 - accuracy: 0.78 - ETA: 0s - loss: 1.1697 - accuracy: 0.77 - ETA: 0s - loss: 1.0533 - accuracy: 0.76 - ETA: 0s - loss: 0.9730 - accuracy: 0.76 - 0s 4ms/step - loss: 0.9295 - accuracy: 0.7656 - val_loss: 0.7222 - val_accuracy: 0.7358\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.87 - ETA: 0s - loss: 0.7758 - accuracy: 0.80 - ETA: 0s - loss: 0.7799 - accuracy: 0.78 - ETA: 0s - loss: 0.7196 - accuracy: 0.79 - ETA: 0s - loss: 0.7318 - accuracy: 0.79 - ETA: 0s - loss: 0.7889 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7729 - accuracy: 0.7835 - val_loss: 0.7172 - val_accuracy: 0.7313\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.78 - ETA: 0s - loss: 0.9408 - accuracy: 0.77 - ETA: 0s - loss: 0.9039 - accuracy: 0.74 - ETA: 0s - loss: 0.8295 - accuracy: 0.73 - ETA: 0s - loss: 0.7997 - accuracy: 0.73 - ETA: 0s - loss: 0.7748 - accuracy: 0.73 - 0s 4ms/step - loss: 0.8095 - accuracy: 0.7294 - val_loss: 0.7057 - val_accuracy: 0.7194\n",
      "Epoch 22/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.8154 - accuracy: 0.50 - ETA: 0s - loss: 0.7167 - accuracy: 0.71 - ETA: 0s - loss: 0.7323 - accuracy: 0.71 - ETA: 0s - loss: 0.7965 - accuracy: 0.72 - ETA: 0s - loss: 0.7752 - accuracy: 0.72 - ETA: 0s - loss: 0.7957 - accuracy: 0.7247Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7738 - accuracy: 0.7294 - val_loss: 0.6537 - val_accuracy: 0.7179\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 519c339adbed828597a07be2163b5f58</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064703305563</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7955469839600595</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8918 - accuracy: 0.53 - ETA: 0s - loss: 2.0514 - accuracy: 0.62 - ETA: 0s - loss: 1.3027 - accuracy: 0.65 - ETA: 0s - loss: 1.0813 - accuracy: 0.67 - ETA: 0s - loss: 0.9564 - accuracy: 0.68 - 0s 5ms/step - loss: 0.9536 - accuracy: 0.6838 - val_loss: 0.6483 - val_accuracy: 0.6985\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.68 - ETA: 0s - loss: 0.5364 - accuracy: 0.78 - ETA: 0s - loss: 0.5737 - accuracy: 0.76 - ETA: 0s - loss: 0.5612 - accuracy: 0.77 - ETA: 0s - loss: 0.5666 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5660 - accuracy: 0.7678 - val_loss: 0.5955 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.87 - ETA: 0s - loss: 0.5230 - accuracy: 0.81 - ETA: 0s - loss: 0.5526 - accuracy: 0.80 - ETA: 0s - loss: 0.5509 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5638 - accuracy: 0.7913 - val_loss: 0.5581 - val_accuracy: 0.7537\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.81 - ETA: 0s - loss: 0.5208 - accuracy: 0.77 - ETA: 0s - loss: 0.5226 - accuracy: 0.78 - ETA: 0s - loss: 0.5250 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5403 - accuracy: 0.7865 - val_loss: 0.5995 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.75 - ETA: 0s - loss: 0.5498 - accuracy: 0.76 - ETA: 0s - loss: 0.5119 - accuracy: 0.79 - ETA: 0s - loss: 0.5257 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5105 - accuracy: 0.8100 - val_loss: 0.6147 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4811 - accuracy: 0.84 - ETA: 0s - loss: 0.4358 - accuracy: 0.83 - ETA: 0s - loss: 0.4667 - accuracy: 0.83 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4887 - accuracy: 0.8339 - val_loss: 0.6309 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7310 - accuracy: 0.53 - ETA: 0s - loss: 0.4789 - accuracy: 0.82 - ETA: 0s - loss: 0.4695 - accuracy: 0.84 - ETA: 0s - loss: 0.4779 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4750 - accuracy: 0.8369 - val_loss: 0.6394 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.96 - ETA: 0s - loss: 0.3915 - accuracy: 0.88 - ETA: 0s - loss: 0.4378 - accuracy: 0.86 - ETA: 0s - loss: 0.4639 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4616 - accuracy: 0.8585 - val_loss: 0.5749 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.90 - ETA: 0s - loss: 0.4079 - accuracy: 0.87 - ETA: 0s - loss: 0.4512 - accuracy: 0.85 - ETA: 0s - loss: 0.4412 - accuracy: 0.86 - ETA: 0s - loss: 0.4392 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4462 - accuracy: 0.8593 - val_loss: 1.0853 - val_accuracy: 0.7597\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.93 - ETA: 0s - loss: 0.6559 - accuracy: 0.84 - ETA: 0s - loss: 0.5808 - accuracy: 0.85 - ETA: 0s - loss: 0.5384 - accuracy: 0.84 - ETA: 0s - loss: 0.5904 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5933 - accuracy: 0.8227 - val_loss: 0.9002 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.78 - ETA: 0s - loss: 0.5461 - accuracy: 0.84 - ETA: 0s - loss: 0.7710 - accuracy: 0.82 - ETA: 0s - loss: 0.7509 - accuracy: 0.80 - ETA: 0s - loss: 0.7049 - accuracy: 0.80 - ETA: 0s - loss: 0.6727 - accuracy: 0.80 - ETA: 0s - loss: 0.6643 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6528 - accuracy: 0.7966 - val_loss: 0.8188 - val_accuracy: 0.7164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.81 - ETA: 0s - loss: 0.7361 - accuracy: 0.77 - ETA: 0s - loss: 0.7186 - accuracy: 0.78 - ETA: 0s - loss: 0.6646 - accuracy: 0.79 - ETA: 0s - loss: 0.6491 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6490 - accuracy: 0.8037 - val_loss: 0.6900 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.84 - ETA: 0s - loss: 0.7319 - accuracy: 0.80 - ETA: 0s - loss: 0.7467 - accuracy: 0.79 - ETA: 0s - loss: 0.6959 - accuracy: 0.78 - 0s 3ms/step - loss: 0.7384 - accuracy: 0.7850 - val_loss: 0.6885 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.78 - ETA: 0s - loss: 0.6609 - accuracy: 0.76 - ETA: 0s - loss: 0.7067 - accuracy: 0.76 - ETA: 0s - loss: 0.7185 - accuracy: 0.76 - ETA: 0s - loss: 0.7258 - accuracy: 0.76 - 0s 3ms/step - loss: 0.7258 - accuracy: 0.7630 - val_loss: 0.8146 - val_accuracy: 0.7015\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5872 - accuracy: 0.68 - ETA: 0s - loss: 1.1070 - accuracy: 0.74 - ETA: 0s - loss: 0.8970 - accuracy: 0.75 - ETA: 0s - loss: 0.8253 - accuracy: 0.75 - 0s 3ms/step - loss: 0.7783 - accuracy: 0.7574 - val_loss: 0.7191 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.71 - ETA: 0s - loss: 0.6101 - accuracy: 0.78 - ETA: 0s - loss: 0.6088 - accuracy: 0.78 - ETA: 0s - loss: 0.6919 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6641 - accuracy: 0.7958 - val_loss: 0.9989 - val_accuracy: 0.7313\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.93 - ETA: 0s - loss: 0.5593 - accuracy: 0.81 - ETA: 0s - loss: 0.5421 - accuracy: 0.82 - ETA: 0s - loss: 0.6232 - accuracy: 0.82 - ETA: 0s - loss: 0.6225 - accuracy: 0.82 - 0s 3ms/step - loss: 0.6225 - accuracy: 0.8205 - val_loss: 0.8164 - val_accuracy: 0.7522\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0409 - accuracy: 0.75 - ETA: 0s - loss: 0.5482 - accuracy: 0.83 - ETA: 0s - loss: 0.5182 - accuracy: 0.83 - ETA: 0s - loss: 0.5208 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5291 - accuracy: 0.8305 - val_loss: 0.8188 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.90 - ETA: 0s - loss: 0.4947 - accuracy: 0.82 - ETA: 0s - loss: 0.5051 - accuracy: 0.83 - ETA: 0s - loss: 0.5069 - accuracy: 0.83 - ETA: 0s - loss: 0.4955 - accuracy: 0.8434Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.8421 - val_loss: 0.8754 - val_accuracy: 0.7522\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1257 - accuracy: 0.59 - ETA: 0s - loss: 1.8791 - accuracy: 0.60 - ETA: 0s - loss: 1.2330 - accuracy: 0.63 - ETA: 0s - loss: 1.0160 - accuracy: 0.65 - ETA: 0s - loss: 0.9094 - accuracy: 0.68 - 0s 4ms/step - loss: 0.8905 - accuracy: 0.6846 - val_loss: 0.5766 - val_accuracy: 0.7373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.81 - ETA: 0s - loss: 0.5869 - accuracy: 0.74 - ETA: 0s - loss: 0.5928 - accuracy: 0.73 - ETA: 0s - loss: 0.5712 - accuracy: 0.74 - ETA: 0s - loss: 0.5836 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5866 - accuracy: 0.7424 - val_loss: 0.5716 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.78 - ETA: 0s - loss: 0.5552 - accuracy: 0.76 - ETA: 0s - loss: 0.5281 - accuracy: 0.79 - ETA: 0s - loss: 0.5292 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5444 - accuracy: 0.7865 - val_loss: 0.5670 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.90 - ETA: 0s - loss: 0.5440 - accuracy: 0.78 - ETA: 0s - loss: 0.5721 - accuracy: 0.78 - ETA: 0s - loss: 0.5489 - accuracy: 0.79 - ETA: 0s - loss: 0.5390 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5364 - accuracy: 0.8052 - val_loss: 0.5661 - val_accuracy: 0.7552\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.93 - ETA: 0s - loss: 0.4793 - accuracy: 0.82 - ETA: 0s - loss: 0.4737 - accuracy: 0.82 - ETA: 0s - loss: 0.4827 - accuracy: 0.82 - ETA: 0s - loss: 0.4888 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4859 - accuracy: 0.8227 - val_loss: 0.5564 - val_accuracy: 0.7448\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4313 - accuracy: 0.84 - ETA: 0s - loss: 0.4309 - accuracy: 0.85 - ETA: 0s - loss: 0.4513 - accuracy: 0.83 - ETA: 0s - loss: 0.4565 - accuracy: 0.83 - ETA: 0s - loss: 0.4783 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4787 - accuracy: 0.8331 - val_loss: 0.6654 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.87 - ETA: 0s - loss: 0.4356 - accuracy: 0.84 - ETA: 0s - loss: 0.4534 - accuracy: 0.83 - ETA: 0s - loss: 0.4376 - accuracy: 0.84 - ETA: 0s - loss: 0.4454 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4442 - accuracy: 0.8477 - val_loss: 0.7733 - val_accuracy: 0.7194\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.93 - ETA: 0s - loss: 0.5649 - accuracy: 0.86 - ETA: 0s - loss: 0.5197 - accuracy: 0.86 - ETA: 0s - loss: 0.5267 - accuracy: 0.84 - ETA: 0s - loss: 0.5363 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5318 - accuracy: 0.8380 - val_loss: 0.7923 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.87 - ETA: 0s - loss: 0.4484 - accuracy: 0.84 - ETA: 0s - loss: 0.4637 - accuracy: 0.84 - ETA: 0s - loss: 0.4716 - accuracy: 0.84 - ETA: 0s - loss: 0.4641 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4661 - accuracy: 0.8485 - val_loss: 0.6707 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.96 - ETA: 0s - loss: 0.4356 - accuracy: 0.86 - ETA: 0s - loss: 0.4992 - accuracy: 0.84 - ETA: 0s - loss: 0.4923 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4690 - accuracy: 0.8574 - val_loss: 0.6712 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.84 - ETA: 0s - loss: 0.4757 - accuracy: 0.86 - ETA: 0s - loss: 0.4149 - accuracy: 0.87 - ETA: 0s - loss: 0.4576 - accuracy: 0.86 - ETA: 0s - loss: 0.4612 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4609 - accuracy: 0.8615 - val_loss: 0.8017 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.84 - ETA: 0s - loss: 0.5427 - accuracy: 0.85 - ETA: 0s - loss: 0.5461 - accuracy: 0.84 - ETA: 0s - loss: 0.5140 - accuracy: 0.85 - ETA: 0s - loss: 0.4869 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4924 - accuracy: 0.8533 - val_loss: 0.8061 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.78 - ETA: 0s - loss: 0.3849 - accuracy: 0.88 - ETA: 0s - loss: 0.4772 - accuracy: 0.87 - ETA: 0s - loss: 0.4653 - accuracy: 0.86 - ETA: 0s - loss: 0.4595 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4600 - accuracy: 0.8585 - val_loss: 0.6601 - val_accuracy: 0.7194\n",
      "Epoch 14/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5485 - accuracy: 0.81 - ETA: 0s - loss: 0.3586 - accuracy: 0.89 - ETA: 0s - loss: 0.4122 - accuracy: 0.87 - ETA: 0s - loss: 0.4075 - accuracy: 0.88 - ETA: 0s - loss: 0.4061 - accuracy: 0.8814Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4064 - accuracy: 0.8813 - val_loss: 0.6510 - val_accuracy: 0.7552\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8840 - accuracy: 0.62 - ETA: 0s - loss: 1.8986 - accuracy: 0.62 - ETA: 0s - loss: 1.3861 - accuracy: 0.66 - ETA: 0s - loss: 1.1396 - accuracy: 0.67 - ETA: 0s - loss: 1.0050 - accuracy: 0.68 - 0s 4ms/step - loss: 0.9845 - accuracy: 0.6898 - val_loss: 0.5581 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.84 - ETA: 0s - loss: 0.5742 - accuracy: 0.77 - ETA: 0s - loss: 0.5901 - accuracy: 0.76 - ETA: 0s - loss: 0.5997 - accuracy: 0.74 - ETA: 0s - loss: 0.5928 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5905 - accuracy: 0.7488 - val_loss: 0.5352 - val_accuracy: 0.7582\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.65 - ETA: 0s - loss: 0.5836 - accuracy: 0.74 - ETA: 0s - loss: 0.5589 - accuracy: 0.77 - ETA: 0s - loss: 0.5862 - accuracy: 0.76 - ETA: 0s - loss: 0.5841 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5861 - accuracy: 0.7708 - val_loss: 0.5958 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.87 - ETA: 0s - loss: 0.5468 - accuracy: 0.80 - ETA: 0s - loss: 0.5513 - accuracy: 0.78 - ETA: 0s - loss: 0.5632 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5607 - accuracy: 0.7906 - val_loss: 0.6345 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.84 - ETA: 0s - loss: 0.5071 - accuracy: 0.82 - ETA: 0s - loss: 0.5311 - accuracy: 0.82 - ETA: 0s - loss: 0.5375 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5604 - accuracy: 0.7996 - val_loss: 0.6298 - val_accuracy: 0.7328\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6137 - accuracy: 0.78 - ETA: 0s - loss: 0.5409 - accuracy: 0.81 - ETA: 0s - loss: 0.5447 - accuracy: 0.80 - ETA: 0s - loss: 0.5480 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7973 - val_loss: 0.7052 - val_accuracy: 0.7478\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.78 - ETA: 0s - loss: 0.4748 - accuracy: 0.83 - ETA: 0s - loss: 0.4839 - accuracy: 0.83 - ETA: 0s - loss: 0.5027 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5031 - accuracy: 0.8186 - val_loss: 0.6145 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.84 - ETA: 0s - loss: 0.5072 - accuracy: 0.82 - ETA: 0s - loss: 0.4934 - accuracy: 0.82 - ETA: 0s - loss: 0.4905 - accuracy: 0.82 - ETA: 0s - loss: 0.4792 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4799 - accuracy: 0.8361 - val_loss: 0.8264 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.90 - ETA: 0s - loss: 0.4921 - accuracy: 0.84 - ETA: 0s - loss: 0.5019 - accuracy: 0.83 - ETA: 0s - loss: 0.4983 - accuracy: 0.83 - ETA: 0s - loss: 0.5037 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5037 - accuracy: 0.8384 - val_loss: 0.6789 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.90 - ETA: 0s - loss: 0.7089 - accuracy: 0.82 - ETA: 0s - loss: 0.5790 - accuracy: 0.84 - ETA: 0s - loss: 0.5861 - accuracy: 0.83 - ETA: 0s - loss: 0.5642 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5634 - accuracy: 0.8391 - val_loss: 0.8288 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.84 - ETA: 0s - loss: 0.4914 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.85 - ETA: 0s - loss: 0.4659 - accuracy: 0.85 - ETA: 0s - loss: 0.4603 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4593 - accuracy: 0.8585 - val_loss: 0.5874 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4872 - accuracy: 0.87 - ETA: 0s - loss: 0.5139 - accuracy: 0.84 - ETA: 0s - loss: 0.4800 - accuracy: 0.85 - ETA: 0s - loss: 0.5087 - accuracy: 0.84 - ETA: 0s - loss: 0.5087 - accuracy: 0.8387Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.8402 - val_loss: 0.6068 - val_accuracy: 0.7478\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 14bf1e02cfc5096efd6557ff33aea80d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7577114303906759</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7023975620582114</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 175</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.78 - ETA: 0s - loss: 1.8645 - accuracy: 0.65 - ETA: 0s - loss: 1.2709 - accuracy: 0.66 - ETA: 0s - loss: 1.0400 - accuracy: 0.69 - ETA: 0s - loss: 0.9215 - accuracy: 0.71 - 0s 5ms/step - loss: 0.8954 - accuracy: 0.7107 - val_loss: 0.5408 - val_accuracy: 0.7463\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.68 - ETA: 0s - loss: 0.6123 - accuracy: 0.72 - ETA: 0s - loss: 0.5832 - accuracy: 0.74 - ETA: 0s - loss: 0.5916 - accuracy: 0.75 - ETA: 0s - loss: 0.5959 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5890 - accuracy: 0.7525 - val_loss: 0.6289 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.71 - ETA: 0s - loss: 0.5255 - accuracy: 0.79 - ETA: 0s - loss: 0.5057 - accuracy: 0.80 - ETA: 0s - loss: 0.5000 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5661 - accuracy: 0.7880 - val_loss: 0.5717 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.81 - ETA: 0s - loss: 0.5113 - accuracy: 0.79 - ETA: 0s - loss: 0.4861 - accuracy: 0.80 - ETA: 0s - loss: 0.4757 - accuracy: 0.81 - ETA: 0s - loss: 0.4739 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4739 - accuracy: 0.8108 - val_loss: 0.6316 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.84 - ETA: 0s - loss: 0.4482 - accuracy: 0.81 - ETA: 0s - loss: 0.4627 - accuracy: 0.81 - ETA: 0s - loss: 0.4546 - accuracy: 0.81 - ETA: 0s - loss: 0.4494 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8197 - val_loss: 0.8271 - val_accuracy: 0.6866\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.87 - ETA: 0s - loss: 0.3929 - accuracy: 0.83 - ETA: 0s - loss: 0.4200 - accuracy: 0.82 - ETA: 0s - loss: 0.4475 - accuracy: 0.80 - ETA: 0s - loss: 0.4919 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4946 - accuracy: 0.8052 - val_loss: 1.1474 - val_accuracy: 0.6746\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4228 - accuracy: 0.81 - ETA: 0s - loss: 0.5866 - accuracy: 0.82 - ETA: 0s - loss: 0.5855 - accuracy: 0.80 - ETA: 0s - loss: 0.5546 - accuracy: 0.80 - ETA: 0s - loss: 0.5734 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5734 - accuracy: 0.8096 - val_loss: 0.9088 - val_accuracy: 0.6940\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.93 - ETA: 0s - loss: 0.4728 - accuracy: 0.84 - ETA: 0s - loss: 0.4604 - accuracy: 0.83 - ETA: 0s - loss: 0.4758 - accuracy: 0.82 - ETA: 0s - loss: 0.4979 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4931 - accuracy: 0.8160 - val_loss: 2.1707 - val_accuracy: 0.7672\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1698 - accuracy: 0.84 - ETA: 0s - loss: 0.5853 - accuracy: 0.81 - ETA: 0s - loss: 0.5149 - accuracy: 0.81 - ETA: 0s - loss: 0.4755 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4862 - accuracy: 0.8305 - val_loss: 0.8299 - val_accuracy: 0.7060\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.84 - ETA: 0s - loss: 0.3949 - accuracy: 0.86 - ETA: 0s - loss: 0.4110 - accuracy: 0.86 - ETA: 0s - loss: 0.4326 - accuracy: 0.86 - ETA: 0s - loss: 0.4323 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8623 - val_loss: 0.6820 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.81 - ETA: 0s - loss: 0.4352 - accuracy: 0.85 - ETA: 0s - loss: 0.4255 - accuracy: 0.85 - ETA: 0s - loss: 0.4151 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8552 - val_loss: 0.7808 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.90 - ETA: 0s - loss: 0.5567 - accuracy: 0.84 - ETA: 0s - loss: 0.5132 - accuracy: 0.85 - ETA: 0s - loss: 0.5718 - accuracy: 0.84 - ETA: 0s - loss: 0.5602 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5585 - accuracy: 0.8376 - val_loss: 0.5737 - val_accuracy: 0.7507\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.90 - ETA: 0s - loss: 0.4204 - accuracy: 0.85 - ETA: 0s - loss: 0.4724 - accuracy: 0.83 - ETA: 0s - loss: 0.4765 - accuracy: 0.84 - ETA: 0s - loss: 0.4580 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4580 - accuracy: 0.8541 - val_loss: 0.7572 - val_accuracy: 0.7045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.71 - ETA: 0s - loss: 0.4298 - accuracy: 0.87 - ETA: 0s - loss: 0.4312 - accuracy: 0.85 - ETA: 0s - loss: 0.4101 - accuracy: 0.86 - ETA: 0s - loss: 0.3971 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8694 - val_loss: 1.3406 - val_accuracy: 0.7090\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.96 - ETA: 0s - loss: 0.8220 - accuracy: 0.80 - ETA: 0s - loss: 0.9650 - accuracy: 0.75 - ETA: 0s - loss: 0.8792 - accuracy: 0.75 - ETA: 0s - loss: 0.8743 - accuracy: 0.75 - 0s 3ms/step - loss: 0.8726 - accuracy: 0.7551 - val_loss: 0.6082 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.84 - ETA: 0s - loss: 0.7286 - accuracy: 0.74 - ETA: 0s - loss: 0.7534 - accuracy: 0.75 - ETA: 0s - loss: 0.7470 - accuracy: 0.73 - ETA: 0s - loss: 0.7273 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7273 - accuracy: 0.7327 - val_loss: 0.6547 - val_accuracy: 0.7045\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.81 - ETA: 0s - loss: 0.6373 - accuracy: 0.75 - ETA: 0s - loss: 0.6520 - accuracy: 0.74 - ETA: 0s - loss: 0.6582 - accuracy: 0.73 - ETA: 0s - loss: 0.6698 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6698 - accuracy: 0.7335 - val_loss: 0.6499 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7544 - accuracy: 0.62 - ETA: 0s - loss: 0.6724 - accuracy: 0.74 - ETA: 0s - loss: 0.6827 - accuracy: 0.71 - ETA: 0s - loss: 0.6820 - accuracy: 0.71 - ETA: 0s - loss: 0.6829 - accuracy: 0.7148Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.7122 - val_loss: 0.6813 - val_accuracy: 0.7000\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.78 - ETA: 0s - loss: 1.2413 - accuracy: 0.68 - ETA: 0s - loss: 0.9614 - accuracy: 0.66 - ETA: 0s - loss: 0.8315 - accuracy: 0.67 - ETA: 0s - loss: 0.7748 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7727 - accuracy: 0.6917 - val_loss: 0.6294 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.56 - ETA: 0s - loss: 0.5536 - accuracy: 0.74 - ETA: 0s - loss: 0.5693 - accuracy: 0.73 - ETA: 0s - loss: 0.5739 - accuracy: 0.74 - ETA: 0s - loss: 0.5658 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5641 - accuracy: 0.7492 - val_loss: 0.5430 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.81 - ETA: 0s - loss: 0.4882 - accuracy: 0.83 - ETA: 0s - loss: 0.5223 - accuracy: 0.79 - ETA: 0s - loss: 0.5078 - accuracy: 0.79 - ETA: 0s - loss: 0.5357 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7854 - val_loss: 0.5642 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.81 - ETA: 0s - loss: 0.5415 - accuracy: 0.80 - ETA: 0s - loss: 0.5743 - accuracy: 0.77 - ETA: 0s - loss: 0.5531 - accuracy: 0.78 - ETA: 0s - loss: 0.5594 - accuracy: 0.78 - ETA: 0s - loss: 0.5570 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5577 - accuracy: 0.7790 - val_loss: 0.6929 - val_accuracy: 0.7149\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.93 - ETA: 0s - loss: 0.4477 - accuracy: 0.84 - ETA: 0s - loss: 0.4883 - accuracy: 0.82 - ETA: 0s - loss: 0.4870 - accuracy: 0.82 - ETA: 0s - loss: 0.4961 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4961 - accuracy: 0.8167 - val_loss: 0.5607 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.84 - ETA: 0s - loss: 0.4808 - accuracy: 0.82 - ETA: 0s - loss: 0.4963 - accuracy: 0.79 - ETA: 0s - loss: 0.4849 - accuracy: 0.80 - ETA: 0s - loss: 0.4949 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5019 - accuracy: 0.8052 - val_loss: 0.8702 - val_accuracy: 0.6358\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.81 - ETA: 0s - loss: 0.5348 - accuracy: 0.82 - ETA: 0s - loss: 0.5206 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.82 - ETA: 0s - loss: 0.5158 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5142 - accuracy: 0.8242 - val_loss: 0.7120 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.93 - ETA: 0s - loss: 0.4223 - accuracy: 0.85 - ETA: 0s - loss: 0.4375 - accuracy: 0.86 - ETA: 0s - loss: 0.4768 - accuracy: 0.84 - ETA: 0s - loss: 0.4735 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4949 - accuracy: 0.8402 - val_loss: 0.6115 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.90 - ETA: 0s - loss: 0.4985 - accuracy: 0.83 - ETA: 0s - loss: 0.4955 - accuracy: 0.84 - ETA: 0s - loss: 0.4978 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5041 - accuracy: 0.8275 - val_loss: 0.5959 - val_accuracy: 0.7403\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.87 - ETA: 0s - loss: 0.5083 - accuracy: 0.83 - ETA: 0s - loss: 0.4762 - accuracy: 0.83 - ETA: 0s - loss: 0.4711 - accuracy: 0.84 - ETA: 0s - loss: 0.4977 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4907 - accuracy: 0.8335 - val_loss: 0.7848 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4279 - accuracy: 0.84 - ETA: 0s - loss: 0.4453 - accuracy: 0.85 - ETA: 0s - loss: 0.4377 - accuracy: 0.85 - ETA: 0s - loss: 0.4346 - accuracy: 0.86 - ETA: 0s - loss: 0.4357 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8619 - val_loss: 0.8818 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.87 - ETA: 0s - loss: 0.3971 - accuracy: 0.87 - ETA: 0s - loss: 0.3977 - accuracy: 0.88 - ETA: 0s - loss: 0.4157 - accuracy: 0.87 - ETA: 0s - loss: 0.4354 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8645 - val_loss: 0.7981 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4018 - accuracy: 0.90 - ETA: 0s - loss: 0.3766 - accuracy: 0.89 - ETA: 0s - loss: 0.4007 - accuracy: 0.88 - ETA: 0s - loss: 0.4371 - accuracy: 0.87 - ETA: 0s - loss: 0.4603 - accuracy: 0.8673Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.8604 - val_loss: 1.6105 - val_accuracy: 0.7075\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.78 - ETA: 0s - loss: 1.7907 - accuracy: 0.63 - ETA: 0s - loss: 1.1735 - accuracy: 0.69 - ETA: 0s - loss: 0.9811 - accuracy: 0.70 - ETA: 0s - loss: 0.8813 - accuracy: 0.71 - 0s 4ms/step - loss: 0.8656 - accuracy: 0.7171 - val_loss: 0.6072 - val_accuracy: 0.6925\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3909 - accuracy: 0.87 - ETA: 0s - loss: 0.5293 - accuracy: 0.76 - ETA: 0s - loss: 0.5632 - accuracy: 0.75 - ETA: 0s - loss: 0.5584 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5703 - accuracy: 0.7641 - val_loss: 0.5766 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.62 - ETA: 0s - loss: 0.5691 - accuracy: 0.77 - ETA: 0s - loss: 0.5440 - accuracy: 0.78 - ETA: 0s - loss: 0.5560 - accuracy: 0.77 - ETA: 0s - loss: 0.5539 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5539 - accuracy: 0.7738 - val_loss: 0.7074 - val_accuracy: 0.7254\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.87 - ETA: 0s - loss: 0.5047 - accuracy: 0.79 - ETA: 0s - loss: 0.5328 - accuracy: 0.79 - ETA: 0s - loss: 0.5328 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5318 - accuracy: 0.8007 - val_loss: 0.7609 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.71 - ETA: 0s - loss: 0.4821 - accuracy: 0.84 - ETA: 0s - loss: 0.4761 - accuracy: 0.83 - ETA: 0s - loss: 0.5002 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5116 - accuracy: 0.8197 - val_loss: 0.6559 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.90 - ETA: 0s - loss: 0.4710 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.84 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5222 - accuracy: 0.8253 - val_loss: 0.6395 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.90 - ETA: 0s - loss: 0.5419 - accuracy: 0.83 - ETA: 0s - loss: 0.5016 - accuracy: 0.83 - ETA: 0s - loss: 0.4808 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4799 - accuracy: 0.8414 - val_loss: 0.7511 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4161 - accuracy: 0.96 - ETA: 0s - loss: 0.4764 - accuracy: 0.85 - ETA: 0s - loss: 0.4654 - accuracy: 0.85 - ETA: 0s - loss: 0.4701 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4775 - accuracy: 0.8462 - val_loss: 0.7641 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4031 - accuracy: 0.87 - ETA: 0s - loss: 0.4184 - accuracy: 0.85 - ETA: 0s - loss: 0.4211 - accuracy: 0.85 - ETA: 0s - loss: 0.4294 - accuracy: 0.85 - ETA: 0s - loss: 0.4241 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8559 - val_loss: 0.6673 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.84 - ETA: 0s - loss: 0.4061 - accuracy: 0.85 - ETA: 0s - loss: 0.4244 - accuracy: 0.84 - ETA: 0s - loss: 0.4101 - accuracy: 0.86 - ETA: 0s - loss: 0.3997 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3957 - accuracy: 0.8727 - val_loss: 0.8129 - val_accuracy: 0.7239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.75 - ETA: 0s - loss: 0.3654 - accuracy: 0.89 - ETA: 0s - loss: 0.3686 - accuracy: 0.89 - ETA: 0s - loss: 0.3625 - accuracy: 0.88 - ETA: 0s - loss: 0.3725 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8858 - val_loss: 0.8212 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.87 - ETA: 0s - loss: 0.3824 - accuracy: 0.88 - ETA: 0s - loss: 0.4108 - accuracy: 0.87 - ETA: 0s - loss: 0.4049 - accuracy: 0.88 - ETA: 0s - loss: 0.4944 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4909 - accuracy: 0.8757 - val_loss: 0.7408 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6687 - accuracy: 0.78 - ETA: 0s - loss: 0.3910 - accuracy: 0.87 - ETA: 0s - loss: 0.3824 - accuracy: 0.88 - ETA: 0s - loss: 0.3765 - accuracy: 0.88 - ETA: 0s - loss: 0.4038 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4070 - accuracy: 0.8716 - val_loss: 0.7134 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.87 - ETA: 0s - loss: 0.4607 - accuracy: 0.85 - ETA: 0s - loss: 0.4274 - accuracy: 0.86 - ETA: 0s - loss: 0.4130 - accuracy: 0.87 - ETA: 0s - loss: 0.4059 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4062 - accuracy: 0.8776 - val_loss: 0.8103 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.93 - ETA: 0s - loss: 0.3630 - accuracy: 0.89 - ETA: 0s - loss: 0.3557 - accuracy: 0.89 - ETA: 0s - loss: 0.3652 - accuracy: 0.89 - ETA: 0s - loss: 0.3805 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8929 - val_loss: 0.8580 - val_accuracy: 0.7388\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.84 - ETA: 0s - loss: 0.4196 - accuracy: 0.87 - ETA: 0s - loss: 0.3940 - accuracy: 0.89 - ETA: 0s - loss: 0.3850 - accuracy: 0.89 - ETA: 0s - loss: 0.3978 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8914 - val_loss: 0.9110 - val_accuracy: 0.7104\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.68 - ETA: 0s - loss: 0.3920 - accuracy: 0.88 - ETA: 0s - loss: 0.4114 - accuracy: 0.87 - ETA: 0s - loss: 0.3965 - accuracy: 0.88 - ETA: 0s - loss: 0.4646 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4724 - accuracy: 0.8753 - val_loss: 1.1872 - val_accuracy: 0.7090\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.78 - ETA: 0s - loss: 0.4619 - accuracy: 0.86 - ETA: 0s - loss: 0.4763 - accuracy: 0.84 - ETA: 0s - loss: 0.4712 - accuracy: 0.85 - ETA: 0s - loss: 0.5284 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5185 - accuracy: 0.8507 - val_loss: 0.6158 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.87 - ETA: 0s - loss: 0.5077 - accuracy: 0.84 - ETA: 0s - loss: 0.4740 - accuracy: 0.86 - ETA: 0s - loss: 0.4621 - accuracy: 0.86 - ETA: 0s - loss: 0.4645 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8645 - val_loss: 0.7502 - val_accuracy: 0.7507\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.90 - ETA: 0s - loss: 0.5558 - accuracy: 0.87 - ETA: 0s - loss: 0.4775 - accuracy: 0.88 - ETA: 0s - loss: 0.4399 - accuracy: 0.88 - ETA: 0s - loss: 0.4466 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4447 - accuracy: 0.8806 - val_loss: 1.2484 - val_accuracy: 0.7194\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6220 - accuracy: 0.81 - ETA: 0s - loss: 0.3681 - accuracy: 0.89 - ETA: 0s - loss: 0.3766 - accuracy: 0.89 - ETA: 0s - loss: 0.3864 - accuracy: 0.89 - ETA: 0s - loss: 0.3868 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8951 - val_loss: 1.1622 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.87 - ETA: 0s - loss: 0.3383 - accuracy: 0.91 - ETA: 0s - loss: 0.3409 - accuracy: 0.90 - ETA: 0s - loss: 0.3609 - accuracy: 0.89 - ETA: 0s - loss: 0.3505 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3520 - accuracy: 0.9018 - val_loss: 1.1881 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.84 - ETA: 0s - loss: 0.3302 - accuracy: 0.91 - ETA: 0s - loss: 0.3142 - accuracy: 0.91 - ETA: 0s - loss: 0.3157 - accuracy: 0.91 - ETA: 0s - loss: 0.3081 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3227 - accuracy: 0.9145 - val_loss: 0.8553 - val_accuracy: 0.7388\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.81 - ETA: 0s - loss: 0.4313 - accuracy: 0.88 - ETA: 0s - loss: 0.4270 - accuracy: 0.87 - ETA: 0s - loss: 0.4132 - accuracy: 0.88 - ETA: 0s - loss: 0.4226 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8787 - val_loss: 0.9120 - val_accuracy: 0.7328\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.93 - ETA: 0s - loss: 0.4358 - accuracy: 0.84 - ETA: 0s - loss: 0.6490 - accuracy: 0.84 - ETA: 0s - loss: 0.6055 - accuracy: 0.84 - ETA: 0s - loss: 0.5699 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5508 - accuracy: 0.8511 - val_loss: 0.8672 - val_accuracy: 0.7418\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.81 - ETA: 0s - loss: 0.4476 - accuracy: 0.86 - ETA: 0s - loss: 0.4586 - accuracy: 0.85 - ETA: 0s - loss: 0.4438 - accuracy: 0.86 - ETA: 0s - loss: 0.4478 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8686 - val_loss: 0.9813 - val_accuracy: 0.7493\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.84 - ETA: 0s - loss: 0.4078 - accuracy: 0.88 - ETA: 0s - loss: 0.3948 - accuracy: 0.88 - ETA: 0s - loss: 0.4062 - accuracy: 0.88 - ETA: 0s - loss: 0.4091 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4733 - accuracy: 0.8820 - val_loss: 1.7554 - val_accuracy: 0.7269\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.90 - ETA: 0s - loss: 0.4415 - accuracy: 0.89 - ETA: 0s - loss: 0.4366 - accuracy: 0.88 - ETA: 0s - loss: 0.4228 - accuracy: 0.88 - ETA: 0s - loss: 0.4208 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8817 - val_loss: 1.2397 - val_accuracy: 0.7433\n",
      "Epoch 29/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4921 - accuracy: 0.84 - ETA: 0s - loss: 0.4084 - accuracy: 0.88 - ETA: 0s - loss: 0.4053 - accuracy: 0.88 - ETA: 0s - loss: 0.3969 - accuracy: 0.88 - ETA: 0s - loss: 0.3826 - accuracy: 0.8938Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8966 - val_loss: 1.4061 - val_accuracy: 0.7313\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 4a351a506e1ae8f1543a4cf4b3a9cfbe</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7547263503074646</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6083101447530346</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 75</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4360 - accuracy: 0.87 - ETA: 0s - loss: 0.8732 - accuracy: 0.68 - ETA: 0s - loss: 0.7636 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7033 - accuracy: 0.6965 - val_loss: 0.5433 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.84 - ETA: 0s - loss: 0.5325 - accuracy: 0.76 - ETA: 0s - loss: 0.5485 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5448 - accuracy: 0.7581 - val_loss: 0.6215 - val_accuracy: 0.6791\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.90 - ETA: 0s - loss: 0.4625 - accuracy: 0.76 - ETA: 0s - loss: 0.4740 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7824 - val_loss: 0.5743 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.78 - ETA: 0s - loss: 0.4338 - accuracy: 0.81 - ETA: 0s - loss: 0.4510 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4542 - accuracy: 0.8078 - val_loss: 0.8162 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.71 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - ETA: 0s - loss: 0.3826 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8242 - val_loss: 0.6330 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.3867 - accuracy: 0.83 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8279 - val_loss: 0.6292 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.96 - ETA: 0s - loss: 0.4176 - accuracy: 0.83 - ETA: 0s - loss: 0.4353 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8373 - val_loss: 0.6130 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.93 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3754 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8436 - val_loss: 1.1359 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.90 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.4247 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8645 - val_loss: 0.7083 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.93 - ETA: 0s - loss: 0.5169 - accuracy: 0.83 - ETA: 0s - loss: 0.4432 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8559 - val_loss: 1.0033 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.75 - ETA: 0s - loss: 0.3454 - accuracy: 0.87 - ETA: 0s - loss: 0.3348 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8753 - val_loss: 1.0166 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.93 - ETA: 0s - loss: 0.3236 - accuracy: 0.89 - ETA: 0s - loss: 0.3373 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8750 - val_loss: 0.9927 - val_accuracy: 0.6731\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.87 - ETA: 0s - loss: 0.3118 - accuracy: 0.89 - ETA: 0s - loss: 0.3270 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3279 - accuracy: 0.8753 - val_loss: 1.1303 - val_accuracy: 0.6881\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.81 - ETA: 0s - loss: 0.2915 - accuracy: 0.89 - ETA: 0s - loss: 0.3259 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8761 - val_loss: 4.3810 - val_accuracy: 0.6687\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3717 - accuracy: 0.87 - ETA: 0s - loss: 0.6317 - accuracy: 0.87 - ETA: 0s - loss: 0.5876 - accuracy: 0.86 - 0s 2ms/step - loss: 0.6664 - accuracy: 0.8290 - val_loss: 1.6533 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.81 - ETA: 0s - loss: 0.7426 - accuracy: 0.70 - ETA: 0s - loss: 0.7943 - accuracy: 0.69 - ETA: 0s - loss: 0.7283 - accuracy: 0.66 - 0s 2ms/step - loss: 0.7283 - accuracy: 0.6652 - val_loss: 3.3307 - val_accuracy: 0.4328\n",
      "Epoch 17/50\n",
      "67/84 [======================>.......] - ETA: 0s - loss: 0.5073 - accuracy: 0.50 - ETA: 0s - loss: 0.5813 - accuracy: 0.54 - ETA: 0s - loss: 0.6932 - accuracy: 0.5429Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7425 - accuracy: 0.5483 - val_loss: 1.0669 - val_accuracy: 0.4985\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0327 - accuracy: 0.59 - ETA: 0s - loss: 0.7841 - accuracy: 0.68 - ETA: 0s - loss: 0.7207 - accuracy: 0.68 - 0s 3ms/step - loss: 0.6902 - accuracy: 0.6823 - val_loss: 0.5737 - val_accuracy: 0.6910\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.87 - ETA: 0s - loss: 0.5186 - accuracy: 0.75 - ETA: 0s - loss: 0.5435 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7521 - val_loss: 0.5799 - val_accuracy: 0.6940\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6032 - accuracy: 0.65 - ETA: 0s - loss: 0.4353 - accuracy: 0.79 - ETA: 0s - loss: 0.4566 - accuracy: 0.77 - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7809 - val_loss: 0.6078 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.90 - ETA: 0s - loss: 0.3362 - accuracy: 0.85 - ETA: 0s - loss: 0.4127 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4218 - accuracy: 0.8175 - val_loss: 0.5708 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.78 - ETA: 0s - loss: 0.3853 - accuracy: 0.81 - ETA: 0s - loss: 0.3963 - accuracy: 0.80 - 0s 2ms/step - loss: 0.3974 - accuracy: 0.8089 - val_loss: 0.8523 - val_accuracy: 0.7090\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.90 - ETA: 0s - loss: 0.3623 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8391 - val_loss: 0.6369 - val_accuracy: 0.7209\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.78 - ETA: 0s - loss: 0.3747 - accuracy: 0.82 - ETA: 0s - loss: 0.3865 - accuracy: 0.82 - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8298 - val_loss: 0.6908 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.81 - ETA: 0s - loss: 0.3493 - accuracy: 0.83 - ETA: 0s - loss: 0.3695 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8391 - val_loss: 0.9826 - val_accuracy: 0.6701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.90 - ETA: 0s - loss: 0.3403 - accuracy: 0.83 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8257 - val_loss: 1.6313 - val_accuracy: 0.6791\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7868 - accuracy: 0.84 - ETA: 0s - loss: 0.3747 - accuracy: 0.83 - ETA: 0s - loss: 0.3787 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8399 - val_loss: 1.9883 - val_accuracy: 0.7134\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.87 - ETA: 0s - loss: 0.4473 - accuracy: 0.83 - ETA: 0s - loss: 0.4059 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8395 - val_loss: 0.8713 - val_accuracy: 0.6881\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.71 - ETA: 0s - loss: 0.3135 - accuracy: 0.88 - ETA: 0s - loss: 0.3521 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8630 - val_loss: 1.0452 - val_accuracy: 0.7104\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.96 - ETA: 0s - loss: 0.3391 - accuracy: 0.88 - ETA: 0s - loss: 0.3599 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8585 - val_loss: 0.8680 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.84 - ETA: 0s - loss: 0.3071 - accuracy: 0.87 - ETA: 0s - loss: 0.3183 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8847 - val_loss: 1.9081 - val_accuracy: 0.7194\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.96 - ETA: 0s - loss: 0.3225 - accuracy: 0.91 - ETA: 0s - loss: 0.3440 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8858 - val_loss: 1.4331 - val_accuracy: 0.7448\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.96 - ETA: 0s - loss: 0.7193 - accuracy: 0.78 - ETA: 0s - loss: 0.8065 - accuracy: 0.77 - 0s 2ms/step - loss: 0.7792 - accuracy: 0.7551 - val_loss: 0.6391 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.68 - ETA: 0s - loss: 1.1780 - accuracy: 0.62 - ETA: 0s - loss: 0.9347 - accuracy: 0.53 - 0s 2ms/step - loss: 0.8523 - accuracy: 0.5327 - val_loss: 0.8414 - val_accuracy: 0.4075\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.40 - ETA: 0s - loss: 0.6246 - accuracy: 0.48 - ETA: 0s - loss: 0.6254 - accuracy: 0.48 - 0s 2ms/step - loss: 0.6269 - accuracy: 0.4614 - val_loss: 1.5384 - val_accuracy: 0.4373\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.43 - ETA: 0s - loss: 0.6070 - accuracy: 0.43 - ETA: 0s - loss: 0.6922 - accuracy: 0.42 - ETA: 0s - loss: 0.6999 - accuracy: 0.43 - 0s 2ms/step - loss: 0.6996 - accuracy: 0.4289 - val_loss: 1.6263 - val_accuracy: 0.3642\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.43 - ETA: 0s - loss: 0.6521 - accuracy: 0.37 - ETA: 0s - loss: 0.6576 - accuracy: 0.35 - ETA: 0s - loss: 0.6851 - accuracy: 0.40 - 0s 2ms/step - loss: 0.6806 - accuracy: 0.4128 - val_loss: 6.4266 - val_accuracy: 0.4627\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.59 - ETA: 0s - loss: 0.8636 - accuracy: 0.50 - ETA: 0s - loss: 0.7851 - accuracy: 0.45 - 0s 2ms/step - loss: 0.7433 - accuracy: 0.4166 - val_loss: 0.8240 - val_accuracy: 0.3493\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.37 - ETA: 0s - loss: 0.6710 - accuracy: 0.36 - ETA: 0s - loss: 0.6656 - accuracy: 0.35 - ETA: 0s - loss: 1.2045 - accuracy: 0.37 - 0s 2ms/step - loss: 1.1915 - accuracy: 0.3729 - val_loss: 0.7148 - val_accuracy: 0.3119\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.28 - ETA: 0s - loss: 0.6865 - accuracy: 0.31 - ETA: 0s - loss: 0.6829 - accuracy: 0.31 - ETA: 0s - loss: 0.6821 - accuracy: 0.32 - 0s 2ms/step - loss: 0.6817 - accuracy: 0.3270 - val_loss: 0.8216 - val_accuracy: 0.3537\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5729 - accuracy: 0.15 - ETA: 0s - loss: 0.6635 - accuracy: 0.36 - ETA: 0s - loss: 0.7621 - accuracy: 0.39 - ETA: 0s - loss: 0.7218 - accuracy: 0.37 - 0s 2ms/step - loss: 1.1230 - accuracy: 0.3662 - val_loss: 1.1766 - val_accuracy: 0.3463\n",
      "Epoch 25/50\n",
      "57/84 [===================>..........] - ETA: 0s - loss: 0.6678 - accuracy: 0.37 - ETA: 0s - loss: 0.6946 - accuracy: 0.32 - ETA: 0s - loss: 0.9249 - accuracy: 0.3037Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.8525 - accuracy: 0.3699 - val_loss: 0.6961 - val_accuracy: 0.3045\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.65 - ETA: 0s - loss: 0.9871 - accuracy: 0.63 - ETA: 0s - loss: 0.8222 - accuracy: 0.66 - ETA: 0s - loss: 0.7474 - accuracy: 0.67 - 0s 4ms/step - loss: 0.7434 - accuracy: 0.6790 - val_loss: 0.5536 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.75 - ETA: 0s - loss: 0.5270 - accuracy: 0.73 - ETA: 0s - loss: 0.5114 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7574 - val_loss: 0.6711 - val_accuracy: 0.6463\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.84 - ETA: 0s - loss: 0.4336 - accuracy: 0.79 - ETA: 0s - loss: 0.4627 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7913 - val_loss: 0.6240 - val_accuracy: 0.6866\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.93 - ETA: 0s - loss: 0.4253 - accuracy: 0.81 - ETA: 0s - loss: 0.4599 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7925 - val_loss: 0.9615 - val_accuracy: 0.6358\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7881 - accuracy: 0.90 - ETA: 0s - loss: 0.4960 - accuracy: 0.80 - ETA: 0s - loss: 0.4974 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7981 - val_loss: 0.6357 - val_accuracy: 0.6821\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.84 - ETA: 0s - loss: 0.3754 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.81 - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8182 - val_loss: 0.6312 - val_accuracy: 0.7075\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.68 - ETA: 0s - loss: 0.3603 - accuracy: 0.85 - ETA: 0s - loss: 0.3736 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8440 - val_loss: 0.7838 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.87 - ETA: 0s - loss: 0.3530 - accuracy: 0.87 - ETA: 0s - loss: 0.3746 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8477 - val_loss: 1.1147 - val_accuracy: 0.6776\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.90 - ETA: 0s - loss: 0.4289 - accuracy: 0.82 - ETA: 0s - loss: 0.4165 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8264 - val_loss: 0.5672 - val_accuracy: 0.7358\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.78 - ETA: 0s - loss: 0.3812 - accuracy: 0.85 - ETA: 0s - loss: 0.4015 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8350 - val_loss: 1.2156 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.87 - ETA: 0s - loss: 0.2934 - accuracy: 0.88 - ETA: 0s - loss: 0.3093 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8742 - val_loss: 1.3039 - val_accuracy: 0.6910\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.93 - ETA: 0s - loss: 0.3261 - accuracy: 0.88 - ETA: 0s - loss: 0.3778 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8708 - val_loss: 1.0390 - val_accuracy: 0.7119\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7355 - accuracy: 0.93 - ETA: 0s - loss: 0.3653 - accuracy: 0.87 - ETA: 0s - loss: 0.3341 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8708 - val_loss: 0.9982 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.84 - ETA: 0s - loss: 0.2955 - accuracy: 0.89 - ETA: 0s - loss: 0.2970 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8906 - val_loss: 1.0039 - val_accuracy: 0.6806\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.87 - ETA: 0s - loss: 0.3807 - accuracy: 0.87 - ETA: 0s - loss: 0.4008 - accuracy: 0.86 - ETA: 0s - loss: 0.4687 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4898 - accuracy: 0.8552 - val_loss: 2.7516 - val_accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.93 - ETA: 0s - loss: 0.4731 - accuracy: 0.89 - ETA: 0s - loss: 0.4025 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3875 - accuracy: 0.8873 - val_loss: 2.0308 - val_accuracy: 0.6910\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.90 - ETA: 0s - loss: 0.3029 - accuracy: 0.89 - ETA: 0s - loss: 0.4330 - accuracy: 0.89 - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8888 - val_loss: 1.2310 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.93 - ETA: 0s - loss: 0.3762 - accuracy: 0.89 - ETA: 0s - loss: 0.3772 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8847 - val_loss: 1.2266 - val_accuracy: 0.7194\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.93 - ETA: 0s - loss: 0.2632 - accuracy: 0.91 - ETA: 0s - loss: 0.2898 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3251 - accuracy: 0.9018 - val_loss: 1.1757 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.93 - ETA: 0s - loss: 0.2590 - accuracy: 0.91 - ETA: 0s - loss: 0.3438 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3401 - accuracy: 0.9100 - val_loss: 2.1844 - val_accuracy: 0.6851\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.93 - ETA: 0s - loss: 0.3027 - accuracy: 0.90 - ETA: 0s - loss: 0.2973 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3038 - accuracy: 0.9041 - val_loss: 0.7528 - val_accuracy: 0.7463\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.96 - ETA: 0s - loss: 0.2288 - accuracy: 0.94 - ETA: 0s - loss: 0.2367 - accuracy: 0.93 - ETA: 0s - loss: 0.2664 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2778 - accuracy: 0.9302 - val_loss: 0.9367 - val_accuracy: 0.7104\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.96 - ETA: 0s - loss: 0.3554 - accuracy: 0.91 - ETA: 0s - loss: 0.3931 - accuracy: 0.90 - ETA: 0s - loss: 0.3787 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8940 - val_loss: 1.1396 - val_accuracy: 0.7149\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 1.00 - ETA: 0s - loss: 0.2135 - accuracy: 0.93 - ETA: 0s - loss: 0.2410 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9358 - val_loss: 2.1050 - val_accuracy: 0.6970\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.90 - ETA: 0s - loss: 0.1751 - accuracy: 0.96 - ETA: 0s - loss: 0.1961 - accuracy: 0.95 - ETA: 0s - loss: 0.3242 - accuracy: 0.94 - 0s 2ms/step - loss: 0.3741 - accuracy: 0.9362 - val_loss: 3.1616 - val_accuracy: 0.6567\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6754 - accuracy: 0.90 - ETA: 0s - loss: 0.4226 - accuracy: 0.87 - ETA: 0s - loss: 0.3231 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3086 - accuracy: 0.9119 - val_loss: 1.2237 - val_accuracy: 0.7149\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.90 - ETA: 0s - loss: 0.2122 - accuracy: 0.94 - ETA: 0s - loss: 0.2051 - accuracy: 0.94 - 0s 2ms/step - loss: 0.2895 - accuracy: 0.9324 - val_loss: 1.1577 - val_accuracy: 0.7373\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.75 - ETA: 0s - loss: 0.4500 - accuracy: 0.88 - ETA: 0s - loss: 0.3835 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8925 - val_loss: 1.1386 - val_accuracy: 0.7269\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.90 - ETA: 0s - loss: 0.2562 - accuracy: 0.93 - ETA: 0s - loss: 0.2335 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9369 - val_loss: 2.3861 - val_accuracy: 0.6940\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.90 - ETA: 0s - loss: 0.1810 - accuracy: 0.95 - ETA: 0s - loss: 0.1920 - accuracy: 0.94 - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9462 - val_loss: 2.6427 - val_accuracy: 0.7045\n",
      "Epoch 31/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.1839 - accuracy: 0.96 - ETA: 0s - loss: 0.2986 - accuracy: 0.92 - ETA: 0s - loss: 0.6955 - accuracy: 0.9231Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.8010 - accuracy: 0.8850 - val_loss: 0.7281 - val_accuracy: 0.6955\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ee9f89f142403b5757920bb50c8d36b7</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7417910297711691</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.33063500192729045</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 95</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.78 - ETA: 0s - loss: 1.2328 - accuracy: 0.61 - ETA: 0s - loss: 0.9971 - accuracy: 0.63 - ETA: 0s - loss: 0.8597 - accuracy: 0.67 - ETA: 0s - loss: 0.8368 - accuracy: 0.67 - ETA: 0s - loss: 0.7993 - accuracy: 0.67 - 0s 5ms/step - loss: 0.7958 - accuracy: 0.6797 - val_loss: 0.5663 - val_accuracy: 0.7537\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8226 - accuracy: 0.59 - ETA: 0s - loss: 0.5686 - accuracy: 0.76 - ETA: 0s - loss: 0.5792 - accuracy: 0.77 - ETA: 0s - loss: 0.5987 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5988 - accuracy: 0.7518 - val_loss: 0.6176 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.81 - ETA: 0s - loss: 0.5836 - accuracy: 0.76 - ETA: 0s - loss: 0.5995 - accuracy: 0.75 - ETA: 0s - loss: 0.5879 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5960 - accuracy: 0.7615 - val_loss: 0.6039 - val_accuracy: 0.6970\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.56 - ETA: 0s - loss: 0.5929 - accuracy: 0.76 - ETA: 0s - loss: 0.6096 - accuracy: 0.75 - ETA: 0s - loss: 0.5926 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5917 - accuracy: 0.7607 - val_loss: 0.6179 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.84 - ETA: 0s - loss: 0.5419 - accuracy: 0.80 - ETA: 0s - loss: 0.5657 - accuracy: 0.79 - ETA: 0s - loss: 0.5865 - accuracy: 0.77 - ETA: 0s - loss: 0.5855 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7786 - val_loss: 0.6015 - val_accuracy: 0.7463\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.75 - ETA: 0s - loss: 0.5132 - accuracy: 0.82 - ETA: 0s - loss: 0.5561 - accuracy: 0.80 - ETA: 0s - loss: 0.5507 - accuracy: 0.79 - ETA: 0s - loss: 0.5845 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5838 - accuracy: 0.7865 - val_loss: 1.1933 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.87 - ETA: 0s - loss: 0.8028 - accuracy: 0.77 - ETA: 0s - loss: 0.7191 - accuracy: 0.78 - ETA: 0s - loss: 0.6707 - accuracy: 0.78 - ETA: 0s - loss: 0.6671 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6629 - accuracy: 0.7790 - val_loss: 0.5795 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.81 - ETA: 0s - loss: 0.5292 - accuracy: 0.81 - ETA: 0s - loss: 0.5554 - accuracy: 0.80 - ETA: 0s - loss: 0.5860 - accuracy: 0.78 - ETA: 0s - loss: 0.6236 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6306 - accuracy: 0.7846 - val_loss: 0.7594 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8192 - accuracy: 0.78 - ETA: 0s - loss: 0.6102 - accuracy: 0.80 - ETA: 0s - loss: 0.5688 - accuracy: 0.81 - ETA: 0s - loss: 0.5651 - accuracy: 0.81 - ETA: 0s - loss: 0.5686 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5781 - accuracy: 0.8096 - val_loss: 1.0307 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.87 - ETA: 0s - loss: 0.7340 - accuracy: 0.80 - ETA: 0s - loss: 0.6352 - accuracy: 0.80 - ETA: 0s - loss: 0.6948 - accuracy: 0.81 - ETA: 0s - loss: 0.6670 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6645 - accuracy: 0.8078 - val_loss: 0.5951 - val_accuracy: 0.7597\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7459 - accuracy: 0.71 - ETA: 0s - loss: 0.5317 - accuracy: 0.81 - ETA: 0s - loss: 0.5518 - accuracy: 0.82 - ETA: 0s - loss: 0.5634 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5581 - accuracy: 0.8145 - val_loss: 0.7158 - val_accuracy: 0.7552\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.5371 - accuracy: 0.82 - ETA: 0s - loss: 0.5586 - accuracy: 0.81 - ETA: 0s - loss: 0.5292 - accuracy: 0.82 - ETA: 0s - loss: 0.5313 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5337 - accuracy: 0.8216 - val_loss: 0.6386 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.75 - ETA: 0s - loss: 0.4834 - accuracy: 0.85 - ETA: 0s - loss: 0.6065 - accuracy: 0.83 - ETA: 0s - loss: 0.5827 - accuracy: 0.83 - ETA: 0s - loss: 0.5808 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5776 - accuracy: 0.8302 - val_loss: 0.6856 - val_accuracy: 0.7657\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.81 - ETA: 0s - loss: 0.5419 - accuracy: 0.82 - ETA: 0s - loss: 0.5979 - accuracy: 0.81 - ETA: 0s - loss: 0.5584 - accuracy: 0.82 - ETA: 0s - loss: 0.5554 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5503 - accuracy: 0.8208 - val_loss: 0.7488 - val_accuracy: 0.7597\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.81 - ETA: 0s - loss: 0.4933 - accuracy: 0.83 - ETA: 0s - loss: 0.5051 - accuracy: 0.83 - ETA: 0s - loss: 0.5375 - accuracy: 0.82 - ETA: 0s - loss: 0.5449 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5388 - accuracy: 0.8246 - val_loss: 0.5694 - val_accuracy: 0.7597\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.78 - ETA: 0s - loss: 0.5769 - accuracy: 0.79 - ETA: 0s - loss: 0.5922 - accuracy: 0.80 - ETA: 0s - loss: 0.5609 - accuracy: 0.81 - ETA: 0s - loss: 0.5457 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5457 - accuracy: 0.8227 - val_loss: 0.5989 - val_accuracy: 0.7239\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.78 - ETA: 0s - loss: 0.4498 - accuracy: 0.86 - ETA: 0s - loss: 0.5047 - accuracy: 0.84 - ETA: 0s - loss: 0.5167 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5274 - accuracy: 0.8324 - val_loss: 0.6215 - val_accuracy: 0.7522\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.87 - ETA: 0s - loss: 0.7169 - accuracy: 0.82 - ETA: 0s - loss: 0.6399 - accuracy: 0.82 - ETA: 0s - loss: 0.6547 - accuracy: 0.80 - ETA: 0s - loss: 0.6246 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6194 - accuracy: 0.8085 - val_loss: 1.5506 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.84 - ETA: 0s - loss: 0.6455 - accuracy: 0.80 - ETA: 0s - loss: 0.6834 - accuracy: 0.78 - ETA: 0s - loss: 0.6607 - accuracy: 0.78 - ETA: 0s - loss: 0.6484 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6527 - accuracy: 0.7895 - val_loss: 1.4626 - val_accuracy: 0.7299\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.75 - ETA: 0s - loss: 0.6380 - accuracy: 0.78 - ETA: 0s - loss: 0.6355 - accuracy: 0.79 - ETA: 0s - loss: 0.6711 - accuracy: 0.79 - ETA: 0s - loss: 0.6322 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6299 - accuracy: 0.8055 - val_loss: 1.2652 - val_accuracy: 0.7448\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.93 - ETA: 0s - loss: 0.5879 - accuracy: 0.81 - ETA: 0s - loss: 0.5866 - accuracy: 0.81 - ETA: 0s - loss: 0.5541 - accuracy: 0.82 - ETA: 0s - loss: 0.5396 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5380 - accuracy: 0.8283 - val_loss: 1.0010 - val_accuracy: 0.7463\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.81 - ETA: 0s - loss: 0.4737 - accuracy: 0.84 - ETA: 0s - loss: 0.4797 - accuracy: 0.84 - ETA: 0s - loss: 0.4717 - accuracy: 0.84 - ETA: 0s - loss: 0.4851 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4832 - accuracy: 0.8477 - val_loss: 0.8878 - val_accuracy: 0.7015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.6581 - accuracy: 0.75 - ETA: 0s - loss: 0.4663 - accuracy: 0.84 - ETA: 0s - loss: 0.4825 - accuracy: 0.85 - ETA: 0s - loss: 0.4588 - accuracy: 0.86 - ETA: 0s - loss: 0.4988 - accuracy: 0.8590Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.8544 - val_loss: 1.1852 - val_accuracy: 0.7269\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.84 - ETA: 0s - loss: 1.4895 - accuracy: 0.59 - ETA: 0s - loss: 1.0926 - accuracy: 0.62 - ETA: 0s - loss: 0.9548 - accuracy: 0.65 - ETA: 0s - loss: 0.8604 - accuracy: 0.67 - 0s 4ms/step - loss: 0.8522 - accuracy: 0.6786 - val_loss: 0.5806 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.78 - ETA: 0s - loss: 0.5745 - accuracy: 0.75 - ETA: 0s - loss: 0.5805 - accuracy: 0.75 - ETA: 0s - loss: 0.5741 - accuracy: 0.76 - ETA: 0s - loss: 0.5713 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5712 - accuracy: 0.7645 - val_loss: 0.6852 - val_accuracy: 0.7254\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.84 - ETA: 0s - loss: 0.5236 - accuracy: 0.79 - ETA: 0s - loss: 0.5536 - accuracy: 0.78 - ETA: 0s - loss: 0.5635 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5622 - accuracy: 0.7779 - val_loss: 0.5697 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.65 - ETA: 0s - loss: 0.5436 - accuracy: 0.79 - ETA: 0s - loss: 0.5537 - accuracy: 0.78 - ETA: 0s - loss: 0.5797 - accuracy: 0.77 - ETA: 0s - loss: 0.5787 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5787 - accuracy: 0.7779 - val_loss: 0.5517 - val_accuracy: 0.7507\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.75 - ETA: 0s - loss: 0.5892 - accuracy: 0.78 - ETA: 0s - loss: 0.5975 - accuracy: 0.78 - ETA: 0s - loss: 0.5866 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5873 - accuracy: 0.7820 - val_loss: 0.5632 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.71 - ETA: 0s - loss: 0.5613 - accuracy: 0.81 - ETA: 0s - loss: 0.5666 - accuracy: 0.80 - ETA: 0s - loss: 0.5649 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5799 - accuracy: 0.8025 - val_loss: 0.9159 - val_accuracy: 0.6493\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8498 - accuracy: 0.68 - ETA: 0s - loss: 0.5957 - accuracy: 0.79 - ETA: 0s - loss: 0.6059 - accuracy: 0.78 - ETA: 0s - loss: 0.6042 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6310 - accuracy: 0.7772 - val_loss: 1.0980 - val_accuracy: 0.6090\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.75 - ETA: 0s - loss: 0.6500 - accuracy: 0.79 - ETA: 0s - loss: 0.6212 - accuracy: 0.79 - ETA: 0s - loss: 0.5946 - accuracy: 0.79 - ETA: 0s - loss: 0.5760 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5746 - accuracy: 0.8022 - val_loss: 0.7430 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.84 - ETA: 0s - loss: 0.4866 - accuracy: 0.83 - ETA: 0s - loss: 0.6107 - accuracy: 0.82 - ETA: 0s - loss: 0.6169 - accuracy: 0.81 - ETA: 0s - loss: 0.6109 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6091 - accuracy: 0.8066 - val_loss: 0.5855 - val_accuracy: 0.7716\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6439 - accuracy: 0.75 - ETA: 0s - loss: 0.6293 - accuracy: 0.77 - ETA: 0s - loss: 0.6530 - accuracy: 0.77 - ETA: 0s - loss: 0.6437 - accuracy: 0.77 - ETA: 0s - loss: 0.6384 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6356 - accuracy: 0.7798 - val_loss: 0.8805 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.87 - ETA: 0s - loss: 0.6019 - accuracy: 0.82 - ETA: 0s - loss: 0.5868 - accuracy: 0.80 - ETA: 0s - loss: 0.5866 - accuracy: 0.80 - ETA: 0s - loss: 0.5659 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5590 - accuracy: 0.8115 - val_loss: 0.7209 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.81 - ETA: 0s - loss: 0.4878 - accuracy: 0.83 - ETA: 0s - loss: 0.4888 - accuracy: 0.83 - ETA: 0s - loss: 0.5002 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5032 - accuracy: 0.8290 - val_loss: 0.6193 - val_accuracy: 0.7493\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.96 - ETA: 0s - loss: 0.4657 - accuracy: 0.84 - ETA: 0s - loss: 0.4763 - accuracy: 0.84 - ETA: 0s - loss: 0.4913 - accuracy: 0.83 - ETA: 0s - loss: 0.5004 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4984 - accuracy: 0.8343 - val_loss: 0.9293 - val_accuracy: 0.7269\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.81 - ETA: 0s - loss: 0.4914 - accuracy: 0.84 - ETA: 0s - loss: 0.4755 - accuracy: 0.84 - ETA: 0s - loss: 0.4889 - accuracy: 0.84 - ETA: 0s - loss: 0.4918 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4915 - accuracy: 0.8414 - val_loss: 0.7891 - val_accuracy: 0.7522\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.81 - ETA: 0s - loss: 0.4345 - accuracy: 0.86 - ETA: 0s - loss: 0.4455 - accuracy: 0.86 - ETA: 0s - loss: 0.4525 - accuracy: 0.85 - ETA: 0s - loss: 0.4602 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4616 - accuracy: 0.8507 - val_loss: 0.8707 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.90 - ETA: 0s - loss: 0.4487 - accuracy: 0.85 - ETA: 0s - loss: 0.4536 - accuracy: 0.86 - ETA: 0s - loss: 0.4585 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4481 - accuracy: 0.8582 - val_loss: 0.7607 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.81 - ETA: 0s - loss: 0.4456 - accuracy: 0.86 - ETA: 0s - loss: 0.4374 - accuracy: 0.87 - ETA: 0s - loss: 0.4342 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8682 - val_loss: 0.7758 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.78 - ETA: 0s - loss: 0.4168 - accuracy: 0.87 - ETA: 0s - loss: 0.4261 - accuracy: 0.87 - ETA: 0s - loss: 0.4086 - accuracy: 0.87 - ETA: 0s - loss: 0.4354 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8772 - val_loss: 0.8846 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3441 - accuracy: 0.93 - ETA: 0s - loss: 0.3950 - accuracy: 0.88 - ETA: 0s - loss: 0.4646 - accuracy: 0.86 - ETA: 0s - loss: 0.5333 - accuracy: 0.86 - ETA: 0s - loss: 0.4977 - accuracy: 0.8684Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.8690 - val_loss: 1.1516 - val_accuracy: 0.7448\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5910 - accuracy: 0.75 - ETA: 0s - loss: 1.4954 - accuracy: 0.65 - ETA: 0s - loss: 1.0930 - accuracy: 0.67 - ETA: 0s - loss: 0.9298 - accuracy: 0.68 - ETA: 0s - loss: 0.8806 - accuracy: 0.68 - 0s 5ms/step - loss: 0.8533 - accuracy: 0.6876 - val_loss: 0.5734 - val_accuracy: 0.7552\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1220 - accuracy: 0.62 - ETA: 0s - loss: 0.6683 - accuracy: 0.70 - ETA: 0s - loss: 0.6406 - accuracy: 0.71 - ETA: 0s - loss: 0.6280 - accuracy: 0.73 - ETA: 0s - loss: 0.6133 - accuracy: 0.74 - ETA: 0s - loss: 0.5944 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6024 - accuracy: 0.7525 - val_loss: 0.5555 - val_accuracy: 0.7463\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.68 - ETA: 0s - loss: 0.5336 - accuracy: 0.79 - ETA: 0s - loss: 0.5251 - accuracy: 0.80 - ETA: 0s - loss: 0.5313 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7932 - val_loss: 0.5778 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.81 - ETA: 0s - loss: 0.4902 - accuracy: 0.80 - ETA: 0s - loss: 0.4887 - accuracy: 0.81 - ETA: 0s - loss: 0.4867 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4941 - accuracy: 0.8208 - val_loss: 0.6971 - val_accuracy: 0.6761\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.75 - ETA: 0s - loss: 0.4741 - accuracy: 0.82 - ETA: 0s - loss: 0.4653 - accuracy: 0.83 - ETA: 0s - loss: 0.4654 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4850 - accuracy: 0.8242 - val_loss: 0.5693 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.90 - ETA: 0s - loss: 0.4776 - accuracy: 0.84 - ETA: 0s - loss: 0.4550 - accuracy: 0.85 - ETA: 0s - loss: 0.5181 - accuracy: 0.83 - ETA: 0s - loss: 0.5245 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5283 - accuracy: 0.8115 - val_loss: 0.5642 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5148 - accuracy: 0.84 - ETA: 0s - loss: 0.4835 - accuracy: 0.83 - ETA: 0s - loss: 0.4681 - accuracy: 0.83 - ETA: 0s - loss: 0.4724 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4849 - accuracy: 0.8339 - val_loss: 0.5905 - val_accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.90 - ETA: 0s - loss: 0.4602 - accuracy: 0.85 - ETA: 0s - loss: 0.4436 - accuracy: 0.85 - ETA: 0s - loss: 0.4592 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4866 - accuracy: 0.8443 - val_loss: 0.7309 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.87 - ETA: 0s - loss: 0.3858 - accuracy: 0.86 - ETA: 0s - loss: 0.4858 - accuracy: 0.85 - ETA: 0s - loss: 0.4869 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4879 - accuracy: 0.8507 - val_loss: 0.5496 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.84 - ETA: 0s - loss: 0.4222 - accuracy: 0.86 - ETA: 0s - loss: 0.4394 - accuracy: 0.85 - ETA: 0s - loss: 0.4531 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8559 - val_loss: 0.9145 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.3212 - accuracy: 0.90 - ETA: 0s - loss: 0.3975 - accuracy: 0.88 - ETA: 0s - loss: 0.4314 - accuracy: 0.86 - ETA: 0s - loss: 0.4478 - accuracy: 0.8592Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.8582 - val_loss: 0.9684 - val_accuracy: 0.7104\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d74032051361f6c133412f6be1395099</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7641791105270386</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7483283416535789</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8379 - accuracy: 0.78 - ETA: 0s - loss: 8.7132 - accuracy: 0.59 - ETA: 0s - loss: 6.1729 - accuracy: 0.61 - ETA: 0s - loss: 4.5320 - accuracy: 0.64 - ETA: 0s - loss: 3.5728 - accuracy: 0.59 - ETA: 0s - loss: 2.8859 - accuracy: 0.52 - 0s 5ms/step - loss: 2.8407 - accuracy: 0.5200 - val_loss: 0.6980 - val_accuracy: 0.3776\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.15 - ETA: 0s - loss: 0.7739 - accuracy: 0.67 - ETA: 0s - loss: 0.7413 - accuracy: 0.66 - ETA: 0s - loss: 0.7216 - accuracy: 0.53 - ETA: 0s - loss: 0.7165 - accuracy: 0.48 - 0s 3ms/step - loss: 0.7133 - accuracy: 0.4591 - val_loss: 0.6961 - val_accuracy: 0.3030\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7405 - accuracy: 0.37 - ETA: 0s - loss: 0.7031 - accuracy: 0.31 - ETA: 0s - loss: 0.7031 - accuracy: 0.31 - ETA: 0s - loss: 0.6982 - accuracy: 0.35 - ETA: 0s - loss: 0.6950 - accuracy: 0.45 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.4714 - val_loss: 0.6812 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.68 - ETA: 0s - loss: 0.6731 - accuracy: 0.72 - ETA: 0s - loss: 0.6897 - accuracy: 0.70 - ETA: 0s - loss: 0.6957 - accuracy: 0.61 - ETA: 0s - loss: 0.7292 - accuracy: 0.53 - 0s 3ms/step - loss: 0.7240 - accuracy: 0.5629 - val_loss: 0.6735 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.87 - ETA: 0s - loss: 0.6702 - accuracy: 0.72 - ETA: 0s - loss: 0.6939 - accuracy: 0.66 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.56 - 0s 3ms/step - loss: 0.6952 - accuracy: 0.5704 - val_loss: 0.6944 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7615 - accuracy: 0.40 - ETA: 0s - loss: 0.6745 - accuracy: 0.32 - ETA: 0s - loss: 0.6965 - accuracy: 0.41 - ETA: 0s - loss: 0.6896 - accuracy: 0.39 - ETA: 0s - loss: 0.6955 - accuracy: 0.46 - 0s 3ms/step - loss: 0.6954 - accuracy: 0.4550 - val_loss: 0.7067 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.18 - ETA: 0s - loss: 0.6929 - accuracy: 0.34 - ETA: 0s - loss: 0.6876 - accuracy: 0.33 - ETA: 0s - loss: 0.6898 - accuracy: 0.46 - ETA: 0s - loss: 0.6959 - accuracy: 0.47 - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4449 - val_loss: 0.7024 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.25 - ETA: 0s - loss: 0.7040 - accuracy: 0.31 - ETA: 0s - loss: 0.6969 - accuracy: 0.30 - ETA: 0s - loss: 0.6973 - accuracy: 0.43 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5192 - val_loss: 0.6869 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.75 - ETA: 0s - loss: 0.6840 - accuracy: 0.47 - ETA: 0s - loss: 0.6851 - accuracy: 0.59 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - 0s 3ms/step - loss: 0.6946 - accuracy: 0.5024 - val_loss: 0.7203 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7011 - accuracy: 0.31 - ETA: 0s - loss: 0.6963 - accuracy: 0.30 - ETA: 0s - loss: 0.6947 - accuracy: 0.39 - ETA: 0s - loss: 0.6871 - accuracy: 0.46 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - 0s 3ms/step - loss: 0.6942 - accuracy: 0.5144 - val_loss: 0.7140 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.21 - ETA: 0s - loss: 0.7019 - accuracy: 0.31 - ETA: 0s - loss: 0.6955 - accuracy: 0.40 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.48 - 0s 3ms/step - loss: 0.6942 - accuracy: 0.4591 - val_loss: 0.7029 - val_accuracy: 0.3045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.34 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6878 - accuracy: 0.60 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6952 - accuracy: 0.47 - 0s 3ms/step - loss: 0.6939 - accuracy: 0.4576 - val_loss: 0.7066 - val_accuracy: 0.3045\n",
      "Epoch 13/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.7547 - accuracy: 0.40 - ETA: 0s - loss: 0.6811 - accuracy: 0.46 - ETA: 0s - loss: 0.6854 - accuracy: 0.59 - ETA: 0s - loss: 0.6876 - accuracy: 0.63 - ETA: 0s - loss: 0.6941 - accuracy: 0.6186Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.6152 - val_loss: 0.7246 - val_accuracy: 0.3045\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7757 - accuracy: 0.71 - ETA: 0s - loss: 5.9992 - accuracy: 0.59 - ETA: 0s - loss: 5.4987 - accuracy: 0.62 - ETA: 0s - loss: 4.4211 - accuracy: 0.64 - ETA: 0s - loss: 3.5602 - accuracy: 0.60 - 0s 5ms/step - loss: 3.2278 - accuracy: 0.6144 - val_loss: 0.6867 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.68 - ETA: 0s - loss: 0.6911 - accuracy: 0.70 - ETA: 0s - loss: 0.6895 - accuracy: 0.70 - ETA: 0s - loss: 0.6864 - accuracy: 0.70 - ETA: 0s - loss: 0.6938 - accuracy: 0.68 - 0s 3ms/step - loss: 0.6940 - accuracy: 0.6342 - val_loss: 0.7190 - val_accuracy: 0.3045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.31 - ETA: 0s - loss: 0.6993 - accuracy: 0.30 - ETA: 0s - loss: 0.6965 - accuracy: 0.30 - ETA: 0s - loss: 0.6913 - accuracy: 0.41 - ETA: 0s - loss: 0.6931 - accuracy: 0.48 - 0s 3ms/step - loss: 0.6936 - accuracy: 0.4856 - val_loss: 0.6937 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7620 - accuracy: 0.40 - ETA: 0s - loss: 0.7006 - accuracy: 0.31 - ETA: 0s - loss: 0.6935 - accuracy: 0.36 - ETA: 0s - loss: 0.6897 - accuracy: 0.47 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - 0s 3ms/step - loss: 0.6940 - accuracy: 0.5099 - val_loss: 0.7063 - val_accuracy: 0.3045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.25 - ETA: 0s - loss: 0.6835 - accuracy: 0.31 - ETA: 0s - loss: 0.6875 - accuracy: 0.50 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - 0s 3ms/step - loss: 0.6936 - accuracy: 0.4912 - val_loss: 0.7126 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.18 - ETA: 0s - loss: 0.7029 - accuracy: 0.31 - ETA: 0s - loss: 0.6992 - accuracy: 0.30 - ETA: 0s - loss: 0.6900 - accuracy: 0.40 - ETA: 0s - loss: 0.6963 - accuracy: 0.46 - 0s 3ms/step - loss: 0.6949 - accuracy: 0.4457 - val_loss: 0.7029 - val_accuracy: 0.3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.28 - ETA: 0s - loss: 0.6905 - accuracy: 0.29 - ETA: 0s - loss: 0.6929 - accuracy: 0.47 - ETA: 0s - loss: 0.6948 - accuracy: 0.43 - ETA: 0s - loss: 0.6946 - accuracy: 0.43 - 0s 3ms/step - loss: 0.6939 - accuracy: 0.4856 - val_loss: 0.6843 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.84 - ETA: 0s - loss: 0.6873 - accuracy: 0.70 - ETA: 0s - loss: 0.6918 - accuracy: 0.66 - ETA: 0s - loss: 0.6904 - accuracy: 0.62 - ETA: 0s - loss: 0.6941 - accuracy: 0.59 - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5741 - val_loss: 0.7035 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.31 - ETA: 0s - loss: 0.6893 - accuracy: 0.40 - ETA: 0s - loss: 0.6976 - accuracy: 0.41 - ETA: 0s - loss: 0.7068 - accuracy: 0.40 - ETA: 0s - loss: 0.7020 - accuracy: 0.47 - 0s 3ms/step - loss: 0.7031 - accuracy: 0.4793 - val_loss: 0.6825 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6341 - accuracy: 0.78 - ETA: 0s - loss: 0.6948 - accuracy: 0.66 - ETA: 0s - loss: 0.6906 - accuracy: 0.56 - ETA: 0s - loss: 0.6954 - accuracy: 0.59 - ETA: 0s - loss: 0.6968 - accuracy: 0.53 - 0s 3ms/step - loss: 0.6943 - accuracy: 0.4979 - val_loss: 0.6934 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6803 - accuracy: 0.28 - ETA: 0s - loss: 0.6934 - accuracy: 0.68 - ETA: 0s - loss: 0.6872 - accuracy: 0.69 - ETA: 0s - loss: 0.6934 - accuracy: 0.68 - ETA: 0s - loss: 0.6949 - accuracy: 0.5995Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.5883 - val_loss: 0.7097 - val_accuracy: 0.3045\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7833 - accuracy: 0.65 - ETA: 0s - loss: 7.4724 - accuracy: 0.57 - ETA: 0s - loss: 6.8603 - accuracy: 0.62 - ETA: 0s - loss: 4.8722 - accuracy: 0.63 - ETA: 0s - loss: 3.7675 - accuracy: 0.56 - 0s 5ms/step - loss: 3.1637 - accuracy: 0.5539 - val_loss: 0.6742 - val_accuracy: 0.3687\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6253 - accuracy: 0.18 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.56 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.46 - 0s 3ms/step - loss: 0.6946 - accuracy: 0.4737 - val_loss: 0.6947 - val_accuracy: 0.3045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.31 - ETA: 0s - loss: 0.6961 - accuracy: 0.30 - ETA: 0s - loss: 0.6909 - accuracy: 0.41 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - 0s 3ms/step - loss: 0.6938 - accuracy: 0.4979 - val_loss: 0.7040 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.37 - ETA: 0s - loss: 0.6995 - accuracy: 0.30 - ETA: 0s - loss: 0.6982 - accuracy: 0.30 - ETA: 0s - loss: 0.6989 - accuracy: 0.31 - ETA: 0s - loss: 0.6958 - accuracy: 0.39 - 0s 4ms/step - loss: 0.6951 - accuracy: 0.4274 - val_loss: 0.6785 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.7009 - accuracy: 0.68 - ETA: 0s - loss: 0.6931 - accuracy: 0.48 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6935 - accuracy: 0.53 - 0s 3ms/step - loss: 0.6947 - accuracy: 0.5121 - val_loss: 0.7064 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.28 - ETA: 0s - loss: 0.7031 - accuracy: 0.31 - ETA: 0s - loss: 0.6908 - accuracy: 0.34 - ETA: 0s - loss: 0.6934 - accuracy: 0.46 - ETA: 0s - loss: 0.6958 - accuracy: 0.47 - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4517 - val_loss: 0.7059 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.28 - ETA: 0s - loss: 0.6974 - accuracy: 0.34 - ETA: 0s - loss: 0.6928 - accuracy: 0.40 - ETA: 0s - loss: 0.6903 - accuracy: 0.47 - ETA: 0s - loss: 0.6942 - accuracy: 0.48 - 0s 3ms/step - loss: 0.6939 - accuracy: 0.4748 - val_loss: 0.7029 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.40 - ETA: 0s - loss: 0.6814 - accuracy: 0.43 - ETA: 0s - loss: 0.6895 - accuracy: 0.57 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6942 - accuracy: 0.48 - 0s 3ms/step - loss: 0.6938 - accuracy: 0.4681 - val_loss: 0.6993 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.31 - ETA: 0s - loss: 0.6959 - accuracy: 0.30 - ETA: 0s - loss: 0.6931 - accuracy: 0.45 - ETA: 0s - loss: 0.6941 - accuracy: 0.47 - ETA: 0s - loss: 0.6928 - accuracy: 0.42 - 0s 3ms/step - loss: 0.6935 - accuracy: 0.4334 - val_loss: 0.6880 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.65 - ETA: 0s - loss: 0.6882 - accuracy: 0.70 - ETA: 0s - loss: 0.6975 - accuracy: 0.58 - ETA: 0s - loss: 0.6952 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.55 - 0s 3ms/step - loss: 0.6943 - accuracy: 0.5588 - val_loss: 0.6817 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.71 - ETA: 0s - loss: 0.7112 - accuracy: 0.51 - ETA: 0s - loss: 0.7036 - accuracy: 0.41 - ETA: 0s - loss: 0.6998 - accuracy: 0.46 - ETA: 0s - loss: 0.6979 - accuracy: 0.52 - 0s 3ms/step - loss: 0.6943 - accuracy: 0.5573 - val_loss: 0.6811 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.78 - ETA: 0s - loss: 0.6739 - accuracy: 0.72 - ETA: 0s - loss: 0.6786 - accuracy: 0.72 - ETA: 0s - loss: 0.6810 - accuracy: 0.66 - ETA: 0s - loss: 0.6894 - accuracy: 0.59 - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5659 - val_loss: 0.7221 - val_accuracy: 0.3045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6394 - accuracy: 0.18 - ETA: 0s - loss: 0.6957 - accuracy: 0.30 - ETA: 0s - loss: 0.6966 - accuracy: 0.42 - ETA: 0s - loss: 0.6991 - accuracy: 0.39 - ETA: 0s - loss: 0.6937 - accuracy: 0.43 - 0s 3ms/step - loss: 0.6949 - accuracy: 0.4476 - val_loss: 0.6765 - val_accuracy: 0.6970\n",
      "Epoch 14/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.7744 - accuracy: 0.59 - ETA: 0s - loss: 0.6955 - accuracy: 0.62 - ETA: 0s - loss: 0.6902 - accuracy: 0.57 - ETA: 0s - loss: 0.6930 - accuracy: 0.56 - ETA: 0s - loss: 0.6954 - accuracy: 0.5140Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.4972 - val_loss: 0.7099 - val_accuracy: 0.3045\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f592f9dd93c98e44bf91cd5b6b0d60f4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6960198879241943</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9522620931529241</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 105</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7457 - accuracy: 0.65 - ETA: 0s - loss: 12.9239 - accuracy: 0.619 - ETA: 0s - loss: 10.9215 - accuracy: 0.616 - ETA: 0s - loss: 9.2005 - accuracy: 0.618 - ETA: 0s - loss: 8.0811 - accuracy: 0.61 - ETA: 0s - loss: 6.9027 - accuracy: 0.59 - ETA: 0s - loss: 6.1068 - accuracy: 0.60 - 1s 6ms/step - loss: 6.0726 - accuracy: 0.6021 - val_loss: 0.6461 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.1259 - accuracy: 0.62 - ETA: 0s - loss: 1.6187 - accuracy: 0.61 - ETA: 0s - loss: 1.6720 - accuracy: 0.63 - ETA: 0s - loss: 1.6651 - accuracy: 0.61 - ETA: 0s - loss: 1.6168 - accuracy: 0.60 - ETA: 0s - loss: 1.5676 - accuracy: 0.59 - ETA: 0s - loss: 1.5063 - accuracy: 0.59 - 0s 4ms/step - loss: 1.5063 - accuracy: 0.5913 - val_loss: 0.7249 - val_accuracy: 0.3045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5923 - accuracy: 0.59 - ETA: 0s - loss: 0.9996 - accuracy: 0.57 - ETA: 0s - loss: 1.1328 - accuracy: 0.55 - ETA: 0s - loss: 1.1033 - accuracy: 0.54 - ETA: 0s - loss: 1.0501 - accuracy: 0.54 - ETA: 0s - loss: 0.9873 - accuracy: 0.53 - ETA: 0s - loss: 0.9487 - accuracy: 0.53 - 0s 4ms/step - loss: 0.9471 - accuracy: 0.5390 - val_loss: 0.7339 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8181 - accuracy: 0.53 - ETA: 0s - loss: 1.0105 - accuracy: 0.42 - ETA: 0s - loss: 0.9191 - accuracy: 0.45 - ETA: 0s - loss: 0.8512 - accuracy: 0.47 - ETA: 0s - loss: 0.8359 - accuracy: 0.49 - ETA: 0s - loss: 0.8123 - accuracy: 0.50 - 0s 4ms/step - loss: 0.8072 - accuracy: 0.5069 - val_loss: 0.7059 - val_accuracy: 0.3045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.43 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.7591 - accuracy: 0.52 - ETA: 0s - loss: 0.7683 - accuracy: 0.50 - ETA: 0s - loss: 0.8004 - accuracy: 0.50 - ETA: 0s - loss: 0.9240 - accuracy: 0.49 - 0s 4ms/step - loss: 0.9349 - accuracy: 0.4987 - val_loss: 0.7019 - val_accuracy: 0.3075\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.59 - ETA: 0s - loss: 0.7595 - accuracy: 0.52 - ETA: 0s - loss: 0.8809 - accuracy: 0.48 - ETA: 0s - loss: 0.8271 - accuracy: 0.45 - ETA: 0s - loss: 0.7883 - accuracy: 0.52 - ETA: 0s - loss: 0.8232 - accuracy: 0.55 - 0s 5ms/step - loss: 0.8060 - accuracy: 0.5808 - val_loss: 0.6750 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.84 - ETA: 0s - loss: 0.9447 - accuracy: 0.68 - ETA: 0s - loss: 0.8822 - accuracy: 0.68 - ETA: 0s - loss: 0.8109 - accuracy: 0.69 - ETA: 0s - loss: 0.8990 - accuracy: 0.71 - ETA: 0s - loss: 0.9280 - accuracy: 0.71 - 0s 4ms/step - loss: 0.9622 - accuracy: 0.7122 - val_loss: 0.6970 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.71 - ETA: 0s - loss: 0.7622 - accuracy: 0.66 - ETA: 0s - loss: 0.7590 - accuracy: 0.70 - ETA: 0s - loss: 0.7536 - accuracy: 0.72 - ETA: 0s - loss: 0.7434 - accuracy: 0.71 - ETA: 0s - loss: 0.7624 - accuracy: 0.71 - ETA: 0s - loss: 0.7453 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7440 - accuracy: 0.7144 - val_loss: 0.6650 - val_accuracy: 0.7164\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7695 - accuracy: 0.75 - ETA: 0s - loss: 0.6726 - accuracy: 0.75 - ETA: 0s - loss: 0.6488 - accuracy: 0.75 - ETA: 0s - loss: 0.6636 - accuracy: 0.75 - ETA: 0s - loss: 0.7114 - accuracy: 0.75 - ETA: 0s - loss: 0.8099 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8065 - accuracy: 0.7518 - val_loss: 0.6564 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.68 - ETA: 0s - loss: 0.6651 - accuracy: 0.75 - ETA: 0s - loss: 0.9295 - accuracy: 0.73 - ETA: 0s - loss: 0.8884 - accuracy: 0.72 - ETA: 0s - loss: 0.8469 - accuracy: 0.73 - ETA: 0s - loss: 0.8569 - accuracy: 0.73 - 0s 4ms/step - loss: 0.8839 - accuracy: 0.7324 - val_loss: 0.6609 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.71 - ETA: 0s - loss: 0.8423 - accuracy: 0.73 - ETA: 0s - loss: 0.8074 - accuracy: 0.73 - ETA: 0s - loss: 0.8777 - accuracy: 0.74 - ETA: 0s - loss: 0.9318 - accuracy: 0.73 - ETA: 0s - loss: 0.9456 - accuracy: 0.73 - 0s 4ms/step - loss: 0.9189 - accuracy: 0.7253 - val_loss: 0.6645 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.71 - ETA: 0s - loss: 0.7247 - accuracy: 0.69 - ETA: 0s - loss: 0.7094 - accuracy: 0.69 - ETA: 0s - loss: 0.7108 - accuracy: 0.70 - ETA: 0s - loss: 0.7012 - accuracy: 0.71 - ETA: 0s - loss: 0.7050 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7105 - accuracy: 0.7062 - val_loss: 0.6846 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.78 - ETA: 0s - loss: 0.7003 - accuracy: 0.70 - ETA: 0s - loss: 0.6809 - accuracy: 0.72 - ETA: 0s - loss: 0.7103 - accuracy: 0.71 - ETA: 0s - loss: 0.7064 - accuracy: 0.70 - ETA: 0s - loss: 0.7019 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7028 - accuracy: 0.7062 - val_loss: 0.6870 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.71 - ETA: 0s - loss: 0.6832 - accuracy: 0.71 - ETA: 0s - loss: 0.6777 - accuracy: 0.71 - ETA: 0s - loss: 0.6806 - accuracy: 0.71 - ETA: 0s - loss: 0.6839 - accuracy: 0.71 - ETA: 0s - loss: 0.6913 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6917 - accuracy: 0.6999 - val_loss: 0.6933 - val_accuracy: 0.3045\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7525 - accuracy: 0.25 - ETA: 0s - loss: 0.6969 - accuracy: 0.30 - ETA: 0s - loss: 0.6855 - accuracy: 0.35 - ETA: 0s - loss: 0.6912 - accuracy: 0.40 - ETA: 0s - loss: 0.6969 - accuracy: 0.37 - ETA: 0s - loss: 0.6941 - accuracy: 0.36 - 0s 4ms/step - loss: 0.6890 - accuracy: 0.3975 - val_loss: 0.6876 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.75 - ETA: 0s - loss: 0.7022 - accuracy: 0.68 - ETA: 0s - loss: 0.6990 - accuracy: 0.69 - ETA: 0s - loss: 0.6907 - accuracy: 0.70 - ETA: 0s - loss: 0.6843 - accuracy: 0.70 - ETA: 0s - loss: 0.6877 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6859 - accuracy: 0.7055 - val_loss: 0.6836 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6478 - accuracy: 0.75 - ETA: 0s - loss: 0.6713 - accuracy: 0.72 - ETA: 0s - loss: 0.6885 - accuracy: 0.70 - ETA: 0s - loss: 0.6894 - accuracy: 0.70 - ETA: 0s - loss: 0.6843 - accuracy: 0.70 - ETA: 0s - loss: 0.6855 - accuracy: 0.70 - ETA: 0s - loss: 0.6823 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6823 - accuracy: 0.7100 - val_loss: 0.6832 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.81 - ETA: 0s - loss: 0.6783 - accuracy: 0.72 - ETA: 0s - loss: 0.6937 - accuracy: 0.70 - ETA: 0s - loss: 0.6991 - accuracy: 0.69 - ETA: 0s - loss: 0.6950 - accuracy: 0.70 - ETA: 0s - loss: 0.6931 - accuracy: 0.70 - ETA: 0s - loss: 0.6872 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6876 - accuracy: 0.7115 - val_loss: 0.6795 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6779 - accuracy: 0.72 - ETA: 0s - loss: 0.6926 - accuracy: 0.70 - ETA: 0s - loss: 0.6825 - accuracy: 0.71 - ETA: 0s - loss: 0.6838 - accuracy: 0.71 - ETA: 0s - loss: 0.6902 - accuracy: 0.71 - ETA: 0s - loss: 0.6919 - accuracy: 0.7092Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.7096 - val_loss: 0.6830 - val_accuracy: 0.6955\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.4690 - accuracy: 0.53 - ETA: 0s - loss: 9.6781 - accuracy: 0.61 - ETA: 0s - loss: 14.0868 - accuracy: 0.622 - ETA: 0s - loss: 12.2759 - accuracy: 0.611 - ETA: 0s - loss: 10.7731 - accuracy: 0.602 - ETA: 0s - loss: 9.7154 - accuracy: 0.611 - ETA: 0s - loss: 8.9166 - accuracy: 0.60 - 0s 5ms/step - loss: 8.7734 - accuracy: 0.6058 - val_loss: 0.7693 - val_accuracy: 0.5134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7022 - accuracy: 0.65 - ETA: 0s - loss: 3.2537 - accuracy: 0.69 - ETA: 0s - loss: 3.0124 - accuracy: 0.63 - ETA: 0s - loss: 2.8500 - accuracy: 0.63 - ETA: 0s - loss: 2.5213 - accuracy: 0.59 - ETA: 0s - loss: 2.4016 - accuracy: 0.59 - 0s 4ms/step - loss: 2.2218 - accuracy: 0.5786 - val_loss: 0.7353 - val_accuracy: 0.3045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9509 - accuracy: 0.40 - ETA: 0s - loss: 1.2992 - accuracy: 0.55 - ETA: 0s - loss: 1.1040 - accuracy: 0.55 - ETA: 0s - loss: 0.9640 - accuracy: 0.55 - ETA: 0s - loss: 0.9629 - accuracy: 0.54 - ETA: 0s - loss: 0.9218 - accuracy: 0.55 - 0s 4ms/step - loss: 0.9272 - accuracy: 0.5461 - val_loss: 0.7554 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2600 - accuracy: 0.50 - ETA: 0s - loss: 0.7874 - accuracy: 0.55 - ETA: 0s - loss: 0.8304 - accuracy: 0.52 - ETA: 0s - loss: 0.7931 - accuracy: 0.52 - ETA: 0s - loss: 0.8216 - accuracy: 0.51 - ETA: 0s - loss: 0.8436 - accuracy: 0.50 - ETA: 0s - loss: 0.8647 - accuracy: 0.51 - 0s 4ms/step - loss: 0.8647 - accuracy: 0.5118 - val_loss: 0.7464 - val_accuracy: 0.3045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.53 - ETA: 0s - loss: 0.7106 - accuracy: 0.44 - ETA: 0s - loss: 0.7525 - accuracy: 0.46 - ETA: 0s - loss: 0.7506 - accuracy: 0.47 - ETA: 0s - loss: 0.8361 - accuracy: 0.46 - ETA: 0s - loss: 0.8146 - accuracy: 0.45 - 0s 4ms/step - loss: 0.8100 - accuracy: 0.4610 - val_loss: 0.6962 - val_accuracy: 0.3209\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.59 - ETA: 0s - loss: 0.7087 - accuracy: 0.49 - ETA: 0s - loss: 0.6869 - accuracy: 0.50 - ETA: 0s - loss: 0.6769 - accuracy: 0.54 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6627 - accuracy: 0.6185 - val_loss: 0.6486 - val_accuracy: 0.7299\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5402 - accuracy: 0.81 - ETA: 0s - loss: 1.4495 - accuracy: 0.66 - ETA: 0s - loss: 1.0548 - accuracy: 0.67 - ETA: 0s - loss: 0.9180 - accuracy: 0.69 - ETA: 0s - loss: 0.8585 - accuracy: 0.70 - ETA: 0s - loss: 0.8215 - accuracy: 0.71 - 0s 4ms/step - loss: 0.8177 - accuracy: 0.7107 - val_loss: 0.6311 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.59 - ETA: 0s - loss: 0.6834 - accuracy: 0.73 - ETA: 0s - loss: 0.7145 - accuracy: 0.73 - ETA: 0s - loss: 0.6733 - accuracy: 0.74 - ETA: 0s - loss: 0.8282 - accuracy: 0.73 - ETA: 0s - loss: 0.8562 - accuracy: 0.73 - 0s 5ms/step - loss: 0.8394 - accuracy: 0.7279 - val_loss: 0.6165 - val_accuracy: 0.7313\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7705 - accuracy: 0.75 - ETA: 0s - loss: 1.0587 - accuracy: 0.70 - ETA: 0s - loss: 0.8583 - accuracy: 0.71 - ETA: 0s - loss: 0.8121 - accuracy: 0.72 - ETA: 0s - loss: 0.7797 - accuracy: 0.71 - ETA: 0s - loss: 0.7608 - accuracy: 0.72 - 0s 4ms/step - loss: 0.7507 - accuracy: 0.7324 - val_loss: 0.6263 - val_accuracy: 0.7269\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.78 - ETA: 0s - loss: 0.8255 - accuracy: 0.73 - ETA: 0s - loss: 0.7649 - accuracy: 0.73 - ETA: 0s - loss: 0.7215 - accuracy: 0.74 - ETA: 0s - loss: 0.7465 - accuracy: 0.74 - ETA: 0s - loss: 0.7292 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7202 - accuracy: 0.7495 - val_loss: 0.6246 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.84 - ETA: 0s - loss: 0.7992 - accuracy: 0.75 - ETA: 0s - loss: 0.9495 - accuracy: 0.75 - ETA: 0s - loss: 1.0231 - accuracy: 0.75 - ETA: 0s - loss: 0.9425 - accuracy: 0.75 - ETA: 0s - loss: 0.9699 - accuracy: 0.75 - 0s 4ms/step - loss: 0.9597 - accuracy: 0.7604 - val_loss: 0.6180 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.78 - ETA: 0s - loss: 0.6535 - accuracy: 0.78 - ETA: 0s - loss: 0.6424 - accuracy: 0.77 - ETA: 0s - loss: 0.6498 - accuracy: 0.77 - ETA: 0s - loss: 0.6687 - accuracy: 0.75 - ETA: 0s - loss: 0.8777 - accuracy: 0.76 - 0s 4ms/step - loss: 0.8697 - accuracy: 0.7611 - val_loss: 0.6179 - val_accuracy: 0.7313\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.75 - ETA: 0s - loss: 0.7353 - accuracy: 0.74 - ETA: 0s - loss: 0.7600 - accuracy: 0.73 - ETA: 0s - loss: 0.7282 - accuracy: 0.74 - ETA: 0s - loss: 0.7029 - accuracy: 0.74 - ETA: 0s - loss: 0.6981 - accuracy: 0.75 - ETA: 0s - loss: 0.7042 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6999 - accuracy: 0.7592 - val_loss: 0.6374 - val_accuracy: 0.7284\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.75 - ETA: 0s - loss: 0.6518 - accuracy: 0.76 - ETA: 0s - loss: 0.6398 - accuracy: 0.76 - ETA: 0s - loss: 1.0083 - accuracy: 0.75 - ETA: 0s - loss: 0.9536 - accuracy: 0.75 - ETA: 0s - loss: 0.9135 - accuracy: 0.75 - ETA: 0s - loss: 0.8948 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8933 - accuracy: 0.7570 - val_loss: 0.6384 - val_accuracy: 0.7104\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5819 - accuracy: 0.81 - ETA: 0s - loss: 1.1421 - accuracy: 0.76 - ETA: 0s - loss: 1.2946 - accuracy: 0.75 - ETA: 0s - loss: 1.1466 - accuracy: 0.74 - ETA: 0s - loss: 1.0248 - accuracy: 0.74 - ETA: 0s - loss: 0.9739 - accuracy: 0.74 - ETA: 0s - loss: 0.9249 - accuracy: 0.74 - 0s 4ms/step - loss: 0.9283 - accuracy: 0.7436 - val_loss: 0.6524 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.75 - ETA: 0s - loss: 0.6435 - accuracy: 0.75 - ETA: 0s - loss: 0.6600 - accuracy: 0.75 - ETA: 0s - loss: 0.6525 - accuracy: 0.76 - ETA: 0s - loss: 0.6804 - accuracy: 0.75 - ETA: 0s - loss: 0.9003 - accuracy: 0.75 - ETA: 0s - loss: 0.8801 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8729 - accuracy: 0.7525 - val_loss: 0.6412 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.71 - ETA: 0s - loss: 0.6552 - accuracy: 0.78 - ETA: 0s - loss: 0.6447 - accuracy: 0.77 - ETA: 0s - loss: 0.6852 - accuracy: 0.76 - ETA: 0s - loss: 0.9190 - accuracy: 0.76 - ETA: 0s - loss: 1.1343 - accuracy: 0.76 - ETA: 0s - loss: 1.4011 - accuracy: 0.76 - 0s 4ms/step - loss: 1.3939 - accuracy: 0.7645 - val_loss: 0.6375 - val_accuracy: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.81 - ETA: 0s - loss: 0.8302 - accuracy: 0.76 - ETA: 0s - loss: 0.8509 - accuracy: 0.75 - ETA: 0s - loss: 0.8347 - accuracy: 0.74 - ETA: 0s - loss: 0.8659 - accuracy: 0.74 - ETA: 0s - loss: 0.8452 - accuracy: 0.74 - ETA: 0s - loss: 0.8288 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8201 - accuracy: 0.7529 - val_loss: 0.6248 - val_accuracy: 0.7313\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.84 - ETA: 0s - loss: 0.8900 - accuracy: 0.76 - ETA: 0s - loss: 0.7827 - accuracy: 0.75 - ETA: 0s - loss: 1.0224 - accuracy: 0.74 - ETA: 0s - loss: 0.9449 - accuracy: 0.74 - ETA: 0s - loss: 0.9169 - accuracy: 0.73 - ETA: 0s - loss: 0.9751 - accuracy: 0.73 - 0s 4ms/step - loss: 0.9659 - accuracy: 0.7327 - val_loss: 0.6630 - val_accuracy: 0.6970\n",
      "Epoch 20/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7327 - accuracy: 0.62 - ETA: 0s - loss: 0.8905 - accuracy: 0.67 - ETA: 0s - loss: 0.9150 - accuracy: 0.70 - ETA: 0s - loss: 0.9869 - accuracy: 0.70 - ETA: 0s - loss: 0.9542 - accuracy: 0.71 - ETA: 0s - loss: 0.8982 - accuracy: 0.72 - ETA: 0s - loss: 1.0775 - accuracy: 0.7191Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.0649 - accuracy: 0.7193 - val_loss: 0.6570 - val_accuracy: 0.7313\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1752 - accuracy: 0.59 - ETA: 0s - loss: 11.0975 - accuracy: 0.599 - ETA: 0s - loss: 11.0162 - accuracy: 0.589 - ETA: 0s - loss: 9.6273 - accuracy: 0.587 - ETA: 0s - loss: 8.2162 - accuracy: 0.57 - ETA: 0s - loss: 6.9987 - accuracy: 0.58 - ETA: 0s - loss: 6.3292 - accuracy: 0.58 - 0s 6ms/step - loss: 6.1167 - accuracy: 0.5831 - val_loss: 0.6770 - val_accuracy: 0.6104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 5.4002 - accuracy: 0.56 - ETA: 0s - loss: 2.1296 - accuracy: 0.61 - ETA: 0s - loss: 2.2564 - accuracy: 0.61 - ETA: 0s - loss: 2.0896 - accuracy: 0.60 - ETA: 0s - loss: 1.8797 - accuracy: 0.60 - ETA: 0s - loss: 1.7682 - accuracy: 0.60 - 0s 4ms/step - loss: 1.6342 - accuracy: 0.6043 - val_loss: 0.6886 - val_accuracy: 0.4761\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5349 - accuracy: 0.59 - ETA: 0s - loss: 1.1061 - accuracy: 0.57 - ETA: 0s - loss: 0.9614 - accuracy: 0.52 - ETA: 0s - loss: 0.9302 - accuracy: 0.52 - ETA: 0s - loss: 0.9007 - accuracy: 0.50 - ETA: 0s - loss: 0.9102 - accuracy: 0.50 - 0s 4ms/step - loss: 0.8854 - accuracy: 0.5084 - val_loss: 0.7140 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8977 - accuracy: 0.50 - ETA: 0s - loss: 0.8137 - accuracy: 0.49 - ETA: 0s - loss: 0.8665 - accuracy: 0.48 - ETA: 0s - loss: 0.8129 - accuracy: 0.49 - ETA: 0s - loss: 0.7706 - accuracy: 0.49 - ETA: 0s - loss: 0.7773 - accuracy: 0.53 - 0s 4ms/step - loss: 0.7763 - accuracy: 0.5330 - val_loss: 0.6557 - val_accuracy: 0.6940\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.9765 - accuracy: 0.71 - ETA: 0s - loss: 0.9069 - accuracy: 0.66 - ETA: 0s - loss: 0.8494 - accuracy: 0.67 - ETA: 0s - loss: 0.8048 - accuracy: 0.69 - ETA: 0s - loss: 0.7819 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7642 - accuracy: 0.6928 - val_loss: 0.6397 - val_accuracy: 0.7179\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.84 - ETA: 0s - loss: 0.6097 - accuracy: 0.75 - ETA: 0s - loss: 0.6303 - accuracy: 0.75 - ETA: 0s - loss: 0.6409 - accuracy: 0.75 - ETA: 0s - loss: 0.6364 - accuracy: 0.75 - ETA: 0s - loss: 0.6339 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6411 - accuracy: 0.7462 - val_loss: 0.6237 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6457 - accuracy: 0.75 - ETA: 0s - loss: 0.6829 - accuracy: 0.70 - ETA: 0s - loss: 0.6741 - accuracy: 0.70 - ETA: 0s - loss: 0.6552 - accuracy: 0.72 - ETA: 0s - loss: 0.6711 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6684 - accuracy: 0.7462 - val_loss: 0.6747 - val_accuracy: 0.6985\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.81 - ETA: 0s - loss: 0.6305 - accuracy: 0.75 - ETA: 0s - loss: 0.6357 - accuracy: 0.75 - ETA: 0s - loss: 0.6308 - accuracy: 0.75 - ETA: 0s - loss: 0.6261 - accuracy: 0.75 - ETA: 0s - loss: 0.6229 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6214 - accuracy: 0.7604 - val_loss: 0.6020 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.75 - ETA: 0s - loss: 0.5906 - accuracy: 0.77 - ETA: 0s - loss: 0.6032 - accuracy: 0.76 - ETA: 0s - loss: 0.5947 - accuracy: 0.77 - ETA: 0s - loss: 0.5862 - accuracy: 0.78 - ETA: 0s - loss: 0.5842 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5863 - accuracy: 0.7805 - val_loss: 0.6103 - val_accuracy: 0.7284\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.78 - ETA: 0s - loss: 0.5607 - accuracy: 0.80 - ETA: 0s - loss: 0.5714 - accuracy: 0.79 - ETA: 0s - loss: 0.5920 - accuracy: 0.79 - ETA: 0s - loss: 0.5984 - accuracy: 0.78 - ETA: 0s - loss: 0.6056 - accuracy: 0.77 - ETA: 0s - loss: 0.6794 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6794 - accuracy: 0.7671 - val_loss: 0.6132 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.78 - ETA: 0s - loss: 0.6284 - accuracy: 0.78 - ETA: 0s - loss: 0.6233 - accuracy: 0.79 - ETA: 0s - loss: 0.6341 - accuracy: 0.79 - ETA: 0s - loss: 0.7451 - accuracy: 0.77 - ETA: 0s - loss: 0.7274 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7358 - accuracy: 0.7704 - val_loss: 0.6346 - val_accuracy: 0.7164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.78 - ETA: 0s - loss: 0.6976 - accuracy: 0.73 - ETA: 0s - loss: 1.0651 - accuracy: 0.74 - ETA: 0s - loss: 1.1378 - accuracy: 0.73 - ETA: 0s - loss: 1.0462 - accuracy: 0.73 - ETA: 0s - loss: 1.1916 - accuracy: 0.72 - 0s 4ms/step - loss: 1.1226 - accuracy: 0.7339 - val_loss: 1.0854 - val_accuracy: 0.6821\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.9438 - accuracy: 0.68 - ETA: 0s - loss: 1.4504 - accuracy: 0.72 - ETA: 0s - loss: 1.1666 - accuracy: 0.72 - ETA: 0s - loss: 1.0791 - accuracy: 0.73 - ETA: 0s - loss: 0.9860 - accuracy: 0.73 - ETA: 0s - loss: 0.9787 - accuracy: 0.73 - 0s 4ms/step - loss: 0.9485 - accuracy: 0.7339 - val_loss: 0.6509 - val_accuracy: 0.7254\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6684 - accuracy: 0.71 - ETA: 0s - loss: 0.7481 - accuracy: 0.76 - ETA: 0s - loss: 0.9696 - accuracy: 0.75 - ETA: 0s - loss: 0.8822 - accuracy: 0.74 - ETA: 0s - loss: 1.2175 - accuracy: 0.75 - ETA: 0s - loss: 1.1210 - accuracy: 0.74 - ETA: 0s - loss: 1.0541 - accuracy: 0.74 - 0s 4ms/step - loss: 1.0541 - accuracy: 0.7417 - val_loss: 0.7205 - val_accuracy: 0.6925\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.68 - ETA: 0s - loss: 1.3256 - accuracy: 0.71 - ETA: 0s - loss: 1.4064 - accuracy: 0.72 - ETA: 0s - loss: 1.2234 - accuracy: 0.71 - ETA: 0s - loss: 1.2139 - accuracy: 0.72 - ETA: 0s - loss: 1.1182 - accuracy: 0.72 - 0s 4ms/step - loss: 1.0756 - accuracy: 0.7186 - val_loss: 0.6834 - val_accuracy: 0.7164\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.75 - ETA: 0s - loss: 0.8682 - accuracy: 0.76 - ETA: 0s - loss: 0.9688 - accuracy: 0.74 - ETA: 0s - loss: 1.2312 - accuracy: 0.72 - ETA: 0s - loss: 1.6994 - accuracy: 0.72 - ETA: 0s - loss: 1.6048 - accuracy: 0.71 - ETA: 0s - loss: 1.4742 - accuracy: 0.70 - 0s 4ms/step - loss: 1.4576 - accuracy: 0.7059 - val_loss: 0.6793 - val_accuracy: 0.3731\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9937 - accuracy: 0.65 - ETA: 0s - loss: 0.7711 - accuracy: 0.67 - ETA: 0s - loss: 0.9250 - accuracy: 0.70 - ETA: 0s - loss: 0.8891 - accuracy: 0.70 - ETA: 0s - loss: 0.9326 - accuracy: 0.65 - ETA: 0s - loss: 0.8851 - accuracy: 0.57 - 0s 4ms/step - loss: 0.8872 - accuracy: 0.5674 - val_loss: 0.6894 - val_accuracy: 0.3000\n",
      "Epoch 18/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.6223 - accuracy: 0.68 - ETA: 0s - loss: 0.7608 - accuracy: 0.64 - ETA: 0s - loss: 0.7417 - accuracy: 0.50 - ETA: 0s - loss: 0.7346 - accuracy: 0.42 - ETA: 0s - loss: 0.7310 - accuracy: 0.49 - ETA: 0s - loss: 0.7385 - accuracy: 0.5409Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7429 - accuracy: 0.5622 - val_loss: 0.6802 - val_accuracy: 0.7000\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 455a2395aed3942792a65bafdf1a7667</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7328358093897501</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9371730001013145</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 55</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.62 - ETA: 0s - loss: 1.1911 - accuracy: 0.63 - ETA: 0s - loss: 0.9106 - accuracy: 0.66 - ETA: 0s - loss: 0.7984 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7775 - accuracy: 0.6917 - val_loss: 0.5968 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.71 - ETA: 0s - loss: 0.5340 - accuracy: 0.73 - ETA: 0s - loss: 0.5380 - accuracy: 0.74 - ETA: 0s - loss: 0.5462 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7432 - val_loss: 0.5813 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7755 - accuracy: 0.65 - ETA: 0s - loss: 0.4508 - accuracy: 0.77 - ETA: 0s - loss: 0.4752 - accuracy: 0.79 - ETA: 0s - loss: 0.4836 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4844 - accuracy: 0.7902 - val_loss: 0.5776 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.71 - ETA: 0s - loss: 0.4076 - accuracy: 0.81 - ETA: 0s - loss: 0.4276 - accuracy: 0.81 - ETA: 0s - loss: 0.4461 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8040 - val_loss: 0.9496 - val_accuracy: 0.6627\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.87 - ETA: 0s - loss: 0.4978 - accuracy: 0.81 - ETA: 0s - loss: 0.4623 - accuracy: 0.81 - ETA: 0s - loss: 0.4461 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4427 - accuracy: 0.8238 - val_loss: 0.7642 - val_accuracy: 0.6910\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.90 - ETA: 0s - loss: 0.3598 - accuracy: 0.84 - ETA: 0s - loss: 0.3724 - accuracy: 0.84 - ETA: 0s - loss: 0.3769 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8447 - val_loss: 0.6696 - val_accuracy: 0.6701\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.93 - ETA: 0s - loss: 0.3261 - accuracy: 0.85 - ETA: 0s - loss: 0.3332 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3482 - accuracy: 0.8492 - val_loss: 0.6654 - val_accuracy: 0.7134\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.93 - ETA: 0s - loss: 0.3816 - accuracy: 0.87 - ETA: 0s - loss: 0.4165 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8451 - val_loss: 1.1662 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4632 - accuracy: 0.84 - ETA: 0s - loss: 0.3751 - accuracy: 0.86 - ETA: 0s - loss: 0.4716 - accuracy: 0.83 - ETA: 0s - loss: 0.5869 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5855 - accuracy: 0.8081 - val_loss: 0.7831 - val_accuracy: 0.7418\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6577 - accuracy: 0.71 - ETA: 0s - loss: 0.8702 - accuracy: 0.76 - ETA: 0s - loss: 0.8924 - accuracy: 0.75 - 0s 2ms/step - loss: 0.7747 - accuracy: 0.7768 - val_loss: 0.9694 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.87 - ETA: 0s - loss: 0.7408 - accuracy: 0.74 - ETA: 0s - loss: 0.8084 - accuracy: 0.68 - 0s 2ms/step - loss: 0.8020 - accuracy: 0.5842 - val_loss: 0.7116 - val_accuracy: 0.3045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.40 - ETA: 0s - loss: 0.6885 - accuracy: 0.30 - ETA: 0s - loss: 0.6919 - accuracy: 0.30 - ETA: 0s - loss: 0.6903 - accuracy: 0.30 - 0s 2ms/step - loss: 0.6902 - accuracy: 0.3035 - val_loss: 0.6932 - val_accuracy: 0.3060\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.28 - ETA: 0s - loss: 0.6883 - accuracy: 0.64 - ETA: 0s - loss: 0.6952 - accuracy: 0.55 - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5237 - val_loss: 0.7604 - val_accuracy: 0.7522\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.87 - ETA: 0s - loss: 0.6625 - accuracy: 0.78 - ETA: 0s - loss: 0.6677 - accuracy: 0.75 - ETA: 0s - loss: 0.6848 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6854 - accuracy: 0.7230 - val_loss: 0.6882 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.62 - ETA: 0s - loss: 0.6899 - accuracy: 0.37 - ETA: 0s - loss: 0.6759 - accuracy: 0.38 - 0s 2ms/step - loss: 0.6676 - accuracy: 0.5035 - val_loss: 0.6618 - val_accuracy: 0.7104\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5904 - accuracy: 0.81 - ETA: 0s - loss: 0.6802 - accuracy: 0.70 - ETA: 0s - loss: 0.6677 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6555 - accuracy: 0.7380 - val_loss: 0.8917 - val_accuracy: 0.7328\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.84 - ETA: 0s - loss: 0.6065 - accuracy: 0.73 - ETA: 0s - loss: 0.6446 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6622 - accuracy: 0.7361 - val_loss: 0.6481 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.81 - ETA: 0s - loss: 0.6431 - accuracy: 0.73 - ETA: 0s - loss: 0.6452 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6455 - accuracy: 0.7387 - val_loss: 0.6439 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.75 - ETA: 0s - loss: 0.6193 - accuracy: 0.76 - ETA: 0s - loss: 0.6197 - accuracy: 0.77 - ETA: 0s - loss: 0.6316 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6316 - accuracy: 0.7712 - val_loss: 0.6161 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6267 - accuracy: 0.75 - ETA: 0s - loss: 0.6146 - accuracy: 0.78 - ETA: 0s - loss: 0.6439 - accuracy: 0.74 - ETA: 0s - loss: 0.6573 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6560 - accuracy: 0.7376 - val_loss: 0.6536 - val_accuracy: 0.7269\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.75 - ETA: 0s - loss: 0.7021 - accuracy: 0.75 - ETA: 0s - loss: 0.7298 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6863 - accuracy: 0.7585 - val_loss: 0.6578 - val_accuracy: 0.7507\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.75 - ETA: 0s - loss: 0.5952 - accuracy: 0.78 - ETA: 0s - loss: 0.5887 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6083 - accuracy: 0.7954 - val_loss: 0.6637 - val_accuracy: 0.7358\n",
      "Epoch 23/50\n",
      "56/84 [===================>..........] - ETA: 0s - loss: 0.5923 - accuracy: 0.81 - ETA: 0s - loss: 0.5325 - accuracy: 0.81 - ETA: 0s - loss: 0.5699 - accuracy: 0.8119Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.8059 - val_loss: 0.6332 - val_accuracy: 0.7328\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0711 - accuracy: 0.56 - ETA: 0s - loss: 1.0576 - accuracy: 0.68 - ETA: 0s - loss: 0.8366 - accuracy: 0.68 - ETA: 0s - loss: 0.7523 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7320 - accuracy: 0.6973 - val_loss: 0.5849 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.84 - ETA: 0s - loss: 0.5238 - accuracy: 0.74 - ETA: 0s - loss: 0.5431 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7454 - val_loss: 0.5880 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.71 - ETA: 0s - loss: 0.4833 - accuracy: 0.77 - ETA: 0s - loss: 0.4788 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7913 - val_loss: 0.5694 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.90 - ETA: 0s - loss: 0.4614 - accuracy: 0.82 - ETA: 0s - loss: 0.4547 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8160 - val_loss: 0.9952 - val_accuracy: 0.5985\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.59 - ETA: 0s - loss: 0.3981 - accuracy: 0.85 - ETA: 0s - loss: 0.4342 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4687 - accuracy: 0.8219 - val_loss: 1.0848 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.90 - ETA: 0s - loss: 0.5734 - accuracy: 0.83 - ETA: 0s - loss: 0.5251 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5019 - accuracy: 0.8130 - val_loss: 0.7967 - val_accuracy: 0.7104\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.78 - ETA: 0s - loss: 0.3715 - accuracy: 0.85 - ETA: 0s - loss: 0.3890 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8455 - val_loss: 0.6713 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.90 - ETA: 0s - loss: 0.4238 - accuracy: 0.83 - ETA: 0s - loss: 0.3839 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8429 - val_loss: 0.8155 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.84 - ETA: 0s - loss: 0.2959 - accuracy: 0.87 - ETA: 0s - loss: 0.3372 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8511 - val_loss: 0.9511 - val_accuracy: 0.6194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 1.00 - ETA: 0s - loss: 0.3690 - accuracy: 0.85 - ETA: 0s - loss: 0.3723 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8574 - val_loss: 0.7152 - val_accuracy: 0.6776\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.75 - ETA: 0s - loss: 0.3303 - accuracy: 0.85 - ETA: 0s - loss: 0.2996 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8723 - val_loss: 2.1200 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.93 - ETA: 0s - loss: 0.5446 - accuracy: 0.83 - ETA: 0s - loss: 0.5035 - accuracy: 0.82 - ETA: 0s - loss: 0.4720 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4720 - accuracy: 0.8365 - val_loss: 1.1069 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.75 - ETA: 0s - loss: 0.2991 - accuracy: 0.88 - ETA: 0s - loss: 0.4575 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8384 - val_loss: 1.4949 - val_accuracy: 0.6627\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.78 - ETA: 0s - loss: 0.4682 - accuracy: 0.87 - ETA: 0s - loss: 0.3741 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8712 - val_loss: 1.7818 - val_accuracy: 0.7119\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.84 - ETA: 0s - loss: 0.4451 - accuracy: 0.85 - ETA: 0s - loss: 0.4224 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8623 - val_loss: 1.0442 - val_accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.87 - ETA: 0s - loss: 0.4875 - accuracy: 0.87 - ETA: 0s - loss: 0.5093 - accuracy: 0.86 - 0s 2ms/step - loss: 0.5274 - accuracy: 0.8611 - val_loss: 0.8583 - val_accuracy: 0.7179\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.93 - ETA: 0s - loss: 0.3668 - accuracy: 0.87 - ETA: 0s - loss: 0.3756 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4636 - accuracy: 0.8563 - val_loss: 0.5660 - val_accuracy: 0.7313\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.68 - ETA: 0s - loss: 0.5586 - accuracy: 0.81 - ETA: 0s - loss: 0.7236 - accuracy: 0.82 - 0s 2ms/step - loss: 0.6317 - accuracy: 0.8339 - val_loss: 0.7086 - val_accuracy: 0.7299\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.90 - ETA: 0s - loss: 0.4793 - accuracy: 0.87 - ETA: 0s - loss: 0.5032 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8384 - val_loss: 3.2104 - val_accuracy: 0.7030\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.75 - ETA: 0s - loss: 0.8604 - accuracy: 0.80 - ETA: 0s - loss: 0.8014 - accuracy: 0.75 - 0s 2ms/step - loss: 0.7392 - accuracy: 0.7413 - val_loss: 0.7715 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.59 - ETA: 0s - loss: 0.9796 - accuracy: 0.56 - ETA: 0s - loss: 0.8202 - accuracy: 0.53 - ETA: 0s - loss: 0.8682 - accuracy: 0.50 - 0s 2ms/step - loss: 0.8642 - accuracy: 0.5065 - val_loss: 0.7483 - val_accuracy: 0.3642\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.21 - ETA: 0s - loss: 0.7625 - accuracy: 0.35 - ETA: 0s - loss: 0.9760 - accuracy: 0.35 - ETA: 0s - loss: 0.8850 - accuracy: 0.34 - 0s 2ms/step - loss: 0.8850 - accuracy: 0.3438 - val_loss: 0.7007 - val_accuracy: 0.3104\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.21 - ETA: 0s - loss: 0.7035 - accuracy: 0.30 - ETA: 0s - loss: 0.6970 - accuracy: 0.43 - 0s 2ms/step - loss: 0.6958 - accuracy: 0.4520 - val_loss: 0.7523 - val_accuracy: 0.3224\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6108 - accuracy: 0.28 - ETA: 0s - loss: 0.6724 - accuracy: 0.36 - ETA: 0s - loss: 0.7548 - accuracy: 0.36 - 0s 2ms/step - loss: 1.1081 - accuracy: 0.3740 - val_loss: 1.9138 - val_accuracy: 0.3284\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7171 - accuracy: 0.40 - ETA: 0s - loss: 0.8688 - accuracy: 0.36 - ETA: 0s - loss: 0.7676 - accuracy: 0.35 - 0s 2ms/step - loss: 0.7393 - accuracy: 0.3595 - val_loss: 0.9400 - val_accuracy: 0.3373\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.43 - ETA: 0s - loss: 0.6588 - accuracy: 0.36 - ETA: 0s - loss: 0.6639 - accuracy: 0.36 - 0s 2ms/step - loss: 0.6640 - accuracy: 0.3643 - val_loss: 1.0546 - val_accuracy: 0.3418\n",
      "Epoch 27/50\n",
      "59/84 [====================>.........] - ETA: 0s - loss: 0.7501 - accuracy: 0.40 - ETA: 0s - loss: 0.6699 - accuracy: 0.39 - ETA: 0s - loss: 0.8399 - accuracy: 0.3697Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7937 - accuracy: 0.3434 - val_loss: 0.6962 - val_accuracy: 0.3045\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.9037 - accuracy: 0.65 - ETA: 0s - loss: 1.0563 - accuracy: 0.64 - ETA: 0s - loss: 0.8455 - accuracy: 0.67 - ETA: 0s - loss: 0.7688 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7406 - accuracy: 0.6932 - val_loss: 0.5504 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6678 - accuracy: 0.75 - ETA: 0s - loss: 0.5142 - accuracy: 0.76 - ETA: 0s - loss: 0.5078 - accuracy: 0.77 - ETA: 0s - loss: 0.5359 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7645 - val_loss: 0.5783 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.84 - ETA: 0s - loss: 0.4724 - accuracy: 0.78 - ETA: 0s - loss: 0.4869 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7768 - val_loss: 0.5488 - val_accuracy: 0.7433\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.93 - ETA: 0s - loss: 0.4236 - accuracy: 0.81 - ETA: 0s - loss: 0.4411 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8119 - val_loss: 0.5961 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.71 - ETA: 0s - loss: 0.4542 - accuracy: 0.80 - ETA: 0s - loss: 0.4805 - accuracy: 0.80 - ETA: 0s - loss: 0.5132 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7943 - val_loss: 0.8459 - val_accuracy: 0.7149\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.87 - ETA: 0s - loss: 0.4406 - accuracy: 0.82 - ETA: 0s - loss: 0.4448 - accuracy: 0.81 - ETA: 0s - loss: 0.4313 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8268 - val_loss: 1.4814 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.84 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - ETA: 0s - loss: 0.4217 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4996 - accuracy: 0.8163 - val_loss: 0.9632 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.81 - ETA: 0s - loss: 0.5275 - accuracy: 0.81 - ETA: 0s - loss: 0.4895 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5378 - accuracy: 0.8025 - val_loss: 0.5916 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.75 - ETA: 0s - loss: 0.5874 - accuracy: 0.81 - ETA: 0s - loss: 0.6004 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5902 - accuracy: 0.8197 - val_loss: 1.6455 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.84 - ETA: 0s - loss: 0.5209 - accuracy: 0.84 - ETA: 0s - loss: 0.5723 - accuracy: 0.82 - 0s 2ms/step - loss: 0.6055 - accuracy: 0.8010 - val_loss: 0.6148 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.81 - ETA: 0s - loss: 0.5581 - accuracy: 0.76 - ETA: 0s - loss: 0.5359 - accuracy: 0.77 - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7835 - val_loss: 0.7074 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.68 - ETA: 0s - loss: 0.5464 - accuracy: 0.76 - ETA: 0s - loss: 0.5514 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5975 - accuracy: 0.7936 - val_loss: 0.6318 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.87 - ETA: 0s - loss: 0.5369 - accuracy: 0.82 - ETA: 0s - loss: 0.5168 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5130 - accuracy: 0.8294 - val_loss: 0.8183 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7540 - accuracy: 0.71 - ETA: 0s - loss: 0.5066 - accuracy: 0.83 - ETA: 0s - loss: 0.5307 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5446 - accuracy: 0.8216 - val_loss: 0.6337 - val_accuracy: 0.7537\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.90 - ETA: 0s - loss: 0.4711 - accuracy: 0.85 - ETA: 0s - loss: 0.4962 - accuracy: 0.83 - ETA: 0s - loss: 0.4912 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4912 - accuracy: 0.8402 - val_loss: 0.6865 - val_accuracy: 0.7612\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.78 - ETA: 0s - loss: 0.4949 - accuracy: 0.83 - ETA: 0s - loss: 0.4997 - accuracy: 0.83 - ETA: 0s - loss: 0.4949 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4948 - accuracy: 0.8324 - val_loss: 0.6765 - val_accuracy: 0.7627\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.93 - ETA: 0s - loss: 0.6222 - accuracy: 0.83 - ETA: 0s - loss: 0.6575 - accuracy: 0.81 - 0s 2ms/step - loss: 0.6594 - accuracy: 0.8108 - val_loss: 0.6884 - val_accuracy: 0.7507\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5625 - accuracy: 0.81 - ETA: 0s - loss: 0.5991 - accuracy: 0.80 - ETA: 0s - loss: 0.5867 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5636 - accuracy: 0.8208 - val_loss: 0.8948 - val_accuracy: 0.7537\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.96 - ETA: 0s - loss: 0.4923 - accuracy: 0.85 - ETA: 0s - loss: 0.4855 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5045 - accuracy: 0.8421 - val_loss: 1.2373 - val_accuracy: 0.7239\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.87 - ETA: 0s - loss: 0.6142 - accuracy: 0.84 - ETA: 0s - loss: 0.5688 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5671 - accuracy: 0.8384 - val_loss: 0.6394 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.78 - ETA: 0s - loss: 0.5223 - accuracy: 0.83 - ETA: 0s - loss: 0.5338 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5338 - accuracy: 0.8350 - val_loss: 1.1419 - val_accuracy: 0.7284\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.87 - ETA: 0s - loss: 0.5934 - accuracy: 0.81 - ETA: 0s - loss: 0.5760 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5698 - accuracy: 0.8167 - val_loss: 0.6489 - val_accuracy: 0.7522\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.84 - ETA: 0s - loss: 0.5094 - accuracy: 0.83 - ETA: 0s - loss: 0.5044 - accuracy: 0.83 - ETA: 0s - loss: 0.5055 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5055 - accuracy: 0.8406 - val_loss: 0.7543 - val_accuracy: 0.7552\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4925 - accuracy: 0.87 - ETA: 0s - loss: 0.4759 - accuracy: 0.85 - ETA: 0s - loss: 0.4698 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5754 - accuracy: 0.8522 - val_loss: 0.7817 - val_accuracy: 0.7433\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.81 - ETA: 0s - loss: 0.6005 - accuracy: 0.84 - ETA: 0s - loss: 0.5768 - accuracy: 0.83 - 0s 2ms/step - loss: 0.6202 - accuracy: 0.8160 - val_loss: 0.6947 - val_accuracy: 0.7493\n",
      "Epoch 26/50\n",
      "59/84 [====================>.........] - ETA: 0s - loss: 0.7322 - accuracy: 0.68 - ETA: 0s - loss: 0.6111 - accuracy: 0.81 - ETA: 0s - loss: 0.6284 - accuracy: 0.7987Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.7977 - val_loss: 0.6035 - val_accuracy: 0.7299\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 6467ab320771d114d8119ccd948ffbd7</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.748756210009257</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.39584737176469387</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 240</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 155</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8804 - accuracy: 0.65 - ETA: 0s - loss: 0.9171 - accuracy: 0.64 - ETA: 0s - loss: 0.7753 - accuracy: 0.65 - ETA: 0s - loss: 0.7053 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6962 - accuracy: 0.6827 - val_loss: 0.5624 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5812 - accuracy: 0.81 - ETA: 0s - loss: 0.5436 - accuracy: 0.76 - ETA: 0s - loss: 0.5297 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7675 - val_loss: 0.6640 - val_accuracy: 0.6746\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.81 - ETA: 0s - loss: 0.5007 - accuracy: 0.76 - ETA: 0s - loss: 0.4795 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7876 - val_loss: 0.6673 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.84 - ETA: 0s - loss: 0.3979 - accuracy: 0.81 - ETA: 0s - loss: 0.4490 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8025 - val_loss: 0.8335 - val_accuracy: 0.6507\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.75 - ETA: 0s - loss: 0.4102 - accuracy: 0.81 - ETA: 0s - loss: 0.4231 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8178 - val_loss: 0.7364 - val_accuracy: 0.6806\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.75 - ETA: 0s - loss: 0.4470 - accuracy: 0.81 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8223 - val_loss: 0.7954 - val_accuracy: 0.6985\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.93 - ETA: 0s - loss: 0.3152 - accuracy: 0.85 - ETA: 0s - loss: 0.3399 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8492 - val_loss: 0.7042 - val_accuracy: 0.6746\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.84 - ETA: 0s - loss: 0.2889 - accuracy: 0.87 - ETA: 0s - loss: 0.3021 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8671 - val_loss: 0.7782 - val_accuracy: 0.7000\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.87 - ETA: 0s - loss: 0.2551 - accuracy: 0.88 - ETA: 0s - loss: 0.2745 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2708 - accuracy: 0.8809 - val_loss: 1.0421 - val_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.96 - ETA: 0s - loss: 0.2501 - accuracy: 0.91 - ETA: 0s - loss: 0.2564 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2552 - accuracy: 0.8970 - val_loss: 1.2672 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.1405 - accuracy: 0.87 - ETA: 0s - loss: 0.1875 - accuracy: 0.91 - ETA: 0s - loss: 0.2132 - accuracy: 0.9062Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9011 - val_loss: 1.1958 - val_accuracy: 0.6910\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7888 - accuracy: 0.65 - ETA: 0s - loss: 1.0228 - accuracy: 0.66 - ETA: 0s - loss: 0.8306 - accuracy: 0.68 - ETA: 0s - loss: 0.7428 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7434 - accuracy: 0.7047 - val_loss: 0.5806 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.84 - ETA: 0s - loss: 0.5376 - accuracy: 0.75 - ETA: 0s - loss: 0.5246 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7652 - val_loss: 0.5920 - val_accuracy: 0.6746\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.93 - ETA: 0s - loss: 0.4713 - accuracy: 0.79 - ETA: 0s - loss: 0.4749 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7727 - val_loss: 0.5877 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.81 - ETA: 0s - loss: 0.4341 - accuracy: 0.79 - ETA: 0s - loss: 0.4469 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8018 - val_loss: 0.6521 - val_accuracy: 0.6507\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.87 - ETA: 0s - loss: 0.3506 - accuracy: 0.84 - ETA: 0s - loss: 0.3818 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8414 - val_loss: 0.8954 - val_accuracy: 0.6433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.87 - ETA: 0s - loss: 0.4355 - accuracy: 0.84 - ETA: 0s - loss: 0.4016 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8361 - val_loss: 0.7921 - val_accuracy: 0.6881\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.93 - ETA: 0s - loss: 0.3701 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8432 - val_loss: 0.6289 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.90 - ETA: 0s - loss: 0.3319 - accuracy: 0.85 - ETA: 0s - loss: 0.3255 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8664 - val_loss: 0.7747 - val_accuracy: 0.7134\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.93 - ETA: 0s - loss: 0.2779 - accuracy: 0.87 - ETA: 0s - loss: 0.3013 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8522 - val_loss: 0.6182 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.78 - ETA: 0s - loss: 0.3921 - accuracy: 0.84 - ETA: 0s - loss: 0.3674 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8496 - val_loss: 1.0052 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.90 - ETA: 0s - loss: 0.3002 - accuracy: 0.87 - ETA: 0s - loss: 0.2996 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8820 - val_loss: 1.1430 - val_accuracy: 0.6925\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.78 - ETA: 0s - loss: 0.2485 - accuracy: 0.89 - ETA: 0s - loss: 0.2618 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8899 - val_loss: 0.8303 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.96 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9029 - val_loss: 0.9921 - val_accuracy: 0.6896\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.96 - ETA: 0s - loss: 0.2433 - accuracy: 0.90 - ETA: 0s - loss: 0.2911 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8973 - val_loss: 2.2567 - val_accuracy: 0.7224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.90 - ETA: 0s - loss: 0.3174 - accuracy: 0.85 - ETA: 0s - loss: 0.2887 - accuracy: 0.86 - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8690 - val_loss: 3.0115 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4585 - accuracy: 0.87 - ETA: 0s - loss: 0.2983 - accuracy: 0.88 - ETA: 0s - loss: 0.2883 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2916 - accuracy: 0.8906 - val_loss: 1.4292 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 1.00 - ETA: 0s - loss: 0.2598 - accuracy: 0.91 - ETA: 0s - loss: 0.2557 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8985 - val_loss: 2.9524 - val_accuracy: 0.7075\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.81 - ETA: 0s - loss: 0.3152 - accuracy: 0.87 - ETA: 0s - loss: 0.3818 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8869 - val_loss: 2.9966 - val_accuracy: 0.6731\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.75 - ETA: 0s - loss: 0.3999 - accuracy: 0.87 - ETA: 0s - loss: 0.3705 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8865 - val_loss: 4.2091 - val_accuracy: 0.7448\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.6782 - accuracy: 0.86 - ETA: 0s - loss: 0.6527 - accuracy: 0.86 - 0s 2ms/step - loss: 0.6276 - accuracy: 0.8619 - val_loss: 0.6352 - val_accuracy: 0.7269\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.84 - ETA: 0s - loss: 0.7139 - accuracy: 0.82 - ETA: 0s - loss: 0.8736 - accuracy: 0.84 - 0s 2ms/step - loss: 0.7666 - accuracy: 0.8455 - val_loss: 3.1252 - val_accuracy: 0.7194\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.87 - ETA: 0s - loss: 1.2049 - accuracy: 0.80 - ETA: 0s - loss: 1.0509 - accuracy: 0.74 - 0s 2ms/step - loss: 0.9486 - accuracy: 0.7402 - val_loss: 0.6306 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9535 - accuracy: 0.50 - ETA: 0s - loss: 0.7459 - accuracy: 0.67 - ETA: 0s - loss: 0.7172 - accuracy: 0.69 - 0s 2ms/step - loss: 0.7144 - accuracy: 0.6980 - val_loss: 0.6802 - val_accuracy: 0.6955\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6562 - accuracy: 0.75 - ETA: 0s - loss: 0.7016 - accuracy: 0.68 - ETA: 0s - loss: 0.6988 - accuracy: 0.51 - 0s 2ms/step - loss: 0.6960 - accuracy: 0.4879 - val_loss: 0.6897 - val_accuracy: 0.6955\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7222 - accuracy: 0.65 - ETA: 0s - loss: 0.7081 - accuracy: 0.69 - ETA: 0s - loss: 0.7012 - accuracy: 0.62 - 0s 2ms/step - loss: 0.7010 - accuracy: 0.5315 - val_loss: 0.6951 - val_accuracy: 0.3045\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.28 - ETA: 0s - loss: 0.7012 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.52 - 0s 2ms/step - loss: 0.7070 - accuracy: 0.5569 - val_loss: 0.6972 - val_accuracy: 0.3045\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.34 - ETA: 0s - loss: 0.6975 - accuracy: 0.30 - ETA: 0s - loss: 0.6923 - accuracy: 0.31 - 0s 2ms/step - loss: 0.7004 - accuracy: 0.3169 - val_loss: 0.6965 - val_accuracy: 0.3045\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.28 - ETA: 0s - loss: 0.6966 - accuracy: 0.30 - ETA: 0s - loss: 0.6938 - accuracy: 0.30 - 0s 2ms/step - loss: 0.6924 - accuracy: 0.3035 - val_loss: 0.6955 - val_accuracy: 0.3045\n",
      "Epoch 29/50\n",
      "60/84 [====================>.........] - ETA: 0s - loss: 0.6605 - accuracy: 0.25 - ETA: 0s - loss: 0.6912 - accuracy: 0.59 - ETA: 0s - loss: 0.6871 - accuracy: 0.6531Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.6614 - val_loss: 0.6924 - val_accuracy: 0.6955\n",
      "Epoch 00029: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7568 - accuracy: 0.65 - ETA: 0s - loss: 1.1842 - accuracy: 0.61 - ETA: 0s - loss: 0.8743 - accuracy: 0.67 - ETA: 0s - loss: 0.7791 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7667 - accuracy: 0.6879 - val_loss: 0.6490 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.65 - ETA: 0s - loss: 0.5295 - accuracy: 0.76 - ETA: 0s - loss: 0.5241 - accuracy: 0.76 - ETA: 0s - loss: 0.5251 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7548 - val_loss: 0.5869 - val_accuracy: 0.6672\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.87 - ETA: 0s - loss: 0.4384 - accuracy: 0.80 - ETA: 0s - loss: 0.4364 - accuracy: 0.80 - ETA: 0s - loss: 0.4627 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7977 - val_loss: 0.5503 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.71 - ETA: 0s - loss: 0.3921 - accuracy: 0.83 - ETA: 0s - loss: 0.4318 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4344 - accuracy: 0.8108 - val_loss: 0.5930 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.93 - ETA: 0s - loss: 0.3752 - accuracy: 0.84 - ETA: 0s - loss: 0.3878 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8287 - val_loss: 0.7273 - val_accuracy: 0.6627\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.84 - ETA: 0s - loss: 0.4241 - accuracy: 0.81 - ETA: 0s - loss: 0.4357 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8145 - val_loss: 0.6441 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.90 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8361 - val_loss: 0.6019 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.93 - ETA: 0s - loss: 0.3034 - accuracy: 0.86 - ETA: 0s - loss: 0.3527 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8563 - val_loss: 1.0465 - val_accuracy: 0.6806\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.84 - ETA: 0s - loss: 0.2879 - accuracy: 0.87 - ETA: 0s - loss: 0.2795 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8798 - val_loss: 0.9669 - val_accuracy: 0.6478\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.90 - ETA: 0s - loss: 0.2913 - accuracy: 0.87 - ETA: 0s - loss: 0.3048 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8794 - val_loss: 0.9404 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.93 - ETA: 0s - loss: 0.2736 - accuracy: 0.89 - ETA: 0s - loss: 0.5057 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5407 - accuracy: 0.8246 - val_loss: 0.6356 - val_accuracy: 0.7478\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5475 - accuracy: 0.75 - ETA: 0s - loss: 0.6331 - accuracy: 0.83 - ETA: 0s - loss: 0.7291 - accuracy: 0.77 - 0s 2ms/step - loss: 0.8193 - accuracy: 0.7443 - val_loss: 0.6540 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9339 - accuracy: 0.65 - ETA: 0s - loss: 0.7121 - accuracy: 0.70 - ETA: 0s - loss: 0.7071 - accuracy: 0.70 - 0s 2ms/step - loss: 0.7159 - accuracy: 0.6558 - val_loss: 0.6942 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.15 - ETA: 0s - loss: 0.7193 - accuracy: 0.57 - ETA: 0s - loss: 0.7188 - accuracy: 0.45 - 0s 2ms/step - loss: 0.7803 - accuracy: 0.4177 - val_loss: 0.6983 - val_accuracy: 0.3045\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.21 - ETA: 0s - loss: 0.6966 - accuracy: 0.59 - ETA: 0s - loss: 0.6954 - accuracy: 0.47 - 0s 2ms/step - loss: 0.6957 - accuracy: 0.4371 - val_loss: 0.6971 - val_accuracy: 0.3045\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.25 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6880 - accuracy: 0.63 - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5532 - val_loss: 0.6976 - val_accuracy: 0.3045\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7595 - accuracy: 0.40 - ETA: 0s - loss: 0.7090 - accuracy: 0.31 - ETA: 0s - loss: 0.7013 - accuracy: 0.30 - 0s 2ms/step - loss: 0.6989 - accuracy: 0.3109 - val_loss: 0.6930 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.87 - ETA: 0s - loss: 0.6896 - accuracy: 0.70 - ETA: 0s - loss: 0.6919 - accuracy: 0.63 - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5528 - val_loss: 0.6952 - val_accuracy: 0.3045\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.31 - ETA: 0s - loss: 0.6952 - accuracy: 0.30 - ETA: 0s - loss: 0.7052 - accuracy: 0.39 - 0s 2ms/step - loss: 0.7210 - accuracy: 0.4166 - val_loss: 0.7031 - val_accuracy: 0.3045\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.34 - ETA: 0s - loss: 0.6887 - accuracy: 0.29 - ETA: 0s - loss: 0.7185 - accuracy: 0.30 - 0s 2ms/step - loss: 0.7165 - accuracy: 0.3289 - val_loss: 1.1326 - val_accuracy: 0.3672\n",
      "Epoch 21/50\n",
      "61/84 [====================>.........] - ETA: 0s - loss: 0.6186 - accuracy: 0.40 - ETA: 0s - loss: 0.7528 - accuracy: 0.35 - ETA: 0s - loss: 0.7306 - accuracy: 0.3309Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7276 - accuracy: 0.3233 - val_loss: 0.6978 - val_accuracy: 0.3045\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: fdb8503780c2c9fcab6545286d944990</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7422885497411092</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.1505430678384716</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.59 - ETA: 0s - loss: 1.3629 - accuracy: 0.64 - ETA: 0s - loss: 1.0610 - accuracy: 0.63 - ETA: 0s - loss: 0.9369 - accuracy: 0.65 - 0s 4ms/step - loss: 0.9369 - accuracy: 0.6529 - val_loss: 0.5832 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6099 - accuracy: 0.75 - ETA: 0s - loss: 0.5986 - accuracy: 0.71 - ETA: 0s - loss: 0.5969 - accuracy: 0.71 - 0s 2ms/step - loss: 0.5847 - accuracy: 0.7279 - val_loss: 0.5630 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.81 - ETA: 0s - loss: 0.5886 - accuracy: 0.74 - ETA: 0s - loss: 0.5586 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5449 - accuracy: 0.7779 - val_loss: 0.5885 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.84 - ETA: 0s - loss: 0.4889 - accuracy: 0.81 - ETA: 0s - loss: 0.5061 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5131 - accuracy: 0.8059 - val_loss: 0.5817 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.81 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.4394 - accuracy: 0.81 - ETA: 0s - loss: 0.4444 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8052 - val_loss: 0.6474 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.80 - ETA: 0s - loss: 0.4498 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4587 - accuracy: 0.8175 - val_loss: 0.7962 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.90 - ETA: 0s - loss: 0.5121 - accuracy: 0.82 - ETA: 0s - loss: 0.5139 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5398 - accuracy: 0.8085 - val_loss: 0.8719 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4395 - accuracy: 0.84 - ETA: 0s - loss: 0.4602 - accuracy: 0.82 - ETA: 0s - loss: 0.4467 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4584 - accuracy: 0.8175 - val_loss: 0.6293 - val_accuracy: 0.7119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.93 - ETA: 0s - loss: 0.4172 - accuracy: 0.83 - ETA: 0s - loss: 0.4241 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8257 - val_loss: 0.8413 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.78 - ETA: 0s - loss: 0.4365 - accuracy: 0.80 - ETA: 0s - loss: 0.5190 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5199 - accuracy: 0.8122 - val_loss: 0.7650 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.75 - ETA: 0s - loss: 0.6139 - accuracy: 0.80 - ETA: 0s - loss: 0.5436 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5167 - accuracy: 0.8175 - val_loss: 0.9391 - val_accuracy: 0.7433\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.81 - ETA: 0s - loss: 0.5212 - accuracy: 0.83 - ETA: 0s - loss: 0.5330 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5078 - accuracy: 0.8264 - val_loss: 1.0157 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.87 - ETA: 0s - loss: 0.4063 - accuracy: 0.83 - ETA: 0s - loss: 0.4532 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8425 - val_loss: 0.9051 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.81 - ETA: 0s - loss: 0.4339 - accuracy: 0.84 - ETA: 0s - loss: 0.3822 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8436 - val_loss: 1.6576 - val_accuracy: 0.7448\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3018 - accuracy: 0.87 - ETA: 0s - loss: 0.4625 - accuracy: 0.84 - ETA: 0s - loss: 0.5309 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5576 - accuracy: 0.8346 - val_loss: 1.5031 - val_accuracy: 0.6657\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.78 - ETA: 0s - loss: 0.4772 - accuracy: 0.83 - ETA: 0s - loss: 0.4584 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4607 - accuracy: 0.8414 - val_loss: 0.6520 - val_accuracy: 0.7209\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 1.00 - ETA: 0s - loss: 0.7268 - accuracy: 0.87 - ETA: 0s - loss: 0.5703 - accuracy: 0.86 - ETA: 0s - loss: 0.5369 - accuracy: 0.86 - 0s 2ms/step - loss: 0.5333 - accuracy: 0.8615 - val_loss: 0.7066 - val_accuracy: 0.7119\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.87 - ETA: 0s - loss: 0.4442 - accuracy: 0.85 - ETA: 0s - loss: 0.4220 - accuracy: 0.86 - ETA: 0s - loss: 0.4262 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8514 - val_loss: 1.9889 - val_accuracy: 0.7224\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.93 - ETA: 0s - loss: 0.6057 - accuracy: 0.81 - ETA: 0s - loss: 0.5974 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5936 - accuracy: 0.8018 - val_loss: 0.6797 - val_accuracy: 0.7299\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6062 - accuracy: 0.78 - ETA: 0s - loss: 0.5704 - accuracy: 0.83 - ETA: 0s - loss: 0.5470 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5634 - accuracy: 0.8290 - val_loss: 1.1331 - val_accuracy: 0.7254\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.84 - ETA: 0s - loss: 0.4400 - accuracy: 0.85 - ETA: 0s - loss: 0.4280 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5013 - accuracy: 0.8492 - val_loss: 0.6592 - val_accuracy: 0.7478\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.81 - ETA: 0s - loss: 0.5403 - accuracy: 0.80 - ETA: 0s - loss: 0.5686 - accuracy: 0.81 - ETA: 0s - loss: 0.5414 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5414 - accuracy: 0.8246 - val_loss: 0.6465 - val_accuracy: 0.7299\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.87 - ETA: 0s - loss: 0.5107 - accuracy: 0.84 - ETA: 0s - loss: 0.5887 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5591 - accuracy: 0.8414 - val_loss: 1.2478 - val_accuracy: 0.7134\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4081 - accuracy: 0.87 - ETA: 0s - loss: 0.5990 - accuracy: 0.82 - ETA: 0s - loss: 0.5877 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5767 - accuracy: 0.8231 - val_loss: 0.6591 - val_accuracy: 0.7313\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.87 - ETA: 0s - loss: 0.5828 - accuracy: 0.83 - ETA: 0s - loss: 0.6470 - accuracy: 0.82 - 0s 2ms/step - loss: 0.6184 - accuracy: 0.8287 - val_loss: 0.9816 - val_accuracy: 0.7373\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.87 - ETA: 0s - loss: 0.5392 - accuracy: 0.81 - ETA: 0s - loss: 0.5233 - accuracy: 0.83 - 0s 2ms/step - loss: 0.6511 - accuracy: 0.8328 - val_loss: 0.9144 - val_accuracy: 0.7343\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.87 - ETA: 0s - loss: 0.6439 - accuracy: 0.76 - ETA: 0s - loss: 0.6395 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6266 - accuracy: 0.7757 - val_loss: 1.0615 - val_accuracy: 0.7373\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.75 - ETA: 0s - loss: 0.5495 - accuracy: 0.82 - ETA: 0s - loss: 0.5603 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5809 - accuracy: 0.7984 - val_loss: 0.7447 - val_accuracy: 0.7418\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.71 - ETA: 0s - loss: 0.5959 - accuracy: 0.79 - ETA: 0s - loss: 0.5667 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7984 - val_loss: 0.7571 - val_accuracy: 0.7537\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.81 - ETA: 0s - loss: 0.5754 - accuracy: 0.79 - ETA: 0s - loss: 0.5574 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5669 - accuracy: 0.8022 - val_loss: 0.7028 - val_accuracy: 0.7463\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.75 - ETA: 0s - loss: 0.5613 - accuracy: 0.82 - ETA: 0s - loss: 0.5424 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5382 - accuracy: 0.8268 - val_loss: 0.8067 - val_accuracy: 0.7478\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.84 - ETA: 0s - loss: 0.5229 - accuracy: 0.82 - ETA: 0s - loss: 0.5022 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8387 - val_loss: 1.2045 - val_accuracy: 0.7567\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.87 - ETA: 0s - loss: 0.4507 - accuracy: 0.85 - ETA: 0s - loss: 0.5077 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5426 - accuracy: 0.8499 - val_loss: 0.9407 - val_accuracy: 0.7373\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.87 - ETA: 0s - loss: 0.7175 - accuracy: 0.79 - ETA: 0s - loss: 0.7072 - accuracy: 0.75 - 0s 2ms/step - loss: 0.9661 - accuracy: 0.7436 - val_loss: 0.6631 - val_accuracy: 0.6955\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.65 - ETA: 0s - loss: 0.6999 - accuracy: 0.69 - ETA: 0s - loss: 0.6987 - accuracy: 0.56 - 0s 2ms/step - loss: 0.6939 - accuracy: 0.5390 - val_loss: 0.6872 - val_accuracy: 0.6955\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7447 - accuracy: 0.62 - ETA: 0s - loss: 0.7200 - accuracy: 0.57 - ETA: 0s - loss: 0.7049 - accuracy: 0.49 - 0s 2ms/step - loss: 0.7027 - accuracy: 0.5476 - val_loss: 0.6866 - val_accuracy: 0.6955\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7667 - accuracy: 0.59 - ETA: 0s - loss: 0.6911 - accuracy: 0.70 - ETA: 0s - loss: 0.6903 - accuracy: 0.51 - 0s 2ms/step - loss: 0.6940 - accuracy: 0.5644 - val_loss: 0.6936 - val_accuracy: 0.3045\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.43 - ETA: 0s - loss: 0.6901 - accuracy: 0.34 - ETA: 0s - loss: 0.6883 - accuracy: 0.47 - 0s 2ms/step - loss: 0.6926 - accuracy: 0.4811 - val_loss: 0.6969 - val_accuracy: 0.3045\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7006 - accuracy: 0.31 - ETA: 0s - loss: 0.6940 - accuracy: 0.32 - ETA: 0s - loss: 0.6950 - accuracy: 0.31 - 0s 2ms/step - loss: 0.6937 - accuracy: 0.3177 - val_loss: 0.6929 - val_accuracy: 0.6955\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.65 - ETA: 0s - loss: 0.7186 - accuracy: 0.37 - ETA: 0s - loss: 0.7015 - accuracy: 0.32 - 0s 2ms/step - loss: 0.6931 - accuracy: 0.3901 - val_loss: 0.6807 - val_accuracy: 0.6955\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.71 - ETA: 0s - loss: 0.7075 - accuracy: 0.64 - ETA: 0s - loss: 0.7013 - accuracy: 0.47 - 0s 2ms/step - loss: 0.7326 - accuracy: 0.4733 - val_loss: 0.6857 - val_accuracy: 0.6955\n",
      "Epoch 42/50\n",
      "60/84 [====================>.........] - ETA: 0s - loss: 0.7015 - accuracy: 0.68 - ETA: 0s - loss: 0.6819 - accuracy: 0.71 - ETA: 0s - loss: 0.6919 - accuracy: 0.7016Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5920 - val_loss: 0.6986 - val_accuracy: 0.3045\n",
      "Epoch 00042: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8631 - accuracy: 0.62 - ETA: 0s - loss: 1.3161 - accuracy: 0.61 - ETA: 0s - loss: 0.9805 - accuracy: 0.65 - ETA: 0s - loss: 0.8765 - accuracy: 0.65 - 0s 3ms/step - loss: 0.8659 - accuracy: 0.6603 - val_loss: 0.5759 - val_accuracy: 0.7418\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.75 - ETA: 0s - loss: 0.5999 - accuracy: 0.75 - ETA: 0s - loss: 0.6017 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5959 - accuracy: 0.7492 - val_loss: 0.5676 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.75 - ETA: 0s - loss: 0.5166 - accuracy: 0.76 - ETA: 0s - loss: 0.5427 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7555 - val_loss: 0.5582 - val_accuracy: 0.7433\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.96 - ETA: 0s - loss: 0.5053 - accuracy: 0.82 - ETA: 0s - loss: 0.5049 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5232 - accuracy: 0.8022 - val_loss: 0.7900 - val_accuracy: 0.6881\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.90 - ETA: 0s - loss: 0.5636 - accuracy: 0.77 - ETA: 0s - loss: 0.5669 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6051 - accuracy: 0.7656 - val_loss: 0.6616 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5168 - accuracy: 0.68 - ETA: 0s - loss: 0.5455 - accuracy: 0.80 - ETA: 0s - loss: 0.5231 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5112 - accuracy: 0.8141 - val_loss: 0.7102 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.96 - ETA: 0s - loss: 0.5068 - accuracy: 0.83 - ETA: 0s - loss: 0.4799 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4771 - accuracy: 0.8264 - val_loss: 0.6870 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.78 - ETA: 0s - loss: 0.4421 - accuracy: 0.82 - ETA: 0s - loss: 0.5060 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4875 - accuracy: 0.8324 - val_loss: 0.7577 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.84 - ETA: 0s - loss: 0.3983 - accuracy: 0.85 - ETA: 0s - loss: 0.4805 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5116 - accuracy: 0.8238 - val_loss: 0.7706 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.93 - ETA: 0s - loss: 0.4636 - accuracy: 0.84 - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4445 - accuracy: 0.8533 - val_loss: 0.7620 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.93 - ETA: 0s - loss: 0.5008 - accuracy: 0.85 - ETA: 0s - loss: 0.4192 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8723 - val_loss: 0.8311 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7447 - accuracy: 0.81 - ETA: 0s - loss: 0.3693 - accuracy: 0.88 - ETA: 0s - loss: 0.4801 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8406 - val_loss: 0.6851 - val_accuracy: 0.7433\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/84 [====================>.........] - ETA: 0s - loss: 0.5170 - accuracy: 0.81 - ETA: 0s - loss: 0.5751 - accuracy: 0.84 - ETA: 0s - loss: 0.5263 - accuracy: 0.8489Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8529 - val_loss: 1.8868 - val_accuracy: 0.7149\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9462 - accuracy: 0.56 - ETA: 0s - loss: 1.0972 - accuracy: 0.64 - ETA: 0s - loss: 0.9248 - accuracy: 0.64 - ETA: 0s - loss: 0.8150 - accuracy: 0.66 - 0s 4ms/step - loss: 0.8005 - accuracy: 0.6697 - val_loss: 0.5690 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.78 - ETA: 0s - loss: 0.5844 - accuracy: 0.74 - ETA: 0s - loss: 0.5710 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5932 - accuracy: 0.7368 - val_loss: 0.5877 - val_accuracy: 0.7254\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.75 - ETA: 0s - loss: 0.5720 - accuracy: 0.77 - ETA: 0s - loss: 0.5670 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5753 - accuracy: 0.7503 - val_loss: 0.5918 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.81 - ETA: 0s - loss: 0.5355 - accuracy: 0.78 - ETA: 0s - loss: 0.5244 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7940 - val_loss: 0.5861 - val_accuracy: 0.7104\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.81 - ETA: 0s - loss: 0.4897 - accuracy: 0.81 - ETA: 0s - loss: 0.4952 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4941 - accuracy: 0.8048 - val_loss: 0.5978 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - ETA: 0s - loss: 0.4936 - accuracy: 0.83 - ETA: 0s - loss: 0.4997 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4894 - accuracy: 0.8149 - val_loss: 1.2391 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.81 - ETA: 0s - loss: 0.4944 - accuracy: 0.82 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4825 - accuracy: 0.8149 - val_loss: 0.6589 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0265 - accuracy: 0.68 - ETA: 0s - loss: 0.5592 - accuracy: 0.79 - ETA: 0s - loss: 0.5088 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5066 - accuracy: 0.8141 - val_loss: 0.5829 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.71 - ETA: 0s - loss: 0.4868 - accuracy: 0.81 - ETA: 0s - loss: 0.4979 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5355 - accuracy: 0.8163 - val_loss: 0.5642 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.84 - ETA: 0s - loss: 0.6628 - accuracy: 0.80 - ETA: 0s - loss: 0.7196 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6558 - accuracy: 0.8044 - val_loss: 0.8248 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4944 - accuracy: 0.84 - ETA: 0s - loss: 0.4308 - accuracy: 0.86 - ETA: 0s - loss: 0.4536 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4709 - accuracy: 0.8466 - val_loss: 1.0906 - val_accuracy: 0.6328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.81 - ETA: 0s - loss: 0.5488 - accuracy: 0.81 - ETA: 0s - loss: 0.4708 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4685 - accuracy: 0.8473 - val_loss: 2.1526 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.93 - ETA: 0s - loss: 0.5345 - accuracy: 0.85 - ETA: 0s - loss: 0.5485 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5378 - accuracy: 0.8485 - val_loss: 0.6937 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.93 - ETA: 0s - loss: 0.4910 - accuracy: 0.85 - ETA: 0s - loss: 0.5424 - accuracy: 0.84 - 0s 2ms/step - loss: 0.6295 - accuracy: 0.8313 - val_loss: 0.9634 - val_accuracy: 0.6881\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.87 - ETA: 0s - loss: 0.5049 - accuracy: 0.85 - ETA: 0s - loss: 0.5130 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5189 - accuracy: 0.8503 - val_loss: 0.8423 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.93 - ETA: 0s - loss: 1.1720 - accuracy: 0.84 - ETA: 0s - loss: 0.9241 - accuracy: 0.82 - 0s 2ms/step - loss: 0.8181 - accuracy: 0.8309 - val_loss: 0.5971 - val_accuracy: 0.7657\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.81 - ETA: 0s - loss: 0.5454 - accuracy: 0.82 - ETA: 0s - loss: 0.5110 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8447 - val_loss: 0.8256 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.81 - ETA: 0s - loss: 0.4890 - accuracy: 0.84 - ETA: 0s - loss: 0.6576 - accuracy: 0.84 - 0s 2ms/step - loss: 0.6408 - accuracy: 0.8309 - val_loss: 0.9376 - val_accuracy: 0.7313\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.71 - ETA: 0s - loss: 0.5793 - accuracy: 0.81 - ETA: 0s - loss: 0.5894 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6091 - accuracy: 0.7947 - val_loss: 0.8424 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.81 - ETA: 0s - loss: 0.6151 - accuracy: 0.77 - ETA: 0s - loss: 0.6368 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6382 - accuracy: 0.7510 - val_loss: 0.6653 - val_accuracy: 0.7224\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5273 - accuracy: 0.84 - ETA: 0s - loss: 0.6391 - accuracy: 0.74 - ETA: 0s - loss: 0.6417 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6395 - accuracy: 0.7477 - val_loss: 0.8403 - val_accuracy: 0.7179\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.90 - ETA: 0s - loss: 0.6369 - accuracy: 0.76 - ETA: 0s - loss: 0.6706 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6644 - accuracy: 0.7492 - val_loss: 0.6782 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.75 - ETA: 0s - loss: 0.8447 - accuracy: 0.75 - ETA: 0s - loss: 0.7633 - accuracy: 0.74 - 0s 2ms/step - loss: 0.7466 - accuracy: 0.7286 - val_loss: 0.6751 - val_accuracy: 0.7030\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.68 - ETA: 0s - loss: 0.7478 - accuracy: 0.69 - ETA: 0s - loss: 0.8220 - accuracy: 0.63 - 0s 2ms/step - loss: 0.7823 - accuracy: 0.6573 - val_loss: 0.6799 - val_accuracy: 0.7000\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.75 - ETA: 0s - loss: 0.7213 - accuracy: 0.73 - ETA: 0s - loss: 0.7607 - accuracy: 0.71 - 0s 2ms/step - loss: 0.7429 - accuracy: 0.7148 - val_loss: 0.6832 - val_accuracy: 0.6955\n",
      "Epoch 26/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.5895 - accuracy: 0.84 - ETA: 0s - loss: 0.6910 - accuracy: 0.70 - ETA: 0s - loss: 0.6937 - accuracy: 0.6230Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5390 - val_loss: 0.6948 - val_accuracy: 0.3045\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 3625a89eb17bb3ba2e230e3e1ad31618</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7552238901456197</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7231006284254293</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 50</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7985 - accuracy: 0.62 - ETA: 0s - loss: 2.0639 - accuracy: 0.64 - ETA: 0s - loss: 1.8050 - accuracy: 0.62 - ETA: 0s - loss: 1.4408 - accuracy: 0.64 - ETA: 0s - loss: 1.2336 - accuracy: 0.65 - ETA: 0s - loss: 1.0954 - accuracy: 0.66 - 0s 5ms/step - loss: 1.0512 - accuracy: 0.6719 - val_loss: 0.5846 - val_accuracy: 0.7045\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8310 - accuracy: 0.78 - ETA: 0s - loss: 0.6005 - accuracy: 0.73 - ETA: 0s - loss: 0.6105 - accuracy: 0.74 - ETA: 0s - loss: 0.5994 - accuracy: 0.73 - ETA: 0s - loss: 0.6038 - accuracy: 0.73 - 0s 4ms/step - loss: 0.5962 - accuracy: 0.7346 - val_loss: 0.5759 - val_accuracy: 0.7254\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.84 - ETA: 0s - loss: 0.5245 - accuracy: 0.78 - ETA: 0s - loss: 0.5283 - accuracy: 0.78 - ETA: 0s - loss: 0.5334 - accuracy: 0.79 - ETA: 0s - loss: 0.5444 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5371 - accuracy: 0.7876 - val_loss: 0.6992 - val_accuracy: 0.6925\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.81 - ETA: 0s - loss: 0.4734 - accuracy: 0.82 - ETA: 0s - loss: 0.4816 - accuracy: 0.82 - ETA: 0s - loss: 0.4900 - accuracy: 0.82 - ETA: 0s - loss: 0.5384 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5400 - accuracy: 0.8063 - val_loss: 0.6273 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.87 - ETA: 0s - loss: 0.4778 - accuracy: 0.81 - ETA: 0s - loss: 0.5153 - accuracy: 0.81 - ETA: 0s - loss: 0.5153 - accuracy: 0.81 - ETA: 0s - loss: 0.5158 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5077 - accuracy: 0.8175 - val_loss: 0.6461 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.87 - ETA: 0s - loss: 0.4767 - accuracy: 0.84 - ETA: 0s - loss: 0.5193 - accuracy: 0.83 - ETA: 0s - loss: 0.5094 - accuracy: 0.83 - ETA: 0s - loss: 0.5120 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4971 - accuracy: 0.8358 - val_loss: 0.7128 - val_accuracy: 0.7209\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4774 - accuracy: 0.78 - ETA: 0s - loss: 0.4880 - accuracy: 0.81 - ETA: 0s - loss: 0.4681 - accuracy: 0.83 - ETA: 0s - loss: 0.4779 - accuracy: 0.82 - ETA: 0s - loss: 0.4814 - accuracy: 0.82 - ETA: 0s - loss: 0.4842 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4857 - accuracy: 0.8249 - val_loss: 0.6802 - val_accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.93 - ETA: 0s - loss: 0.3820 - accuracy: 0.87 - ETA: 0s - loss: 0.4298 - accuracy: 0.86 - ETA: 0s - loss: 0.4809 - accuracy: 0.85 - ETA: 0s - loss: 0.4890 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4873 - accuracy: 0.8429 - val_loss: 0.8120 - val_accuracy: 0.6851\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8796 - accuracy: 0.78 - ETA: 0s - loss: 0.5803 - accuracy: 0.81 - ETA: 0s - loss: 0.5324 - accuracy: 0.82 - ETA: 0s - loss: 0.5317 - accuracy: 0.82 - ETA: 0s - loss: 0.5171 - accuracy: 0.82 - ETA: 0s - loss: 0.5035 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5012 - accuracy: 0.8320 - val_loss: 0.8673 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7561 - accuracy: 0.75 - ETA: 0s - loss: 0.5236 - accuracy: 0.85 - ETA: 0s - loss: 0.4628 - accuracy: 0.85 - ETA: 0s - loss: 0.4744 - accuracy: 0.85 - ETA: 0s - loss: 0.4735 - accuracy: 0.85 - ETA: 0s - loss: 0.4723 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4702 - accuracy: 0.8511 - val_loss: 0.7201 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.96 - ETA: 0s - loss: 0.4166 - accuracy: 0.87 - ETA: 0s - loss: 0.4307 - accuracy: 0.86 - ETA: 0s - loss: 0.4355 - accuracy: 0.86 - ETA: 0s - loss: 0.4319 - accuracy: 0.86 - ETA: 0s - loss: 0.4410 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4405 - accuracy: 0.8585 - val_loss: 0.8590 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.93 - ETA: 0s - loss: 0.4065 - accuracy: 0.87 - ETA: 0s - loss: 0.4264 - accuracy: 0.86 - ETA: 0s - loss: 0.4356 - accuracy: 0.86 - ETA: 0s - loss: 0.4318 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4337 - accuracy: 0.8574 - val_loss: 0.6717 - val_accuracy: 0.7433\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.90 - ETA: 0s - loss: 0.4726 - accuracy: 0.85 - ETA: 0s - loss: 0.4332 - accuracy: 0.87 - ETA: 0s - loss: 0.4643 - accuracy: 0.86 - ETA: 0s - loss: 0.4600 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4685 - accuracy: 0.8589 - val_loss: 0.8390 - val_accuracy: 0.7343\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.93 - ETA: 0s - loss: 0.4521 - accuracy: 0.85 - ETA: 0s - loss: 0.4707 - accuracy: 0.84 - ETA: 0s - loss: 0.4648 - accuracy: 0.85 - ETA: 0s - loss: 0.4697 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4718 - accuracy: 0.8470 - val_loss: 0.8805 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.87 - ETA: 0s - loss: 0.4270 - accuracy: 0.86 - ETA: 0s - loss: 0.4149 - accuracy: 0.87 - ETA: 0s - loss: 0.4242 - accuracy: 0.87 - ETA: 0s - loss: 0.4193 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8731 - val_loss: 0.7285 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.84 - ETA: 0s - loss: 0.3728 - accuracy: 0.87 - ETA: 0s - loss: 0.3988 - accuracy: 0.87 - ETA: 0s - loss: 0.3868 - accuracy: 0.87 - ETA: 0s - loss: 0.3947 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8787 - val_loss: 0.9422 - val_accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.78 - ETA: 0s - loss: 0.4645 - accuracy: 0.87 - ETA: 0s - loss: 0.5011 - accuracy: 0.85 - ETA: 0s - loss: 0.4539 - accuracy: 0.87 - ETA: 0s - loss: 0.4776 - accuracy: 0.86 - ETA: 0s - loss: 0.4678 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4602 - accuracy: 0.8697 - val_loss: 0.8338 - val_accuracy: 0.7343\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.87 - ETA: 0s - loss: 0.4135 - accuracy: 0.85 - ETA: 0s - loss: 0.4195 - accuracy: 0.86 - ETA: 0s - loss: 0.4243 - accuracy: 0.86 - ETA: 0s - loss: 0.4264 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8638 - val_loss: 0.8890 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.84 - ETA: 0s - loss: 0.4344 - accuracy: 0.86 - ETA: 0s - loss: 0.4105 - accuracy: 0.87 - ETA: 0s - loss: 0.4179 - accuracy: 0.87 - ETA: 0s - loss: 0.4131 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8802 - val_loss: 0.8938 - val_accuracy: 0.7328\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.87 - ETA: 0s - loss: 0.3813 - accuracy: 0.88 - ETA: 0s - loss: 0.4072 - accuracy: 0.89 - ETA: 0s - loss: 0.4129 - accuracy: 0.88 - ETA: 0s - loss: 0.4347 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4366 - accuracy: 0.8768 - val_loss: 0.7650 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.78 - ETA: 0s - loss: 0.4046 - accuracy: 0.87 - ETA: 0s - loss: 0.3941 - accuracy: 0.88 - ETA: 0s - loss: 0.3920 - accuracy: 0.88 - ETA: 0s - loss: 0.4074 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4045 - accuracy: 0.8817 - val_loss: 0.7579 - val_accuracy: 0.7672\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8119 - accuracy: 0.71 - ETA: 0s - loss: 0.3935 - accuracy: 0.89 - ETA: 0s - loss: 0.4011 - accuracy: 0.88 - ETA: 0s - loss: 0.4109 - accuracy: 0.88 - ETA: 0s - loss: 0.4107 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8854 - val_loss: 0.9645 - val_accuracy: 0.7537\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.93 - ETA: 0s - loss: 0.3825 - accuracy: 0.89 - ETA: 0s - loss: 0.3779 - accuracy: 0.90 - ETA: 0s - loss: 0.4097 - accuracy: 0.89 - ETA: 0s - loss: 0.4096 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8858 - val_loss: 0.8387 - val_accuracy: 0.7537\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.87 - ETA: 0s - loss: 0.3723 - accuracy: 0.89 - ETA: 0s - loss: 0.3894 - accuracy: 0.89 - ETA: 0s - loss: 0.3988 - accuracy: 0.89 - ETA: 0s - loss: 0.3902 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8850 - val_loss: 1.0811 - val_accuracy: 0.7627\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.87 - ETA: 0s - loss: 0.4283 - accuracy: 0.89 - ETA: 0s - loss: 0.4357 - accuracy: 0.88 - ETA: 0s - loss: 0.4322 - accuracy: 0.88 - ETA: 0s - loss: 0.4167 - accuracy: 0.88 - ETA: 0s - loss: 0.4219 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8817 - val_loss: 0.7779 - val_accuracy: 0.7522\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.90 - ETA: 0s - loss: 0.4112 - accuracy: 0.88 - ETA: 0s - loss: 0.4148 - accuracy: 0.88 - ETA: 0s - loss: 0.3955 - accuracy: 0.88 - ETA: 0s - loss: 0.3947 - accuracy: 0.88 - ETA: 0s - loss: 0.3909 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8884 - val_loss: 1.2456 - val_accuracy: 0.7552\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.87 - ETA: 0s - loss: 0.4392 - accuracy: 0.89 - ETA: 0s - loss: 0.7343 - accuracy: 0.90 - ETA: 0s - loss: 0.6482 - accuracy: 0.88 - ETA: 0s - loss: 0.6113 - accuracy: 0.87 - ETA: 0s - loss: 0.5763 - accuracy: 0.87 - 0s 4ms/step - loss: 0.5734 - accuracy: 0.8779 - val_loss: 1.1579 - val_accuracy: 0.7433\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.87 - ETA: 0s - loss: 0.4735 - accuracy: 0.85 - ETA: 0s - loss: 0.4494 - accuracy: 0.86 - ETA: 0s - loss: 0.4434 - accuracy: 0.86 - ETA: 0s - loss: 0.4530 - accuracy: 0.86 - ETA: 0s - loss: 0.4458 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4464 - accuracy: 0.8660 - val_loss: 1.1380 - val_accuracy: 0.7642\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.93 - ETA: 0s - loss: 0.4435 - accuracy: 0.88 - ETA: 0s - loss: 0.4523 - accuracy: 0.87 - ETA: 0s - loss: 0.4862 - accuracy: 0.86 - ETA: 0s - loss: 0.4692 - accuracy: 0.87 - ETA: 0s - loss: 0.4750 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4733 - accuracy: 0.8652 - val_loss: 1.3668 - val_accuracy: 0.7537\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5452 - accuracy: 0.81 - ETA: 0s - loss: 0.5206 - accuracy: 0.87 - ETA: 0s - loss: 0.5247 - accuracy: 0.85 - ETA: 0s - loss: 0.5070 - accuracy: 0.85 - ETA: 0s - loss: 0.4892 - accuracy: 0.86 - ETA: 0s - loss: 0.4882 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4840 - accuracy: 0.8600 - val_loss: 0.8692 - val_accuracy: 0.7597\n",
      "Epoch 31/50\n",
      "68/84 [=======================>......] - ETA: 0s - loss: 0.3546 - accuracy: 0.87 - ETA: 0s - loss: 0.3885 - accuracy: 0.89 - ETA: 0s - loss: 0.4108 - accuracy: 0.89 - ETA: 0s - loss: 0.4272 - accuracy: 0.89 - ETA: 0s - loss: 0.4284 - accuracy: 0.8879Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8817 - val_loss: 0.5611 - val_accuracy: 0.7627\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9523 - accuracy: 0.40 - ETA: 0s - loss: 2.1581 - accuracy: 0.57 - ETA: 0s - loss: 1.5581 - accuracy: 0.59 - ETA: 0s - loss: 1.2557 - accuracy: 0.62 - ETA: 0s - loss: 1.1080 - accuracy: 0.66 - ETA: 0s - loss: 0.9972 - accuracy: 0.68 - 0s 5ms/step - loss: 0.9782 - accuracy: 0.6879 - val_loss: 0.6204 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6413 - accuracy: 0.78 - ETA: 0s - loss: 0.6065 - accuracy: 0.78 - ETA: 0s - loss: 0.6251 - accuracy: 0.74 - ETA: 0s - loss: 0.6178 - accuracy: 0.75 - ETA: 0s - loss: 0.6083 - accuracy: 0.75 - ETA: 0s - loss: 0.6083 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6083 - accuracy: 0.7540 - val_loss: 0.6409 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.75 - ETA: 0s - loss: 0.5706 - accuracy: 0.77 - ETA: 0s - loss: 0.6004 - accuracy: 0.74 - ETA: 0s - loss: 0.5747 - accuracy: 0.76 - ETA: 0s - loss: 0.5639 - accuracy: 0.77 - ETA: 0s - loss: 0.5611 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5624 - accuracy: 0.7704 - val_loss: 0.6019 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.78 - ETA: 0s - loss: 0.5258 - accuracy: 0.80 - ETA: 0s - loss: 0.5239 - accuracy: 0.80 - ETA: 0s - loss: 0.5201 - accuracy: 0.80 - ETA: 0s - loss: 0.5282 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5280 - accuracy: 0.7973 - val_loss: 0.6762 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.68 - ETA: 0s - loss: 0.4601 - accuracy: 0.82 - ETA: 0s - loss: 0.4854 - accuracy: 0.81 - ETA: 0s - loss: 0.4825 - accuracy: 0.82 - ETA: 0s - loss: 0.4933 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5034 - accuracy: 0.8201 - val_loss: 0.6841 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.81 - ETA: 0s - loss: 0.4976 - accuracy: 0.80 - ETA: 0s - loss: 0.4907 - accuracy: 0.81 - ETA: 0s - loss: 0.5093 - accuracy: 0.81 - ETA: 0s - loss: 0.4902 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4900 - accuracy: 0.8242 - val_loss: 0.6578 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4557 - accuracy: 0.81 - ETA: 0s - loss: 0.4720 - accuracy: 0.83 - ETA: 0s - loss: 0.4509 - accuracy: 0.84 - ETA: 0s - loss: 0.4628 - accuracy: 0.84 - ETA: 0s - loss: 0.4702 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4673 - accuracy: 0.8395 - val_loss: 0.8025 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.93 - ETA: 0s - loss: 0.4030 - accuracy: 0.87 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - ETA: 0s - loss: 0.4721 - accuracy: 0.84 - ETA: 0s - loss: 0.4812 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4803 - accuracy: 0.8425 - val_loss: 0.7672 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.90 - ETA: 0s - loss: 0.4542 - accuracy: 0.85 - ETA: 0s - loss: 0.4534 - accuracy: 0.85 - ETA: 0s - loss: 0.4387 - accuracy: 0.85 - ETA: 0s - loss: 0.4403 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4507 - accuracy: 0.8548 - val_loss: 0.6378 - val_accuracy: 0.7179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5997 - accuracy: 0.81 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4426 - accuracy: 0.85 - ETA: 0s - loss: 0.4379 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4582 - accuracy: 0.8511 - val_loss: 0.7698 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.81 - ETA: 0s - loss: 0.4493 - accuracy: 0.88 - ETA: 0s - loss: 0.4844 - accuracy: 0.85 - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.4943 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5004 - accuracy: 0.8447 - val_loss: 0.6136 - val_accuracy: 0.7448\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.87 - ETA: 0s - loss: 0.4535 - accuracy: 0.84 - ETA: 0s - loss: 0.4558 - accuracy: 0.85 - ETA: 0s - loss: 0.4860 - accuracy: 0.84 - ETA: 0s - loss: 0.5072 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4981 - accuracy: 0.8410 - val_loss: 0.8315 - val_accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.84 - ETA: 0s - loss: 0.4452 - accuracy: 0.86 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.4444 - accuracy: 0.85 - ETA: 0s - loss: 0.4588 - accuracy: 0.85 - ETA: 0s - loss: 0.4566 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4566 - accuracy: 0.8582 - val_loss: 0.7537 - val_accuracy: 0.7358\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.93 - ETA: 0s - loss: 0.4130 - accuracy: 0.88 - ETA: 0s - loss: 0.4349 - accuracy: 0.87 - ETA: 0s - loss: 0.4372 - accuracy: 0.87 - ETA: 0s - loss: 0.4512 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4543 - accuracy: 0.8671 - val_loss: 0.8437 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.93 - ETA: 0s - loss: 0.4643 - accuracy: 0.85 - ETA: 0s - loss: 0.4405 - accuracy: 0.86 - ETA: 0s - loss: 0.4785 - accuracy: 0.85 - ETA: 0s - loss: 0.4725 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5347 - accuracy: 0.8563 - val_loss: 0.6063 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.87 - ETA: 0s - loss: 0.5827 - accuracy: 0.87 - ETA: 0s - loss: 0.8563 - accuracy: 0.85 - ETA: 0s - loss: 0.7535 - accuracy: 0.84 - ETA: 0s - loss: 0.7108 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6740 - accuracy: 0.8387 - val_loss: 0.6224 - val_accuracy: 0.7418\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5855 - accuracy: 0.78 - ETA: 0s - loss: 0.4174 - accuracy: 0.86 - ETA: 0s - loss: 0.4221 - accuracy: 0.86 - ETA: 0s - loss: 0.4514 - accuracy: 0.86 - ETA: 0s - loss: 0.4626 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4619 - accuracy: 0.8548 - val_loss: 0.8063 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.87 - ETA: 0s - loss: 0.4232 - accuracy: 0.87 - ETA: 0s - loss: 0.4258 - accuracy: 0.87 - ETA: 0s - loss: 0.4600 - accuracy: 0.87 - ETA: 0s - loss: 0.4763 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4872 - accuracy: 0.8582 - val_loss: 0.5534 - val_accuracy: 0.7552\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.90 - ETA: 0s - loss: 0.5457 - accuracy: 0.83 - ETA: 0s - loss: 0.4924 - accuracy: 0.85 - ETA: 0s - loss: 0.4997 - accuracy: 0.84 - ETA: 0s - loss: 0.4949 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4909 - accuracy: 0.8425 - val_loss: 0.7715 - val_accuracy: 0.7254\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.87 - ETA: 0s - loss: 0.4897 - accuracy: 0.84 - ETA: 0s - loss: 0.4421 - accuracy: 0.86 - ETA: 0s - loss: 0.4493 - accuracy: 0.86 - ETA: 0s - loss: 0.4528 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8660 - val_loss: 0.7359 - val_accuracy: 0.7358\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.93 - ETA: 0s - loss: 0.3924 - accuracy: 0.87 - ETA: 0s - loss: 0.3987 - accuracy: 0.87 - ETA: 0s - loss: 0.4169 - accuracy: 0.87 - ETA: 0s - loss: 0.4244 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4418 - accuracy: 0.8660 - val_loss: 0.6441 - val_accuracy: 0.7463\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.84 - ETA: 0s - loss: 0.4287 - accuracy: 0.87 - ETA: 0s - loss: 0.4516 - accuracy: 0.86 - ETA: 0s - loss: 0.4413 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4549 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8679 - val_loss: 0.7384 - val_accuracy: 0.7597\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.87 - ETA: 0s - loss: 0.4256 - accuracy: 0.88 - ETA: 0s - loss: 0.4232 - accuracy: 0.88 - ETA: 0s - loss: 0.4121 - accuracy: 0.88 - ETA: 0s - loss: 0.4514 - accuracy: 0.88 - ETA: 0s - loss: 0.4804 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4827 - accuracy: 0.8746 - val_loss: 0.8457 - val_accuracy: 0.7567\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.87 - ETA: 0s - loss: 0.4882 - accuracy: 0.86 - ETA: 0s - loss: 0.5137 - accuracy: 0.87 - ETA: 0s - loss: 0.5646 - accuracy: 0.86 - ETA: 0s - loss: 0.5932 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6043 - accuracy: 0.8227 - val_loss: 0.7268 - val_accuracy: 0.7343\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.75 - ETA: 0s - loss: 0.7930 - accuracy: 0.73 - ETA: 0s - loss: 0.7016 - accuracy: 0.75 - ETA: 0s - loss: 0.6664 - accuracy: 0.77 - ETA: 0s - loss: 0.6687 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6666 - accuracy: 0.7540 - val_loss: 0.7410 - val_accuracy: 0.7104\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.62 - ETA: 0s - loss: 0.6299 - accuracy: 0.75 - ETA: 0s - loss: 0.6153 - accuracy: 0.77 - ETA: 0s - loss: 0.7321 - accuracy: 0.78 - ETA: 0s - loss: 0.7253 - accuracy: 0.77 - ETA: 0s - loss: 0.7091 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7094 - accuracy: 0.7645 - val_loss: 0.7043 - val_accuracy: 0.7403\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6272 - accuracy: 0.75 - ETA: 0s - loss: 0.6676 - accuracy: 0.77 - ETA: 0s - loss: 0.6599 - accuracy: 0.77 - ETA: 0s - loss: 0.6446 - accuracy: 0.76 - ETA: 0s - loss: 0.6552 - accuracy: 0.75 - ETA: 0s - loss: 0.7619 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7610 - accuracy: 0.7536 - val_loss: 0.6857 - val_accuracy: 0.7358\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.75 - ETA: 0s - loss: 0.6171 - accuracy: 0.75 - ETA: 0s - loss: 0.6341 - accuracy: 0.75 - ETA: 0s - loss: 0.6336 - accuracy: 0.75 - ETA: 0s - loss: 0.6297 - accuracy: 0.75 - ETA: 0s - loss: 0.6320 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6330 - accuracy: 0.7577 - val_loss: 0.6305 - val_accuracy: 0.7313\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.78 - ETA: 0s - loss: 0.6172 - accuracy: 0.75 - ETA: 0s - loss: 0.6359 - accuracy: 0.73 - ETA: 0s - loss: 0.6494 - accuracy: 0.75 - ETA: 0s - loss: 0.6456 - accuracy: 0.75 - ETA: 0s - loss: 0.6457 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6460 - accuracy: 0.7492 - val_loss: 0.6341 - val_accuracy: 0.7269\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.68 - ETA: 0s - loss: 0.6539 - accuracy: 0.73 - ETA: 0s - loss: 0.6527 - accuracy: 0.73 - ETA: 0s - loss: 0.6468 - accuracy: 0.73 - ETA: 0s - loss: 0.6358 - accuracy: 0.74 - ETA: 0s - loss: 0.6242 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6239 - accuracy: 0.7585 - val_loss: 0.9019 - val_accuracy: 0.7388\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.68 - ETA: 0s - loss: 0.6408 - accuracy: 0.74 - ETA: 0s - loss: 0.6298 - accuracy: 0.75 - ETA: 0s - loss: 0.6203 - accuracy: 0.76 - ETA: 0s - loss: 0.6126 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6165 - accuracy: 0.7678 - val_loss: 0.7698 - val_accuracy: 0.7403\n",
      "Epoch 32/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.7129 - accuracy: 0.65 - ETA: 0s - loss: 0.6465 - accuracy: 0.78 - ETA: 0s - loss: 0.6355 - accuracy: 0.76 - ETA: 0s - loss: 0.6332 - accuracy: 0.76 - ETA: 0s - loss: 0.6351 - accuracy: 0.7607Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.7585 - val_loss: 0.6380 - val_accuracy: 0.7179\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0430 - accuracy: 0.59 - ETA: 0s - loss: 1.8349 - accuracy: 0.62 - ETA: 0s - loss: 1.3034 - accuracy: 0.62 - ETA: 0s - loss: 1.0783 - accuracy: 0.64 - ETA: 0s - loss: 0.9586 - accuracy: 0.66 - ETA: 0s - loss: 0.9144 - accuracy: 0.68 - 0s 5ms/step - loss: 0.8941 - accuracy: 0.6850 - val_loss: 0.5932 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7189 - accuracy: 0.71 - ETA: 0s - loss: 0.5788 - accuracy: 0.72 - ETA: 0s - loss: 0.5671 - accuracy: 0.74 - ETA: 0s - loss: 0.5687 - accuracy: 0.74 - ETA: 0s - loss: 0.5802 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5771 - accuracy: 0.7402 - val_loss: 0.5808 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.90 - ETA: 0s - loss: 0.5063 - accuracy: 0.76 - ETA: 0s - loss: 0.5549 - accuracy: 0.74 - ETA: 0s - loss: 0.5666 - accuracy: 0.74 - ETA: 0s - loss: 0.5562 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5525 - accuracy: 0.7544 - val_loss: 0.5819 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.84 - ETA: 0s - loss: 0.5185 - accuracy: 0.81 - ETA: 0s - loss: 0.5103 - accuracy: 0.80 - ETA: 0s - loss: 0.5174 - accuracy: 0.80 - ETA: 0s - loss: 0.5100 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5131 - accuracy: 0.7981 - val_loss: 0.5757 - val_accuracy: 0.7284\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4116 - accuracy: 0.84 - ETA: 0s - loss: 0.4762 - accuracy: 0.82 - ETA: 0s - loss: 0.4527 - accuracy: 0.83 - ETA: 0s - loss: 0.4611 - accuracy: 0.83 - ETA: 0s - loss: 0.4674 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4786 - accuracy: 0.8257 - val_loss: 0.5766 - val_accuracy: 0.7164\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.87 - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - ETA: 0s - loss: 0.4377 - accuracy: 0.84 - ETA: 0s - loss: 0.4422 - accuracy: 0.84 - ETA: 0s - loss: 0.4662 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4659 - accuracy: 0.8302 - val_loss: 0.6673 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.81 - ETA: 0s - loss: 0.3953 - accuracy: 0.85 - ETA: 0s - loss: 0.4814 - accuracy: 0.84 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - ETA: 0s - loss: 0.5331 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5300 - accuracy: 0.8175 - val_loss: 0.7747 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.78 - ETA: 0s - loss: 0.5039 - accuracy: 0.81 - ETA: 0s - loss: 0.4498 - accuracy: 0.83 - ETA: 0s - loss: 0.4503 - accuracy: 0.83 - ETA: 0s - loss: 0.4602 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4630 - accuracy: 0.8331 - val_loss: 0.7626 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.87 - ETA: 0s - loss: 0.3919 - accuracy: 0.85 - ETA: 0s - loss: 0.3962 - accuracy: 0.85 - ETA: 0s - loss: 0.4141 - accuracy: 0.86 - ETA: 0s - loss: 0.4223 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4605 - accuracy: 0.8533 - val_loss: 0.8850 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.93 - ETA: 0s - loss: 0.3935 - accuracy: 0.87 - ETA: 0s - loss: 0.4088 - accuracy: 0.86 - ETA: 0s - loss: 0.4226 - accuracy: 0.85 - ETA: 0s - loss: 0.4315 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8518 - val_loss: 0.8258 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.84 - ETA: 0s - loss: 0.4122 - accuracy: 0.88 - ETA: 0s - loss: 0.4050 - accuracy: 0.88 - ETA: 0s - loss: 0.4098 - accuracy: 0.87 - ETA: 0s - loss: 0.4041 - accuracy: 0.87 - 0s 3ms/step - loss: 0.6298 - accuracy: 0.8738 - val_loss: 3.9751 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.90 - ETA: 0s - loss: 1.0445 - accuracy: 0.83 - ETA: 0s - loss: 0.8476 - accuracy: 0.81 - ETA: 0s - loss: 0.7329 - accuracy: 0.82 - ETA: 0s - loss: 0.6951 - accuracy: 0.82 - 0s 3ms/step - loss: 0.6865 - accuracy: 0.8320 - val_loss: 0.7027 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.84 - ETA: 0s - loss: 0.7434 - accuracy: 0.81 - ETA: 0s - loss: 0.6027 - accuracy: 0.84 - ETA: 0s - loss: 0.6016 - accuracy: 0.83 - ETA: 0s - loss: 0.5655 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5493 - accuracy: 0.8432 - val_loss: 0.8687 - val_accuracy: 0.7254\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.87 - ETA: 0s - loss: 0.4123 - accuracy: 0.87 - ETA: 0s - loss: 0.4442 - accuracy: 0.85 - ETA: 0s - loss: 0.4163 - accuracy: 0.86 - ETA: 0s - loss: 0.4198 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4286 - accuracy: 0.8638 - val_loss: 0.7527 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.87 - ETA: 0s - loss: 0.4381 - accuracy: 0.86 - ETA: 0s - loss: 0.4295 - accuracy: 0.86 - ETA: 0s - loss: 0.4589 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4617 - accuracy: 0.8645 - val_loss: 0.9080 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.90 - ETA: 0s - loss: 0.3932 - accuracy: 0.88 - ETA: 0s - loss: 0.3964 - accuracy: 0.88 - ETA: 0s - loss: 0.4075 - accuracy: 0.87 - ETA: 0s - loss: 0.3987 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8750 - val_loss: 0.9272 - val_accuracy: 0.7328\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.93 - ETA: 0s - loss: 0.4515 - accuracy: 0.88 - ETA: 0s - loss: 0.4471 - accuracy: 0.87 - ETA: 0s - loss: 0.4358 - accuracy: 0.87 - ETA: 0s - loss: 0.4402 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8723 - val_loss: 0.6995 - val_accuracy: 0.7418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.81 - ETA: 0s - loss: 0.3731 - accuracy: 0.89 - ETA: 0s - loss: 0.4824 - accuracy: 0.89 - ETA: 0s - loss: 0.4896 - accuracy: 0.87 - ETA: 0s - loss: 0.4595 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4554 - accuracy: 0.8750 - val_loss: 0.8327 - val_accuracy: 0.7224\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.90 - ETA: 0s - loss: 0.3966 - accuracy: 0.89 - ETA: 0s - loss: 0.4091 - accuracy: 0.88 - ETA: 0s - loss: 0.4404 - accuracy: 0.87 - ETA: 0s - loss: 0.4286 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4195 - accuracy: 0.8764 - val_loss: 0.9114 - val_accuracy: 0.7313\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.90 - ETA: 0s - loss: 0.3502 - accuracy: 0.89 - ETA: 0s - loss: 0.3712 - accuracy: 0.88 - ETA: 0s - loss: 0.3743 - accuracy: 0.89 - ETA: 0s - loss: 0.3811 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8880 - val_loss: 0.7512 - val_accuracy: 0.7343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.87 - ETA: 0s - loss: 0.2836 - accuracy: 0.93 - ETA: 0s - loss: 0.3192 - accuracy: 0.91 - ETA: 0s - loss: 0.3249 - accuracy: 0.91 - ETA: 0s - loss: 0.3395 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3472 - accuracy: 0.9067 - val_loss: 1.2781 - val_accuracy: 0.7254\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.93 - ETA: 0s - loss: 0.2899 - accuracy: 0.92 - ETA: 0s - loss: 0.3358 - accuracy: 0.91 - ETA: 0s - loss: 0.3808 - accuracy: 0.90 - ETA: 0s - loss: 0.3861 - accuracy: 0.90 - 0s 3ms/step - loss: 0.4501 - accuracy: 0.8843 - val_loss: 1.0867 - val_accuracy: 0.7254\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.90 - ETA: 0s - loss: 0.6921 - accuracy: 0.77 - ETA: 0s - loss: 0.7204 - accuracy: 0.75 - ETA: 0s - loss: 0.7092 - accuracy: 0.75 - ETA: 0s - loss: 0.7155 - accuracy: 0.76 - ETA: 0s - loss: 0.7045 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7005 - accuracy: 0.7686 - val_loss: 0.9004 - val_accuracy: 0.7254\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.78 - ETA: 0s - loss: 0.5660 - accuracy: 0.81 - ETA: 0s - loss: 0.5892 - accuracy: 0.80 - ETA: 0s - loss: 0.6111 - accuracy: 0.79 - ETA: 0s - loss: 0.6192 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6320 - accuracy: 0.7675 - val_loss: 0.7521 - val_accuracy: 0.7224\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.78 - ETA: 0s - loss: 0.7120 - accuracy: 0.72 - ETA: 0s - loss: 0.6963 - accuracy: 0.71 - ETA: 0s - loss: 0.6763 - accuracy: 0.72 - ETA: 0s - loss: 0.6613 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6623 - accuracy: 0.7409 - val_loss: 1.1199 - val_accuracy: 0.7358\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.81 - ETA: 0s - loss: 0.6121 - accuracy: 0.76 - ETA: 0s - loss: 0.7753 - accuracy: 0.76 - ETA: 0s - loss: 0.7214 - accuracy: 0.77 - ETA: 0s - loss: 0.6996 - accuracy: 0.76 - ETA: 0s - loss: 0.6823 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6811 - accuracy: 0.7667 - val_loss: 0.8485 - val_accuracy: 0.7358\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.93 - ETA: 0s - loss: 0.5751 - accuracy: 0.80 - ETA: 0s - loss: 0.5779 - accuracy: 0.79 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.5817 - accuracy: 0.79 - ETA: 0s - loss: 0.5813 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5814 - accuracy: 0.7969 - val_loss: 1.0299 - val_accuracy: 0.7507\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.87 - ETA: 0s - loss: 0.9177 - accuracy: 0.81 - ETA: 0s - loss: 0.7358 - accuracy: 0.81 - ETA: 0s - loss: 0.6629 - accuracy: 0.82 - ETA: 0s - loss: 0.6197 - accuracy: 0.82 - ETA: 0s - loss: 0.5926 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5917 - accuracy: 0.8305 - val_loss: 0.7719 - val_accuracy: 0.7597\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.87 - ETA: 0s - loss: 0.4636 - accuracy: 0.86 - ETA: 0s - loss: 0.4694 - accuracy: 0.86 - ETA: 0s - loss: 0.4662 - accuracy: 0.86 - ETA: 0s - loss: 0.4840 - accuracy: 0.85 - ETA: 0s - loss: 0.4840 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4843 - accuracy: 0.8511 - val_loss: 1.0002 - val_accuracy: 0.7463\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.93 - ETA: 0s - loss: 0.4340 - accuracy: 0.87 - ETA: 0s - loss: 0.4444 - accuracy: 0.87 - ETA: 0s - loss: 0.4678 - accuracy: 0.85 - ETA: 0s - loss: 0.4667 - accuracy: 0.85 - ETA: 0s - loss: 0.4696 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4800 - accuracy: 0.8533 - val_loss: 0.9740 - val_accuracy: 0.7418\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.71 - ETA: 0s - loss: 0.4508 - accuracy: 0.85 - ETA: 0s - loss: 0.4515 - accuracy: 0.86 - ETA: 0s - loss: 0.4493 - accuracy: 0.86 - ETA: 0s - loss: 0.4441 - accuracy: 0.86 - ETA: 0s - loss: 0.4482 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4521 - accuracy: 0.8664 - val_loss: 1.0468 - val_accuracy: 0.7537\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.87 - ETA: 0s - loss: 0.4959 - accuracy: 0.85 - ETA: 0s - loss: 0.4583 - accuracy: 0.86 - ETA: 0s - loss: 0.4641 - accuracy: 0.86 - ETA: 0s - loss: 0.4616 - accuracy: 0.86 - ETA: 0s - loss: 0.4774 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4774 - accuracy: 0.8582 - val_loss: 1.0518 - val_accuracy: 0.7418\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.81 - ETA: 0s - loss: 0.5130 - accuracy: 0.83 - ETA: 0s - loss: 0.5082 - accuracy: 0.83 - ETA: 0s - loss: 0.4848 - accuracy: 0.84 - ETA: 0s - loss: 0.4669 - accuracy: 0.85 - ETA: 0s - loss: 0.4670 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4686 - accuracy: 0.8555 - val_loss: 1.5961 - val_accuracy: 0.7358\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.93 - ETA: 0s - loss: 0.5593 - accuracy: 0.84 - ETA: 0s - loss: 0.5290 - accuracy: 0.83 - ETA: 0s - loss: 0.5008 - accuracy: 0.84 - ETA: 0s - loss: 0.4931 - accuracy: 0.85 - ETA: 0s - loss: 0.4760 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4810 - accuracy: 0.8596 - val_loss: 0.6625 - val_accuracy: 0.7403\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0446 - accuracy: 0.87 - ETA: 0s - loss: 0.5317 - accuracy: 0.86 - ETA: 0s - loss: 0.4878 - accuracy: 0.86 - ETA: 0s - loss: 0.5155 - accuracy: 0.86 - ETA: 0s - loss: 0.5006 - accuracy: 0.85 - ETA: 0s - loss: 0.4888 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4767 - accuracy: 0.8638 - val_loss: 1.4284 - val_accuracy: 0.7403\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.81 - ETA: 0s - loss: 0.5417 - accuracy: 0.84 - ETA: 0s - loss: 0.4868 - accuracy: 0.85 - ETA: 0s - loss: 0.4772 - accuracy: 0.86 - ETA: 0s - loss: 0.4551 - accuracy: 0.86 - ETA: 0s - loss: 0.4643 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4697 - accuracy: 0.8615 - val_loss: 0.9629 - val_accuracy: 0.7328\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.90 - ETA: 0s - loss: 0.4982 - accuracy: 0.84 - ETA: 0s - loss: 0.4795 - accuracy: 0.85 - ETA: 0s - loss: 0.4661 - accuracy: 0.86 - ETA: 0s - loss: 0.4666 - accuracy: 0.85 - ETA: 0s - loss: 0.4702 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4632 - accuracy: 0.8619 - val_loss: 1.4450 - val_accuracy: 0.7090\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.81 - ETA: 0s - loss: 0.6066 - accuracy: 0.86 - ETA: 0s - loss: 0.5599 - accuracy: 0.85 - ETA: 0s - loss: 0.5470 - accuracy: 0.84 - ETA: 0s - loss: 0.5369 - accuracy: 0.84 - ETA: 0s - loss: 0.5262 - accuracy: 0.8470Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5262 - accuracy: 0.8470 - val_loss: 3.1281 - val_accuracy: 0.7433\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b69a375adbf352c6790b20ff2128d808</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7621890505154928</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.550773413066413</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 65</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9715 - accuracy: 0.56 - ETA: 0s - loss: 2.6376 - accuracy: 0.61 - ETA: 0s - loss: 2.5631 - accuracy: 0.60 - ETA: 0s - loss: 2.2218 - accuracy: 0.62 - 0s 4ms/step - loss: 1.9607 - accuracy: 0.6334 - val_loss: 0.6123 - val_accuracy: 0.6940\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7533 - accuracy: 0.62 - ETA: 0s - loss: 0.7315 - accuracy: 0.70 - ETA: 0s - loss: 0.7016 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6972 - accuracy: 0.7103 - val_loss: 0.6419 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9643 - accuracy: 0.71 - ETA: 0s - loss: 0.6602 - accuracy: 0.73 - ETA: 0s - loss: 0.6679 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6752 - accuracy: 0.7230 - val_loss: 0.6420 - val_accuracy: 0.6925\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.68 - ETA: 0s - loss: 0.6761 - accuracy: 0.71 - ETA: 0s - loss: 0.6596 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6582 - accuracy: 0.7305 - val_loss: 0.6462 - val_accuracy: 0.6687\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.78 - ETA: 0s - loss: 0.7306 - accuracy: 0.69 - ETA: 0s - loss: 0.6847 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6838 - accuracy: 0.7242 - val_loss: 0.6385 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6260 - accuracy: 0.75 - ETA: 0s - loss: 0.6253 - accuracy: 0.77 - ETA: 0s - loss: 0.6386 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6519 - accuracy: 0.7484 - val_loss: 0.6488 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7087 - accuracy: 0.78 - ETA: 0s - loss: 0.6689 - accuracy: 0.73 - ETA: 0s - loss: 0.6486 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6390 - accuracy: 0.7540 - val_loss: 0.6224 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.75 - ETA: 0s - loss: 0.6261 - accuracy: 0.76 - ETA: 0s - loss: 0.6395 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6338 - accuracy: 0.7518 - val_loss: 0.6142 - val_accuracy: 0.7507\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.65 - ETA: 0s - loss: 0.6161 - accuracy: 0.76 - ETA: 0s - loss: 0.6258 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6230 - accuracy: 0.7600 - val_loss: 0.6117 - val_accuracy: 0.7284\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.75 - ETA: 0s - loss: 0.5894 - accuracy: 0.78 - ETA: 0s - loss: 0.5996 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7749 - val_loss: 0.5968 - val_accuracy: 0.7478\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.84 - ETA: 0s - loss: 0.5557 - accuracy: 0.80 - ETA: 0s - loss: 0.5658 - accuracy: 0.80 - ETA: 0s - loss: 0.5611 - accuracy: 0.80 - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5820 - accuracy: 0.7932 - val_loss: 0.5914 - val_accuracy: 0.7358\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.78 - ETA: 0s - loss: 0.5888 - accuracy: 0.78 - ETA: 0s - loss: 0.5862 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7902 - val_loss: 0.5814 - val_accuracy: 0.7493\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.84 - ETA: 0s - loss: 0.5763 - accuracy: 0.79 - ETA: 0s - loss: 0.5995 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5967 - accuracy: 0.7869 - val_loss: 0.5892 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.5642 - accuracy: 0.80 - ETA: 0s - loss: 0.5708 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5751 - accuracy: 0.7981 - val_loss: 0.6000 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9739 - accuracy: 0.56 - ETA: 0s - loss: 0.5963 - accuracy: 0.79 - ETA: 0s - loss: 0.5800 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5816 - accuracy: 0.7936 - val_loss: 0.5851 - val_accuracy: 0.7358\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.71 - ETA: 0s - loss: 0.5474 - accuracy: 0.80 - ETA: 0s - loss: 0.6028 - accuracy: 0.81 - ETA: 0s - loss: 0.5937 - accuracy: 0.80 - ETA: 0s - loss: 0.5861 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5831 - accuracy: 0.8066 - val_loss: 0.5876 - val_accuracy: 0.7313\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.81 - ETA: 0s - loss: 0.6184 - accuracy: 0.82 - ETA: 0s - loss: 0.6279 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6267 - accuracy: 0.8033 - val_loss: 0.6032 - val_accuracy: 0.7119\n",
      "Epoch 18/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.5287 - accuracy: 0.81 - ETA: 0s - loss: 0.6289 - accuracy: 0.78 - ETA: 0s - loss: 0.6345 - accuracy: 0.7878Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.7925 - val_loss: 0.6087 - val_accuracy: 0.7269\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4735 - accuracy: 0.46 - ETA: 0s - loss: 3.5874 - accuracy: 0.57 - ETA: 0s - loss: 2.5611 - accuracy: 0.60 - ETA: 0s - loss: 2.0109 - accuracy: 0.62 - 0s 4ms/step - loss: 1.9824 - accuracy: 0.6230 - val_loss: 0.5737 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0140 - accuracy: 0.50 - ETA: 0s - loss: 0.7476 - accuracy: 0.66 - ETA: 0s - loss: 0.7536 - accuracy: 0.67 - 0s 2ms/step - loss: 0.7401 - accuracy: 0.6857 - val_loss: 0.6321 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.75 - ETA: 0s - loss: 0.6806 - accuracy: 0.70 - ETA: 0s - loss: 0.6722 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6757 - accuracy: 0.7137 - val_loss: 0.6389 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.71 - ETA: 0s - loss: 0.6716 - accuracy: 0.71 - ETA: 0s - loss: 0.6687 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6680 - accuracy: 0.7238 - val_loss: 0.6389 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.68 - ETA: 0s - loss: 0.6500 - accuracy: 0.72 - ETA: 0s - loss: 0.6516 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6579 - accuracy: 0.7268 - val_loss: 0.6347 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7537 - accuracy: 0.65 - ETA: 0s - loss: 0.6189 - accuracy: 0.75 - ETA: 0s - loss: 0.6564 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6573 - accuracy: 0.7469 - val_loss: 0.6459 - val_accuracy: 0.6537\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.75 - ETA: 0s - loss: 0.6473 - accuracy: 0.75 - ETA: 0s - loss: 0.6446 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6401 - accuracy: 0.7596 - val_loss: 0.5495 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.7731 - accuracy: 0.68 - ETA: 0s - loss: 0.6806 - accuracy: 0.75 - ETA: 0s - loss: 0.6499 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6441 - accuracy: 0.7633 - val_loss: 0.6004 - val_accuracy: 0.7537\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.78 - ETA: 0s - loss: 0.5955 - accuracy: 0.77 - ETA: 0s - loss: 0.6130 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6101 - accuracy: 0.7630 - val_loss: 0.6038 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.84 - ETA: 0s - loss: 0.5847 - accuracy: 0.77 - ETA: 0s - loss: 0.5989 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6054 - accuracy: 0.7730 - val_loss: 0.5839 - val_accuracy: 0.7463\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7453 - accuracy: 0.59 - ETA: 0s - loss: 0.5938 - accuracy: 0.76 - ETA: 0s - loss: 0.5840 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5850 - accuracy: 0.7779 - val_loss: 0.5686 - val_accuracy: 0.7627\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.96 - ETA: 0s - loss: 0.5944 - accuracy: 0.78 - ETA: 0s - loss: 0.6071 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7794 - val_loss: 0.6048 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.84 - ETA: 0s - loss: 0.5572 - accuracy: 0.80 - ETA: 0s - loss: 0.5975 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6052 - accuracy: 0.7895 - val_loss: 0.5888 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7655 - accuracy: 0.68 - ETA: 0s - loss: 0.6216 - accuracy: 0.77 - ETA: 0s - loss: 0.6505 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6372 - accuracy: 0.7813 - val_loss: 0.5908 - val_accuracy: 0.7433\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.78 - ETA: 0s - loss: 0.5696 - accuracy: 0.80 - ETA: 0s - loss: 0.6495 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6244 - accuracy: 0.7973 - val_loss: 0.5837 - val_accuracy: 0.7478\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.84 - ETA: 0s - loss: 0.6352 - accuracy: 0.79 - ETA: 0s - loss: 0.6576 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6517 - accuracy: 0.7869 - val_loss: 0.7163 - val_accuracy: 0.7224\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6278 - accuracy: 0.81 - ETA: 0s - loss: 0.6084 - accuracy: 0.80 - ETA: 0s - loss: 0.6440 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6558 - accuracy: 0.7898 - val_loss: 0.6131 - val_accuracy: 0.7567\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.78 - ETA: 0s - loss: 0.5906 - accuracy: 0.78 - ETA: 0s - loss: 0.5983 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6322 - accuracy: 0.7943 - val_loss: 0.6402 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8842 - accuracy: 0.71 - ETA: 0s - loss: 0.8263 - accuracy: 0.76 - ETA: 0s - loss: 0.8880 - accuracy: 0.75 - 0s 2ms/step - loss: 0.8241 - accuracy: 0.7413 - val_loss: 0.6571 - val_accuracy: 0.7478\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.75 - ETA: 0s - loss: 0.6901 - accuracy: 0.72 - ETA: 0s - loss: 0.6763 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6859 - accuracy: 0.7271 - val_loss: 0.6699 - val_accuracy: 0.7164\n",
      "Epoch 21/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.6070 - accuracy: 0.78 - ETA: 0s - loss: 0.6967 - accuracy: 0.73 - ETA: 0s - loss: 0.6826 - accuracy: 0.7267Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.7305 - val_loss: 0.6799 - val_accuracy: 0.7075\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7213 - accuracy: 0.78 - ETA: 0s - loss: 3.2645 - accuracy: 0.59 - ETA: 0s - loss: 2.4464 - accuracy: 0.62 - 0s 4ms/step - loss: 2.1047 - accuracy: 0.6331 - val_loss: 0.6156 - val_accuracy: 0.6567\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8633 - accuracy: 0.62 - ETA: 0s - loss: 0.7565 - accuracy: 0.67 - ETA: 0s - loss: 0.7296 - accuracy: 0.68 - ETA: 0s - loss: 0.7277 - accuracy: 0.68 - ETA: 0s - loss: 0.7220 - accuracy: 0.69 - 0s 3ms/step - loss: 0.7129 - accuracy: 0.6988 - val_loss: 0.6580 - val_accuracy: 0.6925\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.78 - ETA: 0s - loss: 0.6696 - accuracy: 0.74 - ETA: 0s - loss: 0.6796 - accuracy: 0.72 - ETA: 0s - loss: 0.6757 - accuracy: 0.72 - ETA: 0s - loss: 0.6754 - accuracy: 0.72 - ETA: 0s - loss: 0.6819 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6819 - accuracy: 0.7197 - val_loss: 0.6350 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7336 - accuracy: 0.56 - ETA: 0s - loss: 0.6495 - accuracy: 0.73 - ETA: 0s - loss: 0.6569 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6607 - accuracy: 0.7316 - val_loss: 0.6374 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7270 - accuracy: 0.65 - ETA: 0s - loss: 0.6702 - accuracy: 0.72 - ETA: 0s - loss: 0.6731 - accuracy: 0.71 - ETA: 0s - loss: 0.6570 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6589 - accuracy: 0.7297 - val_loss: 0.6348 - val_accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.78 - ETA: 0s - loss: 0.6237 - accuracy: 0.75 - ETA: 0s - loss: 0.6279 - accuracy: 0.75 - ETA: 0s - loss: 0.6286 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6259 - accuracy: 0.7563 - val_loss: 0.6105 - val_accuracy: 0.7478\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.84 - ETA: 0s - loss: 0.6252 - accuracy: 0.75 - ETA: 0s - loss: 0.6297 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6236 - accuracy: 0.7559 - val_loss: 0.6071 - val_accuracy: 0.7507\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.71 - ETA: 0s - loss: 0.6043 - accuracy: 0.76 - ETA: 0s - loss: 0.6042 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6084 - accuracy: 0.7633 - val_loss: 0.6002 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.84 - ETA: 0s - loss: 0.5705 - accuracy: 0.79 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6046 - accuracy: 0.7794 - val_loss: 0.6018 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.87 - ETA: 0s - loss: 0.6143 - accuracy: 0.76 - ETA: 0s - loss: 0.6166 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6114 - accuracy: 0.7798 - val_loss: 0.5843 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8174 - accuracy: 0.78 - ETA: 0s - loss: 0.5817 - accuracy: 0.79 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7988 - val_loss: 0.5977 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7982 - accuracy: 0.71 - ETA: 0s - loss: 0.5926 - accuracy: 0.79 - ETA: 0s - loss: 0.5800 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7973 - val_loss: 0.5905 - val_accuracy: 0.7582\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7961 - accuracy: 0.81 - ETA: 0s - loss: 0.6076 - accuracy: 0.78 - ETA: 0s - loss: 0.6762 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6733 - accuracy: 0.7828 - val_loss: 0.5874 - val_accuracy: 0.7254\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7996 - accuracy: 0.65 - ETA: 0s - loss: 0.7406 - accuracy: 0.74 - ETA: 0s - loss: 0.7331 - accuracy: 0.76 - 0s 2ms/step - loss: 0.7867 - accuracy: 0.7581 - val_loss: 0.7136 - val_accuracy: 0.6507\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.87 - ETA: 0s - loss: 0.8386 - accuracy: 0.73 - ETA: 0s - loss: 0.7584 - accuracy: 0.73 - 0s 2ms/step - loss: 0.7549 - accuracy: 0.7376 - val_loss: 0.6707 - val_accuracy: 0.6836\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8293 - accuracy: 0.59 - ETA: 0s - loss: 0.6970 - accuracy: 0.71 - ETA: 0s - loss: 0.6791 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6659 - accuracy: 0.7391 - val_loss: 0.6251 - val_accuracy: 0.7493\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7064 - accuracy: 0.71 - ETA: 0s - loss: 0.6442 - accuracy: 0.75 - ETA: 0s - loss: 0.8094 - accuracy: 0.75 - 0s 2ms/step - loss: 0.7888 - accuracy: 0.7566 - val_loss: 0.6351 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.78 - ETA: 0s - loss: 0.7426 - accuracy: 0.77 - ETA: 0s - loss: 0.7054 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6987 - accuracy: 0.7622 - val_loss: 0.6358 - val_accuracy: 0.7299\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.78 - ETA: 0s - loss: 0.6671 - accuracy: 0.75 - ETA: 0s - loss: 0.6431 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6436 - accuracy: 0.7619 - val_loss: 0.6326 - val_accuracy: 0.7418\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.81 - ETA: 0s - loss: 0.6407 - accuracy: 0.75 - ETA: 0s - loss: 0.6499 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6476 - accuracy: 0.7675 - val_loss: 0.6202 - val_accuracy: 0.7299\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.87 - ETA: 0s - loss: 0.6089 - accuracy: 0.77 - ETA: 0s - loss: 0.6169 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6333 - accuracy: 0.7682 - val_loss: 0.6851 - val_accuracy: 0.7045\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.78 - ETA: 0s - loss: 0.7223 - accuracy: 0.75 - ETA: 0s - loss: 0.6760 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6742 - accuracy: 0.7611 - val_loss: 0.6113 - val_accuracy: 0.7627\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.65 - ETA: 0s - loss: 0.6176 - accuracy: 0.77 - ETA: 0s - loss: 0.6409 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6432 - accuracy: 0.7712 - val_loss: 0.9791 - val_accuracy: 0.6836\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.5643 - accuracy: 0.62 - ETA: 0s - loss: 0.6782 - accuracy: 0.77 - ETA: 0s - loss: 0.6512 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6433 - accuracy: 0.7779 - val_loss: 0.6635 - val_accuracy: 0.7627\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.68 - ETA: 0s - loss: 0.7507 - accuracy: 0.76 - ETA: 0s - loss: 0.6835 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6735 - accuracy: 0.7742 - val_loss: 0.6834 - val_accuracy: 0.7522\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.78 - ETA: 0s - loss: 0.6429 - accuracy: 0.77 - ETA: 0s - loss: 0.7123 - accuracy: 0.76 - 0s 2ms/step - loss: 0.7035 - accuracy: 0.7682 - val_loss: 0.6197 - val_accuracy: 0.7403\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.71 - ETA: 0s - loss: 0.7689 - accuracy: 0.75 - ETA: 0s - loss: 0.7014 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6910 - accuracy: 0.7704 - val_loss: 0.6271 - val_accuracy: 0.7493\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.71 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.6396 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6346 - accuracy: 0.7809 - val_loss: 0.8001 - val_accuracy: 0.7060\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.87 - ETA: 0s - loss: 0.8838 - accuracy: 0.79 - ETA: 0s - loss: 0.7996 - accuracy: 0.77 - 0s 2ms/step - loss: 0.7454 - accuracy: 0.7749 - val_loss: 0.6736 - val_accuracy: 0.7358\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.81 - ETA: 0s - loss: 0.5794 - accuracy: 0.79 - ETA: 0s - loss: 0.5941 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5911 - accuracy: 0.7895 - val_loss: 0.6448 - val_accuracy: 0.7507\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.84 - ETA: 0s - loss: 0.6234 - accuracy: 0.79 - ETA: 0s - loss: 0.6332 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6303 - accuracy: 0.7768 - val_loss: 0.6710 - val_accuracy: 0.7000\n",
      "Epoch 32/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.4887 - accuracy: 0.90 - ETA: 0s - loss: 0.6284 - accuracy: 0.78 - ETA: 0s - loss: 0.6515 - accuracy: 0.7798Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.7775 - val_loss: 0.6279 - val_accuracy: 0.7597\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: bfede7fdb4a63b5944940f6dff23f1a2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064504623413</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9206970548274799</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 45</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2975 - accuracy: 0.62 - ETA: 0s - loss: 1.5427 - accuracy: 0.61 - ETA: 0s - loss: 1.1080 - accuracy: 0.67 - ETA: 0s - loss: 0.9765 - accuracy: 0.68 - ETA: 0s - loss: 0.8919 - accuracy: 0.68 - ETA: 0s - loss: 0.8369 - accuracy: 0.69 - ETA: 0s - loss: 0.7899 - accuracy: 0.71 - 1s 7ms/step - loss: 0.7734 - accuracy: 0.7118 - val_loss: 0.6456 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6046 - accuracy: 0.65 - ETA: 0s - loss: 0.5514 - accuracy: 0.77 - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - ETA: 0s - loss: 0.5688 - accuracy: 0.76 - ETA: 0s - loss: 0.5717 - accuracy: 0.77 - ETA: 0s - loss: 0.5685 - accuracy: 0.77 - ETA: 0s - loss: 0.5679 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5657 - accuracy: 0.7730 - val_loss: 0.7316 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.84 - ETA: 0s - loss: 0.5082 - accuracy: 0.80 - ETA: 0s - loss: 0.5218 - accuracy: 0.81 - ETA: 0s - loss: 0.5055 - accuracy: 0.82 - ETA: 0s - loss: 0.5143 - accuracy: 0.81 - ETA: 0s - loss: 0.5305 - accuracy: 0.80 - ETA: 0s - loss: 0.5326 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5328 - accuracy: 0.8022 - val_loss: 0.6350 - val_accuracy: 0.7284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.84 - ETA: 0s - loss: 0.4734 - accuracy: 0.82 - ETA: 0s - loss: 0.4781 - accuracy: 0.83 - ETA: 0s - loss: 0.5016 - accuracy: 0.82 - ETA: 0s - loss: 0.5005 - accuracy: 0.82 - ETA: 0s - loss: 0.4992 - accuracy: 0.82 - ETA: 0s - loss: 0.4955 - accuracy: 0.82 - ETA: 0s - loss: 0.4992 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4992 - accuracy: 0.8227 - val_loss: 0.6271 - val_accuracy: 0.7284\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.90 - ETA: 0s - loss: 0.4990 - accuracy: 0.83 - ETA: 0s - loss: 0.4866 - accuracy: 0.84 - ETA: 0s - loss: 0.4981 - accuracy: 0.83 - ETA: 0s - loss: 0.4971 - accuracy: 0.82 - ETA: 0s - loss: 0.4919 - accuracy: 0.82 - ETA: 0s - loss: 0.4955 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4909 - accuracy: 0.8298 - val_loss: 0.7424 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4916 - accuracy: 0.81 - ETA: 0s - loss: 0.5823 - accuracy: 0.77 - ETA: 0s - loss: 0.5253 - accuracy: 0.80 - ETA: 0s - loss: 0.4854 - accuracy: 0.82 - ETA: 0s - loss: 0.4918 - accuracy: 0.82 - ETA: 0s - loss: 0.4799 - accuracy: 0.83 - ETA: 0s - loss: 0.4782 - accuracy: 0.83 - 0s 6ms/step - loss: 0.4855 - accuracy: 0.8354 - val_loss: 0.6881 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.81 - ETA: 0s - loss: 0.4544 - accuracy: 0.86 - ETA: 0s - loss: 0.4577 - accuracy: 0.85 - ETA: 0s - loss: 0.4621 - accuracy: 0.85 - ETA: 0s - loss: 0.4758 - accuracy: 0.84 - ETA: 0s - loss: 0.4799 - accuracy: 0.84 - ETA: 0s - loss: 0.4736 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4746 - accuracy: 0.8429 - val_loss: 0.5956 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.90 - ETA: 0s - loss: 0.4261 - accuracy: 0.86 - ETA: 0s - loss: 0.4244 - accuracy: 0.86 - ETA: 0s - loss: 0.4509 - accuracy: 0.85 - ETA: 0s - loss: 0.4523 - accuracy: 0.85 - ETA: 0s - loss: 0.4493 - accuracy: 0.86 - ETA: 0s - loss: 0.4481 - accuracy: 0.86 - ETA: 0s - loss: 0.4457 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4457 - accuracy: 0.8623 - val_loss: 0.6674 - val_accuracy: 0.7552\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.90 - ETA: 0s - loss: 0.4508 - accuracy: 0.84 - ETA: 0s - loss: 0.4471 - accuracy: 0.86 - ETA: 0s - loss: 0.4563 - accuracy: 0.85 - ETA: 0s - loss: 0.4714 - accuracy: 0.85 - ETA: 0s - loss: 0.4699 - accuracy: 0.85 - ETA: 0s - loss: 0.4704 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4697 - accuracy: 0.8518 - val_loss: 0.7890 - val_accuracy: 0.7418\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4099 - accuracy: 0.87 - ETA: 0s - loss: 0.4643 - accuracy: 0.83 - ETA: 0s - loss: 0.4386 - accuracy: 0.85 - ETA: 0s - loss: 0.4525 - accuracy: 0.85 - ETA: 0s - loss: 0.4519 - accuracy: 0.85 - ETA: 0s - loss: 0.4511 - accuracy: 0.85 - ETA: 0s - loss: 0.4918 - accuracy: 0.86 - ETA: 0s - loss: 0.5007 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4995 - accuracy: 0.8526 - val_loss: 0.6386 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.90 - ETA: 0s - loss: 0.4269 - accuracy: 0.86 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - ETA: 0s - loss: 0.5067 - accuracy: 0.84 - ETA: 0s - loss: 0.5166 - accuracy: 0.83 - ETA: 0s - loss: 0.5201 - accuracy: 0.83 - ETA: 0s - loss: 0.5187 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5196 - accuracy: 0.8335 - val_loss: 0.6057 - val_accuracy: 0.7552\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.87 - ETA: 0s - loss: 0.4629 - accuracy: 0.85 - ETA: 0s - loss: 0.4520 - accuracy: 0.86 - ETA: 0s - loss: 0.4536 - accuracy: 0.85 - ETA: 0s - loss: 0.4488 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4486 - accuracy: 0.85 - ETA: 0s - loss: 0.4541 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4541 - accuracy: 0.8559 - val_loss: 0.8595 - val_accuracy: 0.7104\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.87 - ETA: 0s - loss: 0.4528 - accuracy: 0.85 - ETA: 0s - loss: 0.4562 - accuracy: 0.85 - ETA: 0s - loss: 0.4344 - accuracy: 0.86 - ETA: 0s - loss: 0.4634 - accuracy: 0.86 - ETA: 0s - loss: 0.4729 - accuracy: 0.85 - ETA: 0s - loss: 0.4733 - accuracy: 0.85 - ETA: 0s - loss: 0.4611 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4608 - accuracy: 0.8555 - val_loss: 0.7048 - val_accuracy: 0.7239\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.81 - ETA: 0s - loss: 0.4312 - accuracy: 0.87 - ETA: 0s - loss: 0.4295 - accuracy: 0.87 - ETA: 0s - loss: 0.4278 - accuracy: 0.86 - ETA: 0s - loss: 0.4252 - accuracy: 0.86 - ETA: 0s - loss: 0.4205 - accuracy: 0.86 - ETA: 0s - loss: 0.4224 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8701 - val_loss: 0.7805 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.90 - ETA: 0s - loss: 0.4239 - accuracy: 0.87 - ETA: 0s - loss: 0.4084 - accuracy: 0.88 - ETA: 0s - loss: 0.4092 - accuracy: 0.88 - ETA: 0s - loss: 0.3982 - accuracy: 0.88 - ETA: 0s - loss: 0.4058 - accuracy: 0.88 - ETA: 0s - loss: 0.4023 - accuracy: 0.88 - 0s 6ms/step - loss: 0.4119 - accuracy: 0.8809 - val_loss: 0.7584 - val_accuracy: 0.7582\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.93 - ETA: 0s - loss: 0.4075 - accuracy: 0.88 - ETA: 0s - loss: 0.3978 - accuracy: 0.88 - ETA: 0s - loss: 0.3912 - accuracy: 0.89 - ETA: 0s - loss: 0.3978 - accuracy: 0.88 - ETA: 0s - loss: 0.4026 - accuracy: 0.88 - ETA: 0s - loss: 0.4083 - accuracy: 0.88 - ETA: 0s - loss: 0.4046 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4120 - accuracy: 0.8802 - val_loss: 0.9103 - val_accuracy: 0.7313\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 1.00 - ETA: 0s - loss: 0.3784 - accuracy: 0.88 - ETA: 0s - loss: 0.3725 - accuracy: 0.89 - ETA: 0s - loss: 0.3937 - accuracy: 0.88 - ETA: 0s - loss: 0.3879 - accuracy: 0.88 - ETA: 0s - loss: 0.3941 - accuracy: 0.89 - ETA: 0s - loss: 0.4038 - accuracy: 0.88 - ETA: 0s - loss: 0.4141 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4135 - accuracy: 0.8813 - val_loss: 0.6998 - val_accuracy: 0.7403\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.93 - ETA: 0s - loss: 0.4094 - accuracy: 0.88 - ETA: 0s - loss: 0.4641 - accuracy: 0.86 - ETA: 0s - loss: 0.4845 - accuracy: 0.85 - ETA: 0s - loss: 0.4613 - accuracy: 0.86 - ETA: 0s - loss: 0.4707 - accuracy: 0.86 - ETA: 0s - loss: 0.4816 - accuracy: 0.85 - ETA: 0s - loss: 0.4728 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4683 - accuracy: 0.8615 - val_loss: 0.8367 - val_accuracy: 0.7313\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.87 - ETA: 0s - loss: 0.4141 - accuracy: 0.87 - ETA: 0s - loss: 0.4217 - accuracy: 0.87 - ETA: 0s - loss: 0.4434 - accuracy: 0.86 - ETA: 0s - loss: 0.4398 - accuracy: 0.86 - ETA: 0s - loss: 0.4698 - accuracy: 0.86 - ETA: 0s - loss: 0.4719 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4721 - accuracy: 0.8615 - val_loss: 0.6422 - val_accuracy: 0.7463\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.87 - ETA: 0s - loss: 0.5330 - accuracy: 0.83 - ETA: 0s - loss: 0.4577 - accuracy: 0.86 - ETA: 0s - loss: 0.4611 - accuracy: 0.86 - ETA: 0s - loss: 0.4604 - accuracy: 0.87 - ETA: 0s - loss: 0.4497 - accuracy: 0.87 - ETA: 0s - loss: 0.4657 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4643 - accuracy: 0.8649 - val_loss: 1.1726 - val_accuracy: 0.7373\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.87 - ETA: 0s - loss: 0.4182 - accuracy: 0.88 - ETA: 0s - loss: 0.4998 - accuracy: 0.84 - ETA: 0s - loss: 0.5028 - accuracy: 0.84 - ETA: 0s - loss: 0.4897 - accuracy: 0.84 - ETA: 0s - loss: 0.4882 - accuracy: 0.84 - ETA: 0s - loss: 0.4794 - accuracy: 0.85 - ETA: 0s - loss: 0.4752 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8582 - val_loss: 1.3262 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.93 - ETA: 0s - loss: 0.4817 - accuracy: 0.85 - ETA: 0s - loss: 0.4829 - accuracy: 0.85 - ETA: 0s - loss: 0.4854 - accuracy: 0.85 - ETA: 0s - loss: 0.4969 - accuracy: 0.85 - ETA: 0s - loss: 0.5171 - accuracy: 0.83 - ETA: 0s - loss: 0.5283 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5553 - accuracy: 0.8063 - val_loss: 0.8160 - val_accuracy: 0.7254\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.87 - ETA: 0s - loss: 0.6053 - accuracy: 0.77 - ETA: 0s - loss: 0.5935 - accuracy: 0.72 - ETA: 0s - loss: 0.5688 - accuracy: 0.76 - ETA: 0s - loss: 0.5495 - accuracy: 0.78 - ETA: 0s - loss: 0.5410 - accuracy: 0.80 - ETA: 0s - loss: 0.5451 - accuracy: 0.80 - ETA: 0s - loss: 0.5478 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5478 - accuracy: 0.8040 - val_loss: 0.9035 - val_accuracy: 0.7701\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.90 - ETA: 0s - loss: 0.4507 - accuracy: 0.86 - ETA: 0s - loss: 0.4859 - accuracy: 0.84 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.4927 - accuracy: 0.84 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4882 - accuracy: 0.84 - ETA: 0s - loss: 0.4747 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4747 - accuracy: 0.8567 - val_loss: 0.7808 - val_accuracy: 0.7403\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.90 - ETA: 0s - loss: 0.4759 - accuracy: 0.86 - ETA: 0s - loss: 0.4606 - accuracy: 0.86 - ETA: 0s - loss: 0.4600 - accuracy: 0.86 - ETA: 0s - loss: 0.4481 - accuracy: 0.87 - ETA: 0s - loss: 0.4543 - accuracy: 0.87 - ETA: 0s - loss: 0.4855 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4847 - accuracy: 0.8694 - val_loss: 0.7969 - val_accuracy: 0.7313\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.84 - ETA: 0s - loss: 0.5675 - accuracy: 0.81 - ETA: 0s - loss: 0.5630 - accuracy: 0.82 - ETA: 0s - loss: 0.5334 - accuracy: 0.83 - ETA: 0s - loss: 0.5082 - accuracy: 0.84 - ETA: 0s - loss: 0.5191 - accuracy: 0.83 - ETA: 0s - loss: 0.5563 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5549 - accuracy: 0.8343 - val_loss: 0.7563 - val_accuracy: 0.7313\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.78 - ETA: 0s - loss: 0.4992 - accuracy: 0.84 - ETA: 0s - loss: 0.5476 - accuracy: 0.83 - ETA: 0s - loss: 0.5465 - accuracy: 0.82 - ETA: 0s - loss: 0.5380 - accuracy: 0.82 - ETA: 0s - loss: 0.5355 - accuracy: 0.82 - ETA: 0s - loss: 0.5420 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5498 - accuracy: 0.8298 - val_loss: 0.9712 - val_accuracy: 0.7358\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7930 - accuracy: 0.75 - ETA: 0s - loss: 0.5031 - accuracy: 0.85 - ETA: 0s - loss: 0.5137 - accuracy: 0.85 - ETA: 0s - loss: 0.5354 - accuracy: 0.84 - ETA: 0s - loss: 0.5411 - accuracy: 0.83 - ETA: 0s - loss: 0.5401 - accuracy: 0.83 - ETA: 0s - loss: 0.5404 - accuracy: 0.82 - ETA: 0s - loss: 0.5369 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5363 - accuracy: 0.8305 - val_loss: 0.7252 - val_accuracy: 0.7463\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.87 - ETA: 0s - loss: 0.5555 - accuracy: 0.81 - ETA: 0s - loss: 0.5495 - accuracy: 0.81 - ETA: 0s - loss: 0.5271 - accuracy: 0.82 - ETA: 0s - loss: 0.5282 - accuracy: 0.82 - ETA: 0s - loss: 0.5335 - accuracy: 0.82 - ETA: 0s - loss: 0.5285 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5234 - accuracy: 0.8268 - val_loss: 0.7012 - val_accuracy: 0.7433\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.71 - ETA: 0s - loss: 0.5451 - accuracy: 0.82 - ETA: 0s - loss: 0.5189 - accuracy: 0.83 - ETA: 0s - loss: 0.5054 - accuracy: 0.83 - ETA: 0s - loss: 0.4975 - accuracy: 0.84 - ETA: 0s - loss: 0.4925 - accuracy: 0.84 - ETA: 0s - loss: 0.4847 - accuracy: 0.84 - ETA: 0s - loss: 0.4790 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4776 - accuracy: 0.8537 - val_loss: 0.6775 - val_accuracy: 0.7493\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.90 - ETA: 0s - loss: 0.4554 - accuracy: 0.86 - ETA: 0s - loss: 0.4532 - accuracy: 0.86 - ETA: 0s - loss: 0.4466 - accuracy: 0.87 - ETA: 0s - loss: 0.4418 - accuracy: 0.87 - ETA: 0s - loss: 0.4388 - accuracy: 0.87 - ETA: 0s - loss: 0.4380 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4419 - accuracy: 0.8705 - val_loss: 0.8447 - val_accuracy: 0.7433\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.93 - ETA: 0s - loss: 0.4769 - accuracy: 0.86 - ETA: 0s - loss: 0.4908 - accuracy: 0.85 - ETA: 0s - loss: 0.4848 - accuracy: 0.85 - ETA: 0s - loss: 0.4827 - accuracy: 0.85 - ETA: 0s - loss: 0.4860 - accuracy: 0.85 - ETA: 0s - loss: 0.4754 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4734 - accuracy: 0.8604 - val_loss: 0.6092 - val_accuracy: 0.7582\n",
      "Epoch 33/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.4854 - accuracy: 0.84 - ETA: 0s - loss: 0.4192 - accuracy: 0.88 - ETA: 0s - loss: 0.4215 - accuracy: 0.88 - ETA: 0s - loss: 0.4256 - accuracy: 0.88 - ETA: 0s - loss: 0.4443 - accuracy: 0.87 - ETA: 0s - loss: 0.4679 - accuracy: 0.86 - ETA: 0s - loss: 0.4690 - accuracy: 0.8643Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4741 - accuracy: 0.8604 - val_loss: 0.7173 - val_accuracy: 0.7493\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.62 - ETA: 0s - loss: 2.1651 - accuracy: 0.63 - ETA: 0s - loss: 1.3444 - accuracy: 0.67 - ETA: 0s - loss: 1.1286 - accuracy: 0.69 - ETA: 0s - loss: 1.0084 - accuracy: 0.70 - ETA: 0s - loss: 0.9237 - accuracy: 0.71 - ETA: 0s - loss: 0.8677 - accuracy: 0.72 - ETA: 0s - loss: 0.8307 - accuracy: 0.72 - 1s 6ms/step - loss: 0.8284 - accuracy: 0.7208 - val_loss: 0.5771 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.78 - ETA: 0s - loss: 0.5470 - accuracy: 0.77 - ETA: 0s - loss: 0.5359 - accuracy: 0.79 - ETA: 0s - loss: 0.5583 - accuracy: 0.79 - ETA: 0s - loss: 0.5754 - accuracy: 0.77 - ETA: 0s - loss: 0.5761 - accuracy: 0.77 - ETA: 0s - loss: 0.5739 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5734 - accuracy: 0.7719 - val_loss: 0.5553 - val_accuracy: 0.7567\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.87 - ETA: 0s - loss: 0.5534 - accuracy: 0.80 - ETA: 0s - loss: 0.5701 - accuracy: 0.78 - ETA: 0s - loss: 0.5568 - accuracy: 0.78 - ETA: 0s - loss: 0.5479 - accuracy: 0.79 - ETA: 0s - loss: 0.5476 - accuracy: 0.79 - ETA: 0s - loss: 0.5536 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5536 - accuracy: 0.7906 - val_loss: 0.6492 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.78 - ETA: 0s - loss: 0.5117 - accuracy: 0.80 - ETA: 0s - loss: 0.4991 - accuracy: 0.80 - ETA: 0s - loss: 0.4942 - accuracy: 0.81 - ETA: 0s - loss: 0.5042 - accuracy: 0.81 - ETA: 0s - loss: 0.5099 - accuracy: 0.81 - ETA: 0s - loss: 0.5233 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5246 - accuracy: 0.8085 - val_loss: 0.7067 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.75 - ETA: 0s - loss: 0.4779 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.83 - ETA: 0s - loss: 0.4981 - accuracy: 0.83 - ETA: 0s - loss: 0.5099 - accuracy: 0.82 - ETA: 0s - loss: 0.5108 - accuracy: 0.81 - ETA: 0s - loss: 0.5047 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5006 - accuracy: 0.8193 - val_loss: 0.6280 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.90 - ETA: 0s - loss: 0.4592 - accuracy: 0.86 - ETA: 0s - loss: 0.4474 - accuracy: 0.86 - ETA: 0s - loss: 0.4661 - accuracy: 0.85 - ETA: 0s - loss: 0.4988 - accuracy: 0.83 - ETA: 0s - loss: 0.4982 - accuracy: 0.83 - ETA: 0s - loss: 0.5002 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5016 - accuracy: 0.8309 - val_loss: 0.7288 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.75 - ETA: 0s - loss: 0.5016 - accuracy: 0.84 - ETA: 0s - loss: 0.5115 - accuracy: 0.83 - ETA: 0s - loss: 0.5065 - accuracy: 0.83 - ETA: 0s - loss: 0.5046 - accuracy: 0.83 - ETA: 0s - loss: 0.4933 - accuracy: 0.83 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5003 - accuracy: 0.8294 - val_loss: 0.6424 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.81 - ETA: 0s - loss: 0.4396 - accuracy: 0.86 - ETA: 0s - loss: 0.4411 - accuracy: 0.86 - ETA: 0s - loss: 0.4529 - accuracy: 0.85 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - ETA: 0s - loss: 0.4892 - accuracy: 0.83 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4947 - accuracy: 0.8354 - val_loss: 0.5893 - val_accuracy: 0.7552\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.87 - ETA: 0s - loss: 0.4758 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.85 - ETA: 0s - loss: 0.4863 - accuracy: 0.84 - ETA: 0s - loss: 0.4692 - accuracy: 0.85 - ETA: 0s - loss: 0.4619 - accuracy: 0.85 - ETA: 0s - loss: 0.4681 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4714 - accuracy: 0.8511 - val_loss: 0.5947 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.81 - ETA: 0s - loss: 0.4915 - accuracy: 0.83 - ETA: 0s - loss: 0.4710 - accuracy: 0.84 - ETA: 0s - loss: 0.4543 - accuracy: 0.85 - ETA: 0s - loss: 0.4584 - accuracy: 0.85 - ETA: 0s - loss: 0.4534 - accuracy: 0.86 - ETA: 0s - loss: 0.4569 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4679 - accuracy: 0.8541 - val_loss: 0.6138 - val_accuracy: 0.7537\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.84 - ETA: 0s - loss: 0.5145 - accuracy: 0.82 - ETA: 0s - loss: 0.5215 - accuracy: 0.82 - ETA: 0s - loss: 0.5048 - accuracy: 0.83 - ETA: 0s - loss: 0.4880 - accuracy: 0.84 - ETA: 0s - loss: 0.4856 - accuracy: 0.84 - ETA: 0s - loss: 0.4738 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4654 - accuracy: 0.8574 - val_loss: 0.7288 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3852 - accuracy: 0.90 - ETA: 0s - loss: 0.4083 - accuracy: 0.87 - ETA: 0s - loss: 0.4181 - accuracy: 0.87 - ETA: 0s - loss: 0.4176 - accuracy: 0.87 - ETA: 0s - loss: 0.4199 - accuracy: 0.87 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - ETA: 0s - loss: 0.4394 - accuracy: 0.8643Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4384 - accuracy: 0.8634 - val_loss: 0.9061 - val_accuracy: 0.7313\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9645 - accuracy: 0.50 - ETA: 0s - loss: 2.0771 - accuracy: 0.66 - ETA: 0s - loss: 1.3310 - accuracy: 0.70 - ETA: 0s - loss: 1.0786 - accuracy: 0.71 - ETA: 0s - loss: 0.9625 - accuracy: 0.71 - ETA: 0s - loss: 0.8833 - accuracy: 0.73 - ETA: 0s - loss: 0.8385 - accuracy: 0.72 - 1s 6ms/step - loss: 0.8187 - accuracy: 0.7249 - val_loss: 0.5964 - val_accuracy: 0.6970\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7200 - accuracy: 0.65 - ETA: 0s - loss: 0.6041 - accuracy: 0.77 - ETA: 0s - loss: 0.6083 - accuracy: 0.75 - ETA: 0s - loss: 0.5902 - accuracy: 0.76 - ETA: 0s - loss: 0.5995 - accuracy: 0.76 - ETA: 0s - loss: 0.5943 - accuracy: 0.75 - ETA: 0s - loss: 0.5830 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5815 - accuracy: 0.7697 - val_loss: 0.5819 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.81 - ETA: 0s - loss: 0.5399 - accuracy: 0.80 - ETA: 0s - loss: 0.5626 - accuracy: 0.78 - ETA: 0s - loss: 0.5502 - accuracy: 0.79 - ETA: 0s - loss: 0.5635 - accuracy: 0.78 - ETA: 0s - loss: 0.5678 - accuracy: 0.78 - ETA: 0s - loss: 0.5689 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5669 - accuracy: 0.7835 - val_loss: 0.6453 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.84 - ETA: 0s - loss: 0.5225 - accuracy: 0.82 - ETA: 0s - loss: 0.5279 - accuracy: 0.81 - ETA: 0s - loss: 0.5151 - accuracy: 0.82 - ETA: 0s - loss: 0.5342 - accuracy: 0.80 - ETA: 0s - loss: 0.5329 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5306 - accuracy: 0.8025 - val_loss: 0.5741 - val_accuracy: 0.7179\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.87 - ETA: 0s - loss: 0.4829 - accuracy: 0.83 - ETA: 0s - loss: 0.4878 - accuracy: 0.83 - ETA: 0s - loss: 0.4841 - accuracy: 0.83 - ETA: 0s - loss: 0.4773 - accuracy: 0.83 - ETA: 0s - loss: 0.4855 - accuracy: 0.82 - ETA: 0s - loss: 0.4943 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4956 - accuracy: 0.8253 - val_loss: 0.6384 - val_accuracy: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.84 - ETA: 0s - loss: 0.5206 - accuracy: 0.82 - ETA: 0s - loss: 0.5222 - accuracy: 0.82 - ETA: 0s - loss: 0.5137 - accuracy: 0.83 - ETA: 0s - loss: 0.5175 - accuracy: 0.83 - ETA: 0s - loss: 0.5156 - accuracy: 0.83 - ETA: 0s - loss: 0.5157 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5177 - accuracy: 0.8309 - val_loss: 0.6759 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.90 - ETA: 0s - loss: 0.4531 - accuracy: 0.86 - ETA: 0s - loss: 0.4696 - accuracy: 0.84 - ETA: 0s - loss: 0.4906 - accuracy: 0.83 - ETA: 0s - loss: 0.4940 - accuracy: 0.83 - ETA: 0s - loss: 0.4911 - accuracy: 0.83 - ETA: 0s - loss: 0.5019 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5049 - accuracy: 0.8287 - val_loss: 0.5791 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.75 - ETA: 0s - loss: 0.5417 - accuracy: 0.80 - ETA: 0s - loss: 0.5248 - accuracy: 0.82 - ETA: 0s - loss: 0.5308 - accuracy: 0.82 - ETA: 0s - loss: 0.5220 - accuracy: 0.82 - ETA: 0s - loss: 0.5046 - accuracy: 0.83 - ETA: 0s - loss: 0.5002 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5014 - accuracy: 0.8302 - val_loss: 0.5815 - val_accuracy: 0.7239\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.75 - ETA: 0s - loss: 0.4726 - accuracy: 0.83 - ETA: 0s - loss: 0.4617 - accuracy: 0.85 - ETA: 0s - loss: 0.4852 - accuracy: 0.84 - ETA: 0s - loss: 0.4879 - accuracy: 0.83 - ETA: 0s - loss: 0.4746 - accuracy: 0.84 - ETA: 0s - loss: 0.4691 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8425 - val_loss: 0.7351 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.81 - ETA: 0s - loss: 0.4501 - accuracy: 0.84 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - ETA: 0s - loss: 0.4278 - accuracy: 0.85 - ETA: 0s - loss: 0.4363 - accuracy: 0.85 - ETA: 0s - loss: 0.4429 - accuracy: 0.85 - ETA: 0s - loss: 0.4473 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4549 - accuracy: 0.8529 - val_loss: 0.7282 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.93 - ETA: 0s - loss: 0.4502 - accuracy: 0.84 - ETA: 0s - loss: 0.4332 - accuracy: 0.86 - ETA: 0s - loss: 0.4193 - accuracy: 0.87 - ETA: 0s - loss: 0.4497 - accuracy: 0.85 - ETA: 0s - loss: 0.4546 - accuracy: 0.85 - ETA: 0s - loss: 0.4646 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4657 - accuracy: 0.8481 - val_loss: 0.6255 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.81 - ETA: 0s - loss: 0.4745 - accuracy: 0.83 - ETA: 0s - loss: 0.4532 - accuracy: 0.84 - ETA: 0s - loss: 0.4458 - accuracy: 0.85 - ETA: 0s - loss: 0.4460 - accuracy: 0.85 - ETA: 0s - loss: 0.4335 - accuracy: 0.86 - ETA: 0s - loss: 0.4440 - accuracy: 0.86 - ETA: 0s - loss: 0.4553 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4629 - accuracy: 0.8559 - val_loss: 1.0052 - val_accuracy: 0.6836\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4751 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.85 - ETA: 0s - loss: 0.4861 - accuracy: 0.84 - ETA: 0s - loss: 0.4813 - accuracy: 0.85 - ETA: 0s - loss: 0.4889 - accuracy: 0.85 - ETA: 0s - loss: 0.4857 - accuracy: 0.85 - ETA: 0s - loss: 0.4837 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4826 - accuracy: 0.8499 - val_loss: 0.8962 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5205 - accuracy: 0.84 - ETA: 0s - loss: 0.4487 - accuracy: 0.88 - ETA: 0s - loss: 0.4791 - accuracy: 0.88 - ETA: 0s - loss: 0.4711 - accuracy: 0.87 - ETA: 0s - loss: 0.4740 - accuracy: 0.87 - ETA: 0s - loss: 0.5496 - accuracy: 0.85 - ETA: 0s - loss: 0.5556 - accuracy: 0.84 - 0s 5ms/step - loss: 0.6206 - accuracy: 0.8432 - val_loss: 0.6590 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.6851 - accuracy: 0.68 - ETA: 0s - loss: 0.5147 - accuracy: 0.83 - ETA: 0s - loss: 0.5628 - accuracy: 0.81 - ETA: 0s - loss: 0.5630 - accuracy: 0.81 - ETA: 0s - loss: 0.5431 - accuracy: 0.82 - ETA: 0s - loss: 0.5591 - accuracy: 0.81 - ETA: 0s - loss: 0.5702 - accuracy: 0.8074Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5807 - accuracy: 0.8074 - val_loss: 0.6656 - val_accuracy: 0.7373\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 57b83ca9ef7034c54fb554affce1ac6a</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7572139302889506</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6233683474880358</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 145</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7745 - accuracy: 0.75 - ETA: 0s - loss: 2.3454 - accuracy: 0.64 - ETA: 0s - loss: 1.6268 - accuracy: 0.64 - ETA: 0s - loss: 1.3513 - accuracy: 0.63 - ETA: 0s - loss: 1.1796 - accuracy: 0.64 - ETA: 0s - loss: 1.0690 - accuracy: 0.66 - 1s 6ms/step - loss: 1.0012 - accuracy: 0.6723 - val_loss: 0.5875 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.87 - ETA: 0s - loss: 0.6627 - accuracy: 0.73 - ETA: 0s - loss: 0.6469 - accuracy: 0.74 - ETA: 0s - loss: 0.6242 - accuracy: 0.75 - ETA: 0s - loss: 0.6112 - accuracy: 0.75 - ETA: 0s - loss: 0.6226 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6254 - accuracy: 0.7443 - val_loss: 0.5892 - val_accuracy: 0.7493\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.90 - ETA: 0s - loss: 0.5223 - accuracy: 0.82 - ETA: 0s - loss: 0.5545 - accuracy: 0.79 - ETA: 0s - loss: 0.5419 - accuracy: 0.80 - ETA: 0s - loss: 0.5411 - accuracy: 0.80 - ETA: 0s - loss: 0.5603 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5646 - accuracy: 0.7902 - val_loss: 0.5786 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.90 - ETA: 0s - loss: 0.5344 - accuracy: 0.81 - ETA: 0s - loss: 0.5623 - accuracy: 0.79 - ETA: 0s - loss: 0.5524 - accuracy: 0.80 - ETA: 0s - loss: 0.5533 - accuracy: 0.80 - ETA: 0s - loss: 0.5490 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5486 - accuracy: 0.8022 - val_loss: 0.5689 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.81 - ETA: 0s - loss: 0.5024 - accuracy: 0.83 - ETA: 0s - loss: 0.5569 - accuracy: 0.80 - ETA: 0s - loss: 0.5368 - accuracy: 0.81 - ETA: 0s - loss: 0.5342 - accuracy: 0.81 - ETA: 0s - loss: 0.5382 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5358 - accuracy: 0.8163 - val_loss: 0.6091 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.96 - ETA: 0s - loss: 0.5466 - accuracy: 0.81 - ETA: 0s - loss: 0.5427 - accuracy: 0.81 - ETA: 0s - loss: 0.5147 - accuracy: 0.82 - ETA: 0s - loss: 0.5048 - accuracy: 0.83 - ETA: 0s - loss: 0.5148 - accuracy: 0.82 - ETA: 0s - loss: 0.5179 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5197 - accuracy: 0.8264 - val_loss: 0.6032 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.78 - ETA: 0s - loss: 0.5873 - accuracy: 0.79 - ETA: 0s - loss: 0.5419 - accuracy: 0.81 - ETA: 0s - loss: 0.5571 - accuracy: 0.80 - ETA: 0s - loss: 0.5500 - accuracy: 0.80 - ETA: 0s - loss: 0.5488 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5403 - accuracy: 0.8093 - val_loss: 0.5625 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.90 - ETA: 0s - loss: 0.4860 - accuracy: 0.84 - ETA: 0s - loss: 0.5057 - accuracy: 0.83 - ETA: 0s - loss: 0.5287 - accuracy: 0.81 - ETA: 0s - loss: 0.5286 - accuracy: 0.81 - ETA: 0s - loss: 0.5469 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5510 - accuracy: 0.8085 - val_loss: 0.7540 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.87 - ETA: 0s - loss: 0.5054 - accuracy: 0.82 - ETA: 0s - loss: 0.5225 - accuracy: 0.81 - ETA: 0s - loss: 0.5304 - accuracy: 0.80 - ETA: 0s - loss: 0.5456 - accuracy: 0.80 - ETA: 0s - loss: 0.5312 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5288 - accuracy: 0.8186 - val_loss: 0.8563 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.78 - ETA: 0s - loss: 0.6823 - accuracy: 0.77 - ETA: 0s - loss: 0.6886 - accuracy: 0.75 - ETA: 0s - loss: 0.6867 - accuracy: 0.74 - ETA: 0s - loss: 0.6552 - accuracy: 0.75 - ETA: 0s - loss: 0.6431 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6394 - accuracy: 0.7637 - val_loss: 0.6922 - val_accuracy: 0.7418\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.87 - ETA: 0s - loss: 0.5770 - accuracy: 0.80 - ETA: 0s - loss: 0.5769 - accuracy: 0.80 - ETA: 0s - loss: 0.5691 - accuracy: 0.81 - ETA: 0s - loss: 0.5818 - accuracy: 0.80 - ETA: 0s - loss: 0.5725 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5736 - accuracy: 0.8070 - val_loss: 0.6649 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.81 - ETA: 0s - loss: 0.5256 - accuracy: 0.81 - ETA: 0s - loss: 0.5073 - accuracy: 0.83 - ETA: 0s - loss: 0.5454 - accuracy: 0.82 - ETA: 0s - loss: 0.5590 - accuracy: 0.81 - ETA: 0s - loss: 0.5685 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5694 - accuracy: 0.8081 - val_loss: 0.7728 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.78 - ETA: 0s - loss: 0.5138 - accuracy: 0.82 - ETA: 0s - loss: 0.5146 - accuracy: 0.82 - ETA: 0s - loss: 0.5296 - accuracy: 0.82 - ETA: 0s - loss: 0.5448 - accuracy: 0.81 - ETA: 0s - loss: 0.5528 - accuracy: 0.80 - ETA: 0s - loss: 0.5606 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5782 - accuracy: 0.7954 - val_loss: 0.5933 - val_accuracy: 0.7567\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.87 - ETA: 0s - loss: 0.5549 - accuracy: 0.79 - ETA: 0s - loss: 0.5649 - accuracy: 0.80 - ETA: 0s - loss: 0.5668 - accuracy: 0.79 - ETA: 0s - loss: 0.5814 - accuracy: 0.79 - ETA: 0s - loss: 0.5767 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5709 - accuracy: 0.8029 - val_loss: 0.7897 - val_accuracy: 0.7403\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.81 - ETA: 0s - loss: 0.5457 - accuracy: 0.82 - ETA: 0s - loss: 0.5559 - accuracy: 0.81 - ETA: 0s - loss: 0.5596 - accuracy: 0.81 - ETA: 0s - loss: 0.5497 - accuracy: 0.82 - ETA: 0s - loss: 0.5511 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5500 - accuracy: 0.8205 - val_loss: 0.8646 - val_accuracy: 0.7657\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.90 - ETA: 0s - loss: 0.4949 - accuracy: 0.83 - ETA: 0s - loss: 0.5166 - accuracy: 0.82 - ETA: 0s - loss: 0.5280 - accuracy: 0.82 - ETA: 0s - loss: 0.5052 - accuracy: 0.82 - ETA: 0s - loss: 0.5098 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5191 - accuracy: 0.8257 - val_loss: 0.9667 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.75 - ETA: 0s - loss: 0.6343 - accuracy: 0.78 - ETA: 0s - loss: 0.6123 - accuracy: 0.79 - ETA: 0s - loss: 0.5935 - accuracy: 0.80 - ETA: 0s - loss: 0.5808 - accuracy: 0.80 - ETA: 0s - loss: 0.5749 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5778 - accuracy: 0.8093 - val_loss: 0.5907 - val_accuracy: 0.7552\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.78 - ETA: 0s - loss: 0.5944 - accuracy: 0.79 - ETA: 0s - loss: 0.5862 - accuracy: 0.80 - ETA: 0s - loss: 0.5568 - accuracy: 0.81 - ETA: 0s - loss: 0.5579 - accuracy: 0.81 - ETA: 0s - loss: 0.5705 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5827 - accuracy: 0.8145 - val_loss: 0.6521 - val_accuracy: 0.7627\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6425 - accuracy: 0.87 - ETA: 0s - loss: 0.6337 - accuracy: 0.78 - ETA: 0s - loss: 0.6432 - accuracy: 0.77 - ETA: 0s - loss: 0.6155 - accuracy: 0.77 - ETA: 0s - loss: 0.6024 - accuracy: 0.78 - ETA: 0s - loss: 0.5911 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5916 - accuracy: 0.7936 - val_loss: 0.7609 - val_accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.84 - ETA: 0s - loss: 0.5943 - accuracy: 0.79 - ETA: 0s - loss: 0.5709 - accuracy: 0.80 - ETA: 0s - loss: 0.5679 - accuracy: 0.80 - ETA: 0s - loss: 0.5457 - accuracy: 0.81 - ETA: 0s - loss: 0.5431 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5443 - accuracy: 0.8186 - val_loss: 0.7940 - val_accuracy: 0.7522\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.78 - ETA: 0s - loss: 0.4653 - accuracy: 0.86 - ETA: 0s - loss: 0.4718 - accuracy: 0.85 - ETA: 0s - loss: 0.4821 - accuracy: 0.85 - ETA: 0s - loss: 0.5371 - accuracy: 0.84 - ETA: 0s - loss: 0.5473 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5536 - accuracy: 0.8305 - val_loss: 0.9449 - val_accuracy: 0.7522\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.1753 - accuracy: 0.78 - ETA: 0s - loss: 0.8882 - accuracy: 0.76 - ETA: 0s - loss: 0.7497 - accuracy: 0.77 - ETA: 0s - loss: 0.6976 - accuracy: 0.77 - ETA: 0s - loss: 0.6612 - accuracy: 0.78 - ETA: 0s - loss: 0.6559 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6608 - accuracy: 0.7813 - val_loss: 0.9219 - val_accuracy: 0.7209\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.87 - ETA: 0s - loss: 0.6897 - accuracy: 0.79 - ETA: 0s - loss: 0.6715 - accuracy: 0.76 - ETA: 0s - loss: 0.6567 - accuracy: 0.76 - ETA: 0s - loss: 0.6423 - accuracy: 0.76 - ETA: 0s - loss: 0.6390 - accuracy: 0.76 - ETA: 0s - loss: 0.6366 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6361 - accuracy: 0.7660 - val_loss: 0.7974 - val_accuracy: 0.7284\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.84 - ETA: 0s - loss: 0.6531 - accuracy: 0.72 - ETA: 0s - loss: 0.6213 - accuracy: 0.74 - ETA: 0s - loss: 0.6128 - accuracy: 0.75 - ETA: 0s - loss: 0.6170 - accuracy: 0.75 - ETA: 0s - loss: 0.6093 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6162 - accuracy: 0.7667 - val_loss: 1.0587 - val_accuracy: 0.7403\n",
      "Epoch 25/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.4479 - accuracy: 0.90 - ETA: 0s - loss: 0.5614 - accuracy: 0.80 - ETA: 0s - loss: 0.5770 - accuracy: 0.79 - ETA: 0s - loss: 0.5777 - accuracy: 0.79 - ETA: 0s - loss: 0.5700 - accuracy: 0.80 - ETA: 0s - loss: 0.6015 - accuracy: 0.8054Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6031 - accuracy: 0.8033 - val_loss: 0.6696 - val_accuracy: 0.7313\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0398 - accuracy: 0.62 - ETA: 0s - loss: 2.5909 - accuracy: 0.57 - ETA: 0s - loss: 1.6891 - accuracy: 0.63 - ETA: 0s - loss: 1.3124 - accuracy: 0.66 - ETA: 0s - loss: 1.1249 - accuracy: 0.68 - ETA: 0s - loss: 1.0189 - accuracy: 0.69 - 0s 5ms/step - loss: 0.9553 - accuracy: 0.7032 - val_loss: 0.5808 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.81 - ETA: 0s - loss: 0.5545 - accuracy: 0.79 - ETA: 0s - loss: 0.5929 - accuracy: 0.76 - ETA: 0s - loss: 0.5813 - accuracy: 0.77 - ETA: 0s - loss: 0.5840 - accuracy: 0.76 - ETA: 0s - loss: 0.5884 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5930 - accuracy: 0.7600 - val_loss: 0.5714 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6452 - accuracy: 0.75 - ETA: 0s - loss: 0.5867 - accuracy: 0.76 - ETA: 0s - loss: 0.5552 - accuracy: 0.79 - ETA: 0s - loss: 0.5461 - accuracy: 0.80 - ETA: 0s - loss: 0.5571 - accuracy: 0.79 - ETA: 0s - loss: 0.5677 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5665 - accuracy: 0.7951 - val_loss: 0.5600 - val_accuracy: 0.7612\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.81 - ETA: 0s - loss: 0.5723 - accuracy: 0.78 - ETA: 0s - loss: 0.5500 - accuracy: 0.78 - ETA: 0s - loss: 0.5549 - accuracy: 0.78 - ETA: 0s - loss: 0.5492 - accuracy: 0.79 - ETA: 0s - loss: 0.5392 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5409 - accuracy: 0.8033 - val_loss: 0.5812 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.87 - ETA: 0s - loss: 0.4907 - accuracy: 0.83 - ETA: 0s - loss: 0.4838 - accuracy: 0.84 - ETA: 0s - loss: 0.5140 - accuracy: 0.83 - ETA: 0s - loss: 0.5329 - accuracy: 0.81 - ETA: 0s - loss: 0.5321 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5382 - accuracy: 0.8182 - val_loss: 0.5640 - val_accuracy: 0.7552\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.78 - ETA: 0s - loss: 0.5118 - accuracy: 0.82 - ETA: 0s - loss: 0.5128 - accuracy: 0.82 - ETA: 0s - loss: 0.5107 - accuracy: 0.82 - ETA: 0s - loss: 0.5093 - accuracy: 0.82 - ETA: 0s - loss: 0.5007 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5072 - accuracy: 0.8264 - val_loss: 0.5847 - val_accuracy: 0.7597\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.87 - ETA: 0s - loss: 0.4960 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.83 - ETA: 0s - loss: 0.4972 - accuracy: 0.83 - ETA: 0s - loss: 0.4988 - accuracy: 0.84 - ETA: 0s - loss: 0.5066 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5073 - accuracy: 0.8376 - val_loss: 0.6170 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.87 - ETA: 0s - loss: 0.5136 - accuracy: 0.82 - ETA: 0s - loss: 0.5057 - accuracy: 0.82 - ETA: 0s - loss: 0.4995 - accuracy: 0.83 - ETA: 0s - loss: 0.4928 - accuracy: 0.84 - ETA: 0s - loss: 0.4845 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4807 - accuracy: 0.8447 - val_loss: 0.6440 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.84 - ETA: 0s - loss: 0.4489 - accuracy: 0.85 - ETA: 0s - loss: 0.4473 - accuracy: 0.86 - ETA: 0s - loss: 0.4631 - accuracy: 0.85 - ETA: 0s - loss: 0.4772 - accuracy: 0.84 - ETA: 0s - loss: 0.4939 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4966 - accuracy: 0.8339 - val_loss: 0.5718 - val_accuracy: 0.7522\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.93 - ETA: 0s - loss: 0.4748 - accuracy: 0.85 - ETA: 0s - loss: 0.5501 - accuracy: 0.83 - ETA: 0s - loss: 0.5541 - accuracy: 0.83 - ETA: 0s - loss: 0.5471 - accuracy: 0.82 - ETA: 0s - loss: 0.5405 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5398 - accuracy: 0.8309 - val_loss: 0.6045 - val_accuracy: 0.7552\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.93 - ETA: 0s - loss: 0.5286 - accuracy: 0.82 - ETA: 0s - loss: 0.5243 - accuracy: 0.82 - ETA: 0s - loss: 0.5349 - accuracy: 0.81 - ETA: 0s - loss: 0.5293 - accuracy: 0.82 - ETA: 0s - loss: 0.5303 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5320 - accuracy: 0.8216 - val_loss: 0.6586 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.84 - ETA: 0s - loss: 0.5082 - accuracy: 0.84 - ETA: 0s - loss: 0.4891 - accuracy: 0.85 - ETA: 0s - loss: 0.4797 - accuracy: 0.85 - ETA: 0s - loss: 0.4951 - accuracy: 0.85 - ETA: 0s - loss: 0.5042 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5048 - accuracy: 0.8443 - val_loss: 0.5939 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.6199 - accuracy: 0.75 - ETA: 0s - loss: 0.4909 - accuracy: 0.83 - ETA: 0s - loss: 0.4842 - accuracy: 0.84 - ETA: 0s - loss: 0.4953 - accuracy: 0.84 - ETA: 0s - loss: 0.4900 - accuracy: 0.84 - ETA: 0s - loss: 0.4922 - accuracy: 0.8433Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.8432 - val_loss: 0.7548 - val_accuracy: 0.7299\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.62 - ETA: 0s - loss: 2.5857 - accuracy: 0.52 - ETA: 0s - loss: 1.7312 - accuracy: 0.56 - ETA: 0s - loss: 1.3716 - accuracy: 0.60 - ETA: 0s - loss: 1.1931 - accuracy: 0.62 - ETA: 0s - loss: 1.0713 - accuracy: 0.65 - 0s 5ms/step - loss: 1.0141 - accuracy: 0.6655 - val_loss: 0.5925 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.68 - ETA: 0s - loss: 0.5657 - accuracy: 0.77 - ETA: 0s - loss: 0.5932 - accuracy: 0.76 - ETA: 0s - loss: 0.5803 - accuracy: 0.76 - ETA: 0s - loss: 0.5837 - accuracy: 0.76 - ETA: 0s - loss: 0.5892 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5848 - accuracy: 0.7611 - val_loss: 0.5834 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.87 - ETA: 0s - loss: 0.5255 - accuracy: 0.79 - ETA: 0s - loss: 0.5608 - accuracy: 0.77 - ETA: 0s - loss: 0.5703 - accuracy: 0.78 - ETA: 0s - loss: 0.5704 - accuracy: 0.77 - ETA: 0s - loss: 0.5722 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5683 - accuracy: 0.7801 - val_loss: 0.6046 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5726 - accuracy: 0.81 - ETA: 0s - loss: 0.5005 - accuracy: 0.81 - ETA: 0s - loss: 0.5222 - accuracy: 0.80 - ETA: 0s - loss: 0.5388 - accuracy: 0.80 - ETA: 0s - loss: 0.5358 - accuracy: 0.80 - ETA: 0s - loss: 0.5369 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5334 - accuracy: 0.8014 - val_loss: 0.5556 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5340 - accuracy: 0.81 - ETA: 0s - loss: 0.4504 - accuracy: 0.83 - ETA: 0s - loss: 0.4629 - accuracy: 0.83 - ETA: 0s - loss: 0.4955 - accuracy: 0.83 - ETA: 0s - loss: 0.5002 - accuracy: 0.82 - ETA: 0s - loss: 0.5144 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5154 - accuracy: 0.8231 - val_loss: 0.6335 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.78 - ETA: 0s - loss: 0.5356 - accuracy: 0.80 - ETA: 0s - loss: 0.5244 - accuracy: 0.81 - ETA: 0s - loss: 0.5335 - accuracy: 0.80 - ETA: 0s - loss: 0.5155 - accuracy: 0.81 - ETA: 0s - loss: 0.5144 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5141 - accuracy: 0.8186 - val_loss: 0.6296 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.81 - ETA: 0s - loss: 0.5026 - accuracy: 0.81 - ETA: 0s - loss: 0.5169 - accuracy: 0.82 - ETA: 0s - loss: 0.5266 - accuracy: 0.81 - ETA: 0s - loss: 0.5195 - accuracy: 0.82 - ETA: 0s - loss: 0.5130 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5292 - accuracy: 0.8182 - val_loss: 0.6612 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.84 - ETA: 0s - loss: 0.5086 - accuracy: 0.83 - ETA: 0s - loss: 0.5292 - accuracy: 0.81 - ETA: 0s - loss: 0.5497 - accuracy: 0.81 - ETA: 0s - loss: 0.5418 - accuracy: 0.81 - ETA: 0s - loss: 0.5318 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5262 - accuracy: 0.8257 - val_loss: 0.7908 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.84 - ETA: 0s - loss: 0.5520 - accuracy: 0.80 - ETA: 0s - loss: 0.4891 - accuracy: 0.82 - ETA: 0s - loss: 0.4700 - accuracy: 0.83 - ETA: 0s - loss: 0.5131 - accuracy: 0.82 - ETA: 0s - loss: 0.5099 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5175 - accuracy: 0.8305 - val_loss: 1.0346 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.84 - ETA: 0s - loss: 0.4257 - accuracy: 0.88 - ETA: 0s - loss: 0.4547 - accuracy: 0.86 - ETA: 0s - loss: 0.4694 - accuracy: 0.85 - ETA: 0s - loss: 0.4910 - accuracy: 0.84 - ETA: 0s - loss: 0.5149 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5212 - accuracy: 0.8261 - val_loss: 0.6381 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.81 - ETA: 0s - loss: 0.5692 - accuracy: 0.79 - ETA: 0s - loss: 0.5572 - accuracy: 0.80 - ETA: 0s - loss: 0.5326 - accuracy: 0.81 - ETA: 0s - loss: 0.5473 - accuracy: 0.81 - ETA: 0s - loss: 0.5488 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5449 - accuracy: 0.8167 - val_loss: 0.7059 - val_accuracy: 0.7463\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.75 - ETA: 0s - loss: 0.5498 - accuracy: 0.80 - ETA: 0s - loss: 0.6386 - accuracy: 0.81 - ETA: 0s - loss: 0.5933 - accuracy: 0.82 - ETA: 0s - loss: 0.5722 - accuracy: 0.82 - ETA: 0s - loss: 0.5771 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5843 - accuracy: 0.8227 - val_loss: 0.7774 - val_accuracy: 0.7209\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3630 - accuracy: 0.90 - ETA: 0s - loss: 0.4570 - accuracy: 0.85 - ETA: 0s - loss: 0.5385 - accuracy: 0.83 - ETA: 0s - loss: 0.5613 - accuracy: 0.81 - ETA: 0s - loss: 0.5856 - accuracy: 0.81 - ETA: 0s - loss: 0.5832 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5801 - accuracy: 0.8052 - val_loss: 0.7529 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.3933 - accuracy: 0.87 - ETA: 0s - loss: 0.4943 - accuracy: 0.83 - ETA: 0s - loss: 0.5889 - accuracy: 0.82 - ETA: 0s - loss: 0.5778 - accuracy: 0.82 - ETA: 0s - loss: 0.6183 - accuracy: 0.82 - ETA: 0s - loss: 0.6386 - accuracy: 0.8075Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.8066 - val_loss: 0.8039 - val_accuracy: 0.7284\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 18efa8d9c189a9bf687ada8ec573c897</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7597015102704366</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8237770244597469</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 85</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9861 - accuracy: 0.59 - ETA: 0s - loss: 6.6236 - accuracy: 0.61 - ETA: 0s - loss: 3.5481 - accuracy: 0.61 - ETA: 0s - loss: 2.6247 - accuracy: 0.64 - ETA: 0s - loss: 2.0779 - accuracy: 0.66 - ETA: 0s - loss: 1.7866 - accuracy: 0.67 - ETA: 0s - loss: 1.5889 - accuracy: 0.69 - ETA: 0s - loss: 1.4615 - accuracy: 0.69 - 1s 7ms/step - loss: 1.3836 - accuracy: 0.6980 - val_loss: 0.5914 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.71 - ETA: 0s - loss: 0.6790 - accuracy: 0.71 - ETA: 0s - loss: 0.6291 - accuracy: 0.74 - ETA: 0s - loss: 0.6247 - accuracy: 0.75 - ETA: 0s - loss: 0.6228 - accuracy: 0.75 - ETA: 0s - loss: 0.6166 - accuracy: 0.75 - ETA: 0s - loss: 0.6141 - accuracy: 0.75 - ETA: 0s - loss: 0.6082 - accuracy: 0.76 - 0s 6ms/step - loss: 0.6118 - accuracy: 0.7592 - val_loss: 0.5781 - val_accuracy: 0.7478\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.78 - ETA: 0s - loss: 0.5161 - accuracy: 0.80 - ETA: 0s - loss: 0.5418 - accuracy: 0.79 - ETA: 0s - loss: 0.5632 - accuracy: 0.78 - ETA: 0s - loss: 0.5599 - accuracy: 0.78 - ETA: 0s - loss: 0.5645 - accuracy: 0.78 - ETA: 0s - loss: 0.5741 - accuracy: 0.78 - ETA: 0s - loss: 0.5828 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5881 - accuracy: 0.7753 - val_loss: 0.5873 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6266 - accuracy: 0.68 - ETA: 0s - loss: 0.5796 - accuracy: 0.77 - ETA: 0s - loss: 0.5822 - accuracy: 0.77 - ETA: 0s - loss: 0.6011 - accuracy: 0.77 - ETA: 0s - loss: 0.6177 - accuracy: 0.76 - ETA: 0s - loss: 0.6179 - accuracy: 0.76 - ETA: 0s - loss: 0.6248 - accuracy: 0.76 - ETA: 0s - loss: 0.6187 - accuracy: 0.76 - 0s 6ms/step - loss: 0.6175 - accuracy: 0.7675 - val_loss: 0.6132 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5319 - accuracy: 0.84 - ETA: 0s - loss: 0.5919 - accuracy: 0.78 - ETA: 0s - loss: 0.6111 - accuracy: 0.77 - ETA: 0s - loss: 0.5959 - accuracy: 0.77 - ETA: 0s - loss: 0.5943 - accuracy: 0.78 - ETA: 0s - loss: 0.5865 - accuracy: 0.78 - ETA: 0s - loss: 0.5854 - accuracy: 0.79 - ETA: 0s - loss: 0.5873 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5854 - accuracy: 0.7895 - val_loss: 0.5963 - val_accuracy: 0.7448\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.75 - ETA: 0s - loss: 0.6303 - accuracy: 0.76 - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - ETA: 0s - loss: 0.5762 - accuracy: 0.79 - ETA: 0s - loss: 0.5766 - accuracy: 0.79 - ETA: 0s - loss: 0.5748 - accuracy: 0.79 - ETA: 0s - loss: 0.6115 - accuracy: 0.78 - ETA: 0s - loss: 0.6124 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6129 - accuracy: 0.7835 - val_loss: 0.6053 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.84 - ETA: 0s - loss: 0.6450 - accuracy: 0.78 - ETA: 0s - loss: 0.6680 - accuracy: 0.75 - ETA: 0s - loss: 0.6402 - accuracy: 0.76 - ETA: 0s - loss: 0.6115 - accuracy: 0.78 - ETA: 0s - loss: 0.6119 - accuracy: 0.78 - ETA: 0s - loss: 0.6281 - accuracy: 0.76 - ETA: 0s - loss: 0.6311 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6344 - accuracy: 0.7663 - val_loss: 0.6106 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.71 - ETA: 0s - loss: 0.6602 - accuracy: 0.74 - ETA: 0s - loss: 0.6217 - accuracy: 0.76 - ETA: 0s - loss: 0.6152 - accuracy: 0.76 - ETA: 0s - loss: 0.6159 - accuracy: 0.77 - ETA: 0s - loss: 0.6133 - accuracy: 0.77 - ETA: 0s - loss: 0.6327 - accuracy: 0.77 - ETA: 0s - loss: 0.6349 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6342 - accuracy: 0.7719 - val_loss: 0.6116 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.84 - ETA: 0s - loss: 0.6751 - accuracy: 0.75 - ETA: 0s - loss: 0.6746 - accuracy: 0.74 - ETA: 0s - loss: 0.6983 - accuracy: 0.75 - ETA: 0s - loss: 0.6910 - accuracy: 0.75 - ETA: 0s - loss: 0.7005 - accuracy: 0.74 - ETA: 0s - loss: 0.7069 - accuracy: 0.72 - ETA: 0s - loss: 0.7066 - accuracy: 0.67 - 0s 5ms/step - loss: 0.7050 - accuracy: 0.6588 - val_loss: 0.6737 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.50 - ETA: 0s - loss: 0.6869 - accuracy: 0.60 - ETA: 0s - loss: 0.7013 - accuracy: 0.57 - ETA: 0s - loss: 0.6926 - accuracy: 0.55 - ETA: 0s - loss: 0.6912 - accuracy: 0.58 - ETA: 0s - loss: 0.6891 - accuracy: 0.60 - ETA: 0s - loss: 0.6898 - accuracy: 0.60 - ETA: 0s - loss: 0.6918 - accuracy: 0.59 - 0s 5ms/step - loss: 0.6920 - accuracy: 0.6006 - val_loss: 0.6565 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.68 - ETA: 0s - loss: 0.7016 - accuracy: 0.59 - ETA: 0s - loss: 0.6895 - accuracy: 0.61 - ETA: 0s - loss: 0.6867 - accuracy: 0.62 - ETA: 0s - loss: 0.6901 - accuracy: 0.61 - ETA: 0s - loss: 0.6862 - accuracy: 0.60 - ETA: 0s - loss: 0.6849 - accuracy: 0.60 - ETA: 0s - loss: 0.6869 - accuracy: 0.61 - ETA: 0s - loss: 0.6875 - accuracy: 0.61 - 0s 6ms/step - loss: 0.6881 - accuracy: 0.6148 - val_loss: 0.6655 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.65 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6669 - accuracy: 0.65 - ETA: 0s - loss: 0.6760 - accuracy: 0.65 - ETA: 0s - loss: 0.6785 - accuracy: 0.65 - ETA: 0s - loss: 0.6858 - accuracy: 0.63 - ETA: 0s - loss: 0.6823 - accuracy: 0.62 - ETA: 0s - loss: 0.6840 - accuracy: 0.61 - 0s 5ms/step - loss: 0.6861 - accuracy: 0.6062 - val_loss: 0.6728 - val_accuracy: 0.7015\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.46 - ETA: 0s - loss: 0.6801 - accuracy: 0.49 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.7037 - accuracy: 0.53 - ETA: 0s - loss: 0.6974 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.50 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - 0s 5ms/step - loss: 0.6804 - accuracy: 0.5786 - val_loss: 0.6876 - val_accuracy: 0.7224\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7077 - accuracy: 0.65 - ETA: 0s - loss: 0.6561 - accuracy: 0.71 - ETA: 0s - loss: 0.8903 - accuracy: 0.71 - ETA: 0s - loss: 0.8352 - accuracy: 0.70 - ETA: 0s - loss: 0.7970 - accuracy: 0.67 - ETA: 0s - loss: 0.7751 - accuracy: 0.65 - ETA: 0s - loss: 0.7590 - accuracy: 0.63 - ETA: 0s - loss: 0.7451 - accuracy: 0.63 - ETA: 0s - loss: 0.7367 - accuracy: 0.6476Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7367 - accuracy: 0.6476 - val_loss: 0.6607 - val_accuracy: 0.7015\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.8572 - accuracy: 0.56 - ETA: 0s - loss: 5.0291 - accuracy: 0.61 - ETA: 0s - loss: 3.8186 - accuracy: 0.60 - ETA: 0s - loss: 2.7557 - accuracy: 0.61 - ETA: 0s - loss: 2.2100 - accuracy: 0.63 - ETA: 0s - loss: 1.8844 - accuracy: 0.65 - ETA: 0s - loss: 1.6706 - accuracy: 0.67 - ETA: 0s - loss: 1.5365 - accuracy: 0.68 - 1s 7ms/step - loss: 1.4407 - accuracy: 0.6891 - val_loss: 0.5740 - val_accuracy: 0.7284\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6149 - accuracy: 0.75 - ETA: 0s - loss: 0.6150 - accuracy: 0.74 - ETA: 0s - loss: 0.6197 - accuracy: 0.74 - ETA: 0s - loss: 0.6155 - accuracy: 0.75 - ETA: 0s - loss: 0.6147 - accuracy: 0.75 - ETA: 0s - loss: 0.6214 - accuracy: 0.75 - ETA: 0s - loss: 0.6175 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6137 - accuracy: 0.7563 - val_loss: 0.6059 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4678 - accuracy: 0.87 - ETA: 0s - loss: 0.5559 - accuracy: 0.79 - ETA: 0s - loss: 0.5618 - accuracy: 0.78 - ETA: 0s - loss: 0.5571 - accuracy: 0.78 - ETA: 0s - loss: 0.5696 - accuracy: 0.78 - ETA: 0s - loss: 0.5813 - accuracy: 0.77 - ETA: 0s - loss: 0.5766 - accuracy: 0.78 - ETA: 0s - loss: 0.5818 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5813 - accuracy: 0.7760 - val_loss: 0.5874 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.93 - ETA: 0s - loss: 0.5455 - accuracy: 0.80 - ETA: 0s - loss: 0.5256 - accuracy: 0.82 - ETA: 0s - loss: 0.5292 - accuracy: 0.81 - ETA: 0s - loss: 0.5394 - accuracy: 0.81 - ETA: 0s - loss: 0.5370 - accuracy: 0.81 - ETA: 0s - loss: 0.5465 - accuracy: 0.80 - ETA: 0s - loss: 0.5556 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5571 - accuracy: 0.7984 - val_loss: 0.5854 - val_accuracy: 0.7418\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.81 - ETA: 0s - loss: 0.5154 - accuracy: 0.80 - ETA: 0s - loss: 0.5378 - accuracy: 0.79 - ETA: 0s - loss: 0.5186 - accuracy: 0.81 - ETA: 0s - loss: 0.5323 - accuracy: 0.81 - ETA: 0s - loss: 0.5358 - accuracy: 0.80 - ETA: 0s - loss: 0.5434 - accuracy: 0.80 - ETA: 0s - loss: 0.5503 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5492 - accuracy: 0.8037 - val_loss: 0.5670 - val_accuracy: 0.7687\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.71 - ETA: 0s - loss: 0.5116 - accuracy: 0.82 - ETA: 0s - loss: 0.5265 - accuracy: 0.81 - ETA: 0s - loss: 0.5528 - accuracy: 0.80 - ETA: 0s - loss: 0.5441 - accuracy: 0.81 - ETA: 0s - loss: 0.5274 - accuracy: 0.82 - ETA: 0s - loss: 0.5266 - accuracy: 0.82 - ETA: 0s - loss: 0.5283 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5320 - accuracy: 0.8219 - val_loss: 0.5634 - val_accuracy: 0.7582\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.87 - ETA: 0s - loss: 0.5165 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - ETA: 0s - loss: 0.5000 - accuracy: 0.83 - ETA: 0s - loss: 0.4986 - accuracy: 0.83 - ETA: 0s - loss: 0.5157 - accuracy: 0.82 - ETA: 0s - loss: 0.5153 - accuracy: 0.82 - ETA: 0s - loss: 0.5195 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5184 - accuracy: 0.8272 - val_loss: 0.5739 - val_accuracy: 0.7582\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4811 - accuracy: 0.84 - ETA: 0s - loss: 0.5395 - accuracy: 0.81 - ETA: 0s - loss: 0.5608 - accuracy: 0.80 - ETA: 0s - loss: 0.5508 - accuracy: 0.81 - ETA: 0s - loss: 0.5476 - accuracy: 0.81 - ETA: 0s - loss: 0.5549 - accuracy: 0.81 - ETA: 0s - loss: 0.5524 - accuracy: 0.81 - ETA: 0s - loss: 0.5499 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5529 - accuracy: 0.8141 - val_loss: 0.5831 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.5625 - accuracy: 0.79 - ETA: 0s - loss: 0.5723 - accuracy: 0.79 - ETA: 0s - loss: 0.5602 - accuracy: 0.79 - ETA: 0s - loss: 0.5474 - accuracy: 0.80 - ETA: 0s - loss: 0.5481 - accuracy: 0.80 - ETA: 0s - loss: 0.5511 - accuracy: 0.80 - ETA: 0s - loss: 0.5460 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5465 - accuracy: 0.8096 - val_loss: 0.6327 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.78 - ETA: 0s - loss: 0.5013 - accuracy: 0.82 - ETA: 0s - loss: 0.5062 - accuracy: 0.83 - ETA: 0s - loss: 0.5114 - accuracy: 0.82 - ETA: 0s - loss: 0.5214 - accuracy: 0.82 - ETA: 0s - loss: 0.5130 - accuracy: 0.82 - ETA: 0s - loss: 0.5152 - accuracy: 0.82 - ETA: 0s - loss: 0.5148 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5153 - accuracy: 0.8302 - val_loss: 0.5864 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8497 - accuracy: 0.75 - ETA: 0s - loss: 0.5896 - accuracy: 0.80 - ETA: 0s - loss: 0.5586 - accuracy: 0.80 - ETA: 0s - loss: 0.5559 - accuracy: 0.80 - ETA: 0s - loss: 0.5472 - accuracy: 0.81 - ETA: 0s - loss: 0.5212 - accuracy: 0.82 - ETA: 0s - loss: 0.5115 - accuracy: 0.83 - ETA: 0s - loss: 0.5129 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5129 - accuracy: 0.8331 - val_loss: 0.6282 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.84 - ETA: 0s - loss: 0.5193 - accuracy: 0.83 - ETA: 0s - loss: 0.5088 - accuracy: 0.84 - ETA: 0s - loss: 0.5477 - accuracy: 0.83 - ETA: 0s - loss: 0.5512 - accuracy: 0.83 - ETA: 0s - loss: 0.5616 - accuracy: 0.82 - ETA: 0s - loss: 0.5685 - accuracy: 0.81 - ETA: 0s - loss: 0.5712 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5683 - accuracy: 0.8163 - val_loss: 0.6640 - val_accuracy: 0.7612\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.78 - ETA: 0s - loss: 0.5032 - accuracy: 0.83 - ETA: 0s - loss: 0.5072 - accuracy: 0.83 - ETA: 0s - loss: 0.5337 - accuracy: 0.82 - ETA: 0s - loss: 0.5343 - accuracy: 0.81 - ETA: 0s - loss: 0.5341 - accuracy: 0.81 - ETA: 0s - loss: 0.5328 - accuracy: 0.81 - ETA: 0s - loss: 0.5298 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5340 - accuracy: 0.8141 - val_loss: 0.6433 - val_accuracy: 0.7552\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9464 - accuracy: 0.68 - ETA: 0s - loss: 0.5280 - accuracy: 0.82 - ETA: 0s - loss: 0.5670 - accuracy: 0.80 - ETA: 0s - loss: 0.5391 - accuracy: 0.82 - ETA: 0s - loss: 0.5338 - accuracy: 0.82 - ETA: 0s - loss: 0.5382 - accuracy: 0.82 - ETA: 0s - loss: 0.5411 - accuracy: 0.81 - ETA: 0s - loss: 0.5355 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5325 - accuracy: 0.8223 - val_loss: 0.6081 - val_accuracy: 0.7627\n",
      "Epoch 15/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.4392 - accuracy: 0.84 - ETA: 0s - loss: 0.4838 - accuracy: 0.83 - ETA: 0s - loss: 0.5039 - accuracy: 0.82 - ETA: 0s - loss: 0.4869 - accuracy: 0.83 - ETA: 0s - loss: 0.5070 - accuracy: 0.83 - ETA: 0s - loss: 0.5099 - accuracy: 0.83 - ETA: 0s - loss: 0.5137 - accuracy: 0.83 - ETA: 0s - loss: 0.5181 - accuracy: 0.8304Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.8305 - val_loss: 0.6892 - val_accuracy: 0.7627\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0538 - accuracy: 0.56 - ETA: 0s - loss: 3.7182 - accuracy: 0.56 - ETA: 0s - loss: 2.3359 - accuracy: 0.59 - ETA: 0s - loss: 1.8489 - accuracy: 0.62 - ETA: 0s - loss: 1.5417 - accuracy: 0.65 - ETA: 0s - loss: 1.3652 - accuracy: 0.66 - ETA: 0s - loss: 1.2500 - accuracy: 0.66 - ETA: 0s - loss: 1.1578 - accuracy: 0.67 - 1s 7ms/step - loss: 1.1078 - accuracy: 0.6812 - val_loss: 0.5750 - val_accuracy: 0.7493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.71 - ETA: 0s - loss: 0.5422 - accuracy: 0.76 - ETA: 0s - loss: 0.5408 - accuracy: 0.77 - ETA: 0s - loss: 0.5499 - accuracy: 0.76 - ETA: 0s - loss: 0.5512 - accuracy: 0.77 - ETA: 0s - loss: 0.5798 - accuracy: 0.76 - ETA: 0s - loss: 0.5882 - accuracy: 0.75 - ETA: 0s - loss: 0.5935 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5915 - accuracy: 0.7559 - val_loss: 0.5734 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.90 - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - ETA: 0s - loss: 0.5575 - accuracy: 0.81 - ETA: 0s - loss: 0.5603 - accuracy: 0.81 - ETA: 0s - loss: 0.5644 - accuracy: 0.79 - ETA: 0s - loss: 0.5716 - accuracy: 0.78 - ETA: 0s - loss: 0.5726 - accuracy: 0.78 - ETA: 0s - loss: 0.5712 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5687 - accuracy: 0.7876 - val_loss: 0.5787 - val_accuracy: 0.7403\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4548 - accuracy: 0.84 - ETA: 0s - loss: 0.5117 - accuracy: 0.83 - ETA: 0s - loss: 0.5215 - accuracy: 0.82 - ETA: 0s - loss: 0.5384 - accuracy: 0.81 - ETA: 0s - loss: 0.5354 - accuracy: 0.81 - ETA: 0s - loss: 0.5363 - accuracy: 0.81 - ETA: 0s - loss: 0.5481 - accuracy: 0.81 - ETA: 0s - loss: 0.5519 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5573 - accuracy: 0.8003 - val_loss: 0.6098 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.84 - ETA: 0s - loss: 0.5462 - accuracy: 0.79 - ETA: 0s - loss: 0.5456 - accuracy: 0.79 - ETA: 0s - loss: 0.5449 - accuracy: 0.80 - ETA: 0s - loss: 0.5468 - accuracy: 0.80 - ETA: 0s - loss: 0.5672 - accuracy: 0.80 - ETA: 0s - loss: 0.5615 - accuracy: 0.80 - ETA: 0s - loss: 0.5828 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5778 - accuracy: 0.7988 - val_loss: 0.6028 - val_accuracy: 0.7418\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.84 - ETA: 0s - loss: 0.5382 - accuracy: 0.81 - ETA: 0s - loss: 0.5384 - accuracy: 0.81 - ETA: 0s - loss: 0.5520 - accuracy: 0.80 - ETA: 0s - loss: 0.5347 - accuracy: 0.81 - ETA: 0s - loss: 0.5346 - accuracy: 0.81 - ETA: 0s - loss: 0.5430 - accuracy: 0.81 - ETA: 0s - loss: 0.5441 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5566 - accuracy: 0.8115 - val_loss: 0.5722 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.81 - ETA: 0s - loss: 0.5597 - accuracy: 0.79 - ETA: 0s - loss: 0.5800 - accuracy: 0.79 - ETA: 0s - loss: 0.5973 - accuracy: 0.78 - ETA: 0s - loss: 0.6125 - accuracy: 0.77 - ETA: 0s - loss: 0.6117 - accuracy: 0.78 - ETA: 0s - loss: 0.6148 - accuracy: 0.77 - ETA: 0s - loss: 0.6136 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6134 - accuracy: 0.7786 - val_loss: 0.6192 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.75 - ETA: 0s - loss: 0.5609 - accuracy: 0.78 - ETA: 0s - loss: 0.5800 - accuracy: 0.78 - ETA: 0s - loss: 0.5745 - accuracy: 0.79 - ETA: 0s - loss: 0.5620 - accuracy: 0.80 - ETA: 0s - loss: 0.5656 - accuracy: 0.80 - ETA: 0s - loss: 0.5738 - accuracy: 0.79 - ETA: 0s - loss: 0.6001 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6029 - accuracy: 0.7805 - val_loss: 0.6241 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.84 - ETA: 0s - loss: 0.5512 - accuracy: 0.80 - ETA: 0s - loss: 0.6250 - accuracy: 0.77 - ETA: 0s - loss: 0.6108 - accuracy: 0.77 - ETA: 0s - loss: 0.6210 - accuracy: 0.77 - ETA: 0s - loss: 0.6256 - accuracy: 0.77 - ETA: 0s - loss: 0.6256 - accuracy: 0.78 - ETA: 0s - loss: 0.6195 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6176 - accuracy: 0.7839 - val_loss: 0.6406 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.71 - ETA: 0s - loss: 0.5958 - accuracy: 0.78 - ETA: 0s - loss: 0.5840 - accuracy: 0.80 - ETA: 0s - loss: 0.5656 - accuracy: 0.81 - ETA: 0s - loss: 0.5698 - accuracy: 0.81 - ETA: 0s - loss: 0.5695 - accuracy: 0.81 - ETA: 0s - loss: 0.5884 - accuracy: 0.80 - ETA: 0s - loss: 0.5887 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5881 - accuracy: 0.8018 - val_loss: 0.6275 - val_accuracy: 0.7672\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.71 - ETA: 0s - loss: 0.6283 - accuracy: 0.77 - ETA: 0s - loss: 0.6096 - accuracy: 0.79 - ETA: 0s - loss: 0.6008 - accuracy: 0.79 - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - ETA: 0s - loss: 0.6158 - accuracy: 0.78 - ETA: 0s - loss: 0.6169 - accuracy: 0.77 - ETA: 0s - loss: 0.6204 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6163 - accuracy: 0.7764 - val_loss: 0.6242 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.81 - ETA: 0s - loss: 0.6654 - accuracy: 0.78 - ETA: 0s - loss: 0.6345 - accuracy: 0.79 - ETA: 0s - loss: 0.5943 - accuracy: 0.80 - ETA: 0s - loss: 0.5834 - accuracy: 0.80 - ETA: 0s - loss: 0.5828 - accuracy: 0.80 - ETA: 0s - loss: 0.5748 - accuracy: 0.80 - ETA: 0s - loss: 0.5798 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5900 - accuracy: 0.7951 - val_loss: 0.6009 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.68 - ETA: 0s - loss: 0.6611 - accuracy: 0.73 - ETA: 0s - loss: 0.6291 - accuracy: 0.74 - ETA: 0s - loss: 0.6237 - accuracy: 0.75 - ETA: 0s - loss: 0.6255 - accuracy: 0.75 - ETA: 0s - loss: 0.6272 - accuracy: 0.75 - ETA: 0s - loss: 0.6187 - accuracy: 0.75 - ETA: 0s - loss: 0.6210 - accuracy: 0.75 - ETA: 0s - loss: 0.6193 - accuracy: 0.75 - 0s 6ms/step - loss: 0.6193 - accuracy: 0.7585 - val_loss: 0.6256 - val_accuracy: 0.7582\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.84 - ETA: 0s - loss: 0.9439 - accuracy: 0.75 - ETA: 0s - loss: 0.8597 - accuracy: 0.75 - ETA: 0s - loss: 0.7755 - accuracy: 0.76 - ETA: 0s - loss: 0.7421 - accuracy: 0.76 - ETA: 0s - loss: 0.7412 - accuracy: 0.75 - ETA: 0s - loss: 0.7346 - accuracy: 0.74 - ETA: 0s - loss: 0.7208 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7124 - accuracy: 0.7062 - val_loss: 0.9999 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.9447 - accuracy: 0.53 - ETA: 0s - loss: 0.7236 - accuracy: 0.71 - ETA: 0s - loss: 0.6838 - accuracy: 0.72 - ETA: 0s - loss: 0.6819 - accuracy: 0.73 - ETA: 0s - loss: 0.7066 - accuracy: 0.73 - ETA: 0s - loss: 0.7038 - accuracy: 0.71 - ETA: 0s - loss: 0.6997 - accuracy: 0.68 - ETA: 0s - loss: 0.6987 - accuracy: 0.67 - 0s 5ms/step - loss: 0.6964 - accuracy: 0.6655 - val_loss: 0.6718 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.56 - ETA: 0s - loss: 0.6359 - accuracy: 0.61 - ETA: 0s - loss: 0.7147 - accuracy: 0.63 - ETA: 0s - loss: 0.7033 - accuracy: 0.64 - ETA: 0s - loss: 0.6986 - accuracy: 0.61 - ETA: 0s - loss: 0.6911 - accuracy: 0.61 - ETA: 0s - loss: 0.6862 - accuracy: 0.63 - ETA: 0s - loss: 0.6862 - accuracy: 0.63 - 0s 5ms/step - loss: 0.6873 - accuracy: 0.6390 - val_loss: 0.6410 - val_accuracy: 0.7090\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.66 - ETA: 0s - loss: 0.6908 - accuracy: 0.62 - ETA: 0s - loss: 0.6906 - accuracy: 0.59 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.57 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - 0s 6ms/step - loss: 0.6842 - accuracy: 0.5864 - val_loss: 0.6483 - val_accuracy: 0.7075\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.64 - ETA: 0s - loss: 0.6720 - accuracy: 0.63 - ETA: 0s - loss: 0.6690 - accuracy: 0.64 - ETA: 0s - loss: 0.6643 - accuracy: 0.65 - ETA: 0s - loss: 0.6561 - accuracy: 0.66 - ETA: 0s - loss: 0.6471 - accuracy: 0.67 - ETA: 0s - loss: 0.6437 - accuracy: 0.68 - ETA: 0s - loss: 0.6444 - accuracy: 0.69 - 0s 6ms/step - loss: 0.6444 - accuracy: 0.6965 - val_loss: 0.9744 - val_accuracy: 0.7493\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.71 - ETA: 0s - loss: 0.5573 - accuracy: 0.80 - ETA: 0s - loss: 0.6793 - accuracy: 0.77 - ETA: 0s - loss: 0.6715 - accuracy: 0.77 - ETA: 0s - loss: 0.7089 - accuracy: 0.74 - ETA: 0s - loss: 0.7171 - accuracy: 0.71 - ETA: 0s - loss: 0.7130 - accuracy: 0.69 - ETA: 0s - loss: 0.7115 - accuracy: 0.67 - ETA: 0s - loss: 0.7060 - accuracy: 0.64 - ETA: 0s - loss: 0.7074 - accuracy: 0.63 - 1s 7ms/step - loss: 0.7030 - accuracy: 0.6402 - val_loss: 0.6542 - val_accuracy: 0.6955\n",
      "Epoch 20/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6700 - accuracy: 0.62 - ETA: 0s - loss: 0.6963 - accuracy: 0.66 - ETA: 0s - loss: 0.6904 - accuracy: 0.65 - ETA: 0s - loss: 0.6990 - accuracy: 0.62 - ETA: 0s - loss: 0.6973 - accuracy: 0.58 - ETA: 0s - loss: 0.6975 - accuracy: 0.58 - ETA: 0s - loss: 0.6979 - accuracy: 0.58 - ETA: 0s - loss: 0.6994 - accuracy: 0.58 - ETA: 0s - loss: 0.6976 - accuracy: 0.5744Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5704 - val_loss: 0.6719 - val_accuracy: 0.6955\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 7821731c10bca5148d0a653205a79799</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7616915504137675</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8244673724051693</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.62 - ETA: 0s - loss: 1.4970 - accuracy: 0.58 - ETA: 0s - loss: 1.0746 - accuracy: 0.63 - ETA: 0s - loss: 0.9153 - accuracy: 0.65 - ETA: 0s - loss: 0.8268 - accuracy: 0.66 - ETA: 0s - loss: 0.7741 - accuracy: 0.67 - ETA: 0s - loss: 0.7441 - accuracy: 0.68 - 1s 7ms/step - loss: 0.7250 - accuracy: 0.6846 - val_loss: 0.5275 - val_accuracy: 0.7373\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6327 - accuracy: 0.78 - ETA: 0s - loss: 0.4960 - accuracy: 0.80 - ETA: 0s - loss: 0.4929 - accuracy: 0.78 - ETA: 0s - loss: 0.4863 - accuracy: 0.78 - ETA: 0s - loss: 0.4831 - accuracy: 0.77 - ETA: 0s - loss: 0.4788 - accuracy: 0.78 - ETA: 0s - loss: 0.4894 - accuracy: 0.77 - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7704 - val_loss: 0.6394 - val_accuracy: 0.6642\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.87 - ETA: 0s - loss: 0.4591 - accuracy: 0.79 - ETA: 0s - loss: 0.4515 - accuracy: 0.78 - ETA: 0s - loss: 0.4505 - accuracy: 0.78 - ETA: 0s - loss: 0.4589 - accuracy: 0.79 - ETA: 0s - loss: 0.4499 - accuracy: 0.79 - ETA: 0s - loss: 0.4563 - accuracy: 0.78 - 0s 5ms/step - loss: 0.4577 - accuracy: 0.7869 - val_loss: 0.6402 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.81 - ETA: 0s - loss: 0.3904 - accuracy: 0.81 - ETA: 0s - loss: 0.4637 - accuracy: 0.79 - ETA: 0s - loss: 0.4442 - accuracy: 0.81 - ETA: 0s - loss: 0.4546 - accuracy: 0.81 - ETA: 0s - loss: 0.4694 - accuracy: 0.80 - ETA: 0s - loss: 0.4649 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4555 - accuracy: 0.8108 - val_loss: 0.6931 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.84 - ETA: 0s - loss: 0.3159 - accuracy: 0.86 - ETA: 0s - loss: 0.3415 - accuracy: 0.84 - ETA: 0s - loss: 0.3598 - accuracy: 0.83 - ETA: 0s - loss: 0.3687 - accuracy: 0.83 - ETA: 0s - loss: 0.3792 - accuracy: 0.83 - ETA: 0s - loss: 0.3781 - accuracy: 0.83 - 0s 5ms/step - loss: 0.3812 - accuracy: 0.8302 - val_loss: 0.6820 - val_accuracy: 0.6791\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.93 - ETA: 0s - loss: 0.3114 - accuracy: 0.87 - ETA: 0s - loss: 0.3068 - accuracy: 0.87 - ETA: 0s - loss: 0.3292 - accuracy: 0.86 - ETA: 0s - loss: 0.3281 - accuracy: 0.86 - ETA: 0s - loss: 0.3412 - accuracy: 0.85 - ETA: 0s - loss: 0.3923 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3957 - accuracy: 0.8432 - val_loss: 0.9279 - val_accuracy: 0.6567\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.84 - ETA: 0s - loss: 0.4066 - accuracy: 0.81 - ETA: 0s - loss: 0.4082 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - 1s 7ms/step - loss: 0.3971 - accuracy: 0.8302 - val_loss: 0.7890 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.93 - ETA: 0s - loss: 0.3577 - accuracy: 0.84 - ETA: 0s - loss: 0.3575 - accuracy: 0.84 - ETA: 0s - loss: 0.3739 - accuracy: 0.84 - ETA: 0s - loss: 0.4065 - accuracy: 0.84 - ETA: 0s - loss: 0.3961 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.84 - ETA: 0s - loss: 0.3812 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3748 - accuracy: 0.8544 - val_loss: 1.2179 - val_accuracy: 0.6866\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 1.00 - ETA: 0s - loss: 0.2849 - accuracy: 0.90 - ETA: 0s - loss: 0.3480 - accuracy: 0.89 - ETA: 0s - loss: 0.3298 - accuracy: 0.89 - ETA: 0s - loss: 0.3291 - accuracy: 0.89 - ETA: 0s - loss: 0.3478 - accuracy: 0.88 - ETA: 0s - loss: 0.3441 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3485 - accuracy: 0.8806 - val_loss: 0.5901 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.84 - ETA: 0s - loss: 0.3554 - accuracy: 0.84 - ETA: 0s - loss: 0.3192 - accuracy: 0.85 - ETA: 0s - loss: 0.3075 - accuracy: 0.87 - ETA: 0s - loss: 0.3039 - accuracy: 0.87 - ETA: 0s - loss: 0.3022 - accuracy: 0.87 - ETA: 0s - loss: 0.2949 - accuracy: 0.87 - 0s 5ms/step - loss: 0.2913 - accuracy: 0.8824 - val_loss: 1.0628 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.84 - ETA: 0s - loss: 0.3284 - accuracy: 0.88 - ETA: 0s - loss: 0.3526 - accuracy: 0.87 - ETA: 0s - loss: 0.3775 - accuracy: 0.85 - ETA: 0s - loss: 0.3783 - accuracy: 0.85 - ETA: 0s - loss: 0.3605 - accuracy: 0.85 - ETA: 0s - loss: 0.3701 - accuracy: 0.8578Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8578 - val_loss: 1.6437 - val_accuracy: 0.6985\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.71 - ETA: 0s - loss: 1.4914 - accuracy: 0.63 - ETA: 0s - loss: 1.1132 - accuracy: 0.65 - ETA: 0s - loss: 0.9602 - accuracy: 0.67 - ETA: 0s - loss: 0.8714 - accuracy: 0.67 - ETA: 0s - loss: 0.8180 - accuracy: 0.68 - ETA: 0s - loss: 0.7733 - accuracy: 0.68 - 0s 6ms/step - loss: 0.7650 - accuracy: 0.6928 - val_loss: 0.6376 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.65 - ETA: 0s - loss: 0.5487 - accuracy: 0.75 - ETA: 0s - loss: 0.5360 - accuracy: 0.75 - ETA: 0s - loss: 0.5303 - accuracy: 0.75 - ETA: 0s - loss: 0.5389 - accuracy: 0.76 - ETA: 0s - loss: 0.5483 - accuracy: 0.75 - ETA: 0s - loss: 0.5446 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5404 - accuracy: 0.7566 - val_loss: 0.5515 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.87 - ETA: 0s - loss: 0.4738 - accuracy: 0.80 - ETA: 0s - loss: 0.4738 - accuracy: 0.79 - ETA: 0s - loss: 0.4633 - accuracy: 0.79 - ETA: 0s - loss: 0.4704 - accuracy: 0.78 - ETA: 0s - loss: 0.4724 - accuracy: 0.79 - ETA: 0s - loss: 0.4733 - accuracy: 0.78 - 0s 4ms/step - loss: 0.4676 - accuracy: 0.7902 - val_loss: 0.8229 - val_accuracy: 0.6209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.87 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.4069 - accuracy: 0.82 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4109 - accuracy: 0.8167 - val_loss: 0.6421 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2331 - accuracy: 0.90 - ETA: 0s - loss: 0.4312 - accuracy: 0.82 - ETA: 0s - loss: 0.4112 - accuracy: 0.84 - ETA: 0s - loss: 0.3967 - accuracy: 0.84 - ETA: 0s - loss: 0.4124 - accuracy: 0.83 - ETA: 0s - loss: 0.4217 - accuracy: 0.82 - ETA: 0s - loss: 0.4214 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4193 - accuracy: 0.8283 - val_loss: 0.7565 - val_accuracy: 0.7149\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.87 - ETA: 0s - loss: 0.4388 - accuracy: 0.84 - ETA: 0s - loss: 0.4407 - accuracy: 0.84 - ETA: 0s - loss: 0.4035 - accuracy: 0.85 - ETA: 0s - loss: 0.4159 - accuracy: 0.85 - ETA: 0s - loss: 0.4056 - accuracy: 0.85 - ETA: 0s - loss: 0.4281 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4343 - accuracy: 0.8447 - val_loss: 0.7660 - val_accuracy: 0.6612\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4195 - accuracy: 0.75 - ETA: 0s - loss: 0.3255 - accuracy: 0.88 - ETA: 0s - loss: 0.4114 - accuracy: 0.82 - ETA: 0s - loss: 0.4163 - accuracy: 0.83 - ETA: 0s - loss: 0.4030 - accuracy: 0.83 - ETA: 0s - loss: 0.3935 - accuracy: 0.84 - ETA: 0s - loss: 0.3973 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3983 - accuracy: 0.8414 - val_loss: 1.1051 - val_accuracy: 0.6448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.68 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3424 - accuracy: 0.85 - ETA: 0s - loss: 0.3394 - accuracy: 0.86 - ETA: 0s - loss: 0.3399 - accuracy: 0.87 - ETA: 0s - loss: 0.3503 - accuracy: 0.87 - ETA: 0s - loss: 0.3553 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3604 - accuracy: 0.8675 - val_loss: 0.7482 - val_accuracy: 0.6731\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.87 - ETA: 0s - loss: 0.3246 - accuracy: 0.86 - ETA: 0s - loss: 0.3352 - accuracy: 0.86 - ETA: 0s - loss: 0.3188 - accuracy: 0.87 - ETA: 0s - loss: 0.3159 - accuracy: 0.88 - ETA: 0s - loss: 0.3213 - accuracy: 0.88 - ETA: 0s - loss: 0.3356 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3356 - accuracy: 0.8742 - val_loss: 0.7525 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.78 - ETA: 0s - loss: 0.2919 - accuracy: 0.89 - ETA: 0s - loss: 0.3033 - accuracy: 0.87 - ETA: 0s - loss: 0.3157 - accuracy: 0.88 - ETA: 0s - loss: 0.3334 - accuracy: 0.87 - ETA: 0s - loss: 0.3343 - accuracy: 0.87 - ETA: 0s - loss: 0.3326 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8832 - val_loss: 0.7592 - val_accuracy: 0.7045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.96 - ETA: 0s - loss: 0.2558 - accuracy: 0.92 - ETA: 0s - loss: 0.2783 - accuracy: 0.91 - ETA: 0s - loss: 0.2783 - accuracy: 0.91 - ETA: 0s - loss: 0.3062 - accuracy: 0.90 - ETA: 0s - loss: 0.3083 - accuracy: 0.89 - ETA: 0s - loss: 0.2963 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3076 - accuracy: 0.8970 - val_loss: 0.7948 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.87 - ETA: 0s - loss: 0.2391 - accuracy: 0.89 - ETA: 0s - loss: 0.2825 - accuracy: 0.88 - ETA: 0s - loss: 0.3256 - accuracy: 0.86 - ETA: 0s - loss: 0.3479 - accuracy: 0.86 - ETA: 0s - loss: 0.3487 - accuracy: 0.85 - ETA: 0s - loss: 0.3422 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3402 - accuracy: 0.8641 - val_loss: 0.8119 - val_accuracy: 0.6925\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.93 - ETA: 0s - loss: 0.2374 - accuracy: 0.91 - ETA: 0s - loss: 0.2549 - accuracy: 0.92 - ETA: 0s - loss: 0.2691 - accuracy: 0.91 - ETA: 0s - loss: 0.2776 - accuracy: 0.90 - ETA: 0s - loss: 0.2983 - accuracy: 0.90 - ETA: 0s - loss: 0.2958 - accuracy: 0.90 - 0s 5ms/step - loss: 0.2982 - accuracy: 0.9056 - val_loss: 0.7760 - val_accuracy: 0.7343\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 1.00 - ETA: 0s - loss: 0.3511 - accuracy: 0.89 - ETA: 0s - loss: 0.2973 - accuracy: 0.91 - ETA: 0s - loss: 0.3101 - accuracy: 0.91 - ETA: 0s - loss: 0.3092 - accuracy: 0.91 - ETA: 0s - loss: 0.2955 - accuracy: 0.91 - ETA: 0s - loss: 0.3095 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3073 - accuracy: 0.9033 - val_loss: 0.6927 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.90 - ETA: 0s - loss: 0.3067 - accuracy: 0.89 - ETA: 0s - loss: 0.3921 - accuracy: 0.88 - ETA: 0s - loss: 0.4748 - accuracy: 0.86 - ETA: 0s - loss: 0.4670 - accuracy: 0.85 - ETA: 0s - loss: 0.4730 - accuracy: 0.84 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4390 - accuracy: 0.8514 - val_loss: 0.8767 - val_accuracy: 0.6657\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.90 - ETA: 0s - loss: 0.2565 - accuracy: 0.91 - ETA: 0s - loss: 0.3138 - accuracy: 0.91 - ETA: 0s - loss: 0.3100 - accuracy: 0.91 - ETA: 0s - loss: 0.3221 - accuracy: 0.90 - ETA: 0s - loss: 0.3367 - accuracy: 0.89 - ETA: 0s - loss: 0.3515 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3464 - accuracy: 0.8940 - val_loss: 1.2132 - val_accuracy: 0.7224\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.90 - ETA: 0s - loss: 0.4452 - accuracy: 0.89 - ETA: 0s - loss: 0.5822 - accuracy: 0.84 - ETA: 0s - loss: 0.6289 - accuracy: 0.82 - ETA: 0s - loss: 0.6487 - accuracy: 0.79 - ETA: 0s - loss: 0.6861 - accuracy: 0.78 - ETA: 0s - loss: 0.6762 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6745 - accuracy: 0.7581 - val_loss: 0.5861 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.75 - ETA: 0s - loss: 0.6027 - accuracy: 0.73 - ETA: 0s - loss: 0.6118 - accuracy: 0.71 - ETA: 0s - loss: 0.6286 - accuracy: 0.69 - ETA: 0s - loss: 0.6192 - accuracy: 0.69 - ETA: 0s - loss: 0.6097 - accuracy: 0.69 - ETA: 0s - loss: 0.6125 - accuracy: 0.68 - ETA: 0s - loss: 0.6159 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6149 - accuracy: 0.6801 - val_loss: 0.6549 - val_accuracy: 0.5970\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.81 - ETA: 0s - loss: 0.6102 - accuracy: 0.67 - ETA: 0s - loss: 0.6142 - accuracy: 0.68 - ETA: 0s - loss: 0.6079 - accuracy: 0.66 - ETA: 0s - loss: 0.6161 - accuracy: 0.66 - ETA: 0s - loss: 0.6208 - accuracy: 0.67 - ETA: 0s - loss: 0.6181 - accuracy: 0.67 - ETA: 0s - loss: 0.6193 - accuracy: 0.67 - 0s 5ms/step - loss: 0.6210 - accuracy: 0.6693 - val_loss: 0.6940 - val_accuracy: 0.5821\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.53 - ETA: 0s - loss: 0.5835 - accuracy: 0.69 - ETA: 0s - loss: 0.5809 - accuracy: 0.67 - ETA: 0s - loss: 0.5928 - accuracy: 0.64 - ETA: 0s - loss: 0.6106 - accuracy: 0.61 - ETA: 0s - loss: 0.6144 - accuracy: 0.60 - ETA: 0s - loss: 0.6175 - accuracy: 0.59 - ETA: 0s - loss: 0.6201 - accuracy: 0.58 - 0s 5ms/step - loss: 0.6459 - accuracy: 0.5775 - val_loss: 0.7187 - val_accuracy: 0.4642\n",
      "Epoch 21/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.7251 - accuracy: 0.34 - ETA: 0s - loss: 0.6502 - accuracy: 0.47 - ETA: 0s - loss: 0.6476 - accuracy: 0.49 - ETA: 0s - loss: 0.6588 - accuracy: 0.47 - ETA: 0s - loss: 0.6590 - accuracy: 0.47 - ETA: 0s - loss: 0.6595 - accuracy: 0.47 - ETA: 0s - loss: 0.6553 - accuracy: 0.4708Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6569 - accuracy: 0.4696 - val_loss: 0.6992 - val_accuracy: 0.4164\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7006 - accuracy: 0.59 - ETA: 0s - loss: 1.3883 - accuracy: 0.62 - ETA: 0s - loss: 0.9865 - accuracy: 0.67 - ETA: 0s - loss: 0.8509 - accuracy: 0.69 - ETA: 0s - loss: 0.8009 - accuracy: 0.69 - ETA: 0s - loss: 0.7600 - accuracy: 0.69 - ETA: 0s - loss: 0.7318 - accuracy: 0.69 - 1s 6ms/step - loss: 0.7109 - accuracy: 0.7029 - val_loss: 0.6280 - val_accuracy: 0.6985\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4132 - accuracy: 0.84 - ETA: 0s - loss: 0.5298 - accuracy: 0.79 - ETA: 0s - loss: 0.5395 - accuracy: 0.77 - ETA: 0s - loss: 0.5394 - accuracy: 0.77 - ETA: 0s - loss: 0.5317 - accuracy: 0.78 - ETA: 0s - loss: 0.5313 - accuracy: 0.77 - ETA: 0s - loss: 0.5321 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5301 - accuracy: 0.7749 - val_loss: 0.5920 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 1.00 - ETA: 0s - loss: 0.4819 - accuracy: 0.82 - ETA: 0s - loss: 0.4951 - accuracy: 0.80 - ETA: 0s - loss: 0.4775 - accuracy: 0.81 - ETA: 0s - loss: 0.4703 - accuracy: 0.82 - ETA: 0s - loss: 0.4785 - accuracy: 0.81 - ETA: 0s - loss: 0.4871 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4871 - accuracy: 0.8160 - val_loss: 0.5613 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.87 - ETA: 0s - loss: 0.4390 - accuracy: 0.85 - ETA: 0s - loss: 0.4390 - accuracy: 0.84 - ETA: 0s - loss: 0.4480 - accuracy: 0.83 - ETA: 0s - loss: 0.4454 - accuracy: 0.83 - ETA: 0s - loss: 0.4375 - accuracy: 0.83 - ETA: 0s - loss: 0.4448 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4518 - accuracy: 0.8287 - val_loss: 0.6080 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6527 - accuracy: 0.68 - ETA: 0s - loss: 0.4209 - accuracy: 0.82 - ETA: 0s - loss: 0.4215 - accuracy: 0.83 - ETA: 0s - loss: 0.4240 - accuracy: 0.84 - ETA: 0s - loss: 0.4327 - accuracy: 0.83 - ETA: 0s - loss: 0.4310 - accuracy: 0.83 - ETA: 0s - loss: 0.4301 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4305 - accuracy: 0.8421 - val_loss: 0.7570 - val_accuracy: 0.7000\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.87 - ETA: 0s - loss: 0.3855 - accuracy: 0.86 - ETA: 0s - loss: 0.4187 - accuracy: 0.85 - ETA: 0s - loss: 0.4478 - accuracy: 0.85 - ETA: 0s - loss: 0.4459 - accuracy: 0.84 - ETA: 0s - loss: 0.4392 - accuracy: 0.85 - ETA: 0s - loss: 0.4562 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4581 - accuracy: 0.8402 - val_loss: 0.6578 - val_accuracy: 0.7119\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.93 - ETA: 0s - loss: 0.4418 - accuracy: 0.84 - ETA: 0s - loss: 0.4107 - accuracy: 0.85 - ETA: 0s - loss: 0.3877 - accuracy: 0.86 - ETA: 0s - loss: 0.3941 - accuracy: 0.86 - ETA: 0s - loss: 0.4006 - accuracy: 0.86 - ETA: 0s - loss: 0.4024 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4031 - accuracy: 0.8596 - val_loss: 0.6745 - val_accuracy: 0.6940\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.71 - ETA: 0s - loss: 0.3754 - accuracy: 0.87 - ETA: 0s - loss: 0.3684 - accuracy: 0.87 - ETA: 0s - loss: 0.3753 - accuracy: 0.87 - ETA: 0s - loss: 0.4933 - accuracy: 0.87 - ETA: 0s - loss: 0.4770 - accuracy: 0.87 - ETA: 0s - loss: 0.4707 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4748 - accuracy: 0.8611 - val_loss: 1.0951 - val_accuracy: 0.6537\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.87 - ETA: 0s - loss: 0.5257 - accuracy: 0.81 - ETA: 0s - loss: 0.4728 - accuracy: 0.84 - ETA: 0s - loss: 0.4776 - accuracy: 0.84 - ETA: 0s - loss: 0.4943 - accuracy: 0.82 - ETA: 0s - loss: 0.4900 - accuracy: 0.83 - ETA: 0s - loss: 0.4765 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4718 - accuracy: 0.8369 - val_loss: 0.6364 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 1.00 - ETA: 0s - loss: 0.4504 - accuracy: 0.86 - ETA: 0s - loss: 0.4489 - accuracy: 0.86 - ETA: 0s - loss: 0.4361 - accuracy: 0.86 - ETA: 0s - loss: 0.4334 - accuracy: 0.86 - ETA: 0s - loss: 0.4266 - accuracy: 0.86 - ETA: 0s - loss: 0.4279 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8600 - val_loss: 0.8401 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.84 - ETA: 0s - loss: 0.3677 - accuracy: 0.87 - ETA: 0s - loss: 0.3659 - accuracy: 0.88 - ETA: 0s - loss: 0.3704 - accuracy: 0.88 - ETA: 0s - loss: 0.3649 - accuracy: 0.88 - ETA: 0s - loss: 0.3629 - accuracy: 0.89 - ETA: 0s - loss: 0.3533 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3523 - accuracy: 0.8951 - val_loss: 1.2466 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.93 - ETA: 0s - loss: 0.3627 - accuracy: 0.90 - ETA: 0s - loss: 0.4000 - accuracy: 0.87 - ETA: 0s - loss: 0.3805 - accuracy: 0.88 - ETA: 0s - loss: 0.3743 - accuracy: 0.89 - ETA: 0s - loss: 0.3703 - accuracy: 0.88 - ETA: 0s - loss: 0.3694 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3689 - accuracy: 0.8876 - val_loss: 0.9895 - val_accuracy: 0.7119\n",
      "Epoch 13/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.4050 - accuracy: 0.84 - ETA: 0s - loss: 0.3696 - accuracy: 0.87 - ETA: 0s - loss: 0.3561 - accuracy: 0.88 - ETA: 0s - loss: 0.3437 - accuracy: 0.89 - ETA: 0s - loss: 0.3323 - accuracy: 0.89 - ETA: 0s - loss: 0.3322 - accuracy: 0.89 - ETA: 0s - loss: 0.3361 - accuracy: 0.8968Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3356 - accuracy: 0.8955 - val_loss: 0.9642 - val_accuracy: 0.6910\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a37fc29f977e9553bc80c1d165baa977</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7338308493296305</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.03983850507660691</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8895 - accuracy: 0.56 - ETA: 0s - loss: 1.4797 - accuracy: 0.63 - ETA: 0s - loss: 1.0199 - accuracy: 0.67 - ETA: 0s - loss: 0.8747 - accuracy: 0.69 - ETA: 0s - loss: 0.8023 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7847 - accuracy: 0.7081 - val_loss: 0.5709 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7121 - accuracy: 0.75 - ETA: 0s - loss: 0.5819 - accuracy: 0.76 - ETA: 0s - loss: 0.5584 - accuracy: 0.77 - ETA: 0s - loss: 0.5719 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5772 - accuracy: 0.7615 - val_loss: 0.7496 - val_accuracy: 0.6552\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.62 - ETA: 0s - loss: 0.5545 - accuracy: 0.78 - ETA: 0s - loss: 0.5450 - accuracy: 0.78 - ETA: 0s - loss: 0.5535 - accuracy: 0.78 - ETA: 0s - loss: 0.5581 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5581 - accuracy: 0.7798 - val_loss: 0.7127 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.78 - ETA: 0s - loss: 0.4965 - accuracy: 0.80 - ETA: 0s - loss: 0.4924 - accuracy: 0.80 - ETA: 0s - loss: 0.4851 - accuracy: 0.81 - ETA: 0s - loss: 0.4987 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4989 - accuracy: 0.8048 - val_loss: 0.6684 - val_accuracy: 0.6925\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.81 - ETA: 0s - loss: 0.4308 - accuracy: 0.84 - ETA: 0s - loss: 0.4720 - accuracy: 0.82 - ETA: 0s - loss: 0.4674 - accuracy: 0.82 - ETA: 0s - loss: 0.4641 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4653 - accuracy: 0.8261 - val_loss: 0.7382 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.81 - ETA: 0s - loss: 0.4300 - accuracy: 0.85 - ETA: 0s - loss: 0.4079 - accuracy: 0.84 - ETA: 0s - loss: 0.4301 - accuracy: 0.84 - ETA: 0s - loss: 0.4344 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8417 - val_loss: 0.5772 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.87 - ETA: 0s - loss: 0.4342 - accuracy: 0.84 - ETA: 0s - loss: 0.4250 - accuracy: 0.84 - ETA: 0s - loss: 0.4217 - accuracy: 0.83 - ETA: 0s - loss: 0.4303 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4306 - accuracy: 0.8417 - val_loss: 1.0484 - val_accuracy: 0.6746\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.71 - ETA: 0s - loss: 0.5351 - accuracy: 0.85 - ETA: 0s - loss: 0.4765 - accuracy: 0.85 - ETA: 0s - loss: 0.4958 - accuracy: 0.84 - 0s 3ms/step - loss: 0.6014 - accuracy: 0.8350 - val_loss: 0.5650 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.84 - ETA: 0s - loss: 0.4272 - accuracy: 0.85 - ETA: 0s - loss: 0.4322 - accuracy: 0.85 - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - ETA: 0s - loss: 0.4416 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4420 - accuracy: 0.8458 - val_loss: 0.9014 - val_accuracy: 0.6866\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.87 - ETA: 0s - loss: 0.3945 - accuracy: 0.88 - ETA: 0s - loss: 0.4233 - accuracy: 0.86 - ETA: 0s - loss: 0.4157 - accuracy: 0.85 - ETA: 0s - loss: 0.4200 - accuracy: 0.86 - 0s 3ms/step - loss: 0.5067 - accuracy: 0.8559 - val_loss: 0.9910 - val_accuracy: 0.6552\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6294 - accuracy: 0.75 - ETA: 0s - loss: 0.4050 - accuracy: 0.84 - ETA: 0s - loss: 0.4143 - accuracy: 0.83 - ETA: 0s - loss: 0.4092 - accuracy: 0.84 - ETA: 0s - loss: 0.3926 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3993 - accuracy: 0.8544 - val_loss: 0.6214 - val_accuracy: 0.7582\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.84 - ETA: 0s - loss: 0.3593 - accuracy: 0.90 - ETA: 0s - loss: 0.3629 - accuracy: 0.89 - ETA: 0s - loss: 0.3805 - accuracy: 0.88 - ETA: 0s - loss: 0.3815 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8813 - val_loss: 0.9301 - val_accuracy: 0.7075\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.81 - ETA: 0s - loss: 0.3723 - accuracy: 0.87 - ETA: 0s - loss: 0.3458 - accuracy: 0.88 - ETA: 0s - loss: 0.3346 - accuracy: 0.89 - ETA: 0s - loss: 0.3404 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8925 - val_loss: 0.9571 - val_accuracy: 0.7224\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - ETA: 0s - loss: 0.3491 - accuracy: 0.89 - ETA: 0s - loss: 0.3368 - accuracy: 0.89 - ETA: 0s - loss: 0.3151 - accuracy: 0.90 - ETA: 0s - loss: 0.3104 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3129 - accuracy: 0.9071 - val_loss: 1.0538 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.90 - ETA: 0s - loss: 0.3025 - accuracy: 0.90 - ETA: 0s - loss: 0.2775 - accuracy: 0.92 - ETA: 0s - loss: 0.2925 - accuracy: 0.91 - ETA: 0s - loss: 0.3028 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3068 - accuracy: 0.9082 - val_loss: 1.3364 - val_accuracy: 0.7179\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.81 - ETA: 0s - loss: 0.2706 - accuracy: 0.92 - ETA: 0s - loss: 0.3000 - accuracy: 0.91 - ETA: 0s - loss: 0.2998 - accuracy: 0.91 - ETA: 0s - loss: 0.3018 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3036 - accuracy: 0.9149 - val_loss: 0.9375 - val_accuracy: 0.7224\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.84 - ETA: 0s - loss: 0.3458 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.91 - ETA: 0s - loss: 0.3064 - accuracy: 0.91 - ETA: 0s - loss: 0.3110 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3077 - accuracy: 0.9108 - val_loss: 1.3370 - val_accuracy: 0.7149\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.90 - ETA: 0s - loss: 0.2566 - accuracy: 0.94 - ETA: 0s - loss: 0.2722 - accuracy: 0.93 - ETA: 0s - loss: 0.2890 - accuracy: 0.92 - 0s 3ms/step - loss: 0.3080 - accuracy: 0.9183 - val_loss: 1.3004 - val_accuracy: 0.7015\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.84 - ETA: 0s - loss: 0.2754 - accuracy: 0.91 - ETA: 0s - loss: 0.2943 - accuracy: 0.91 - ETA: 0s - loss: 0.3091 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3066 - accuracy: 0.9104 - val_loss: 0.9671 - val_accuracy: 0.7284\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.90 - ETA: 0s - loss: 0.3049 - accuracy: 0.91 - ETA: 0s - loss: 0.2714 - accuracy: 0.92 - ETA: 0s - loss: 0.3383 - accuracy: 0.92 - ETA: 0s - loss: 0.3653 - accuracy: 0.92 - 0s 3ms/step - loss: 0.3692 - accuracy: 0.9186 - val_loss: 0.9102 - val_accuracy: 0.7194\n",
      "Epoch 21/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.3639 - accuracy: 0.90 - ETA: 0s - loss: 0.3502 - accuracy: 0.88 - ETA: 0s - loss: 0.3491 - accuracy: 0.89 - ETA: 0s - loss: 0.4264 - accuracy: 0.89 - ETA: 0s - loss: 0.4031 - accuracy: 0.8982Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8988 - val_loss: 0.8380 - val_accuracy: 0.7299\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.68 - ETA: 0s - loss: 1.6846 - accuracy: 0.64 - ETA: 0s - loss: 1.0917 - accuracy: 0.69 - ETA: 0s - loss: 0.9260 - accuracy: 0.71 - ETA: 0s - loss: 0.8324 - accuracy: 0.72 - 0s 4ms/step - loss: 0.8171 - accuracy: 0.7219 - val_loss: 0.7047 - val_accuracy: 0.6597\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.68 - ETA: 0s - loss: 0.5481 - accuracy: 0.77 - ETA: 0s - loss: 0.5532 - accuracy: 0.76 - ETA: 0s - loss: 0.5499 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5535 - accuracy: 0.7570 - val_loss: 0.5584 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.93 - ETA: 0s - loss: 0.5085 - accuracy: 0.79 - ETA: 0s - loss: 0.4984 - accuracy: 0.79 - ETA: 0s - loss: 0.4946 - accuracy: 0.80 - ETA: 0s - loss: 0.4948 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4993 - accuracy: 0.7977 - val_loss: 0.6392 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.90 - ETA: 0s - loss: 0.4960 - accuracy: 0.81 - ETA: 0s - loss: 0.4745 - accuracy: 0.82 - ETA: 0s - loss: 0.4743 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4785 - accuracy: 0.8152 - val_loss: 0.6065 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.84 - ETA: 0s - loss: 0.3671 - accuracy: 0.84 - ETA: 0s - loss: 0.3696 - accuracy: 0.83 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8272 - val_loss: 0.8271 - val_accuracy: 0.7060\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.78 - ETA: 0s - loss: 0.3909 - accuracy: 0.84 - ETA: 0s - loss: 0.3615 - accuracy: 0.86 - ETA: 0s - loss: 0.3798 - accuracy: 0.86 - ETA: 0s - loss: 0.4060 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8499 - val_loss: 0.7150 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.75 - ETA: 0s - loss: 0.4007 - accuracy: 0.84 - ETA: 0s - loss: 0.3811 - accuracy: 0.85 - ETA: 0s - loss: 0.3682 - accuracy: 0.86 - ETA: 0s - loss: 0.3854 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8496 - val_loss: 0.6690 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.90 - ETA: 0s - loss: 0.4667 - accuracy: 0.83 - ETA: 0s - loss: 0.4138 - accuracy: 0.84 - ETA: 0s - loss: 0.4072 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8470 - val_loss: 0.8197 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.93 - ETA: 0s - loss: 0.3255 - accuracy: 0.87 - ETA: 0s - loss: 0.3320 - accuracy: 0.87 - ETA: 0s - loss: 0.3213 - accuracy: 0.87 - ETA: 0s - loss: 0.3384 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8675 - val_loss: 0.8004 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.90 - ETA: 0s - loss: 0.2944 - accuracy: 0.88 - ETA: 0s - loss: 0.3196 - accuracy: 0.87 - ETA: 0s - loss: 0.3344 - accuracy: 0.87 - ETA: 0s - loss: 0.3370 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8731 - val_loss: 1.4653 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.90 - ETA: 0s - loss: 0.3455 - accuracy: 0.88 - ETA: 0s - loss: 0.3916 - accuracy: 0.87 - ETA: 0s - loss: 0.4334 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4894 - accuracy: 0.8384 - val_loss: 1.1599 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.90 - ETA: 0s - loss: 0.6030 - accuracy: 0.82 - ETA: 0s - loss: 0.5985 - accuracy: 0.81 - ETA: 0s - loss: 0.6170 - accuracy: 0.79 - ETA: 0s - loss: 0.6402 - accuracy: 0.80 - 0s 3ms/step - loss: 0.6344 - accuracy: 0.8025 - val_loss: 1.6140 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.71 - ETA: 0s - loss: 0.6135 - accuracy: 0.79 - ETA: 0s - loss: 0.6096 - accuracy: 0.78 - ETA: 0s - loss: 0.6336 - accuracy: 0.76 - ETA: 0s - loss: 0.6359 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6331 - accuracy: 0.7704 - val_loss: 1.1607 - val_accuracy: 0.7269\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.81 - ETA: 0s - loss: 0.5916 - accuracy: 0.77 - ETA: 0s - loss: 0.8930 - accuracy: 0.77 - ETA: 0s - loss: 0.8263 - accuracy: 0.75 - ETA: 0s - loss: 0.7771 - accuracy: 0.76 - 0s 3ms/step - loss: 0.7726 - accuracy: 0.7611 - val_loss: 0.6432 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.81 - ETA: 0s - loss: 0.6258 - accuracy: 0.75 - ETA: 0s - loss: 0.6142 - accuracy: 0.76 - ETA: 0s - loss: 0.5984 - accuracy: 0.78 - ETA: 0s - loss: 0.6028 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6009 - accuracy: 0.7757 - val_loss: 0.7508 - val_accuracy: 0.7612\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.75 - ETA: 0s - loss: 0.6437 - accuracy: 0.78 - ETA: 0s - loss: 0.6264 - accuracy: 0.79 - ETA: 0s - loss: 0.6567 - accuracy: 0.75 - ETA: 0s - loss: 0.6664 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6690 - accuracy: 0.7387 - val_loss: 0.6748 - val_accuracy: 0.6985\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.78 - ETA: 0s - loss: 0.6960 - accuracy: 0.69 - ETA: 0s - loss: 0.6894 - accuracy: 0.68 - ETA: 0s - loss: 0.6922 - accuracy: 0.70 - ETA: 0s - loss: 0.6830 - accuracy: 0.70 - 0s 3ms/step - loss: 0.6818 - accuracy: 0.7103 - val_loss: 0.9381 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.71 - ETA: 0s - loss: 0.6327 - accuracy: 0.74 - ETA: 0s - loss: 0.6355 - accuracy: 0.74 - ETA: 0s - loss: 0.6370 - accuracy: 0.74 - ETA: 0s - loss: 0.6284 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6290 - accuracy: 0.7544 - val_loss: 1.1778 - val_accuracy: 0.7358\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.75 - ETA: 0s - loss: 0.6608 - accuracy: 0.73 - ETA: 0s - loss: 0.7743 - accuracy: 0.73 - ETA: 0s - loss: 0.7434 - accuracy: 0.72 - ETA: 0s - loss: 0.7317 - accuracy: 0.71 - 0s 3ms/step - loss: 0.7305 - accuracy: 0.7171 - val_loss: 0.7316 - val_accuracy: 0.7030\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7169 - accuracy: 0.62 - ETA: 0s - loss: 0.6727 - accuracy: 0.71 - ETA: 0s - loss: 0.6639 - accuracy: 0.72 - ETA: 0s - loss: 0.6585 - accuracy: 0.73 - ETA: 0s - loss: 0.6596 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6586 - accuracy: 0.7361 - val_loss: 1.0424 - val_accuracy: 0.7299\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6658 - accuracy: 0.71 - ETA: 0s - loss: 0.6424 - accuracy: 0.74 - ETA: 0s - loss: 0.6291 - accuracy: 0.75 - ETA: 0s - loss: 0.6289 - accuracy: 0.75 - ETA: 0s - loss: 0.6252 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6240 - accuracy: 0.7592 - val_loss: 2.0927 - val_accuracy: 0.7358\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7135 - accuracy: 0.65 - ETA: 0s - loss: 0.6523 - accuracy: 0.75 - ETA: 0s - loss: 0.6573 - accuracy: 0.74 - ETA: 0s - loss: 0.6488 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6460 - accuracy: 0.7458 - val_loss: 0.6401 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.87 - ETA: 0s - loss: 0.6233 - accuracy: 0.76 - ETA: 0s - loss: 0.6427 - accuracy: 0.73 - ETA: 0s - loss: 0.6378 - accuracy: 0.74 - ETA: 0s - loss: 0.6464 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6451 - accuracy: 0.7417 - val_loss: 0.8761 - val_accuracy: 0.7269\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.84 - ETA: 0s - loss: 0.6415 - accuracy: 0.74 - ETA: 0s - loss: 0.6388 - accuracy: 0.74 - ETA: 0s - loss: 0.6268 - accuracy: 0.75 - ETA: 0s - loss: 0.6389 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6386 - accuracy: 0.7484 - val_loss: 0.8888 - val_accuracy: 0.7104\n",
      "Epoch 25/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7361 - accuracy: 0.65 - ETA: 0s - loss: 0.6553 - accuracy: 0.73 - ETA: 0s - loss: 0.6557 - accuracy: 0.73 - ETA: 0s - loss: 0.6556 - accuracy: 0.73 - ETA: 0s - loss: 0.6500 - accuracy: 0.7365Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.7372 - val_loss: 0.6773 - val_accuracy: 0.7284\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7498 - accuracy: 0.75 - ETA: 0s - loss: 1.7219 - accuracy: 0.67 - ETA: 0s - loss: 1.1377 - accuracy: 0.69 - ETA: 0s - loss: 0.9424 - accuracy: 0.69 - ETA: 0s - loss: 0.8546 - accuracy: 0.71 - 0s 4ms/step - loss: 0.8413 - accuracy: 0.7122 - val_loss: 0.5860 - val_accuracy: 0.7493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.78 - ETA: 0s - loss: 0.5212 - accuracy: 0.78 - ETA: 0s - loss: 0.5207 - accuracy: 0.77 - ETA: 0s - loss: 0.5455 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7656 - val_loss: 0.6758 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.81 - ETA: 0s - loss: 0.5080 - accuracy: 0.80 - ETA: 0s - loss: 0.5611 - accuracy: 0.77 - ETA: 0s - loss: 0.5464 - accuracy: 0.78 - ETA: 0s - loss: 0.5360 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7876 - val_loss: 0.6603 - val_accuracy: 0.7284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6223 - accuracy: 0.71 - ETA: 0s - loss: 0.4726 - accuracy: 0.80 - ETA: 0s - loss: 0.4453 - accuracy: 0.81 - ETA: 0s - loss: 0.4449 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4510 - accuracy: 0.8137 - val_loss: 0.6795 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.68 - ETA: 0s - loss: 0.3898 - accuracy: 0.84 - ETA: 0s - loss: 0.4229 - accuracy: 0.83 - ETA: 0s - loss: 0.4240 - accuracy: 0.83 - ETA: 0s - loss: 0.4200 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4200 - accuracy: 0.8328 - val_loss: 0.6701 - val_accuracy: 0.7075\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4827 - accuracy: 0.78 - ETA: 0s - loss: 0.4676 - accuracy: 0.81 - ETA: 0s - loss: 0.4343 - accuracy: 0.83 - ETA: 0s - loss: 0.4134 - accuracy: 0.84 - ETA: 0s - loss: 0.4098 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8384 - val_loss: 0.9440 - val_accuracy: 0.7149\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.93 - ETA: 0s - loss: 0.3701 - accuracy: 0.84 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.83 - ETA: 0s - loss: 0.3923 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4083 - accuracy: 0.8350 - val_loss: 1.6760 - val_accuracy: 0.6806\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.90 - ETA: 0s - loss: 0.3989 - accuracy: 0.83 - ETA: 0s - loss: 0.3946 - accuracy: 0.84 - ETA: 0s - loss: 0.4067 - accuracy: 0.83 - ETA: 0s - loss: 0.4481 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4452 - accuracy: 0.8290 - val_loss: 4.2716 - val_accuracy: 0.7164\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.93 - ETA: 0s - loss: 0.5917 - accuracy: 0.80 - ETA: 0s - loss: 0.5096 - accuracy: 0.82 - ETA: 0s - loss: 0.5406 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5211 - accuracy: 0.8216 - val_loss: 2.0004 - val_accuracy: 0.6985\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.87 - ETA: 0s - loss: 0.5580 - accuracy: 0.76 - ETA: 0s - loss: 0.5285 - accuracy: 0.79 - ETA: 0s - loss: 0.5373 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5306 - accuracy: 0.7921 - val_loss: 1.5468 - val_accuracy: 0.6925\n",
      "Epoch 11/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.3092 - accuracy: 0.90 - ETA: 0s - loss: 0.4206 - accuracy: 0.83 - ETA: 0s - loss: 0.4008 - accuracy: 0.84 - ETA: 0s - loss: 0.5934 - accuracy: 0.8415Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5963 - accuracy: 0.8298 - val_loss: 0.8200 - val_accuracy: 0.7239\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 98950e26b6ff18a4f9384060d0b8fbd1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7562189102172852</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.37062492975061134</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 290</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 95</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9234 - accuracy: 0.65 - ETA: 0s - loss: 4.9429 - accuracy: 0.57 - ETA: 0s - loss: 4.2099 - accuracy: 0.59 - ETA: 0s - loss: 3.2408 - accuracy: 0.59 - ETA: 0s - loss: 2.6801 - accuracy: 0.61 - ETA: 0s - loss: 2.3359 - accuracy: 0.63 - ETA: 0s - loss: 2.1776 - accuracy: 0.63 - ETA: 0s - loss: 1.9661 - accuracy: 0.60 - 1s 7ms/step - loss: 1.9220 - accuracy: 0.5984 - val_loss: 0.6681 - val_accuracy: 0.5836\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.46 - ETA: 0s - loss: 0.6515 - accuracy: 0.48 - ETA: 0s - loss: 0.7128 - accuracy: 0.58 - ETA: 0s - loss: 0.7072 - accuracy: 0.61 - ETA: 0s - loss: 0.7166 - accuracy: 0.64 - ETA: 0s - loss: 0.7083 - accuracy: 0.65 - ETA: 0s - loss: 0.7043 - accuracy: 0.66 - ETA: 0s - loss: 0.7042 - accuracy: 0.66 - 0s 5ms/step - loss: 0.7044 - accuracy: 0.6521 - val_loss: 0.6473 - val_accuracy: 0.5373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.40 - ETA: 0s - loss: 0.6717 - accuracy: 0.36 - ETA: 0s - loss: 0.6833 - accuracy: 0.36 - ETA: 0s - loss: 0.6719 - accuracy: 0.49 - ETA: 0s - loss: 0.6682 - accuracy: 0.55 - ETA: 0s - loss: 0.6677 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - 0s 5ms/step - loss: 0.6700 - accuracy: 0.6264 - val_loss: 0.6265 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.68 - ETA: 0s - loss: 0.6543 - accuracy: 0.75 - ETA: 0s - loss: 0.6399 - accuracy: 0.75 - ETA: 0s - loss: 0.6544 - accuracy: 0.75 - ETA: 0s - loss: 0.6553 - accuracy: 0.75 - ETA: 0s - loss: 0.6720 - accuracy: 0.75 - ETA: 0s - loss: 0.6903 - accuracy: 0.73 - ETA: 0s - loss: 0.6915 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6882 - accuracy: 0.7361 - val_loss: 0.6137 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2130 - accuracy: 0.65 - ETA: 0s - loss: 0.7108 - accuracy: 0.71 - ETA: 0s - loss: 0.7142 - accuracy: 0.71 - ETA: 0s - loss: 0.7065 - accuracy: 0.71 - ETA: 0s - loss: 0.7088 - accuracy: 0.71 - ETA: 0s - loss: 0.7053 - accuracy: 0.71 - ETA: 0s - loss: 0.6971 - accuracy: 0.72 - ETA: 0s - loss: 0.6878 - accuracy: 0.73 - 0s 6ms/step - loss: 0.6871 - accuracy: 0.7361 - val_loss: 0.6474 - val_accuracy: 0.7493\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.71 - ETA: 0s - loss: 0.6721 - accuracy: 0.75 - ETA: 0s - loss: 0.6805 - accuracy: 0.73 - ETA: 0s - loss: 0.6846 - accuracy: 0.72 - ETA: 0s - loss: 0.6866 - accuracy: 0.72 - ETA: 0s - loss: 0.6840 - accuracy: 0.72 - ETA: 0s - loss: 0.6873 - accuracy: 0.71 - ETA: 0s - loss: 0.6906 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6906 - accuracy: 0.7152 - val_loss: 0.6853 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.68 - ETA: 0s - loss: 0.6790 - accuracy: 0.35 - ETA: 0s - loss: 0.6671 - accuracy: 0.49 - ETA: 0s - loss: 0.6664 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6818 - accuracy: 0.61 - ETA: 0s - loss: 0.6847 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6842 - accuracy: 0.5666 - val_loss: 0.6923 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.37 - ETA: 0s - loss: 0.6373 - accuracy: 0.43 - ETA: 0s - loss: 0.6439 - accuracy: 0.58 - ETA: 0s - loss: 0.6666 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.65 - ETA: 0s - loss: 0.6686 - accuracy: 0.67 - ETA: 0s - loss: 0.6700 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6678 - accuracy: 0.6891 - val_loss: 0.6421 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.75 - ETA: 0s - loss: 0.6397 - accuracy: 0.75 - ETA: 0s - loss: 0.6521 - accuracy: 0.75 - ETA: 0s - loss: 0.6658 - accuracy: 0.73 - ETA: 0s - loss: 0.6669 - accuracy: 0.72 - ETA: 0s - loss: 0.6609 - accuracy: 0.73 - ETA: 0s - loss: 0.6550 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6704 - accuracy: 0.7372 - val_loss: 0.6246 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.68 - ETA: 0s - loss: 0.7717 - accuracy: 0.74 - ETA: 0s - loss: 0.7274 - accuracy: 0.72 - ETA: 0s - loss: 0.7925 - accuracy: 0.72 - ETA: 0s - loss: 0.7666 - accuracy: 0.71 - ETA: 0s - loss: 0.7530 - accuracy: 0.71 - ETA: 0s - loss: 0.7420 - accuracy: 0.71 - ETA: 0s - loss: 0.7351 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7348 - accuracy: 0.7122 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.81 - ETA: 0s - loss: 0.6558 - accuracy: 0.75 - ETA: 0s - loss: 0.7194 - accuracy: 0.72 - ETA: 0s - loss: 0.7082 - accuracy: 0.72 - ETA: 0s - loss: 0.7038 - accuracy: 0.71 - ETA: 0s - loss: 0.7044 - accuracy: 0.68 - ETA: 0s - loss: 0.7049 - accuracy: 0.62 - ETA: 0s - loss: 0.7044 - accuracy: 0.58 - 0s 5ms/step - loss: 0.7049 - accuracy: 0.5812 - val_loss: 0.7124 - val_accuracy: 0.3045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6664 - accuracy: 0.25 - ETA: 0s - loss: 0.6936 - accuracy: 0.29 - ETA: 0s - loss: 0.6926 - accuracy: 0.29 - ETA: 0s - loss: 0.6914 - accuracy: 0.29 - ETA: 0s - loss: 0.6969 - accuracy: 0.33 - ETA: 0s - loss: 0.6962 - accuracy: 0.32 - ETA: 0s - loss: 0.6932 - accuracy: 0.32 - 0s 5ms/step - loss: 0.6941 - accuracy: 0.3654 - val_loss: 0.6868 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7231 - accuracy: 0.65 - ETA: 0s - loss: 0.7061 - accuracy: 0.49 - ETA: 0s - loss: 0.6882 - accuracy: 0.40 - ETA: 0s - loss: 0.6930 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.54 - ETA: 0s - loss: 0.6979 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.47 - ETA: 0s - loss: 0.6960 - accuracy: 0.48 - 0s 5ms/step - loss: 0.6941 - accuracy: 0.4823 - val_loss: 0.6941 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.31 - ETA: 0s - loss: 0.6623 - accuracy: 0.67 - ETA: 0s - loss: 0.6785 - accuracy: 0.68 - ETA: 0s - loss: 0.6872 - accuracy: 0.68 - ETA: 0s - loss: 0.6878 - accuracy: 0.69 - ETA: 0s - loss: 0.6898 - accuracy: 0.69 - ETA: 0s - loss: 0.6903 - accuracy: 0.67 - ETA: 0s - loss: 0.6940 - accuracy: 0.62 - 0s 5ms/step - loss: 0.6935 - accuracy: 0.6010 - val_loss: 0.7114 - val_accuracy: 0.3045\n",
      "Epoch 15/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.6660 - accuracy: 0.25 - ETA: 0s - loss: 0.6797 - accuracy: 0.27 - ETA: 0s - loss: 0.7022 - accuracy: 0.39 - ETA: 0s - loss: 0.7058 - accuracy: 0.37 - ETA: 0s - loss: 0.7030 - accuracy: 0.35 - ETA: 0s - loss: 0.7017 - accuracy: 0.34 - ETA: 0s - loss: 0.6976 - accuracy: 0.36 - ETA: 0s - loss: 0.6958 - accuracy: 0.4119Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6942 - accuracy: 0.4371 - val_loss: 0.6793 - val_accuracy: 0.6955\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 1.2351 - accuracy: 0.59 - ETA: 0s - loss: 4.3150 - accuracy: 0.59 - ETA: 0s - loss: 3.6478 - accuracy: 0.58 - ETA: 0s - loss: 3.0168 - accuracy: 0.61 - ETA: 0s - loss: 2.4926 - accuracy: 0.62 - ETA: 0s - loss: 2.1922 - accuracy: 0.58 - ETA: 0s - loss: 1.9574 - accuracy: 0.55 - ETA: 0s - loss: 1.7802 - accuracy: 0.57 - 1s 6ms/step - loss: 1.7192 - accuracy: 0.5842 - val_loss: 0.6571 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.78 - ETA: 0s - loss: 0.6757 - accuracy: 0.71 - ETA: 0s - loss: 0.6581 - accuracy: 0.73 - ETA: 0s - loss: 0.6669 - accuracy: 0.73 - ETA: 0s - loss: 0.6608 - accuracy: 0.73 - ETA: 0s - loss: 0.6694 - accuracy: 0.73 - ETA: 0s - loss: 0.6694 - accuracy: 0.72 - ETA: 0s - loss: 0.6711 - accuracy: 0.72 - ETA: 0s - loss: 0.6875 - accuracy: 0.72 - 0s 6ms/step - loss: 0.6875 - accuracy: 0.7249 - val_loss: 0.6498 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.75 - ETA: 0s - loss: 0.6370 - accuracy: 0.76 - ETA: 0s - loss: 0.7192 - accuracy: 0.74 - ETA: 0s - loss: 0.7188 - accuracy: 0.73 - ETA: 0s - loss: 0.7190 - accuracy: 0.72 - ETA: 0s - loss: 0.7278 - accuracy: 0.72 - ETA: 0s - loss: 0.7446 - accuracy: 0.72 - ETA: 0s - loss: 0.7303 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7367 - accuracy: 0.7271 - val_loss: 0.6159 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0571 - accuracy: 0.65 - ETA: 0s - loss: 0.7396 - accuracy: 0.69 - ETA: 0s - loss: 0.7269 - accuracy: 0.70 - ETA: 0s - loss: 0.7344 - accuracy: 0.72 - ETA: 0s - loss: 0.7200 - accuracy: 0.73 - ETA: 0s - loss: 0.7815 - accuracy: 0.72 - ETA: 0s - loss: 0.7602 - accuracy: 0.67 - ETA: 0s - loss: 0.7934 - accuracy: 0.66 - 1s 6ms/step - loss: 0.7871 - accuracy: 0.6663 - val_loss: 0.7008 - val_accuracy: 0.7507\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.71 - ETA: 0s - loss: 0.7332 - accuracy: 0.71 - ETA: 0s - loss: 0.7189 - accuracy: 0.69 - ETA: 0s - loss: 0.7086 - accuracy: 0.58 - ETA: 0s - loss: 0.7007 - accuracy: 0.52 - ETA: 0s - loss: 0.6986 - accuracy: 0.55 - ETA: 0s - loss: 0.6991 - accuracy: 0.58 - ETA: 0s - loss: 0.6988 - accuracy: 0.58 - 0s 5ms/step - loss: 0.6979 - accuracy: 0.5636 - val_loss: 0.6983 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.12 - ETA: 0s - loss: 0.6668 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.61 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.47 - ETA: 0s - loss: 0.6940 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.47 - ETA: 0s - loss: 0.6946 - accuracy: 0.45 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4464 - val_loss: 0.6949 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.06 - ETA: 0s - loss: 0.7000 - accuracy: 0.61 - ETA: 0s - loss: 0.6884 - accuracy: 0.45 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6956 - accuracy: 0.56 - ETA: 0s - loss: 0.6951 - accuracy: 0.53 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6970 - accuracy: 0.48 - 0s 5ms/step - loss: 0.6940 - accuracy: 0.4741 - val_loss: 0.6996 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.21 - ETA: 0s - loss: 0.6932 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.63 - ETA: 0s - loss: 0.6845 - accuracy: 0.66 - ETA: 0s - loss: 0.6843 - accuracy: 0.67 - ETA: 0s - loss: 0.6886 - accuracy: 0.67 - ETA: 0s - loss: 0.6901 - accuracy: 0.63 - ETA: 0s - loss: 0.6932 - accuracy: 0.59 - 0s 5ms/step - loss: 0.6930 - accuracy: 0.5827 - val_loss: 0.7151 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.21 - ETA: 0s - loss: 0.6928 - accuracy: 0.29 - ETA: 0s - loss: 0.6833 - accuracy: 0.36 - ETA: 0s - loss: 0.6882 - accuracy: 0.46 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6911 - accuracy: 0.45 - ETA: 0s - loss: 0.6909 - accuracy: 0.42 - ETA: 0s - loss: 0.6934 - accuracy: 0.45 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4427 - val_loss: 0.6996 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7389 - accuracy: 0.37 - ETA: 0s - loss: 0.9499 - accuracy: 0.32 - ETA: 0s - loss: 0.8243 - accuracy: 0.50 - ETA: 0s - loss: 0.7738 - accuracy: 0.58 - ETA: 0s - loss: 0.7640 - accuracy: 0.60 - ETA: 0s - loss: 0.7519 - accuracy: 0.61 - ETA: 0s - loss: 0.7447 - accuracy: 0.57 - ETA: 0s - loss: 0.7384 - accuracy: 0.53 - 0s 5ms/step - loss: 0.7320 - accuracy: 0.5099 - val_loss: 0.6978 - val_accuracy: 0.3060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6427 - accuracy: 0.21 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 1.5228 - accuracy: 0.64 - ETA: 0s - loss: 1.2527 - accuracy: 0.66 - ETA: 0s - loss: 1.1153 - accuracy: 0.64 - ETA: 0s - loss: 1.0325 - accuracy: 0.57 - ETA: 0s - loss: 0.9781 - accuracy: 0.53 - ETA: 0s - loss: 0.9385 - accuracy: 0.50 - 0s 5ms/step - loss: 0.9238 - accuracy: 0.5144 - val_loss: 0.6895 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7647 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.72 - ETA: 0s - loss: 0.6816 - accuracy: 0.71 - ETA: 0s - loss: 0.6881 - accuracy: 0.70 - ETA: 0s - loss: 0.6906 - accuracy: 0.70 - ETA: 0s - loss: 0.6934 - accuracy: 0.62 - ETA: 0s - loss: 0.6930 - accuracy: 0.57 - ETA: 0s - loss: 0.6953 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6937 - accuracy: 0.5151 - val_loss: 0.7009 - val_accuracy: 0.3045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.31 - ETA: 0s - loss: 0.7085 - accuracy: 0.32 - ETA: 0s - loss: 0.6895 - accuracy: 0.32 - ETA: 0s - loss: 0.6930 - accuracy: 0.44 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6926 - accuracy: 0.57 - ETA: 0s - loss: 0.6938 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5233 - val_loss: 0.7018 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6815 - accuracy: 0.28 - ETA: 0s - loss: 0.6990 - accuracy: 0.30 - ETA: 0s - loss: 0.6994 - accuracy: 0.30 - ETA: 0s - loss: 0.6972 - accuracy: 0.30 - ETA: 0s - loss: 0.6950 - accuracy: 0.35 - ETA: 0s - loss: 0.6981 - accuracy: 0.42 - ETA: 0s - loss: 0.6982 - accuracy: 0.40 - ETA: 0s - loss: 0.6936 - accuracy: 0.3930Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4065 - val_loss: 0.6853 - val_accuracy: 0.6955\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.75 - ETA: 0s - loss: 5.7208 - accuracy: 0.64 - ETA: 0s - loss: 4.4779 - accuracy: 0.63 - ETA: 0s - loss: 3.6953 - accuracy: 0.62 - ETA: 0s - loss: 3.0939 - accuracy: 0.62 - ETA: 0s - loss: 2.6980 - accuracy: 0.62 - ETA: 0s - loss: 2.4529 - accuracy: 0.63 - ETA: 0s - loss: 2.2176 - accuracy: 0.62 - 1s 6ms/step - loss: 2.1391 - accuracy: 0.6155 - val_loss: 0.6115 - val_accuracy: 0.6731\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.43 - ETA: 0s - loss: 1.0719 - accuracy: 0.43 - ETA: 0s - loss: 0.9217 - accuracy: 0.55 - ETA: 0s - loss: 0.8604 - accuracy: 0.58 - ETA: 0s - loss: 0.8098 - accuracy: 0.60 - ETA: 0s - loss: 0.7865 - accuracy: 0.62 - ETA: 0s - loss: 0.7617 - accuracy: 0.65 - ETA: 0s - loss: 0.7562 - accuracy: 0.66 - 0s 5ms/step - loss: 0.7479 - accuracy: 0.6685 - val_loss: 0.6533 - val_accuracy: 0.6687\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7703 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.70 - ETA: 0s - loss: 0.6560 - accuracy: 0.70 - ETA: 0s - loss: 0.6258 - accuracy: 0.73 - ETA: 0s - loss: 0.6216 - accuracy: 0.74 - ETA: 0s - loss: 0.6337 - accuracy: 0.74 - ETA: 0s - loss: 0.6420 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6423 - accuracy: 0.7387 - val_loss: 0.6008 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.84 - ETA: 0s - loss: 0.5944 - accuracy: 0.77 - ETA: 0s - loss: 0.6091 - accuracy: 0.75 - ETA: 0s - loss: 0.6122 - accuracy: 0.75 - ETA: 0s - loss: 0.6095 - accuracy: 0.76 - ETA: 0s - loss: 0.6139 - accuracy: 0.76 - ETA: 0s - loss: 0.6104 - accuracy: 0.76 - ETA: 0s - loss: 0.6119 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6061 - accuracy: 0.7630 - val_loss: 0.5988 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.75 - ETA: 0s - loss: 0.6058 - accuracy: 0.77 - ETA: 0s - loss: 0.5869 - accuracy: 0.79 - ETA: 0s - loss: 0.6001 - accuracy: 0.78 - ETA: 0s - loss: 0.6080 - accuracy: 0.77 - ETA: 0s - loss: 0.6074 - accuracy: 0.77 - ETA: 0s - loss: 0.6115 - accuracy: 0.77 - ETA: 0s - loss: 0.6088 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6089 - accuracy: 0.7760 - val_loss: 0.5943 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.90 - ETA: 0s - loss: 0.5710 - accuracy: 0.79 - ETA: 0s - loss: 0.5658 - accuracy: 0.79 - ETA: 0s - loss: 0.5579 - accuracy: 0.80 - ETA: 0s - loss: 0.5716 - accuracy: 0.79 - ETA: 0s - loss: 0.5726 - accuracy: 0.79 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - ETA: 0s - loss: 0.5879 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5884 - accuracy: 0.7861 - val_loss: 0.5993 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.84 - ETA: 0s - loss: 0.6233 - accuracy: 0.75 - ETA: 0s - loss: 0.5933 - accuracy: 0.77 - ETA: 0s - loss: 0.6146 - accuracy: 0.76 - ETA: 0s - loss: 0.6024 - accuracy: 0.77 - ETA: 0s - loss: 0.5920 - accuracy: 0.78 - ETA: 0s - loss: 0.5900 - accuracy: 0.78 - ETA: 0s - loss: 0.5920 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5914 - accuracy: 0.7828 - val_loss: 0.5691 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.71 - ETA: 0s - loss: 0.6425 - accuracy: 0.75 - ETA: 0s - loss: 0.6122 - accuracy: 0.76 - ETA: 0s - loss: 0.6143 - accuracy: 0.77 - ETA: 0s - loss: 0.6053 - accuracy: 0.78 - ETA: 0s - loss: 0.6017 - accuracy: 0.78 - ETA: 0s - loss: 0.6044 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5993 - accuracy: 0.7835 - val_loss: 0.5934 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.87 - ETA: 0s - loss: 0.5620 - accuracy: 0.79 - ETA: 0s - loss: 0.5617 - accuracy: 0.79 - ETA: 0s - loss: 0.5672 - accuracy: 0.79 - ETA: 0s - loss: 0.5635 - accuracy: 0.79 - ETA: 0s - loss: 0.5617 - accuracy: 0.80 - ETA: 0s - loss: 0.5623 - accuracy: 0.80 - ETA: 0s - loss: 0.5575 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5578 - accuracy: 0.8063 - val_loss: 0.5798 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.71 - ETA: 0s - loss: 0.6065 - accuracy: 0.76 - ETA: 0s - loss: 0.6035 - accuracy: 0.76 - ETA: 0s - loss: 0.5879 - accuracy: 0.77 - ETA: 0s - loss: 0.5851 - accuracy: 0.78 - ETA: 0s - loss: 0.5644 - accuracy: 0.79 - ETA: 0s - loss: 0.5640 - accuracy: 0.79 - ETA: 0s - loss: 0.5622 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5605 - accuracy: 0.8044 - val_loss: 0.5641 - val_accuracy: 0.7478\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.87 - ETA: 0s - loss: 0.5522 - accuracy: 0.82 - ETA: 0s - loss: 0.5498 - accuracy: 0.82 - ETA: 0s - loss: 0.5533 - accuracy: 0.82 - ETA: 0s - loss: 0.5674 - accuracy: 0.80 - ETA: 0s - loss: 0.5703 - accuracy: 0.80 - ETA: 0s - loss: 0.5684 - accuracy: 0.80 - ETA: 0s - loss: 0.5693 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5732 - accuracy: 0.8052 - val_loss: 0.5856 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.71 - ETA: 0s - loss: 0.6348 - accuracy: 0.77 - ETA: 0s - loss: 0.6149 - accuracy: 0.79 - ETA: 0s - loss: 0.6016 - accuracy: 0.79 - ETA: 0s - loss: 0.6079 - accuracy: 0.78 - ETA: 0s - loss: 0.5986 - accuracy: 0.79 - ETA: 0s - loss: 0.6009 - accuracy: 0.79 - ETA: 0s - loss: 0.5964 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5917 - accuracy: 0.7996 - val_loss: 0.5961 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.78 - ETA: 0s - loss: 0.5918 - accuracy: 0.78 - ETA: 0s - loss: 0.5824 - accuracy: 0.78 - ETA: 0s - loss: 0.5785 - accuracy: 0.78 - ETA: 0s - loss: 0.5722 - accuracy: 0.79 - ETA: 0s - loss: 0.5777 - accuracy: 0.79 - ETA: 0s - loss: 0.5704 - accuracy: 0.79 - ETA: 0s - loss: 0.5696 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5673 - accuracy: 0.8007 - val_loss: 0.5841 - val_accuracy: 0.7463\n",
      "Epoch 14/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6032 - accuracy: 0.78 - ETA: 0s - loss: 0.5454 - accuracy: 0.81 - ETA: 0s - loss: 0.5704 - accuracy: 0.79 - ETA: 0s - loss: 0.5761 - accuracy: 0.79 - ETA: 0s - loss: 0.5936 - accuracy: 0.78 - ETA: 0s - loss: 0.5950 - accuracy: 0.78 - ETA: 0s - loss: 0.6020 - accuracy: 0.78 - ETA: 0s - loss: 0.6122 - accuracy: 0.7861Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6161 - accuracy: 0.7861 - val_loss: 0.5986 - val_accuracy: 0.7373\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f27d2979d2e80e9fb36c652647776217</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7497512499491373</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8601404160883968</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 55</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.59 - ETA: 0s - loss: 0.8697 - accuracy: 0.67 - ETA: 0s - loss: 0.7399 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6902 - accuracy: 0.6999 - val_loss: 0.5922 - val_accuracy: 0.6776\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.75 - ETA: 0s - loss: 0.4956 - accuracy: 0.77 - ETA: 0s - loss: 0.5150 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7611 - val_loss: 0.5763 - val_accuracy: 0.7313\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.90 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4284 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7996 - val_loss: 0.6088 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.84 - ETA: 0s - loss: 0.4031 - accuracy: 0.80 - ETA: 0s - loss: 0.3921 - accuracy: 0.81 - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8141 - val_loss: 1.0295 - val_accuracy: 0.6433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5581 - accuracy: 0.78 - ETA: 0s - loss: 0.3997 - accuracy: 0.80 - ETA: 0s - loss: 0.3876 - accuracy: 0.80 - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8186 - val_loss: 0.6294 - val_accuracy: 0.7015\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.90 - ETA: 0s - loss: 0.3203 - accuracy: 0.85 - ETA: 0s - loss: 0.3319 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8361 - val_loss: 0.8169 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.87 - ETA: 0s - loss: 0.3084 - accuracy: 0.84 - ETA: 0s - loss: 0.3237 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8481 - val_loss: 0.7716 - val_accuracy: 0.6552\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.78 - ETA: 0s - loss: 0.3103 - accuracy: 0.87 - ETA: 0s - loss: 0.2962 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8511 - val_loss: 0.8342 - val_accuracy: 0.6552\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4385 - accuracy: 0.81 - ETA: 0s - loss: 0.2980 - accuracy: 0.83 - ETA: 0s - loss: 0.3096 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8466 - val_loss: 0.8680 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.87 - ETA: 0s - loss: 0.3153 - accuracy: 0.87 - ETA: 0s - loss: 0.3229 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8623 - val_loss: 0.7533 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.93 - ETA: 0s - loss: 0.3547 - accuracy: 0.87 - ETA: 0s - loss: 0.3176 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8824 - val_loss: 2.2685 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "64/84 [=====================>........] - ETA: 0s - loss: 0.3166 - accuracy: 0.84 - ETA: 0s - loss: 0.3359 - accuracy: 0.89 - ETA: 0s - loss: 0.3051 - accuracy: 0.8911Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8865 - val_loss: 3.4263 - val_accuracy: 0.6925\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6391 - accuracy: 0.78 - ETA: 0s - loss: 0.8111 - accuracy: 0.65 - ETA: 0s - loss: 0.6849 - accuracy: 0.70 - 0s 3ms/step - loss: 0.6541 - accuracy: 0.7085 - val_loss: 0.5430 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.84 - ETA: 0s - loss: 0.5002 - accuracy: 0.75 - ETA: 0s - loss: 0.5200 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7507 - val_loss: 0.7010 - val_accuracy: 0.6537\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.75 - ETA: 0s - loss: 0.4493 - accuracy: 0.80 - ETA: 0s - loss: 0.4416 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4467 - accuracy: 0.7996 - val_loss: 0.7172 - val_accuracy: 0.6761\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.87 - ETA: 0s - loss: 0.4095 - accuracy: 0.83 - ETA: 0s - loss: 0.4269 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8253 - val_loss: 0.6193 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.87 - ETA: 0s - loss: 0.3546 - accuracy: 0.85 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8261 - val_loss: 0.7115 - val_accuracy: 0.6299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.75 - ETA: 0s - loss: 0.3475 - accuracy: 0.85 - ETA: 0s - loss: 0.3508 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8485 - val_loss: 0.6873 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.96 - ETA: 0s - loss: 0.4158 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8507 - val_loss: 2.3718 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.81 - ETA: 0s - loss: 0.4273 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8399 - val_loss: 0.8308 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.90 - ETA: 0s - loss: 0.2773 - accuracy: 0.87 - ETA: 0s - loss: 0.2994 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8645 - val_loss: 0.7634 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.96 - ETA: 0s - loss: 0.2873 - accuracy: 0.87 - ETA: 0s - loss: 0.3181 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8708 - val_loss: 1.2002 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.96 - ETA: 0s - loss: 0.2600 - accuracy: 0.90 - ETA: 0s - loss: 0.2760 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2724 - accuracy: 0.8921 - val_loss: 2.8936 - val_accuracy: 0.7060\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.93 - ETA: 0s - loss: 0.2907 - accuracy: 0.90 - ETA: 0s - loss: 0.3064 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8854 - val_loss: 3.3782 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.90 - ETA: 0s - loss: 0.2636 - accuracy: 0.88 - ETA: 0s - loss: 0.2816 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8813 - val_loss: 1.3318 - val_accuracy: 0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.90 - ETA: 0s - loss: 0.2980 - accuracy: 0.90 - ETA: 0s - loss: 0.2928 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8888 - val_loss: 7.4494 - val_accuracy: 0.6940\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 1.00 - ETA: 0s - loss: 0.4457 - accuracy: 0.85 - ETA: 0s - loss: 0.4346 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8518 - val_loss: 0.9313 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.93 - ETA: 0s - loss: 0.3972 - accuracy: 0.86 - ETA: 0s - loss: 0.4561 - accuracy: 0.87 - 0s 2ms/step - loss: 0.5466 - accuracy: 0.8346 - val_loss: 0.6151 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.68 - ETA: 0s - loss: 0.7553 - accuracy: 0.68 - ETA: 0s - loss: 0.7215 - accuracy: 0.69 - 0s 2ms/step - loss: 0.7132 - accuracy: 0.6991 - val_loss: 0.6767 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6348 - accuracy: 0.78 - ETA: 0s - loss: 0.6947 - accuracy: 0.46 - ETA: 0s - loss: 0.6930 - accuracy: 0.45 - 0s 2ms/step - loss: 0.6853 - accuracy: 0.4233 - val_loss: 0.9383 - val_accuracy: 0.3284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.31 - ETA: 0s - loss: 0.9416 - accuracy: 0.51 - ETA: 0s - loss: 0.8161 - accuracy: 0.61 - 0s 2ms/step - loss: 0.7909 - accuracy: 0.5898 - val_loss: 0.6998 - val_accuracy: 0.3045\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.28 - ETA: 0s - loss: 0.6924 - accuracy: 0.29 - ETA: 0s - loss: 0.6914 - accuracy: 0.29 - 0s 2ms/step - loss: 0.6934 - accuracy: 0.3423 - val_loss: 0.6927 - val_accuracy: 0.6955\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.65 - ETA: 0s - loss: 0.6947 - accuracy: 0.32 - ETA: 0s - loss: 0.6950 - accuracy: 0.43 - 0s 2ms/step - loss: 0.6936 - accuracy: 0.4113 - val_loss: 0.6936 - val_accuracy: 0.3045\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.21 - ETA: 0s - loss: 0.6829 - accuracy: 0.68 - ETA: 0s - loss: 0.6944 - accuracy: 0.67 - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5883 - val_loss: 0.6989 - val_accuracy: 0.3045\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.28 - ETA: 0s - loss: 0.6872 - accuracy: 0.42 - ETA: 0s - loss: 0.6880 - accuracy: 0.56 - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5860 - val_loss: 0.6953 - val_accuracy: 0.3045\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.37 - ETA: 0s - loss: 0.6857 - accuracy: 0.36 - ETA: 0s - loss: 0.6898 - accuracy: 0.51 - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4673 - val_loss: 0.6981 - val_accuracy: 0.3045\n",
      "Epoch 25/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.6222 - accuracy: 0.18 - ETA: 0s - loss: 0.6923 - accuracy: 0.29 - ETA: 0s - loss: 0.6964 - accuracy: 0.3058Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.3009 - val_loss: 0.6990 - val_accuracy: 0.3045\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.78 - ETA: 0s - loss: 0.8218 - accuracy: 0.64 - ETA: 0s - loss: 0.7072 - accuracy: 0.68 - ETA: 0s - loss: 0.6617 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6586 - accuracy: 0.7100 - val_loss: 0.6006 - val_accuracy: 0.6940\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.81 - ETA: 0s - loss: 0.5500 - accuracy: 0.76 - ETA: 0s - loss: 0.5530 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5520 - accuracy: 0.7693 - val_loss: 0.6292 - val_accuracy: 0.7090\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3585 - accuracy: 0.84 - ETA: 0s - loss: 0.5491 - accuracy: 0.78 - ETA: 0s - loss: 0.5458 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7869 - val_loss: 0.6547 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4412 - accuracy: 0.84 - ETA: 0s - loss: 0.4761 - accuracy: 0.82 - ETA: 0s - loss: 0.4901 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4909 - accuracy: 0.8171 - val_loss: 0.6082 - val_accuracy: 0.7134\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.93 - ETA: 0s - loss: 0.4792 - accuracy: 0.81 - ETA: 0s - loss: 0.4684 - accuracy: 0.82 - ETA: 0s - loss: 0.4607 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4580 - accuracy: 0.8309 - val_loss: 0.7355 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.84 - ETA: 0s - loss: 0.4306 - accuracy: 0.85 - ETA: 0s - loss: 0.4543 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4570 - accuracy: 0.8373 - val_loss: 0.5633 - val_accuracy: 0.7239\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.81 - ETA: 0s - loss: 0.4424 - accuracy: 0.85 - ETA: 0s - loss: 0.4402 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8544 - val_loss: 0.7154 - val_accuracy: 0.7000\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.84 - ETA: 0s - loss: 0.3649 - accuracy: 0.87 - ETA: 0s - loss: 0.3912 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3974 - accuracy: 0.8630 - val_loss: 0.5618 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.78 - ETA: 0s - loss: 0.4034 - accuracy: 0.85 - ETA: 0s - loss: 0.3713 - accuracy: 0.87 - ETA: 0s - loss: 0.3672 - accuracy: 0.87 - ETA: 0s - loss: 0.3718 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8772 - val_loss: 0.6217 - val_accuracy: 0.7358\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.87 - ETA: 0s - loss: 0.3469 - accuracy: 0.90 - ETA: 0s - loss: 0.3523 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8884 - val_loss: 0.7930 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.93 - ETA: 0s - loss: 0.3109 - accuracy: 0.91 - ETA: 0s - loss: 0.3516 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8903 - val_loss: 0.5886 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.87 - ETA: 0s - loss: 0.3398 - accuracy: 0.90 - ETA: 0s - loss: 0.3756 - accuracy: 0.88 - ETA: 0s - loss: 0.3546 - accuracy: 0.89 - ETA: 0s - loss: 0.3559 - accuracy: 0.89 - ETA: 0s - loss: 0.3362 - accuracy: 0.90 - ETA: 0s - loss: 0.3442 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3466 - accuracy: 0.8985 - val_loss: 0.7402 - val_accuracy: 0.7149\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.96 - ETA: 0s - loss: 0.3179 - accuracy: 0.90 - ETA: 0s - loss: 0.3528 - accuracy: 0.89 - ETA: 0s - loss: 0.3344 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8955 - val_loss: 0.8721 - val_accuracy: 0.7060\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.84 - ETA: 0s - loss: 0.3376 - accuracy: 0.89 - ETA: 0s - loss: 0.3472 - accuracy: 0.90 - ETA: 0s - loss: 0.3958 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8895 - val_loss: 0.9489 - val_accuracy: 0.6746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.90 - ETA: 0s - loss: 0.3898 - accuracy: 0.86 - ETA: 0s - loss: 0.3964 - accuracy: 0.86 - ETA: 0s - loss: 0.3954 - accuracy: 0.87 - ETA: 0s - loss: 0.3775 - accuracy: 0.88 - ETA: 0s - loss: 0.3729 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8850 - val_loss: 0.6774 - val_accuracy: 0.7134\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.90 - ETA: 0s - loss: 0.2561 - accuracy: 0.93 - ETA: 0s - loss: 0.2986 - accuracy: 0.91 - ETA: 0s - loss: 0.3253 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3310 - accuracy: 0.9063 - val_loss: 1.0167 - val_accuracy: 0.7194\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.96 - ETA: 0s - loss: 0.3111 - accuracy: 0.91 - ETA: 0s - loss: 0.3241 - accuracy: 0.91 - ETA: 0s - loss: 0.3214 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3253 - accuracy: 0.9130 - val_loss: 0.7926 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.96 - ETA: 0s - loss: 0.2912 - accuracy: 0.93 - ETA: 0s - loss: 0.3134 - accuracy: 0.93 - ETA: 0s - loss: 0.3401 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3451 - accuracy: 0.9112 - val_loss: 0.9466 - val_accuracy: 0.7149\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.90 - ETA: 0s - loss: 0.4280 - accuracy: 0.89 - ETA: 0s - loss: 0.4145 - accuracy: 0.88 - ETA: 0s - loss: 0.3754 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8910 - val_loss: 1.0862 - val_accuracy: 0.7104\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.90 - ETA: 0s - loss: 0.3713 - accuracy: 0.90 - ETA: 0s - loss: 0.3811 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8940 - val_loss: 0.7809 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "70/84 [========================>.....] - ETA: 0s - loss: 0.2990 - accuracy: 0.93 - ETA: 0s - loss: 0.3433 - accuracy: 0.90 - ETA: 0s - loss: 0.3469 - accuracy: 0.90 - ETA: 0s - loss: 0.3458 - accuracy: 0.90 - ETA: 0s - loss: 0.3332 - accuracy: 0.9062Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.9063 - val_loss: 1.5863 - val_accuracy: 0.6896\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 5e84c4bdf18ca7080e0a419697fbc1bf</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7368159294128418</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.09544073746840609</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 35</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.8170 - accuracy: 0.21 - ETA: 0s - loss: 8.3395 - accuracy: 0.49 - ETA: 0s - loss: 7.1027 - accuracy: 0.54 - ETA: 0s - loss: 5.5256 - accuracy: 0.57 - ETA: 0s - loss: 4.3826 - accuracy: 0.59 - ETA: 0s - loss: 3.7737 - accuracy: 0.60 - ETA: 0s - loss: 3.4055 - accuracy: 0.60 - ETA: 0s - loss: 3.0489 - accuracy: 0.62 - ETA: 0s - loss: 2.8177 - accuracy: 0.63 - 1s 8ms/step - loss: 2.7586 - accuracy: 0.6297 - val_loss: 0.5683 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.5571 - accuracy: 0.56 - ETA: 0s - loss: 0.9974 - accuracy: 0.58 - ETA: 0s - loss: 0.8653 - accuracy: 0.54 - ETA: 0s - loss: 0.8000 - accuracy: 0.57 - ETA: 0s - loss: 0.7783 - accuracy: 0.60 - ETA: 0s - loss: 0.7593 - accuracy: 0.63 - ETA: 0s - loss: 0.7449 - accuracy: 0.64 - ETA: 0s - loss: 0.7346 - accuracy: 0.65 - 0s 6ms/step - loss: 0.7474 - accuracy: 0.6599 - val_loss: 0.6374 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.68 - ETA: 0s - loss: 0.7350 - accuracy: 0.74 - ETA: 0s - loss: 0.6918 - accuracy: 0.74 - ETA: 0s - loss: 0.6900 - accuracy: 0.73 - ETA: 0s - loss: 0.6830 - accuracy: 0.74 - ETA: 0s - loss: 0.7007 - accuracy: 0.73 - ETA: 0s - loss: 0.6991 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7046 - accuracy: 0.7309 - val_loss: 0.6170 - val_accuracy: 0.7403\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.71 - ETA: 0s - loss: 0.7665 - accuracy: 0.73 - ETA: 0s - loss: 0.7239 - accuracy: 0.73 - ETA: 0s - loss: 0.7090 - accuracy: 0.73 - ETA: 0s - loss: 0.6944 - accuracy: 0.73 - ETA: 0s - loss: 0.6831 - accuracy: 0.74 - ETA: 0s - loss: 0.6743 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6740 - accuracy: 0.7398 - val_loss: 0.6208 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7011 - accuracy: 0.62 - ETA: 0s - loss: 0.5894 - accuracy: 0.78 - ETA: 0s - loss: 0.5900 - accuracy: 0.78 - ETA: 0s - loss: 0.6004 - accuracy: 0.77 - ETA: 0s - loss: 0.6124 - accuracy: 0.76 - ETA: 0s - loss: 0.6204 - accuracy: 0.75 - ETA: 0s - loss: 0.6224 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6256 - accuracy: 0.7563 - val_loss: 0.6114 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6509 - accuracy: 0.71 - ETA: 0s - loss: 0.6455 - accuracy: 0.72 - ETA: 0s - loss: 0.6207 - accuracy: 0.75 - ETA: 0s - loss: 0.6301 - accuracy: 0.74 - ETA: 0s - loss: 0.6200 - accuracy: 0.75 - ETA: 0s - loss: 0.6120 - accuracy: 0.76 - ETA: 0s - loss: 0.6235 - accuracy: 0.76 - ETA: 0s - loss: 0.6263 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6271 - accuracy: 0.7622 - val_loss: 0.6181 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.68 - ETA: 0s - loss: 0.6462 - accuracy: 0.72 - ETA: 0s - loss: 0.6175 - accuracy: 0.75 - ETA: 0s - loss: 0.6182 - accuracy: 0.75 - ETA: 0s - loss: 0.6196 - accuracy: 0.75 - ETA: 0s - loss: 0.6356 - accuracy: 0.76 - ETA: 0s - loss: 0.6441 - accuracy: 0.76 - ETA: 0s - loss: 0.6420 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6391 - accuracy: 0.7626 - val_loss: 0.6198 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.87 - ETA: 0s - loss: 0.6020 - accuracy: 0.76 - ETA: 0s - loss: 0.6417 - accuracy: 0.75 - ETA: 0s - loss: 0.6477 - accuracy: 0.75 - ETA: 0s - loss: 0.6400 - accuracy: 0.75 - ETA: 0s - loss: 0.6523 - accuracy: 0.74 - ETA: 0s - loss: 0.6497 - accuracy: 0.74 - ETA: 0s - loss: 0.6459 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6547 - accuracy: 0.7484 - val_loss: 0.6195 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8806 - accuracy: 0.71 - ETA: 0s - loss: 0.6539 - accuracy: 0.74 - ETA: 0s - loss: 0.6730 - accuracy: 0.74 - ETA: 0s - loss: 0.6627 - accuracy: 0.74 - ETA: 0s - loss: 0.6572 - accuracy: 0.75 - ETA: 0s - loss: 0.6493 - accuracy: 0.75 - ETA: 0s - loss: 0.6594 - accuracy: 0.75 - ETA: 0s - loss: 0.6666 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6676 - accuracy: 0.7555 - val_loss: 0.6354 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.84 - ETA: 0s - loss: 0.7676 - accuracy: 0.75 - ETA: 0s - loss: 0.7168 - accuracy: 0.75 - ETA: 0s - loss: 0.7044 - accuracy: 0.73 - ETA: 0s - loss: 0.6850 - accuracy: 0.74 - ETA: 0s - loss: 0.6834 - accuracy: 0.74 - ETA: 0s - loss: 0.6912 - accuracy: 0.74 - ETA: 0s - loss: 0.6905 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6873 - accuracy: 0.7290 - val_loss: 0.6388 - val_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.75 - ETA: 0s - loss: 0.6436 - accuracy: 0.70 - ETA: 0s - loss: 1.1347 - accuracy: 0.70 - ETA: 0s - loss: 0.9725 - accuracy: 0.72 - ETA: 0s - loss: 0.8998 - accuracy: 0.72 - ETA: 0s - loss: 0.8640 - accuracy: 0.72 - ETA: 0s - loss: 0.8249 - accuracy: 0.71 - ETA: 0s - loss: 0.8022 - accuracy: 0.71 - 0s 5ms/step - loss: 0.8013 - accuracy: 0.7096 - val_loss: 0.6495 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.75 - ETA: 0s - loss: 0.6624 - accuracy: 0.71 - ETA: 0s - loss: 0.6613 - accuracy: 0.70 - ETA: 0s - loss: 0.6662 - accuracy: 0.69 - ETA: 0s - loss: 0.6602 - accuracy: 0.69 - ETA: 0s - loss: 0.6563 - accuracy: 0.69 - ETA: 0s - loss: 0.6551 - accuracy: 0.70 - ETA: 0s - loss: 0.6615 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6632 - accuracy: 0.7066 - val_loss: 0.6485 - val_accuracy: 0.7030\n",
      "Epoch 13/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4906 - accuracy: 0.87 - ETA: 0s - loss: 0.6405 - accuracy: 0.74 - ETA: 0s - loss: 0.6588 - accuracy: 0.72 - ETA: 0s - loss: 0.6560 - accuracy: 0.72 - ETA: 0s - loss: 0.6467 - accuracy: 0.73 - ETA: 0s - loss: 0.6452 - accuracy: 0.73 - ETA: 0s - loss: 0.6431 - accuracy: 0.72 - ETA: 0s - loss: 0.6444 - accuracy: 0.7266Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6437 - accuracy: 0.7275 - val_loss: 0.6324 - val_accuracy: 0.7313\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0178 - accuracy: 0.43 - ETA: 0s - loss: 8.2840 - accuracy: 0.62 - ETA: 0s - loss: 5.7366 - accuracy: 0.62 - ETA: 0s - loss: 4.4163 - accuracy: 0.58 - ETA: 0s - loss: 3.6205 - accuracy: 0.58 - ETA: 0s - loss: 3.0616 - accuracy: 0.57 - ETA: 0s - loss: 2.7017 - accuracy: 0.56 - ETA: 0s - loss: 2.4220 - accuracy: 0.54 - 1s 6ms/step - loss: 2.4070 - accuracy: 0.5420 - val_loss: 0.6600 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.46 - ETA: 0s - loss: 0.6892 - accuracy: 0.66 - ETA: 0s - loss: 0.6786 - accuracy: 0.68 - ETA: 0s - loss: 0.6817 - accuracy: 0.68 - ETA: 0s - loss: 0.6871 - accuracy: 0.68 - ETA: 0s - loss: 0.6964 - accuracy: 0.68 - ETA: 0s - loss: 0.6905 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6927 - accuracy: 0.6861 - val_loss: 0.6243 - val_accuracy: 0.7433\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.65 - ETA: 0s - loss: 0.6731 - accuracy: 0.70 - ETA: 0s - loss: 0.6644 - accuracy: 0.70 - ETA: 0s - loss: 0.6710 - accuracy: 0.70 - ETA: 0s - loss: 0.6723 - accuracy: 0.71 - ETA: 0s - loss: 0.6797 - accuracy: 0.70 - ETA: 0s - loss: 0.6740 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6707 - accuracy: 0.7152 - val_loss: 0.6244 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6640 - accuracy: 0.75 - ETA: 0s - loss: 0.7172 - accuracy: 0.75 - ETA: 0s - loss: 0.8648 - accuracy: 0.73 - ETA: 0s - loss: 0.8017 - accuracy: 0.73 - ETA: 0s - loss: 0.7632 - accuracy: 0.74 - ETA: 0s - loss: 0.7572 - accuracy: 0.73 - ETA: 0s - loss: 0.7461 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7434 - accuracy: 0.7283 - val_loss: 0.6330 - val_accuracy: 0.7045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.81 - ETA: 0s - loss: 0.6528 - accuracy: 0.74 - ETA: 0s - loss: 0.6618 - accuracy: 0.73 - ETA: 0s - loss: 0.6521 - accuracy: 0.75 - ETA: 0s - loss: 0.6602 - accuracy: 0.75 - ETA: 0s - loss: 0.6674 - accuracy: 0.74 - ETA: 0s - loss: 0.6979 - accuracy: 0.73 - ETA: 0s - loss: 0.7271 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7273 - accuracy: 0.7365 - val_loss: 0.6300 - val_accuracy: 0.7149\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7880 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.73 - ETA: 0s - loss: 0.6476 - accuracy: 0.77 - ETA: 0s - loss: 0.6720 - accuracy: 0.74 - ETA: 0s - loss: 0.6751 - accuracy: 0.74 - ETA: 0s - loss: 0.6657 - accuracy: 0.74 - ETA: 0s - loss: 0.6851 - accuracy: 0.74 - ETA: 0s - loss: 0.6814 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6794 - accuracy: 0.7492 - val_loss: 0.6317 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.81 - ETA: 0s - loss: 0.7028 - accuracy: 0.73 - ETA: 0s - loss: 0.7367 - accuracy: 0.72 - ETA: 0s - loss: 0.7151 - accuracy: 0.72 - ETA: 0s - loss: 0.6974 - accuracy: 0.74 - ETA: 0s - loss: 0.6956 - accuracy: 0.74 - ETA: 0s - loss: 0.7047 - accuracy: 0.74 - ETA: 0s - loss: 0.7071 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7053 - accuracy: 0.7413 - val_loss: 0.6311 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.68 - ETA: 0s - loss: 0.7862 - accuracy: 0.69 - ETA: 0s - loss: 0.7347 - accuracy: 0.71 - ETA: 0s - loss: 0.7027 - accuracy: 0.73 - ETA: 0s - loss: 0.6865 - accuracy: 0.74 - ETA: 0s - loss: 0.6769 - accuracy: 0.74 - ETA: 0s - loss: 0.6734 - accuracy: 0.75 - ETA: 0s - loss: 0.7120 - accuracy: 0.75 - 0s 5ms/step - loss: 0.7146 - accuracy: 0.7503 - val_loss: 0.6356 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7492 - accuracy: 0.62 - ETA: 0s - loss: 0.8128 - accuracy: 0.72 - ETA: 0s - loss: 0.7587 - accuracy: 0.72 - ETA: 0s - loss: 0.7398 - accuracy: 0.73 - ETA: 0s - loss: 0.7151 - accuracy: 0.73 - ETA: 0s - loss: 0.7019 - accuracy: 0.74 - ETA: 0s - loss: 0.7045 - accuracy: 0.73 - ETA: 0s - loss: 0.7018 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7005 - accuracy: 0.7365 - val_loss: 0.6451 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.84 - ETA: 0s - loss: 0.6321 - accuracy: 0.75 - ETA: 0s - loss: 0.6366 - accuracy: 0.75 - ETA: 0s - loss: 0.6476 - accuracy: 0.74 - ETA: 0s - loss: 0.6468 - accuracy: 0.74 - ETA: 0s - loss: 0.6503 - accuracy: 0.73 - ETA: 0s - loss: 0.6505 - accuracy: 0.73 - ETA: 0s - loss: 0.6492 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6502 - accuracy: 0.7398 - val_loss: 0.6644 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7257 - accuracy: 0.65 - ETA: 0s - loss: 0.6361 - accuracy: 0.76 - ETA: 0s - loss: 0.6524 - accuracy: 0.74 - ETA: 0s - loss: 0.6462 - accuracy: 0.74 - ETA: 0s - loss: 0.6580 - accuracy: 0.72 - ETA: 0s - loss: 0.6788 - accuracy: 0.73 - ETA: 0s - loss: 0.6775 - accuracy: 0.72 - ETA: 0s - loss: 0.6852 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6895 - accuracy: 0.7253 - val_loss: 0.6724 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6304 - accuracy: 0.78 - ETA: 0s - loss: 0.7039 - accuracy: 0.72 - ETA: 0s - loss: 0.7570 - accuracy: 0.71 - ETA: 0s - loss: 0.7376 - accuracy: 0.71 - ETA: 0s - loss: 0.7358 - accuracy: 0.71 - ETA: 0s - loss: 0.7252 - accuracy: 0.70 - ETA: 0s - loss: 0.7179 - accuracy: 0.69 - ETA: 0s - loss: 0.7103 - accuracy: 0.7012Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.7014 - val_loss: 0.6737 - val_accuracy: 0.7000\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1721 - accuracy: 0.50 - ETA: 0s - loss: 8.7907 - accuracy: 0.54 - ETA: 0s - loss: 6.1142 - accuracy: 0.56 - ETA: 0s - loss: 4.7133 - accuracy: 0.59 - ETA: 0s - loss: 3.8088 - accuracy: 0.61 - ETA: 0s - loss: 3.2734 - accuracy: 0.62 - ETA: 0s - loss: 2.8254 - accuracy: 0.61 - ETA: 0s - loss: 2.5169 - accuracy: 0.58 - 1s 6ms/step - loss: 2.4554 - accuracy: 0.5823 - val_loss: 0.6654 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.43 - ETA: 0s - loss: 0.6685 - accuracy: 0.62 - ETA: 0s - loss: 0.6692 - accuracy: 0.67 - ETA: 0s - loss: 0.6719 - accuracy: 0.70 - ETA: 0s - loss: 0.6655 - accuracy: 0.71 - ETA: 0s - loss: 0.6698 - accuracy: 0.71 - ETA: 0s - loss: 0.6723 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6728 - accuracy: 0.7107 - val_loss: 0.6353 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8495 - accuracy: 0.65 - ETA: 0s - loss: 0.6984 - accuracy: 0.71 - ETA: 0s - loss: 0.6667 - accuracy: 0.72 - ETA: 0s - loss: 0.6958 - accuracy: 0.70 - ETA: 0s - loss: 0.6885 - accuracy: 0.71 - ETA: 0s - loss: 0.6910 - accuracy: 0.70 - ETA: 0s - loss: 0.6868 - accuracy: 0.71 - ETA: 0s - loss: 0.6833 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6822 - accuracy: 0.7178 - val_loss: 0.6256 - val_accuracy: 0.6940\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8537 - accuracy: 0.53 - ETA: 0s - loss: 0.7349 - accuracy: 0.66 - ETA: 0s - loss: 0.7554 - accuracy: 0.69 - ETA: 0s - loss: 0.7327 - accuracy: 0.70 - ETA: 0s - loss: 0.7151 - accuracy: 0.70 - ETA: 0s - loss: 0.7087 - accuracy: 0.70 - ETA: 0s - loss: 0.7173 - accuracy: 0.70 - ETA: 0s - loss: 0.7292 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7289 - accuracy: 0.7081 - val_loss: 0.6281 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.68 - ETA: 0s - loss: 0.6827 - accuracy: 0.72 - ETA: 0s - loss: 0.6821 - accuracy: 0.70 - ETA: 0s - loss: 0.6977 - accuracy: 0.70 - ETA: 0s - loss: 0.6879 - accuracy: 0.71 - ETA: 0s - loss: 0.6847 - accuracy: 0.71 - ETA: 0s - loss: 0.7023 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7019 - accuracy: 0.7174 - val_loss: 0.6237 - val_accuracy: 0.7194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.62 - ETA: 0s - loss: 0.7834 - accuracy: 0.63 - ETA: 0s - loss: 0.7467 - accuracy: 0.69 - ETA: 0s - loss: 0.7331 - accuracy: 0.71 - ETA: 0s - loss: 0.7093 - accuracy: 0.73 - ETA: 0s - loss: 0.7187 - accuracy: 0.72 - ETA: 0s - loss: 0.7250 - accuracy: 0.72 - ETA: 0s - loss: 0.7246 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7276 - accuracy: 0.7260 - val_loss: 0.6370 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 0.81 - ETA: 0s - loss: 0.7100 - accuracy: 0.73 - ETA: 0s - loss: 0.6860 - accuracy: 0.74 - ETA: 0s - loss: 0.6886 - accuracy: 0.74 - ETA: 0s - loss: 0.6900 - accuracy: 0.73 - ETA: 0s - loss: 0.6850 - accuracy: 0.74 - ETA: 0s - loss: 0.6820 - accuracy: 0.73 - ETA: 0s - loss: 0.6863 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6843 - accuracy: 0.7368 - val_loss: 0.6375 - val_accuracy: 0.7104\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.78 - ETA: 0s - loss: 0.6950 - accuracy: 0.72 - ETA: 0s - loss: 0.6531 - accuracy: 0.74 - ETA: 0s - loss: 0.6816 - accuracy: 0.73 - ETA: 0s - loss: 0.6810 - accuracy: 0.72 - ETA: 0s - loss: 0.6798 - accuracy: 0.72 - ETA: 0s - loss: 0.6806 - accuracy: 0.72 - ETA: 0s - loss: 0.6762 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6777 - accuracy: 0.7193 - val_loss: 0.6451 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.68 - ETA: 0s - loss: 0.5989 - accuracy: 0.78 - ETA: 0s - loss: 0.6396 - accuracy: 0.75 - ETA: 0s - loss: 0.6419 - accuracy: 0.74 - ETA: 0s - loss: 0.6589 - accuracy: 0.73 - ETA: 0s - loss: 0.6647 - accuracy: 0.73 - ETA: 0s - loss: 0.6664 - accuracy: 0.73 - ETA: 0s - loss: 0.6732 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6793 - accuracy: 0.7324 - val_loss: 0.6397 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.75 - ETA: 0s - loss: 0.6799 - accuracy: 0.73 - ETA: 0s - loss: 0.6713 - accuracy: 0.72 - ETA: 0s - loss: 0.6858 - accuracy: 0.72 - ETA: 0s - loss: 0.7173 - accuracy: 0.73 - ETA: 0s - loss: 0.7013 - accuracy: 0.73 - ETA: 0s - loss: 0.7276 - accuracy: 0.73 - ETA: 0s - loss: 0.7379 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7396 - accuracy: 0.7268 - val_loss: 0.6438 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 2.4834 - accuracy: 0.71 - ETA: 0s - loss: 0.9108 - accuracy: 0.73 - ETA: 0s - loss: 0.9017 - accuracy: 0.72 - ETA: 0s - loss: 0.8408 - accuracy: 0.71 - ETA: 0s - loss: 0.8031 - accuracy: 0.72 - ETA: 0s - loss: 0.7802 - accuracy: 0.72 - ETA: 0s - loss: 0.7839 - accuracy: 0.71 - ETA: 0s - loss: 0.7703 - accuracy: 0.7090Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7605 - accuracy: 0.7055 - val_loss: 0.6595 - val_accuracy: 0.7045\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 4db4213e218a9b2188a60a8007c757e3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.739303469657898</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9455886139667865</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 240</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 55</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.87 - ETA: 0s - loss: 1.6010 - accuracy: 0.63 - ETA: 0s - loss: 1.1665 - accuracy: 0.65 - ETA: 0s - loss: 1.0044 - accuracy: 0.68 - ETA: 0s - loss: 0.9106 - accuracy: 0.70 - ETA: 0s - loss: 0.8586 - accuracy: 0.70 - ETA: 0s - loss: 0.8247 - accuracy: 0.71 - ETA: 0s - loss: 0.7964 - accuracy: 0.72 - 1s 7ms/step - loss: 0.7807 - accuracy: 0.7212 - val_loss: 0.6531 - val_accuracy: 0.6910\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.87 - ETA: 0s - loss: 0.4973 - accuracy: 0.79 - ETA: 0s - loss: 0.5258 - accuracy: 0.79 - ETA: 0s - loss: 0.5550 - accuracy: 0.78 - ETA: 0s - loss: 0.5604 - accuracy: 0.78 - ETA: 0s - loss: 0.5536 - accuracy: 0.78 - ETA: 0s - loss: 0.5582 - accuracy: 0.78 - ETA: 0s - loss: 0.5689 - accuracy: 0.77 - 0s 6ms/step - loss: 0.5635 - accuracy: 0.7760 - val_loss: 0.5956 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.84 - ETA: 0s - loss: 0.4670 - accuracy: 0.82 - ETA: 0s - loss: 0.4846 - accuracy: 0.82 - ETA: 0s - loss: 0.4975 - accuracy: 0.81 - ETA: 0s - loss: 0.5140 - accuracy: 0.79 - ETA: 0s - loss: 0.5260 - accuracy: 0.78 - ETA: 0s - loss: 0.5226 - accuracy: 0.79 - ETA: 0s - loss: 0.5336 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5338 - accuracy: 0.7865 - val_loss: 0.6497 - val_accuracy: 0.6970\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.84 - ETA: 0s - loss: 0.5303 - accuracy: 0.79 - ETA: 0s - loss: 0.5081 - accuracy: 0.81 - ETA: 0s - loss: 0.4975 - accuracy: 0.82 - ETA: 0s - loss: 0.4983 - accuracy: 0.81 - ETA: 0s - loss: 0.5065 - accuracy: 0.81 - ETA: 0s - loss: 0.5095 - accuracy: 0.81 - ETA: 0s - loss: 0.5130 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5211 - accuracy: 0.8078 - val_loss: 0.7161 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.81 - ETA: 0s - loss: 0.4758 - accuracy: 0.82 - ETA: 0s - loss: 0.4713 - accuracy: 0.83 - ETA: 0s - loss: 0.4701 - accuracy: 0.83 - ETA: 0s - loss: 0.4753 - accuracy: 0.83 - ETA: 0s - loss: 0.4811 - accuracy: 0.82 - ETA: 0s - loss: 0.4834 - accuracy: 0.82 - ETA: 0s - loss: 0.4813 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4818 - accuracy: 0.8328 - val_loss: 1.0115 - val_accuracy: 0.6925\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.90 - ETA: 0s - loss: 0.4503 - accuracy: 0.84 - ETA: 0s - loss: 0.4460 - accuracy: 0.84 - ETA: 0s - loss: 0.4627 - accuracy: 0.84 - ETA: 0s - loss: 0.4814 - accuracy: 0.83 - ETA: 0s - loss: 0.4910 - accuracy: 0.83 - ETA: 0s - loss: 0.4947 - accuracy: 0.82 - ETA: 0s - loss: 0.4956 - accuracy: 0.82 - 0s 6ms/step - loss: 0.4940 - accuracy: 0.8242 - val_loss: 0.7082 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.3303 - accuracy: 0.78 - ETA: 0s - loss: 0.6645 - accuracy: 0.81 - ETA: 0s - loss: 0.5728 - accuracy: 0.84 - ETA: 0s - loss: 0.5697 - accuracy: 0.83 - ETA: 0s - loss: 0.5653 - accuracy: 0.82 - ETA: 0s - loss: 0.5458 - accuracy: 0.82 - ETA: 0s - loss: 0.5382 - accuracy: 0.82 - ETA: 0s - loss: 0.5265 - accuracy: 0.82 - ETA: 0s - loss: 0.5185 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5185 - accuracy: 0.8309 - val_loss: 0.7111 - val_accuracy: 0.7224\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.87 - ETA: 0s - loss: 0.4108 - accuracy: 0.85 - ETA: 0s - loss: 0.4326 - accuracy: 0.84 - ETA: 0s - loss: 0.4389 - accuracy: 0.85 - ETA: 0s - loss: 0.4465 - accuracy: 0.84 - ETA: 0s - loss: 0.4476 - accuracy: 0.84 - ETA: 0s - loss: 0.4570 - accuracy: 0.83 - ETA: 0s - loss: 0.4679 - accuracy: 0.83 - 1s 6ms/step - loss: 0.4714 - accuracy: 0.8358 - val_loss: 0.5667 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.90 - ETA: 0s - loss: 0.4803 - accuracy: 0.85 - ETA: 0s - loss: 0.4419 - accuracy: 0.86 - ETA: 0s - loss: 0.4446 - accuracy: 0.85 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4658 - accuracy: 0.85 - ETA: 0s - loss: 0.4784 - accuracy: 0.85 - ETA: 0s - loss: 0.4764 - accuracy: 0.85 - ETA: 0s - loss: 0.4763 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4797 - accuracy: 0.8455 - val_loss: 0.8370 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4654 - accuracy: 0.87 - ETA: 0s - loss: 0.4995 - accuracy: 0.83 - ETA: 0s - loss: 0.4862 - accuracy: 0.84 - ETA: 0s - loss: 0.4782 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.4701 - accuracy: 0.84 - ETA: 0s - loss: 0.4767 - accuracy: 0.84 - ETA: 0s - loss: 0.4771 - accuracy: 0.84 - ETA: 0s - loss: 0.4787 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4772 - accuracy: 0.8458 - val_loss: 0.8278 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.87 - ETA: 0s - loss: 0.4580 - accuracy: 0.85 - ETA: 0s - loss: 0.4366 - accuracy: 0.86 - ETA: 0s - loss: 0.4279 - accuracy: 0.86 - ETA: 0s - loss: 0.4408 - accuracy: 0.85 - ETA: 0s - loss: 0.4423 - accuracy: 0.85 - ETA: 0s - loss: 0.4538 - accuracy: 0.85 - ETA: 0s - loss: 0.4504 - accuracy: 0.85 - ETA: 0s - loss: 0.4425 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4450 - accuracy: 0.8574 - val_loss: 0.8591 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.87 - ETA: 0s - loss: 0.4096 - accuracy: 0.86 - ETA: 0s - loss: 0.4101 - accuracy: 0.86 - ETA: 0s - loss: 0.3942 - accuracy: 0.87 - ETA: 0s - loss: 0.4127 - accuracy: 0.87 - ETA: 0s - loss: 0.4184 - accuracy: 0.87 - ETA: 0s - loss: 0.4302 - accuracy: 0.86 - ETA: 0s - loss: 0.4273 - accuracy: 0.86 - ETA: 0s - loss: 0.4443 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4440 - accuracy: 0.8615 - val_loss: 0.8156 - val_accuracy: 0.7075\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.90 - ETA: 0s - loss: 0.4033 - accuracy: 0.87 - ETA: 0s - loss: 0.3845 - accuracy: 0.88 - ETA: 0s - loss: 0.3848 - accuracy: 0.88 - ETA: 0s - loss: 0.3718 - accuracy: 0.88 - ETA: 0s - loss: 0.3842 - accuracy: 0.88 - ETA: 0s - loss: 0.3888 - accuracy: 0.88 - ETA: 0s - loss: 0.3886 - accuracy: 0.88 - ETA: 0s - loss: 0.3906 - accuracy: 0.88 - 1s 7ms/step - loss: 0.3950 - accuracy: 0.8820 - val_loss: 0.7387 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2965 - accuracy: 0.93 - ETA: 0s - loss: 0.4194 - accuracy: 0.87 - ETA: 0s - loss: 0.3956 - accuracy: 0.88 - ETA: 0s - loss: 0.4095 - accuracy: 0.87 - ETA: 0s - loss: 0.4262 - accuracy: 0.88 - ETA: 0s - loss: 0.4157 - accuracy: 0.88 - ETA: 0s - loss: 0.4082 - accuracy: 0.88 - ETA: 0s - loss: 0.3998 - accuracy: 0.88 - ETA: 0s - loss: 0.4039 - accuracy: 0.88 - 0s 6ms/step - loss: 0.4039 - accuracy: 0.8850 - val_loss: 1.0273 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.87 - ETA: 0s - loss: 0.3736 - accuracy: 0.87 - ETA: 0s - loss: 0.4448 - accuracy: 0.85 - ETA: 0s - loss: 0.4219 - accuracy: 0.86 - ETA: 0s - loss: 0.4106 - accuracy: 0.87 - ETA: 0s - loss: 0.4115 - accuracy: 0.87 - ETA: 0s - loss: 0.4011 - accuracy: 0.87 - ETA: 0s - loss: 0.4060 - accuracy: 0.87 - ETA: 0s - loss: 0.4014 - accuracy: 0.88 - 0s 6ms/step - loss: 0.4008 - accuracy: 0.8802 - val_loss: 0.9597 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.84 - ETA: 0s - loss: 0.2943 - accuracy: 0.92 - ETA: 0s - loss: 0.3521 - accuracy: 0.90 - ETA: 0s - loss: 0.3633 - accuracy: 0.89 - ETA: 0s - loss: 0.3631 - accuracy: 0.89 - ETA: 0s - loss: 0.3737 - accuracy: 0.89 - ETA: 0s - loss: 0.3816 - accuracy: 0.89 - ETA: 0s - loss: 0.3895 - accuracy: 0.89 - ETA: 0s - loss: 0.4115 - accuracy: 0.88 - 0s 6ms/step - loss: 0.4115 - accuracy: 0.8847 - val_loss: 1.2482 - val_accuracy: 0.6746\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.87 - ETA: 0s - loss: 0.4373 - accuracy: 0.86 - ETA: 0s - loss: 0.4490 - accuracy: 0.86 - ETA: 0s - loss: 0.4559 - accuracy: 0.85 - ETA: 0s - loss: 0.4397 - accuracy: 0.86 - ETA: 0s - loss: 0.4158 - accuracy: 0.87 - ETA: 0s - loss: 0.4133 - accuracy: 0.87 - ETA: 0s - loss: 0.4130 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4211 - accuracy: 0.8731 - val_loss: 0.7471 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.90 - ETA: 0s - loss: 0.3614 - accuracy: 0.89 - ETA: 0s - loss: 0.3699 - accuracy: 0.90 - ETA: 0s - loss: 0.3717 - accuracy: 0.90 - ETA: 0s - loss: 0.4112 - accuracy: 0.88 - ETA: 0s - loss: 0.4226 - accuracy: 0.88 - ETA: 0s - loss: 0.4510 - accuracy: 0.86 - ETA: 0s - loss: 0.4780 - accuracy: 0.85 - ETA: 0s - loss: 0.4983 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4983 - accuracy: 0.8425 - val_loss: 0.7133 - val_accuracy: 0.7149\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.56 - ETA: 0s - loss: 0.6330 - accuracy: 0.67 - ETA: 0s - loss: 0.6631 - accuracy: 0.71 - ETA: 0s - loss: 0.8065 - accuracy: 0.71 - ETA: 0s - loss: 0.7858 - accuracy: 0.71 - ETA: 0s - loss: 0.7772 - accuracy: 0.71 - ETA: 0s - loss: 0.7508 - accuracy: 0.71 - ETA: 0s - loss: 0.7317 - accuracy: 0.72 - ETA: 0s - loss: 0.7101 - accuracy: 0.73 - 0s 6ms/step - loss: 0.7063 - accuracy: 0.7387 - val_loss: 3.7263 - val_accuracy: 0.7179\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.68 - ETA: 0s - loss: 0.6881 - accuracy: 0.71 - ETA: 0s - loss: 0.6909 - accuracy: 0.66 - ETA: 0s - loss: 0.6981 - accuracy: 0.60 - ETA: 0s - loss: 0.6999 - accuracy: 0.57 - ETA: 0s - loss: 0.7012 - accuracy: 0.54 - ETA: 0s - loss: 0.6999 - accuracy: 0.53 - ETA: 0s - loss: 0.6994 - accuracy: 0.55 - ETA: 0s - loss: 0.6984 - accuracy: 0.56 - 0s 6ms/step - loss: 0.6967 - accuracy: 0.5622 - val_loss: 0.6899 - val_accuracy: 0.6955\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7170 - accuracy: 0.46 - ETA: 0s - loss: 0.6968 - accuracy: 0.48 - ETA: 0s - loss: 0.7058 - accuracy: 0.43 - ETA: 0s - loss: 0.6954 - accuracy: 0.45 - ETA: 0s - loss: 0.7009 - accuracy: 0.50 - ETA: 0s - loss: 0.7007 - accuracy: 0.52 - ETA: 0s - loss: 0.6983 - accuracy: 0.52 - ETA: 0s - loss: 0.6979 - accuracy: 0.52 - 0s 6ms/step - loss: 0.6961 - accuracy: 0.5192 - val_loss: 0.6806 - val_accuracy: 0.6955\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6735 - accuracy: 0.65 - ETA: 0s - loss: 0.6821 - accuracy: 0.61 - ETA: 0s - loss: 0.7029 - accuracy: 0.62 - ETA: 0s - loss: 0.6944 - accuracy: 0.56 - ETA: 0s - loss: 0.6969 - accuracy: 0.54 - ETA: 0s - loss: 0.6949 - accuracy: 0.52 - ETA: 0s - loss: 0.6977 - accuracy: 0.53 - ETA: 0s - loss: 0.6977 - accuracy: 0.51 - ETA: 0s - loss: 0.6954 - accuracy: 0.49 - 0s 6ms/step - loss: 0.6955 - accuracy: 0.4890 - val_loss: 0.6896 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.7216 - accuracy: 0.46 - ETA: 0s - loss: 0.6934 - accuracy: 0.66 - ETA: 0s - loss: 0.6991 - accuracy: 0.67 - ETA: 0s - loss: 0.6896 - accuracy: 0.61 - ETA: 0s - loss: 0.6901 - accuracy: 0.62 - ETA: 0s - loss: 0.6964 - accuracy: 0.61 - ETA: 0s - loss: 0.6994 - accuracy: 0.56 - ETA: 0s - loss: 0.6997 - accuracy: 0.53 - ETA: 0s - loss: 0.6959 - accuracy: 0.5179Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6948 - accuracy: 0.5230 - val_loss: 0.6722 - val_accuracy: 0.6955\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.71 - ETA: 0s - loss: 2.0710 - accuracy: 0.64 - ETA: 0s - loss: 1.5261 - accuracy: 0.62 - ETA: 0s - loss: 1.2313 - accuracy: 0.66 - ETA: 0s - loss: 1.0865 - accuracy: 0.68 - ETA: 0s - loss: 0.9973 - accuracy: 0.69 - ETA: 0s - loss: 0.9425 - accuracy: 0.69 - ETA: 0s - loss: 0.8969 - accuracy: 0.70 - 1s 7ms/step - loss: 0.8820 - accuracy: 0.7021 - val_loss: 0.6015 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.84 - ETA: 0s - loss: 0.5542 - accuracy: 0.75 - ETA: 0s - loss: 0.5650 - accuracy: 0.77 - ETA: 0s - loss: 0.5701 - accuracy: 0.76 - ETA: 0s - loss: 0.5746 - accuracy: 0.76 - ETA: 0s - loss: 0.5762 - accuracy: 0.76 - ETA: 0s - loss: 0.5740 - accuracy: 0.76 - ETA: 0s - loss: 0.5715 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5759 - accuracy: 0.7652 - val_loss: 0.6224 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7288 - accuracy: 0.56 - ETA: 0s - loss: 0.5447 - accuracy: 0.77 - ETA: 0s - loss: 0.5388 - accuracy: 0.78 - ETA: 0s - loss: 0.5303 - accuracy: 0.78 - ETA: 0s - loss: 0.5246 - accuracy: 0.78 - ETA: 0s - loss: 0.5232 - accuracy: 0.78 - ETA: 0s - loss: 0.5353 - accuracy: 0.78 - ETA: 0s - loss: 0.5342 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5404 - accuracy: 0.7865 - val_loss: 0.5743 - val_accuracy: 0.7448\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.68 - ETA: 0s - loss: 0.5263 - accuracy: 0.81 - ETA: 0s - loss: 0.5305 - accuracy: 0.79 - ETA: 0s - loss: 0.5213 - accuracy: 0.80 - ETA: 0s - loss: 0.5182 - accuracy: 0.81 - ETA: 0s - loss: 0.5201 - accuracy: 0.81 - ETA: 0s - loss: 0.5247 - accuracy: 0.80 - ETA: 0s - loss: 0.5195 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5229 - accuracy: 0.8033 - val_loss: 0.5896 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.81 - ETA: 0s - loss: 0.5060 - accuracy: 0.82 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - ETA: 0s - loss: 0.4900 - accuracy: 0.83 - ETA: 0s - loss: 0.4895 - accuracy: 0.82 - ETA: 0s - loss: 0.4824 - accuracy: 0.83 - ETA: 0s - loss: 0.4777 - accuracy: 0.83 - ETA: 0s - loss: 0.4824 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8275 - val_loss: 0.7259 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.87 - ETA: 0s - loss: 0.4043 - accuracy: 0.86 - ETA: 0s - loss: 0.5023 - accuracy: 0.83 - ETA: 0s - loss: 0.5130 - accuracy: 0.83 - ETA: 0s - loss: 0.5105 - accuracy: 0.82 - ETA: 0s - loss: 0.5059 - accuracy: 0.82 - ETA: 0s - loss: 0.5120 - accuracy: 0.82 - ETA: 0s - loss: 0.5044 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5033 - accuracy: 0.8268 - val_loss: 0.7087 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.78 - ETA: 0s - loss: 0.4890 - accuracy: 0.82 - ETA: 0s - loss: 0.4590 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.4797 - accuracy: 0.83 - ETA: 0s - loss: 0.4864 - accuracy: 0.83 - ETA: 0s - loss: 0.4937 - accuracy: 0.82 - ETA: 0s - loss: 0.4951 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4907 - accuracy: 0.8305 - val_loss: 0.6501 - val_accuracy: 0.7403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.81 - ETA: 0s - loss: 0.3978 - accuracy: 0.88 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - ETA: 0s - loss: 0.4605 - accuracy: 0.85 - ETA: 0s - loss: 0.4589 - accuracy: 0.85 - ETA: 0s - loss: 0.4704 - accuracy: 0.84 - ETA: 0s - loss: 0.4743 - accuracy: 0.84 - ETA: 0s - loss: 0.4721 - accuracy: 0.84 - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4731 - accuracy: 0.8414 - val_loss: 0.8131 - val_accuracy: 0.7104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4252 - accuracy: 0.84 - ETA: 0s - loss: 0.4594 - accuracy: 0.83 - ETA: 0s - loss: 0.4475 - accuracy: 0.84 - ETA: 0s - loss: 0.4428 - accuracy: 0.84 - ETA: 0s - loss: 0.4515 - accuracy: 0.84 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - ETA: 0s - loss: 0.4417 - accuracy: 0.85 - ETA: 0s - loss: 0.4538 - accuracy: 0.84 - ETA: 0s - loss: 0.4544 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4560 - accuracy: 0.8466 - val_loss: 0.7292 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.93 - ETA: 0s - loss: 0.6867 - accuracy: 0.87 - ETA: 0s - loss: 0.5997 - accuracy: 0.84 - ETA: 0s - loss: 0.5471 - accuracy: 0.84 - ETA: 0s - loss: 0.5266 - accuracy: 0.84 - ETA: 0s - loss: 0.6091 - accuracy: 0.84 - ETA: 0s - loss: 0.5966 - accuracy: 0.84 - ETA: 0s - loss: 0.5841 - accuracy: 0.84 - ETA: 0s - loss: 0.5902 - accuracy: 0.83 - 1s 6ms/step - loss: 0.5815 - accuracy: 0.8335 - val_loss: 0.6548 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.78 - ETA: 0s - loss: 0.4975 - accuracy: 0.81 - ETA: 0s - loss: 0.5168 - accuracy: 0.81 - ETA: 0s - loss: 0.5165 - accuracy: 0.81 - ETA: 0s - loss: 0.5281 - accuracy: 0.80 - ETA: 0s - loss: 0.5241 - accuracy: 0.80 - ETA: 0s - loss: 0.5049 - accuracy: 0.81 - ETA: 0s - loss: 0.5125 - accuracy: 0.81 - ETA: 0s - loss: 0.5086 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5103 - accuracy: 0.8223 - val_loss: 0.7054 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.81 - ETA: 0s - loss: 0.5513 - accuracy: 0.84 - ETA: 0s - loss: 0.5176 - accuracy: 0.83 - ETA: 0s - loss: 0.5256 - accuracy: 0.83 - ETA: 0s - loss: 0.5157 - accuracy: 0.83 - ETA: 0s - loss: 0.5063 - accuracy: 0.83 - ETA: 0s - loss: 0.4992 - accuracy: 0.84 - ETA: 0s - loss: 0.4971 - accuracy: 0.84 - ETA: 0s - loss: 0.5026 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5041 - accuracy: 0.8343 - val_loss: 0.5896 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.90 - ETA: 0s - loss: 0.4588 - accuracy: 0.83 - ETA: 0s - loss: 0.4696 - accuracy: 0.83 - ETA: 0s - loss: 0.4705 - accuracy: 0.83 - ETA: 0s - loss: 0.4786 - accuracy: 0.83 - ETA: 0s - loss: 0.4866 - accuracy: 0.83 - ETA: 0s - loss: 0.4841 - accuracy: 0.84 - ETA: 0s - loss: 0.4880 - accuracy: 0.84 - ETA: 0s - loss: 0.4852 - accuracy: 0.8396Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.8402 - val_loss: 0.5777 - val_accuracy: 0.7388\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7261 - accuracy: 0.68 - ETA: 0s - loss: 2.0206 - accuracy: 0.60 - ETA: 0s - loss: 1.3819 - accuracy: 0.64 - ETA: 0s - loss: 1.1585 - accuracy: 0.65 - ETA: 0s - loss: 1.0123 - accuracy: 0.68 - ETA: 0s - loss: 0.9357 - accuracy: 0.68 - ETA: 0s - loss: 0.8854 - accuracy: 0.69 - ETA: 0s - loss: 0.8454 - accuracy: 0.70 - 1s 7ms/step - loss: 0.8137 - accuracy: 0.7077 - val_loss: 0.5806 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.84 - ETA: 0s - loss: 0.5370 - accuracy: 0.77 - ETA: 0s - loss: 0.5534 - accuracy: 0.76 - ETA: 0s - loss: 0.5549 - accuracy: 0.75 - ETA: 0s - loss: 0.5511 - accuracy: 0.75 - ETA: 0s - loss: 0.5471 - accuracy: 0.76 - ETA: 0s - loss: 0.5513 - accuracy: 0.75 - ETA: 0s - loss: 0.5486 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5486 - accuracy: 0.7615 - val_loss: 0.5939 - val_accuracy: 0.7313\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.87 - ETA: 0s - loss: 0.4893 - accuracy: 0.80 - ETA: 0s - loss: 0.5063 - accuracy: 0.79 - ETA: 0s - loss: 0.5223 - accuracy: 0.79 - ETA: 0s - loss: 0.5084 - accuracy: 0.79 - ETA: 0s - loss: 0.5109 - accuracy: 0.79 - ETA: 0s - loss: 0.4973 - accuracy: 0.80 - ETA: 0s - loss: 0.4988 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5006 - accuracy: 0.8029 - val_loss: 0.6013 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.5262 - accuracy: 0.85 - ETA: 0s - loss: 0.4784 - accuracy: 0.85 - ETA: 0s - loss: 0.4742 - accuracy: 0.84 - ETA: 0s - loss: 0.4725 - accuracy: 0.83 - ETA: 0s - loss: 0.4704 - accuracy: 0.83 - ETA: 0s - loss: 0.4640 - accuracy: 0.84 - ETA: 0s - loss: 0.4782 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4856 - accuracy: 0.8346 - val_loss: 0.5717 - val_accuracy: 0.7358\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.81 - ETA: 0s - loss: 0.4912 - accuracy: 0.84 - ETA: 0s - loss: 0.4812 - accuracy: 0.84 - ETA: 0s - loss: 0.4688 - accuracy: 0.85 - ETA: 0s - loss: 0.4738 - accuracy: 0.85 - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - ETA: 0s - loss: 0.4884 - accuracy: 0.84 - ETA: 0s - loss: 0.4863 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8365 - val_loss: 0.6777 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.78 - ETA: 0s - loss: 0.4443 - accuracy: 0.84 - ETA: 0s - loss: 0.4624 - accuracy: 0.83 - ETA: 0s - loss: 0.4642 - accuracy: 0.83 - ETA: 0s - loss: 0.4579 - accuracy: 0.83 - ETA: 0s - loss: 0.4548 - accuracy: 0.83 - ETA: 0s - loss: 0.4483 - accuracy: 0.84 - ETA: 0s - loss: 0.4514 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4582 - accuracy: 0.8395 - val_loss: 0.6178 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4547 - accuracy: 0.87 - ETA: 0s - loss: 0.3782 - accuracy: 0.87 - ETA: 0s - loss: 0.4025 - accuracy: 0.86 - ETA: 0s - loss: 0.4354 - accuracy: 0.84 - ETA: 0s - loss: 0.4419 - accuracy: 0.85 - ETA: 0s - loss: 0.4580 - accuracy: 0.84 - ETA: 0s - loss: 0.4580 - accuracy: 0.84 - ETA: 0s - loss: 0.4648 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4620 - accuracy: 0.8492 - val_loss: 0.5804 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5319 - accuracy: 0.81 - ETA: 0s - loss: 0.4094 - accuracy: 0.85 - ETA: 0s - loss: 0.4025 - accuracy: 0.86 - ETA: 0s - loss: 0.4043 - accuracy: 0.86 - ETA: 0s - loss: 0.4144 - accuracy: 0.86 - ETA: 0s - loss: 0.4124 - accuracy: 0.86 - ETA: 0s - loss: 0.4192 - accuracy: 0.86 - ETA: 0s - loss: 0.4293 - accuracy: 0.85 - ETA: 0s - loss: 0.4300 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4310 - accuracy: 0.8563 - val_loss: 0.7328 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.87 - ETA: 0s - loss: 0.4068 - accuracy: 0.85 - ETA: 0s - loss: 0.4185 - accuracy: 0.85 - ETA: 0s - loss: 0.3874 - accuracy: 0.87 - ETA: 0s - loss: 0.4010 - accuracy: 0.87 - ETA: 0s - loss: 0.3845 - accuracy: 0.87 - ETA: 0s - loss: 0.3923 - accuracy: 0.87 - ETA: 0s - loss: 0.4117 - accuracy: 0.87 - ETA: 0s - loss: 0.4184 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4168 - accuracy: 0.8649 - val_loss: 0.5507 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.90 - ETA: 0s - loss: 0.4213 - accuracy: 0.85 - ETA: 0s - loss: 0.4266 - accuracy: 0.86 - ETA: 0s - loss: 0.4165 - accuracy: 0.86 - ETA: 0s - loss: 0.4060 - accuracy: 0.86 - ETA: 0s - loss: 0.4187 - accuracy: 0.86 - ETA: 0s - loss: 0.4188 - accuracy: 0.86 - ETA: 0s - loss: 0.4297 - accuracy: 0.86 - ETA: 0s - loss: 0.4336 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4340 - accuracy: 0.8611 - val_loss: 0.7713 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.84 - ETA: 0s - loss: 0.4299 - accuracy: 0.86 - ETA: 0s - loss: 0.4290 - accuracy: 0.86 - ETA: 0s - loss: 0.4187 - accuracy: 0.87 - ETA: 0s - loss: 0.4297 - accuracy: 0.87 - ETA: 0s - loss: 0.4403 - accuracy: 0.86 - ETA: 0s - loss: 0.4826 - accuracy: 0.84 - ETA: 0s - loss: 0.4981 - accuracy: 0.83 - ETA: 0s - loss: 0.4971 - accuracy: 0.83 - 0s 6ms/step - loss: 0.4973 - accuracy: 0.8324 - val_loss: 0.5803 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5685 - accuracy: 0.78 - ETA: 0s - loss: 0.4981 - accuracy: 0.84 - ETA: 0s - loss: 0.4761 - accuracy: 0.84 - ETA: 0s - loss: 0.4862 - accuracy: 0.84 - ETA: 0s - loss: 0.5045 - accuracy: 0.83 - ETA: 0s - loss: 0.4808 - accuracy: 0.84 - ETA: 0s - loss: 0.4718 - accuracy: 0.84 - ETA: 0s - loss: 0.4592 - accuracy: 0.85 - ETA: 0s - loss: 0.4636 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4648 - accuracy: 0.8488 - val_loss: 0.6655 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 1.00 - ETA: 0s - loss: 0.2937 - accuracy: 0.91 - ETA: 0s - loss: 0.3312 - accuracy: 0.90 - ETA: 0s - loss: 0.4621 - accuracy: 0.87 - ETA: 0s - loss: 0.4874 - accuracy: 0.85 - ETA: 0s - loss: 0.4789 - accuracy: 0.85 - ETA: 0s - loss: 0.4760 - accuracy: 0.85 - ETA: 0s - loss: 0.4781 - accuracy: 0.84 - ETA: 0s - loss: 0.4724 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4757 - accuracy: 0.8492 - val_loss: 0.7940 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.87 - ETA: 0s - loss: 0.4502 - accuracy: 0.87 - ETA: 0s - loss: 0.4341 - accuracy: 0.87 - ETA: 0s - loss: 0.4381 - accuracy: 0.87 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - ETA: 0s - loss: 0.4300 - accuracy: 0.86 - ETA: 0s - loss: 0.4294 - accuracy: 0.86 - ETA: 0s - loss: 0.4291 - accuracy: 0.85 - ETA: 0s - loss: 0.4243 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4266 - accuracy: 0.8600 - val_loss: 0.7694 - val_accuracy: 0.7239\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.84 - ETA: 0s - loss: 0.3910 - accuracy: 0.86 - ETA: 0s - loss: 0.3976 - accuracy: 0.86 - ETA: 0s - loss: 0.3861 - accuracy: 0.86 - ETA: 0s - loss: 0.3992 - accuracy: 0.86 - ETA: 0s - loss: 0.4076 - accuracy: 0.85 - ETA: 0s - loss: 0.4074 - accuracy: 0.85 - ETA: 0s - loss: 0.4052 - accuracy: 0.85 - ETA: 0s - loss: 0.4174 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4178 - accuracy: 0.8537 - val_loss: 0.9518 - val_accuracy: 0.7209\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.84 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - ETA: 0s - loss: 0.5103 - accuracy: 0.81 - ETA: 0s - loss: 0.4979 - accuracy: 0.82 - ETA: 0s - loss: 0.4688 - accuracy: 0.83 - ETA: 0s - loss: 0.4484 - accuracy: 0.85 - ETA: 0s - loss: 0.5578 - accuracy: 0.83 - ETA: 0s - loss: 0.5924 - accuracy: 0.81 - ETA: 0s - loss: 0.6039 - accuracy: 0.80 - 0s 6ms/step - loss: 0.6039 - accuracy: 0.8040 - val_loss: 0.5783 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.71 - ETA: 0s - loss: 0.6310 - accuracy: 0.72 - ETA: 0s - loss: 0.5882 - accuracy: 0.77 - ETA: 0s - loss: 0.5694 - accuracy: 0.79 - ETA: 0s - loss: 0.5483 - accuracy: 0.81 - ETA: 0s - loss: 0.5469 - accuracy: 0.81 - ETA: 0s - loss: 0.5753 - accuracy: 0.81 - ETA: 0s - loss: 0.5809 - accuracy: 0.80 - ETA: 0s - loss: 0.5808 - accuracy: 0.80 - 1s 6ms/step - loss: 0.5760 - accuracy: 0.8081 - val_loss: 1.1905 - val_accuracy: 0.7567\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.87 - ETA: 0s - loss: 0.9875 - accuracy: 0.76 - ETA: 0s - loss: 0.8513 - accuracy: 0.70 - ETA: 0s - loss: 0.8079 - accuracy: 0.66 - ETA: 0s - loss: 0.7770 - accuracy: 0.58 - ETA: 0s - loss: 0.7538 - accuracy: 0.57 - ETA: 0s - loss: 0.7514 - accuracy: 0.58 - ETA: 0s - loss: 0.7431 - accuracy: 0.59 - ETA: 0s - loss: 0.7317 - accuracy: 0.61 - 0s 6ms/step - loss: 0.7311 - accuracy: 0.6144 - val_loss: 0.6732 - val_accuracy: 0.7030\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.59 - ETA: 0s - loss: 0.6926 - accuracy: 0.67 - ETA: 0s - loss: 0.6789 - accuracy: 0.68 - ETA: 0s - loss: 0.6817 - accuracy: 0.69 - ETA: 0s - loss: 0.6812 - accuracy: 0.69 - ETA: 0s - loss: 0.6827 - accuracy: 0.68 - ETA: 0s - loss: 0.6842 - accuracy: 0.67 - ETA: 0s - loss: 0.6875 - accuracy: 0.64 - 0s 5ms/step - loss: 0.6838 - accuracy: 0.6096 - val_loss: 0.7319 - val_accuracy: 0.3045\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7176 - accuracy: 0.43 - ETA: 0s - loss: 0.7776 - accuracy: 0.55 - ETA: 0s - loss: 0.7379 - accuracy: 0.60 - ETA: 0s - loss: 0.7203 - accuracy: 0.64 - ETA: 0s - loss: 0.7065 - accuracy: 0.66 - ETA: 0s - loss: 0.6918 - accuracy: 0.68 - ETA: 0s - loss: 0.6798 - accuracy: 0.69 - ETA: 0s - loss: 0.6687 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6608 - accuracy: 0.7230 - val_loss: 2.2398 - val_accuracy: 0.7418\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.78 - ETA: 0s - loss: 0.7182 - accuracy: 0.79 - ETA: 0s - loss: 1.1435 - accuracy: 0.79 - ETA: 0s - loss: 0.9846 - accuracy: 0.79 - ETA: 0s - loss: 0.8964 - accuracy: 0.78 - ETA: 0s - loss: 0.8424 - accuracy: 0.78 - ETA: 0s - loss: 0.8168 - accuracy: 0.78 - ETA: 0s - loss: 0.8005 - accuracy: 0.77 - 0s 5ms/step - loss: 0.7879 - accuracy: 0.7663 - val_loss: 0.7220 - val_accuracy: 0.7194\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.65 - ETA: 0s - loss: 0.6526 - accuracy: 0.53 - ETA: 0s - loss: 0.6423 - accuracy: 0.61 - ETA: 0s - loss: 0.6915 - accuracy: 0.67 - ETA: 0s - loss: 0.6889 - accuracy: 0.69 - ETA: 0s - loss: 0.6767 - accuracy: 0.71 - ETA: 0s - loss: 0.6729 - accuracy: 0.72 - ETA: 0s - loss: 0.6784 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6738 - accuracy: 0.7361 - val_loss: 0.6457 - val_accuracy: 0.7552\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.75 - ETA: 0s - loss: 0.6424 - accuracy: 0.78 - ETA: 0s - loss: 0.6446 - accuracy: 0.78 - ETA: 0s - loss: 0.6494 - accuracy: 0.78 - ETA: 0s - loss: 0.6472 - accuracy: 0.77 - ETA: 0s - loss: 0.6415 - accuracy: 0.76 - ETA: 0s - loss: 0.6389 - accuracy: 0.76 - ETA: 0s - loss: 0.7970 - accuracy: 0.75 - 0s 5ms/step - loss: 0.8163 - accuracy: 0.7533 - val_loss: 0.7907 - val_accuracy: 0.7090\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.84 - ETA: 0s - loss: 1.2307 - accuracy: 0.67 - ETA: 0s - loss: 0.9583 - accuracy: 0.58 - ETA: 0s - loss: 0.8681 - accuracy: 0.62 - ETA: 0s - loss: 0.8255 - accuracy: 0.63 - ETA: 0s - loss: 0.8021 - accuracy: 0.61 - ETA: 0s - loss: 0.7862 - accuracy: 0.58 - ETA: 0s - loss: 0.7747 - accuracy: 0.55 - 0s 5ms/step - loss: 0.7683 - accuracy: 0.5308 - val_loss: 0.7122 - val_accuracy: 0.3045\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.31 - ETA: 0s - loss: 0.7093 - accuracy: 0.33 - ETA: 0s - loss: 0.6997 - accuracy: 0.31 - ETA: 0s - loss: 0.6930 - accuracy: 0.33 - ETA: 0s - loss: 0.6852 - accuracy: 0.43 - ETA: 0s - loss: 0.6922 - accuracy: 0.47 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6962 - accuracy: 0.46 - ETA: 0s - loss: 0.6936 - accuracy: 0.44 - 0s 6ms/step - loss: 0.6936 - accuracy: 0.4431 - val_loss: 0.7059 - val_accuracy: 0.3045\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7011 - accuracy: 0.31 - ETA: 0s - loss: 0.6884 - accuracy: 0.51 - ETA: 0s - loss: 0.6955 - accuracy: 0.59 - ETA: 0s - loss: 0.6939 - accuracy: 0.61 - ETA: 0s - loss: 0.6945 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6905 - accuracy: 0.56 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5394 - val_loss: 0.7045 - val_accuracy: 0.3045\n",
      "Epoch 27/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.28 - ETA: 0s - loss: 0.7054 - accuracy: 0.32 - ETA: 0s - loss: 0.7080 - accuracy: 0.33 - ETA: 0s - loss: 0.6983 - accuracy: 0.31 - ETA: 0s - loss: 0.7008 - accuracy: 0.31 - ETA: 0s - loss: 0.6958 - accuracy: 0.33 - ETA: 0s - loss: 0.6955 - accuracy: 0.39 - ETA: 0s - loss: 0.6961 - accuracy: 0.42 - ETA: 0s - loss: 0.6937 - accuracy: 0.4251Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.4244 - val_loss: 0.6916 - val_accuracy: 0.6955\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b21122f5212f2972207ac939dec43a8f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7487562298774719</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5043691378022364</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 60</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8744 - accuracy: 0.65 - ETA: 0s - loss: 2.1694 - accuracy: 0.58 - ETA: 0s - loss: 1.4675 - accuracy: 0.64 - ETA: 0s - loss: 1.1794 - accuracy: 0.66 - ETA: 0s - loss: 1.0555 - accuracy: 0.67 - ETA: 0s - loss: 0.9846 - accuracy: 0.68 - ETA: 0s - loss: 0.9313 - accuracy: 0.68 - ETA: 0s - loss: 0.8873 - accuracy: 0.69 - 1s 7ms/step - loss: 0.8859 - accuracy: 0.6939 - val_loss: 0.5799 - val_accuracy: 0.7478\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.81 - ETA: 0s - loss: 0.5822 - accuracy: 0.72 - ETA: 0s - loss: 0.5932 - accuracy: 0.74 - ETA: 0s - loss: 0.5924 - accuracy: 0.75 - ETA: 0s - loss: 0.5866 - accuracy: 0.76 - ETA: 0s - loss: 0.5675 - accuracy: 0.77 - ETA: 0s - loss: 0.5726 - accuracy: 0.77 - ETA: 0s - loss: 0.5627 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5627 - accuracy: 0.7760 - val_loss: 0.5893 - val_accuracy: 0.7149\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.81 - ETA: 0s - loss: 0.4697 - accuracy: 0.81 - ETA: 0s - loss: 0.4969 - accuracy: 0.79 - ETA: 0s - loss: 0.4995 - accuracy: 0.79 - ETA: 0s - loss: 0.4954 - accuracy: 0.79 - ETA: 0s - loss: 0.5031 - accuracy: 0.79 - ETA: 0s - loss: 0.4973 - accuracy: 0.79 - ETA: 0s - loss: 0.5014 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5039 - accuracy: 0.7954 - val_loss: 0.6069 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.87 - ETA: 0s - loss: 0.4821 - accuracy: 0.82 - ETA: 0s - loss: 0.4820 - accuracy: 0.80 - ETA: 0s - loss: 0.4812 - accuracy: 0.81 - ETA: 0s - loss: 0.4814 - accuracy: 0.81 - ETA: 0s - loss: 0.4989 - accuracy: 0.81 - ETA: 0s - loss: 0.5020 - accuracy: 0.80 - ETA: 0s - loss: 0.5049 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5046 - accuracy: 0.8126 - val_loss: 0.7921 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.78 - ETA: 0s - loss: 0.4927 - accuracy: 0.80 - ETA: 0s - loss: 0.5068 - accuracy: 0.79 - ETA: 0s - loss: 0.4991 - accuracy: 0.79 - ETA: 0s - loss: 0.4986 - accuracy: 0.80 - ETA: 0s - loss: 0.4970 - accuracy: 0.81 - ETA: 0s - loss: 0.5030 - accuracy: 0.81 - ETA: 0s - loss: 0.5032 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5040 - accuracy: 0.8137 - val_loss: 0.7216 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.84 - ETA: 0s - loss: 0.4977 - accuracy: 0.81 - ETA: 0s - loss: 0.5012 - accuracy: 0.80 - ETA: 0s - loss: 0.5018 - accuracy: 0.81 - ETA: 0s - loss: 0.4870 - accuracy: 0.82 - ETA: 0s - loss: 0.4845 - accuracy: 0.82 - ETA: 0s - loss: 0.4767 - accuracy: 0.82 - ETA: 0s - loss: 0.4822 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4822 - accuracy: 0.8231 - val_loss: 0.6576 - val_accuracy: 0.6881\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.81 - ETA: 0s - loss: 0.4638 - accuracy: 0.79 - ETA: 0s - loss: 0.4298 - accuracy: 0.82 - ETA: 0s - loss: 0.4265 - accuracy: 0.83 - ETA: 0s - loss: 0.4267 - accuracy: 0.83 - ETA: 0s - loss: 0.4373 - accuracy: 0.83 - ETA: 0s - loss: 0.4406 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4370 - accuracy: 0.8317 - val_loss: 0.6466 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.84 - ETA: 0s - loss: 0.3298 - accuracy: 0.88 - ETA: 0s - loss: 0.6469 - accuracy: 0.85 - ETA: 0s - loss: 0.5991 - accuracy: 0.84 - ETA: 0s - loss: 0.6009 - accuracy: 0.83 - ETA: 0s - loss: 0.6038 - accuracy: 0.82 - ETA: 0s - loss: 0.6036 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5924 - accuracy: 0.8208 - val_loss: 0.6374 - val_accuracy: 0.7478\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.78 - ETA: 0s - loss: 0.4961 - accuracy: 0.84 - ETA: 0s - loss: 0.4950 - accuracy: 0.83 - ETA: 0s - loss: 0.5191 - accuracy: 0.82 - ETA: 0s - loss: 0.5203 - accuracy: 0.82 - ETA: 0s - loss: 0.5252 - accuracy: 0.82 - ETA: 0s - loss: 0.5232 - accuracy: 0.82 - ETA: 0s - loss: 0.5245 - accuracy: 0.82 - 0s 6ms/step - loss: 0.5245 - accuracy: 0.8219 - val_loss: 0.7110 - val_accuracy: 0.7522\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.71 - ETA: 0s - loss: 0.5907 - accuracy: 0.79 - ETA: 0s - loss: 0.5325 - accuracy: 0.82 - ETA: 0s - loss: 0.5135 - accuracy: 0.83 - ETA: 0s - loss: 0.5028 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.84 - ETA: 0s - loss: 0.4819 - accuracy: 0.84 - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4918 - accuracy: 0.8410 - val_loss: 0.5639 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.84 - ETA: 0s - loss: 0.4766 - accuracy: 0.85 - ETA: 0s - loss: 0.4545 - accuracy: 0.86 - ETA: 0s - loss: 0.5036 - accuracy: 0.86 - ETA: 0s - loss: 0.5135 - accuracy: 0.85 - ETA: 0s - loss: 0.5183 - accuracy: 0.84 - ETA: 0s - loss: 0.5237 - accuracy: 0.84 - ETA: 0s - loss: 0.5124 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5117 - accuracy: 0.8406 - val_loss: 0.7114 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.75 - ETA: 0s - loss: 0.4579 - accuracy: 0.84 - ETA: 0s - loss: 0.4305 - accuracy: 0.86 - ETA: 0s - loss: 0.4206 - accuracy: 0.87 - ETA: 0s - loss: 0.4255 - accuracy: 0.86 - ETA: 0s - loss: 0.4299 - accuracy: 0.86 - ETA: 0s - loss: 0.4262 - accuracy: 0.86 - ETA: 0s - loss: 0.4226 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4308 - accuracy: 0.8630 - val_loss: 0.7057 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.96 - ETA: 0s - loss: 0.3735 - accuracy: 0.87 - ETA: 0s - loss: 0.3635 - accuracy: 0.88 - ETA: 0s - loss: 0.3765 - accuracy: 0.88 - ETA: 0s - loss: 0.3854 - accuracy: 0.87 - ETA: 0s - loss: 0.4024 - accuracy: 0.86 - ETA: 0s - loss: 0.4004 - accuracy: 0.86 - ETA: 0s - loss: 0.4035 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3988 - accuracy: 0.8671 - val_loss: 1.1249 - val_accuracy: 0.7194\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.87 - ETA: 0s - loss: 0.4271 - accuracy: 0.85 - ETA: 0s - loss: 0.5272 - accuracy: 0.82 - ETA: 0s - loss: 0.5258 - accuracy: 0.82 - ETA: 0s - loss: 0.5047 - accuracy: 0.82 - ETA: 0s - loss: 0.4936 - accuracy: 0.82 - ETA: 0s - loss: 0.4734 - accuracy: 0.83 - ETA: 0s - loss: 0.4608 - accuracy: 0.84 - ETA: 0s - loss: 0.4550 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4555 - accuracy: 0.8443 - val_loss: 0.9289 - val_accuracy: 0.7030\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.81 - ETA: 0s - loss: 0.3751 - accuracy: 0.88 - ETA: 0s - loss: 0.3831 - accuracy: 0.88 - ETA: 0s - loss: 0.3982 - accuracy: 0.87 - ETA: 0s - loss: 0.3801 - accuracy: 0.88 - ETA: 0s - loss: 0.3825 - accuracy: 0.88 - ETA: 0s - loss: 0.3753 - accuracy: 0.88 - ETA: 0s - loss: 0.3826 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8764 - val_loss: 0.7259 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.90 - ETA: 0s - loss: 0.3643 - accuracy: 0.89 - ETA: 0s - loss: 0.4010 - accuracy: 0.88 - ETA: 0s - loss: 0.4254 - accuracy: 0.86 - ETA: 0s - loss: 0.4354 - accuracy: 0.85 - ETA: 0s - loss: 0.4263 - accuracy: 0.85 - ETA: 0s - loss: 0.4291 - accuracy: 0.84 - ETA: 0s - loss: 0.4271 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8391 - val_loss: 0.7691 - val_accuracy: 0.7000\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.90 - ETA: 0s - loss: 0.3371 - accuracy: 0.87 - ETA: 0s - loss: 0.3442 - accuracy: 0.88 - ETA: 0s - loss: 0.3563 - accuracy: 0.86 - ETA: 0s - loss: 0.3660 - accuracy: 0.86 - ETA: 0s - loss: 0.3683 - accuracy: 0.87 - ETA: 0s - loss: 0.3657 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8727 - val_loss: 0.6591 - val_accuracy: 0.7507\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7560 - accuracy: 0.68 - ETA: 0s - loss: 0.4304 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.85 - ETA: 0s - loss: 0.3716 - accuracy: 0.87 - ETA: 0s - loss: 0.3672 - accuracy: 0.88 - ETA: 0s - loss: 0.3680 - accuracy: 0.88 - ETA: 0s - loss: 0.3657 - accuracy: 0.88 - ETA: 0s - loss: 0.3732 - accuracy: 0.88 - ETA: 0s - loss: 0.3694 - accuracy: 0.88 - 0s 6ms/step - loss: 0.3638 - accuracy: 0.8813 - val_loss: 0.8093 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.87 - ETA: 0s - loss: 0.2749 - accuracy: 0.92 - ETA: 0s - loss: 0.3248 - accuracy: 0.91 - ETA: 0s - loss: 0.3227 - accuracy: 0.91 - ETA: 0s - loss: 0.3284 - accuracy: 0.91 - ETA: 0s - loss: 0.3325 - accuracy: 0.90 - ETA: 0s - loss: 0.3441 - accuracy: 0.90 - ETA: 0s - loss: 0.3509 - accuracy: 0.8985Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.8985 - val_loss: 0.6818 - val_accuracy: 0.7358\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8839 - accuracy: 0.59 - ETA: 0s - loss: 2.0281 - accuracy: 0.61 - ETA: 0s - loss: 1.3715 - accuracy: 0.67 - ETA: 0s - loss: 1.0978 - accuracy: 0.70 - ETA: 0s - loss: 0.9742 - accuracy: 0.70 - ETA: 0s - loss: 0.8971 - accuracy: 0.71 - ETA: 0s - loss: 0.8590 - accuracy: 0.70 - ETA: 0s - loss: 0.8226 - accuracy: 0.70 - 1s 7ms/step - loss: 0.8122 - accuracy: 0.7085 - val_loss: 0.5683 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4078 - accuracy: 0.87 - ETA: 0s - loss: 0.5014 - accuracy: 0.76 - ETA: 0s - loss: 0.5403 - accuracy: 0.75 - ETA: 0s - loss: 0.5374 - accuracy: 0.75 - ETA: 0s - loss: 0.5311 - accuracy: 0.75 - ETA: 0s - loss: 0.5541 - accuracy: 0.75 - ETA: 0s - loss: 0.5540 - accuracy: 0.75 - ETA: 0s - loss: 0.5524 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5524 - accuracy: 0.7589 - val_loss: 0.6209 - val_accuracy: 0.6731\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.62 - ETA: 0s - loss: 0.4648 - accuracy: 0.77 - ETA: 0s - loss: 0.4858 - accuracy: 0.79 - ETA: 0s - loss: 0.4964 - accuracy: 0.79 - ETA: 0s - loss: 0.4962 - accuracy: 0.79 - ETA: 0s - loss: 0.4977 - accuracy: 0.79 - ETA: 0s - loss: 0.4940 - accuracy: 0.79 - ETA: 0s - loss: 0.4948 - accuracy: 0.79 - 0s 5ms/step - loss: 0.4967 - accuracy: 0.7928 - val_loss: 0.6270 - val_accuracy: 0.6985\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.84 - ETA: 0s - loss: 0.4293 - accuracy: 0.83 - ETA: 0s - loss: 0.4575 - accuracy: 0.81 - ETA: 0s - loss: 0.4474 - accuracy: 0.82 - ETA: 0s - loss: 0.4635 - accuracy: 0.83 - ETA: 0s - loss: 0.4697 - accuracy: 0.82 - ETA: 0s - loss: 0.4797 - accuracy: 0.82 - ETA: 0s - loss: 0.4875 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4942 - accuracy: 0.8100 - val_loss: 0.7458 - val_accuracy: 0.5910\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6087 - accuracy: 0.68 - ETA: 0s - loss: 0.4637 - accuracy: 0.84 - ETA: 0s - loss: 0.4682 - accuracy: 0.83 - ETA: 0s - loss: 0.4792 - accuracy: 0.83 - ETA: 0s - loss: 0.4705 - accuracy: 0.82 - ETA: 0s - loss: 0.4661 - accuracy: 0.82 - ETA: 0s - loss: 0.4726 - accuracy: 0.82 - ETA: 0s - loss: 0.4733 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8193 - val_loss: 0.6032 - val_accuracy: 0.7179\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3838 - accuracy: 0.84 - ETA: 0s - loss: 0.3803 - accuracy: 0.85 - ETA: 0s - loss: 0.3923 - accuracy: 0.85 - ETA: 0s - loss: 0.4020 - accuracy: 0.84 - ETA: 0s - loss: 0.4081 - accuracy: 0.85 - ETA: 0s - loss: 0.4238 - accuracy: 0.84 - ETA: 0s - loss: 0.4262 - accuracy: 0.84 - ETA: 0s - loss: 0.4259 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8503 - val_loss: 0.9140 - val_accuracy: 0.6806\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.75 - ETA: 0s - loss: 0.4347 - accuracy: 0.83 - ETA: 0s - loss: 0.4109 - accuracy: 0.84 - ETA: 0s - loss: 0.4456 - accuracy: 0.84 - ETA: 0s - loss: 0.5197 - accuracy: 0.83 - ETA: 0s - loss: 0.5161 - accuracy: 0.83 - ETA: 0s - loss: 0.5007 - accuracy: 0.83 - ETA: 0s - loss: 0.4919 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4899 - accuracy: 0.8365 - val_loss: 0.6415 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.93 - ETA: 0s - loss: 0.4671 - accuracy: 0.84 - ETA: 0s - loss: 0.4598 - accuracy: 0.84 - ETA: 0s - loss: 0.4317 - accuracy: 0.85 - ETA: 0s - loss: 0.4456 - accuracy: 0.84 - ETA: 0s - loss: 0.4527 - accuracy: 0.84 - ETA: 0s - loss: 0.4397 - accuracy: 0.84 - ETA: 0s - loss: 0.4328 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4330 - accuracy: 0.8485 - val_loss: 1.1119 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8033 - accuracy: 0.84 - ETA: 0s - loss: 0.5033 - accuracy: 0.79 - ETA: 0s - loss: 0.4733 - accuracy: 0.81 - ETA: 0s - loss: 0.4488 - accuracy: 0.83 - ETA: 0s - loss: 0.4321 - accuracy: 0.83 - ETA: 0s - loss: 0.4408 - accuracy: 0.83 - ETA: 0s - loss: 0.4361 - accuracy: 0.83 - ETA: 0s - loss: 0.4332 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8429 - val_loss: 0.6788 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.96 - ETA: 0s - loss: 0.3585 - accuracy: 0.88 - ETA: 0s - loss: 0.3373 - accuracy: 0.89 - ETA: 0s - loss: 0.3400 - accuracy: 0.90 - ETA: 0s - loss: 0.4012 - accuracy: 0.87 - ETA: 0s - loss: 0.4115 - accuracy: 0.86 - ETA: 0s - loss: 0.4206 - accuracy: 0.85 - ETA: 0s - loss: 0.4207 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4148 - accuracy: 0.8615 - val_loss: 0.8000 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3725 - accuracy: 0.87 - ETA: 0s - loss: 0.3376 - accuracy: 0.88 - ETA: 0s - loss: 0.3363 - accuracy: 0.89 - ETA: 0s - loss: 0.3343 - accuracy: 0.89 - ETA: 0s - loss: 0.3450 - accuracy: 0.88 - ETA: 0s - loss: 0.3516 - accuracy: 0.88 - ETA: 0s - loss: 0.3660 - accuracy: 0.88 - ETA: 0s - loss: 0.3835 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4008 - accuracy: 0.8764 - val_loss: 1.0342 - val_accuracy: 0.6836\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.81 - ETA: 0s - loss: 0.4241 - accuracy: 0.88 - ETA: 0s - loss: 0.3842 - accuracy: 0.89 - ETA: 0s - loss: 0.3880 - accuracy: 0.88 - ETA: 0s - loss: 0.3825 - accuracy: 0.88 - ETA: 0s - loss: 0.3738 - accuracy: 0.88 - ETA: 0s - loss: 0.3678 - accuracy: 0.88 - ETA: 0s - loss: 0.3624 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3601 - accuracy: 0.8862 - val_loss: 1.1568 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.96 - ETA: 0s - loss: 0.2736 - accuracy: 0.91 - ETA: 0s - loss: 0.2847 - accuracy: 0.90 - ETA: 0s - loss: 0.2875 - accuracy: 0.90 - ETA: 0s - loss: 0.3125 - accuracy: 0.90 - ETA: 0s - loss: 0.3307 - accuracy: 0.89 - ETA: 0s - loss: 0.3296 - accuracy: 0.90 - ETA: 0s - loss: 0.3411 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3395 - accuracy: 0.8962 - val_loss: 0.8672 - val_accuracy: 0.7090\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.93 - ETA: 0s - loss: 0.3351 - accuracy: 0.89 - ETA: 0s - loss: 0.3109 - accuracy: 0.91 - ETA: 0s - loss: 0.3164 - accuracy: 0.90 - ETA: 0s - loss: 0.3219 - accuracy: 0.90 - ETA: 0s - loss: 0.3252 - accuracy: 0.90 - ETA: 0s - loss: 0.3276 - accuracy: 0.89 - ETA: 0s - loss: 0.3311 - accuracy: 0.89 - ETA: 0s - loss: 0.3285 - accuracy: 0.89 - 0s 6ms/step - loss: 0.3281 - accuracy: 0.8973 - val_loss: 0.8091 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.81 - ETA: 0s - loss: 0.2953 - accuracy: 0.90 - ETA: 0s - loss: 0.2830 - accuracy: 0.91 - ETA: 0s - loss: 0.2719 - accuracy: 0.92 - ETA: 0s - loss: 0.2680 - accuracy: 0.92 - ETA: 0s - loss: 0.2642 - accuracy: 0.92 - ETA: 0s - loss: 0.2840 - accuracy: 0.92 - ETA: 0s - loss: 0.2856 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2809 - accuracy: 0.9227 - val_loss: 0.9722 - val_accuracy: 0.7090\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.87 - ETA: 0s - loss: 0.2705 - accuracy: 0.91 - ETA: 0s - loss: 0.2435 - accuracy: 0.93 - ETA: 0s - loss: 0.2481 - accuracy: 0.93 - ETA: 0s - loss: 0.2612 - accuracy: 0.92 - ETA: 0s - loss: 0.2618 - accuracy: 0.92 - ETA: 0s - loss: 0.2704 - accuracy: 0.92 - ETA: 0s - loss: 0.2706 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2757 - accuracy: 0.9268 - val_loss: 1.0906 - val_accuracy: 0.7343\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.96 - ETA: 0s - loss: 0.3622 - accuracy: 0.90 - ETA: 0s - loss: 0.3412 - accuracy: 0.91 - ETA: 0s - loss: 0.3070 - accuracy: 0.92 - ETA: 0s - loss: 0.2881 - accuracy: 0.92 - ETA: 0s - loss: 0.3057 - accuracy: 0.92 - ETA: 0s - loss: 0.3107 - accuracy: 0.91 - ETA: 0s - loss: 0.3030 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2993 - accuracy: 0.9212 - val_loss: 0.9164 - val_accuracy: 0.7179\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.90 - ETA: 0s - loss: 0.2406 - accuracy: 0.93 - ETA: 0s - loss: 0.2476 - accuracy: 0.92 - ETA: 0s - loss: 0.2392 - accuracy: 0.93 - ETA: 0s - loss: 0.2318 - accuracy: 0.93 - ETA: 0s - loss: 0.2381 - accuracy: 0.93 - ETA: 0s - loss: 0.2511 - accuracy: 0.93 - ETA: 0s - loss: 0.2492 - accuracy: 0.93 - 0s 5ms/step - loss: 0.2544 - accuracy: 0.9291 - val_loss: 0.9046 - val_accuracy: 0.7209\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.90 - ETA: 0s - loss: 0.2423 - accuracy: 0.94 - ETA: 0s - loss: 0.2784 - accuracy: 0.92 - ETA: 0s - loss: 0.2698 - accuracy: 0.92 - ETA: 0s - loss: 0.2517 - accuracy: 0.93 - ETA: 0s - loss: 0.2469 - accuracy: 0.93 - ETA: 0s - loss: 0.2467 - accuracy: 0.93 - ETA: 0s - loss: 0.2654 - accuracy: 0.93 - 0s 5ms/step - loss: 0.2704 - accuracy: 0.9317 - val_loss: 1.2207 - val_accuracy: 0.7284\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.84 - ETA: 0s - loss: 0.3365 - accuracy: 0.91 - ETA: 0s - loss: 0.3147 - accuracy: 0.92 - ETA: 0s - loss: 0.3136 - accuracy: 0.92 - ETA: 0s - loss: 0.3746 - accuracy: 0.90 - ETA: 0s - loss: 0.4051 - accuracy: 0.89 - ETA: 0s - loss: 0.4155 - accuracy: 0.89 - ETA: 0s - loss: 0.4326 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4293 - accuracy: 0.8843 - val_loss: 0.9035 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.93 - ETA: 0s - loss: 0.4978 - accuracy: 0.87 - ETA: 0s - loss: 0.4699 - accuracy: 0.87 - ETA: 0s - loss: 0.4840 - accuracy: 0.86 - ETA: 0s - loss: 0.4777 - accuracy: 0.86 - ETA: 0s - loss: 0.4845 - accuracy: 0.85 - ETA: 0s - loss: 0.4876 - accuracy: 0.85 - ETA: 0s - loss: 0.4786 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4771 - accuracy: 0.8578 - val_loss: 0.7101 - val_accuracy: 0.7209\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.93 - ETA: 0s - loss: 0.4247 - accuracy: 0.87 - ETA: 0s - loss: 0.4244 - accuracy: 0.88 - ETA: 0s - loss: 0.4205 - accuracy: 0.88 - ETA: 0s - loss: 0.4411 - accuracy: 0.87 - ETA: 0s - loss: 0.4260 - accuracy: 0.88 - ETA: 0s - loss: 0.4290 - accuracy: 0.87 - ETA: 0s - loss: 0.4281 - accuracy: 0.87 - 0s 5ms/step - loss: 0.5478 - accuracy: 0.8768 - val_loss: 0.6576 - val_accuracy: 0.7328\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.90 - ETA: 0s - loss: 0.4581 - accuracy: 0.84 - ETA: 0s - loss: 0.4829 - accuracy: 0.83 - ETA: 0s - loss: 0.4584 - accuracy: 0.85 - ETA: 0s - loss: 0.4528 - accuracy: 0.86 - ETA: 0s - loss: 0.4640 - accuracy: 0.85 - ETA: 0s - loss: 0.4917 - accuracy: 0.84 - ETA: 0s - loss: 0.5060 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5059 - accuracy: 0.8219 - val_loss: 0.6619 - val_accuracy: 0.7388\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.90 - ETA: 0s - loss: 0.6319 - accuracy: 0.78 - ETA: 0s - loss: 0.5899 - accuracy: 0.80 - ETA: 0s - loss: 0.5666 - accuracy: 0.81 - ETA: 0s - loss: 0.5623 - accuracy: 0.82 - ETA: 0s - loss: 0.5514 - accuracy: 0.82 - ETA: 0s - loss: 0.5757 - accuracy: 0.82 - ETA: 0s - loss: 0.6231 - accuracy: 0.82 - 0s 6ms/step - loss: 0.6177 - accuracy: 0.8231 - val_loss: 0.6795 - val_accuracy: 0.7642\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.84 - ETA: 0s - loss: 0.4527 - accuracy: 0.86 - ETA: 0s - loss: 0.4691 - accuracy: 0.85 - ETA: 0s - loss: 0.4850 - accuracy: 0.84 - ETA: 0s - loss: 0.4693 - accuracy: 0.85 - ETA: 0s - loss: 0.4659 - accuracy: 0.85 - ETA: 0s - loss: 0.4636 - accuracy: 0.85 - ETA: 0s - loss: 0.4515 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4469 - accuracy: 0.8623 - val_loss: 1.9042 - val_accuracy: 0.7313\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.90 - ETA: 0s - loss: 0.4111 - accuracy: 0.87 - ETA: 0s - loss: 0.3841 - accuracy: 0.88 - ETA: 0s - loss: 0.4005 - accuracy: 0.87 - ETA: 0s - loss: 0.3862 - accuracy: 0.88 - ETA: 0s - loss: 0.3967 - accuracy: 0.87 - ETA: 0s - loss: 0.5504 - accuracy: 0.87 - ETA: 0s - loss: 0.5420 - accuracy: 0.86 - 0s 5ms/step - loss: 0.5403 - accuracy: 0.8656 - val_loss: 0.6115 - val_accuracy: 0.7149\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 1.00 - ETA: 0s - loss: 0.5268 - accuracy: 0.87 - ETA: 0s - loss: 0.4885 - accuracy: 0.85 - ETA: 0s - loss: 0.4570 - accuracy: 0.86 - ETA: 0s - loss: 0.4453 - accuracy: 0.86 - ETA: 0s - loss: 0.4200 - accuracy: 0.86 - ETA: 0s - loss: 0.4126 - accuracy: 0.86 - ETA: 0s - loss: 0.4109 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4087 - accuracy: 0.8682 - val_loss: 0.7692 - val_accuracy: 0.7299\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.93 - ETA: 0s - loss: 0.3657 - accuracy: 0.88 - ETA: 0s - loss: 0.3386 - accuracy: 0.89 - ETA: 0s - loss: 0.3365 - accuracy: 0.89 - ETA: 0s - loss: 0.3099 - accuracy: 0.90 - ETA: 0s - loss: 0.3068 - accuracy: 0.91 - ETA: 0s - loss: 0.3005 - accuracy: 0.91 - ETA: 0s - loss: 0.3060 - accuracy: 0.91 - 0s 5ms/step - loss: 0.3104 - accuracy: 0.9123 - val_loss: 0.7824 - val_accuracy: 0.7015\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 1.00 - ETA: 0s - loss: 0.3199 - accuracy: 0.91 - ETA: 0s - loss: 0.3341 - accuracy: 0.91 - ETA: 0s - loss: 0.3183 - accuracy: 0.91 - ETA: 0s - loss: 0.3151 - accuracy: 0.91 - ETA: 0s - loss: 0.3644 - accuracy: 0.91 - ETA: 0s - loss: 0.3452 - accuracy: 0.91 - ETA: 0s - loss: 0.3388 - accuracy: 0.91 - 0s 5ms/step - loss: 0.3382 - accuracy: 0.9197 - val_loss: 1.2099 - val_accuracy: 0.7269\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.93 - ETA: 0s - loss: 0.3139 - accuracy: 0.90 - ETA: 0s - loss: 0.2997 - accuracy: 0.91 - ETA: 0s - loss: 0.2828 - accuracy: 0.92 - ETA: 0s - loss: 0.2660 - accuracy: 0.93 - ETA: 0s - loss: 0.2711 - accuracy: 0.92 - ETA: 0s - loss: 0.2679 - accuracy: 0.92 - ETA: 0s - loss: 0.2669 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2620 - accuracy: 0.9306 - val_loss: 1.9221 - val_accuracy: 0.6970\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.96 - ETA: 0s - loss: 0.2273 - accuracy: 0.94 - ETA: 0s - loss: 0.2616 - accuracy: 0.93 - ETA: 0s - loss: 0.3029 - accuracy: 0.93 - ETA: 0s - loss: 0.2819 - accuracy: 0.93 - ETA: 0s - loss: 0.2755 - accuracy: 0.93 - ETA: 0s - loss: 0.2903 - accuracy: 0.92 - ETA: 0s - loss: 0.2899 - accuracy: 0.92 - 0s 5ms/step - loss: 0.2899 - accuracy: 0.9283 - val_loss: 1.3751 - val_accuracy: 0.7254\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.96 - ETA: 0s - loss: 0.1994 - accuracy: 0.96 - ETA: 0s - loss: 0.2260 - accuracy: 0.94 - ETA: 0s - loss: 0.2286 - accuracy: 0.94 - ETA: 0s - loss: 0.2318 - accuracy: 0.94 - ETA: 0s - loss: 0.2273 - accuracy: 0.94 - ETA: 0s - loss: 0.2301 - accuracy: 0.94 - ETA: 0s - loss: 0.2318 - accuracy: 0.94 - 0s 5ms/step - loss: 0.2325 - accuracy: 0.9481 - val_loss: 1.4915 - val_accuracy: 0.7373\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 1.00 - ETA: 0s - loss: 0.2985 - accuracy: 0.93 - ETA: 0s - loss: 0.2330 - accuracy: 0.95 - ETA: 0s - loss: 0.2273 - accuracy: 0.95 - ETA: 0s - loss: 0.2202 - accuracy: 0.95 - ETA: 0s - loss: 0.2290 - accuracy: 0.94 - ETA: 0s - loss: 0.2311 - accuracy: 0.94 - ETA: 0s - loss: 0.2484 - accuracy: 0.94 - 0s 5ms/step - loss: 0.2547 - accuracy: 0.9399 - val_loss: 1.6518 - val_accuracy: 0.7284\n",
      "Epoch 34/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.1722 - accuracy: 0.96 - ETA: 0s - loss: 0.2660 - accuracy: 0.93 - ETA: 0s - loss: 0.3010 - accuracy: 0.93 - ETA: 0s - loss: 0.3202 - accuracy: 0.92 - ETA: 0s - loss: 0.2977 - accuracy: 0.92 - ETA: 0s - loss: 0.2849 - accuracy: 0.93 - ETA: 0s - loss: 0.2743 - accuracy: 0.93 - ETA: 0s - loss: 0.2796 - accuracy: 0.9317Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2784 - accuracy: 0.9324 - val_loss: 2.1081 - val_accuracy: 0.7269\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.68 - ETA: 0s - loss: 1.3922 - accuracy: 0.68 - ETA: 0s - loss: 1.0464 - accuracy: 0.69 - ETA: 0s - loss: 0.9103 - accuracy: 0.70 - ETA: 0s - loss: 0.8386 - accuracy: 0.70 - ETA: 0s - loss: 0.7953 - accuracy: 0.71 - ETA: 0s - loss: 0.7624 - accuracy: 0.71 - 1s 6ms/step - loss: 0.7457 - accuracy: 0.7163 - val_loss: 0.5850 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.84 - ETA: 0s - loss: 0.5435 - accuracy: 0.78 - ETA: 0s - loss: 0.5369 - accuracy: 0.78 - ETA: 0s - loss: 0.5317 - accuracy: 0.77 - ETA: 0s - loss: 0.5350 - accuracy: 0.77 - ETA: 0s - loss: 0.5358 - accuracy: 0.77 - ETA: 0s - loss: 0.5363 - accuracy: 0.77 - ETA: 0s - loss: 0.5411 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5412 - accuracy: 0.7671 - val_loss: 0.5586 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.81 - ETA: 0s - loss: 0.4496 - accuracy: 0.80 - ETA: 0s - loss: 0.5095 - accuracy: 0.78 - ETA: 0s - loss: 0.5165 - accuracy: 0.78 - ETA: 0s - loss: 0.5252 - accuracy: 0.77 - ETA: 0s - loss: 0.5273 - accuracy: 0.77 - ETA: 0s - loss: 0.5195 - accuracy: 0.78 - ETA: 0s - loss: 0.5305 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5305 - accuracy: 0.7891 - val_loss: 0.7856 - val_accuracy: 0.6761\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4882 - accuracy: 0.78 - ETA: 0s - loss: 0.4580 - accuracy: 0.79 - ETA: 0s - loss: 0.4575 - accuracy: 0.81 - ETA: 0s - loss: 0.4609 - accuracy: 0.82 - ETA: 0s - loss: 0.4690 - accuracy: 0.81 - ETA: 0s - loss: 0.4955 - accuracy: 0.81 - ETA: 0s - loss: 0.4920 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4975 - accuracy: 0.8078 - val_loss: 0.5730 - val_accuracy: 0.6866\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.90 - ETA: 0s - loss: 0.5596 - accuracy: 0.72 - ETA: 0s - loss: 0.5262 - accuracy: 0.73 - ETA: 0s - loss: 0.5105 - accuracy: 0.75 - ETA: 0s - loss: 0.5150 - accuracy: 0.77 - ETA: 0s - loss: 0.5247 - accuracy: 0.77 - ETA: 0s - loss: 0.5197 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5111 - accuracy: 0.7865 - val_loss: 0.6810 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.90 - ETA: 0s - loss: 0.3924 - accuracy: 0.87 - ETA: 0s - loss: 0.4137 - accuracy: 0.85 - ETA: 0s - loss: 0.4241 - accuracy: 0.85 - ETA: 0s - loss: 0.4367 - accuracy: 0.84 - ETA: 0s - loss: 0.4929 - accuracy: 0.83 - ETA: 0s - loss: 0.5054 - accuracy: 0.82 - ETA: 0s - loss: 0.5071 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5073 - accuracy: 0.8219 - val_loss: 0.8762 - val_accuracy: 0.6985\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.93 - ETA: 0s - loss: 0.4466 - accuracy: 0.85 - ETA: 0s - loss: 0.4380 - accuracy: 0.84 - ETA: 0s - loss: 0.4351 - accuracy: 0.84 - ETA: 0s - loss: 0.4424 - accuracy: 0.84 - ETA: 0s - loss: 0.4584 - accuracy: 0.83 - ETA: 0s - loss: 0.4691 - accuracy: 0.82 - ETA: 0s - loss: 0.4752 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4726 - accuracy: 0.8234 - val_loss: 0.8098 - val_accuracy: 0.6657\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.71 - ETA: 0s - loss: 0.4679 - accuracy: 0.81 - ETA: 0s - loss: 0.4535 - accuracy: 0.80 - ETA: 0s - loss: 0.4452 - accuracy: 0.81 - ETA: 0s - loss: 0.4590 - accuracy: 0.81 - ETA: 0s - loss: 0.4484 - accuracy: 0.82 - ETA: 0s - loss: 0.4446 - accuracy: 0.82 - ETA: 0s - loss: 0.4457 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4357 - accuracy: 0.8298 - val_loss: 0.7311 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.84 - ETA: 0s - loss: 0.4216 - accuracy: 0.82 - ETA: 0s - loss: 0.4182 - accuracy: 0.83 - ETA: 0s - loss: 0.4214 - accuracy: 0.84 - ETA: 0s - loss: 0.4189 - accuracy: 0.83 - ETA: 0s - loss: 0.4215 - accuracy: 0.83 - ETA: 0s - loss: 0.4221 - accuracy: 0.83 - ETA: 0s - loss: 0.4263 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4431 - accuracy: 0.8395 - val_loss: 1.0240 - val_accuracy: 0.6896\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.87 - ETA: 0s - loss: 0.5450 - accuracy: 0.76 - ETA: 0s - loss: 0.4893 - accuracy: 0.79 - ETA: 0s - loss: 0.4601 - accuracy: 0.81 - ETA: 0s - loss: 0.4362 - accuracy: 0.83 - ETA: 0s - loss: 0.4255 - accuracy: 0.83 - ETA: 0s - loss: 0.4197 - accuracy: 0.83 - ETA: 0s - loss: 0.4279 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4364 - accuracy: 0.8350 - val_loss: 0.5630 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3677 - accuracy: 0.90 - ETA: 0s - loss: 0.5107 - accuracy: 0.78 - ETA: 0s - loss: 0.4714 - accuracy: 0.81 - ETA: 0s - loss: 0.5143 - accuracy: 0.79 - ETA: 0s - loss: 0.5024 - accuracy: 0.79 - ETA: 0s - loss: 0.4814 - accuracy: 0.81 - ETA: 0s - loss: 0.4756 - accuracy: 0.82 - ETA: 0s - loss: 0.4780 - accuracy: 0.81 - ETA: 0s - loss: 0.5058 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5058 - accuracy: 0.8115 - val_loss: 0.9417 - val_accuracy: 0.6925\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.90 - ETA: 0s - loss: 0.4529 - accuracy: 0.83 - ETA: 0s - loss: 0.4516 - accuracy: 0.84 - ETA: 0s - loss: 0.4586 - accuracy: 0.84 - ETA: 0s - loss: 0.5039 - accuracy: 0.83 - ETA: 0s - loss: 0.4996 - accuracy: 0.83 - ETA: 0s - loss: 0.4889 - accuracy: 0.83 - ETA: 0s - loss: 0.4812 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4815 - accuracy: 0.8395 - val_loss: 1.1325 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.84 - ETA: 0s - loss: 0.3851 - accuracy: 0.88 - ETA: 0s - loss: 0.4385 - accuracy: 0.86 - ETA: 0s - loss: 0.4328 - accuracy: 0.87 - ETA: 0s - loss: 0.4402 - accuracy: 0.86 - ETA: 0s - loss: 0.4554 - accuracy: 0.86 - ETA: 0s - loss: 0.4615 - accuracy: 0.85 - ETA: 0s - loss: 0.4668 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4659 - accuracy: 0.8537 - val_loss: 0.9041 - val_accuracy: 0.7358\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.90 - ETA: 0s - loss: 0.4515 - accuracy: 0.86 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - ETA: 0s - loss: 0.4338 - accuracy: 0.86 - ETA: 0s - loss: 0.4247 - accuracy: 0.86 - ETA: 0s - loss: 0.4264 - accuracy: 0.87 - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.4304 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4368 - accuracy: 0.8671 - val_loss: 0.7397 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.93 - ETA: 0s - loss: 0.4202 - accuracy: 0.87 - ETA: 0s - loss: 0.4245 - accuracy: 0.86 - ETA: 0s - loss: 0.4472 - accuracy: 0.86 - ETA: 0s - loss: 0.4740 - accuracy: 0.85 - ETA: 0s - loss: 0.4759 - accuracy: 0.85 - ETA: 0s - loss: 0.4823 - accuracy: 0.85 - ETA: 0s - loss: 0.4678 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4731 - accuracy: 0.8593 - val_loss: 0.7977 - val_accuracy: 0.7269\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4426 - accuracy: 0.81 - ETA: 0s - loss: 0.4626 - accuracy: 0.85 - ETA: 0s - loss: 0.4544 - accuracy: 0.84 - ETA: 0s - loss: 0.4552 - accuracy: 0.84 - ETA: 0s - loss: 0.4506 - accuracy: 0.84 - ETA: 0s - loss: 0.4482 - accuracy: 0.85 - ETA: 0s - loss: 0.4511 - accuracy: 0.85 - ETA: 0s - loss: 0.4694 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4905 - accuracy: 0.8384 - val_loss: 0.6646 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7917 - accuracy: 0.68 - ETA: 0s - loss: 0.5924 - accuracy: 0.78 - ETA: 0s - loss: 0.5939 - accuracy: 0.79 - ETA: 0s - loss: 0.5771 - accuracy: 0.79 - ETA: 0s - loss: 0.5768 - accuracy: 0.79 - ETA: 0s - loss: 0.5559 - accuracy: 0.80 - ETA: 0s - loss: 0.5523 - accuracy: 0.81 - ETA: 0s - loss: 0.5515 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5520 - accuracy: 0.8108 - val_loss: 0.7795 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.71 - ETA: 0s - loss: 0.5043 - accuracy: 0.84 - ETA: 0s - loss: 0.5052 - accuracy: 0.84 - ETA: 0s - loss: 0.5194 - accuracy: 0.82 - ETA: 0s - loss: 0.5063 - accuracy: 0.83 - ETA: 0s - loss: 0.5022 - accuracy: 0.83 - ETA: 0s - loss: 0.4968 - accuracy: 0.84 - ETA: 0s - loss: 0.4910 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4900 - accuracy: 0.8429 - val_loss: 0.6825 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3960 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.86 - ETA: 0s - loss: 0.4533 - accuracy: 0.83 - ETA: 0s - loss: 0.4863 - accuracy: 0.82 - ETA: 0s - loss: 0.4932 - accuracy: 0.82 - ETA: 0s - loss: 0.4874 - accuracy: 0.82 - ETA: 0s - loss: 0.4822 - accuracy: 0.83 - ETA: 0s - loss: 0.4753 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8376 - val_loss: 0.5903 - val_accuracy: 0.7463\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.81 - ETA: 0s - loss: 0.4679 - accuracy: 0.85 - ETA: 0s - loss: 0.4714 - accuracy: 0.85 - ETA: 0s - loss: 0.4662 - accuracy: 0.85 - ETA: 0s - loss: 0.4590 - accuracy: 0.85 - ETA: 0s - loss: 0.4559 - accuracy: 0.86 - ETA: 0s - loss: 0.4590 - accuracy: 0.85 - ETA: 0s - loss: 0.4569 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4583 - accuracy: 0.8596 - val_loss: 0.7158 - val_accuracy: 0.6507\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.78 - ETA: 0s - loss: 0.5285 - accuracy: 0.80 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - ETA: 0s - loss: 0.5876 - accuracy: 0.78 - ETA: 0s - loss: 0.5726 - accuracy: 0.79 - ETA: 0s - loss: 0.5681 - accuracy: 0.79 - ETA: 0s - loss: 0.5671 - accuracy: 0.80 - ETA: 0s - loss: 0.5687 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5686 - accuracy: 0.8014 - val_loss: 0.6663 - val_accuracy: 0.7299\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.84 - ETA: 0s - loss: 0.6287 - accuracy: 0.76 - ETA: 0s - loss: 0.5914 - accuracy: 0.79 - ETA: 0s - loss: 0.6142 - accuracy: 0.77 - ETA: 0s - loss: 0.6141 - accuracy: 0.77 - ETA: 0s - loss: 0.6078 - accuracy: 0.77 - ETA: 0s - loss: 0.6116 - accuracy: 0.77 - ETA: 0s - loss: 0.6090 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6038 - accuracy: 0.7772 - val_loss: 0.7916 - val_accuracy: 0.7313\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6493 - accuracy: 0.71 - ETA: 0s - loss: 0.5927 - accuracy: 0.76 - ETA: 0s - loss: 0.5727 - accuracy: 0.77 - ETA: 0s - loss: 0.5607 - accuracy: 0.79 - ETA: 0s - loss: 0.5665 - accuracy: 0.79 - ETA: 0s - loss: 0.5683 - accuracy: 0.79 - ETA: 0s - loss: 0.5663 - accuracy: 0.79 - ETA: 0s - loss: 0.5662 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5801 - accuracy: 0.7928 - val_loss: 0.9540 - val_accuracy: 0.7463\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.78 - ETA: 0s - loss: 0.5673 - accuracy: 0.78 - ETA: 0s - loss: 0.5693 - accuracy: 0.78 - ETA: 0s - loss: 0.5700 - accuracy: 0.78 - ETA: 0s - loss: 0.5576 - accuracy: 0.79 - ETA: 0s - loss: 0.5554 - accuracy: 0.80 - ETA: 0s - loss: 0.5538 - accuracy: 0.80 - ETA: 0s - loss: 0.5682 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5767 - accuracy: 0.7973 - val_loss: 0.7218 - val_accuracy: 0.7373\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.81 - ETA: 0s - loss: 0.5560 - accuracy: 0.80 - ETA: 0s - loss: 0.5617 - accuracy: 0.80 - ETA: 0s - loss: 0.5457 - accuracy: 0.81 - ETA: 0s - loss: 0.5531 - accuracy: 0.80 - ETA: 0s - loss: 0.5513 - accuracy: 0.81 - ETA: 0s - loss: 0.5578 - accuracy: 0.80 - ETA: 0s - loss: 0.5722 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5747 - accuracy: 0.7928 - val_loss: 0.6374 - val_accuracy: 0.7418\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.75 - ETA: 0s - loss: 0.6037 - accuracy: 0.77 - ETA: 0s - loss: 0.5873 - accuracy: 0.78 - ETA: 0s - loss: 0.6009 - accuracy: 0.77 - ETA: 0s - loss: 0.6859 - accuracy: 0.76 - ETA: 0s - loss: 0.6685 - accuracy: 0.76 - ETA: 0s - loss: 0.6639 - accuracy: 0.76 - ETA: 0s - loss: 0.6556 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6476 - accuracy: 0.7730 - val_loss: 0.6603 - val_accuracy: 0.7418\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.84 - ETA: 0s - loss: 0.5741 - accuracy: 0.79 - ETA: 0s - loss: 0.5885 - accuracy: 0.79 - ETA: 0s - loss: 0.6109 - accuracy: 0.78 - ETA: 0s - loss: 0.6108 - accuracy: 0.79 - ETA: 0s - loss: 0.6091 - accuracy: 0.79 - ETA: 0s - loss: 0.6039 - accuracy: 0.79 - ETA: 0s - loss: 0.5925 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5906 - accuracy: 0.8022 - val_loss: 0.8291 - val_accuracy: 0.7552\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9080 - accuracy: 0.84 - ETA: 0s - loss: 0.7735 - accuracy: 0.78 - ETA: 0s - loss: 0.7330 - accuracy: 0.72 - ETA: 0s - loss: 0.8977 - accuracy: 0.69 - ETA: 0s - loss: 0.8333 - accuracy: 0.70 - ETA: 0s - loss: 0.7877 - accuracy: 0.70 - ETA: 0s - loss: 0.7613 - accuracy: 0.70 - ETA: 0s - loss: 0.7352 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7269 - accuracy: 0.7126 - val_loss: 0.6472 - val_accuracy: 0.6716\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8314 - accuracy: 0.59 - ETA: 0s - loss: 0.6407 - accuracy: 0.70 - ETA: 0s - loss: 0.6309 - accuracy: 0.70 - ETA: 0s - loss: 0.6269 - accuracy: 0.70 - ETA: 0s - loss: 0.6181 - accuracy: 0.70 - ETA: 0s - loss: 0.6213 - accuracy: 0.70 - ETA: 0s - loss: 0.6191 - accuracy: 0.70 - ETA: 0s - loss: 0.6178 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6193 - accuracy: 0.7010 - val_loss: 0.6395 - val_accuracy: 0.6687\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.78 - ETA: 0s - loss: 0.6342 - accuracy: 0.71 - ETA: 0s - loss: 0.6165 - accuracy: 0.71 - ETA: 0s - loss: 0.6128 - accuracy: 0.70 - ETA: 0s - loss: 0.6183 - accuracy: 0.70 - ETA: 0s - loss: 0.6166 - accuracy: 0.70 - ETA: 0s - loss: 0.6171 - accuracy: 0.71 - ETA: 0s - loss: 0.6148 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6153 - accuracy: 0.7133 - val_loss: 0.6564 - val_accuracy: 0.6701\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7335 - accuracy: 0.59 - ETA: 0s - loss: 0.6290 - accuracy: 0.71 - ETA: 0s - loss: 0.6242 - accuracy: 0.71 - ETA: 0s - loss: 0.6254 - accuracy: 0.71 - ETA: 0s - loss: 0.6263 - accuracy: 0.71 - ETA: 0s - loss: 0.6189 - accuracy: 0.71 - ETA: 0s - loss: 0.6169 - accuracy: 0.72 - ETA: 0s - loss: 0.6157 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6125 - accuracy: 0.7245 - val_loss: 0.6844 - val_accuracy: 0.6642\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7153 - accuracy: 0.56 - ETA: 0s - loss: 0.6210 - accuracy: 0.71 - ETA: 0s - loss: 0.6153 - accuracy: 0.70 - ETA: 0s - loss: 0.6156 - accuracy: 0.71 - ETA: 0s - loss: 0.6138 - accuracy: 0.71 - ETA: 0s - loss: 0.6005 - accuracy: 0.71 - ETA: 0s - loss: 0.5954 - accuracy: 0.72 - ETA: 0s - loss: 0.5947 - accuracy: 0.71 - 0s 5ms/step - loss: 0.5947 - accuracy: 0.7148 - val_loss: 0.6601 - val_accuracy: 0.6254\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.71 - ETA: 0s - loss: 0.5949 - accuracy: 0.67 - ETA: 0s - loss: 0.6072 - accuracy: 0.66 - ETA: 0s - loss: 0.6201 - accuracy: 0.67 - ETA: 0s - loss: 0.6116 - accuracy: 0.68 - ETA: 0s - loss: 0.6028 - accuracy: 0.69 - ETA: 0s - loss: 0.6003 - accuracy: 0.69 - ETA: 0s - loss: 0.5981 - accuracy: 0.69 - 0s 5ms/step - loss: 0.5975 - accuracy: 0.6924 - val_loss: 0.6269 - val_accuracy: 0.6388\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.81 - ETA: 0s - loss: 0.5829 - accuracy: 0.71 - ETA: 0s - loss: 0.5833 - accuracy: 0.71 - ETA: 0s - loss: 0.5971 - accuracy: 0.70 - ETA: 0s - loss: 0.5934 - accuracy: 0.70 - ETA: 0s - loss: 0.6009 - accuracy: 0.69 - ETA: 0s - loss: 0.6047 - accuracy: 0.69 - ETA: 0s - loss: 0.6026 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6018 - accuracy: 0.6932 - val_loss: 0.6608 - val_accuracy: 0.6134\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.59 - ETA: 0s - loss: 0.6183 - accuracy: 0.67 - ETA: 0s - loss: 0.6104 - accuracy: 0.69 - ETA: 0s - loss: 0.6097 - accuracy: 0.69 - ETA: 0s - loss: 0.5999 - accuracy: 0.69 - ETA: 0s - loss: 0.6061 - accuracy: 0.69 - ETA: 0s - loss: 0.6042 - accuracy: 0.69 - ETA: 0s - loss: 0.6031 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6080 - accuracy: 0.6898 - val_loss: 0.6572 - val_accuracy: 0.6343\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.65 - ETA: 0s - loss: 0.6641 - accuracy: 0.69 - ETA: 0s - loss: 0.6460 - accuracy: 0.68 - ETA: 0s - loss: 0.6327 - accuracy: 0.69 - ETA: 0s - loss: 0.6291 - accuracy: 0.69 - ETA: 0s - loss: 0.6256 - accuracy: 0.69 - ETA: 0s - loss: 0.6184 - accuracy: 0.69 - ETA: 0s - loss: 0.6157 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6169 - accuracy: 0.6980 - val_loss: 0.7036 - val_accuracy: 0.6164\n",
      "Epoch 37/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5422 - accuracy: 0.71 - ETA: 0s - loss: 0.6259 - accuracy: 0.65 - ETA: 0s - loss: 0.6280 - accuracy: 0.66 - ETA: 0s - loss: 0.6178 - accuracy: 0.66 - ETA: 0s - loss: 0.6180 - accuracy: 0.66 - ETA: 0s - loss: 0.6180 - accuracy: 0.67 - ETA: 0s - loss: 0.6056 - accuracy: 0.68 - ETA: 0s - loss: 0.6023 - accuracy: 0.6883Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6066 - accuracy: 0.6835 - val_loss: 0.6578 - val_accuracy: 0.6418\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 633fdb3274f9e2856fff92c3bd18adeb</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7572139302889506</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.27168383612623515</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8838 - accuracy: 0.65 - ETA: 0s - loss: 1.0710 - accuracy: 0.64 - ETA: 0s - loss: 0.9223 - accuracy: 0.66 - ETA: 0s - loss: 0.8647 - accuracy: 0.67 - ETA: 0s - loss: 0.8397 - accuracy: 0.68 - ETA: 0s - loss: 0.8088 - accuracy: 0.69 - ETA: 0s - loss: 0.7928 - accuracy: 0.69 - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6920 - val_loss: 0.6372 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7938 - accuracy: 0.62 - ETA: 0s - loss: 0.6800 - accuracy: 0.72 - ETA: 0s - loss: 0.6877 - accuracy: 0.71 - ETA: 0s - loss: 0.6909 - accuracy: 0.71 - ETA: 0s - loss: 0.6972 - accuracy: 0.70 - ETA: 0s - loss: 0.6992 - accuracy: 0.70 - ETA: 0s - loss: 0.6999 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6989 - accuracy: 0.6991 - val_loss: 0.6798 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.65 - ETA: 0s - loss: 0.6966 - accuracy: 0.69 - ETA: 0s - loss: 0.6845 - accuracy: 0.71 - ETA: 0s - loss: 0.6933 - accuracy: 0.69 - ETA: 0s - loss: 0.6981 - accuracy: 0.69 - ETA: 0s - loss: 0.6932 - accuracy: 0.64 - ETA: 0s - loss: 0.6930 - accuracy: 0.65 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.6555 - val_loss: 0.6896 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7223 - accuracy: 0.65 - ETA: 0s - loss: 0.6783 - accuracy: 0.72 - ETA: 0s - loss: 0.7004 - accuracy: 0.68 - ETA: 0s - loss: 0.6916 - accuracy: 0.56 - ETA: 0s - loss: 0.7345 - accuracy: 0.58 - ETA: 0s - loss: 0.7420 - accuracy: 0.60 - ETA: 0s - loss: 0.7596 - accuracy: 0.59 - 0s 4ms/step - loss: 0.7579 - accuracy: 0.5931 - val_loss: 0.6957 - val_accuracy: 0.3045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7207 - accuracy: 0.34 - ETA: 0s - loss: 0.6870 - accuracy: 0.29 - ETA: 0s - loss: 0.6961 - accuracy: 0.30 - ETA: 0s - loss: 0.6899 - accuracy: 0.29 - ETA: 0s - loss: 0.6862 - accuracy: 0.35 - ETA: 0s - loss: 0.6901 - accuracy: 0.41 - ETA: 0s - loss: 0.6936 - accuracy: 0.44 - 0s 4ms/step - loss: 0.6936 - accuracy: 0.4498 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.75 - ETA: 0s - loss: 0.6907 - accuracy: 0.39 - ETA: 0s - loss: 0.6928 - accuracy: 0.34 - ETA: 0s - loss: 0.6877 - accuracy: 0.36 - ETA: 0s - loss: 0.6925 - accuracy: 0.42 - ETA: 0s - loss: 0.6920 - accuracy: 0.39 - ETA: 0s - loss: 0.6939 - accuracy: 0.39 - 0s 4ms/step - loss: 0.6933 - accuracy: 0.3912 - val_loss: 0.6942 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.28 - ETA: 0s - loss: 0.6947 - accuracy: 0.34 - ETA: 0s - loss: 0.6881 - accuracy: 0.38 - ETA: 0s - loss: 0.6954 - accuracy: 0.43 - ETA: 0s - loss: 0.6929 - accuracy: 0.39 - ETA: 0s - loss: 0.6924 - accuracy: 0.41 - ETA: 0s - loss: 0.6944 - accuracy: 0.42 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.4173 - val_loss: 0.6962 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6408 - accuracy: 0.21 - ETA: 0s - loss: 0.6906 - accuracy: 0.29 - ETA: 0s - loss: 0.7015 - accuracy: 0.31 - ETA: 0s - loss: 0.6942 - accuracy: 0.30 - ETA: 0s - loss: 0.6901 - accuracy: 0.33 - ETA: 0s - loss: 0.6908 - accuracy: 0.40 - ETA: 0s - loss: 0.6933 - accuracy: 0.44 - 0s 4ms/step - loss: 0.6934 - accuracy: 0.4528 - val_loss: 0.6910 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7637 - accuracy: 0.59 - ETA: 0s - loss: 0.6833 - accuracy: 0.52 - ETA: 0s - loss: 0.6817 - accuracy: 0.62 - ETA: 0s - loss: 0.6837 - accuracy: 0.65 - ETA: 0s - loss: 0.6901 - accuracy: 0.65 - ETA: 0s - loss: 0.6923 - accuracy: 0.62 - ETA: 0s - loss: 0.6931 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5696 - val_loss: 0.6977 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.25 - ETA: 0s - loss: 0.6936 - accuracy: 0.30 - ETA: 0s - loss: 0.7021 - accuracy: 0.31 - ETA: 0s - loss: 0.7025 - accuracy: 0.31 - ETA: 0s - loss: 0.6952 - accuracy: 0.30 - ETA: 0s - loss: 0.6921 - accuracy: 0.31 - ETA: 0s - loss: 0.6940 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.3893 - val_loss: 0.6904 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.7010 - accuracy: 0.68 - ETA: 0s - loss: 0.7013 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.43 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6957 - accuracy: 0.55 - ETA: 0s - loss: 0.6961 - accuracy: 0.53 - ETA: 0s - loss: 0.6933 - accuracy: 0.5000Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5088 - val_loss: 0.6914 - val_accuracy: 0.6955\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8610 - accuracy: 0.65 - ETA: 0s - loss: 1.1721 - accuracy: 0.64 - ETA: 0s - loss: 0.9401 - accuracy: 0.69 - ETA: 0s - loss: 0.8809 - accuracy: 0.69 - ETA: 0s - loss: 0.8514 - accuracy: 0.68 - ETA: 0s - loss: 0.8278 - accuracy: 0.68 - ETA: 0s - loss: 0.8077 - accuracy: 0.68 - 0s 6ms/step - loss: 0.8089 - accuracy: 0.6865 - val_loss: 0.6444 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.81 - ETA: 0s - loss: 0.7016 - accuracy: 0.70 - ETA: 0s - loss: 0.7192 - accuracy: 0.67 - ETA: 0s - loss: 0.7068 - accuracy: 0.69 - ETA: 0s - loss: 0.7030 - accuracy: 0.69 - ETA: 0s - loss: 0.6991 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6973 - accuracy: 0.6991 - val_loss: 0.6796 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7257 - accuracy: 0.65 - ETA: 0s - loss: 0.6880 - accuracy: 0.70 - ETA: 0s - loss: 0.6911 - accuracy: 0.70 - ETA: 0s - loss: 0.6968 - accuracy: 0.69 - ETA: 0s - loss: 0.6946 - accuracy: 0.69 - ETA: 0s - loss: 0.6898 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6936 - accuracy: 0.6991 - val_loss: 0.6882 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7871 - accuracy: 0.56 - ETA: 0s - loss: 0.6946 - accuracy: 0.69 - ETA: 0s - loss: 0.6943 - accuracy: 0.69 - ETA: 0s - loss: 0.6991 - accuracy: 0.64 - ETA: 0s - loss: 0.6982 - accuracy: 0.56 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5274 - val_loss: 0.6903 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.71 - ETA: 0s - loss: 0.7132 - accuracy: 0.59 - ETA: 0s - loss: 0.7073 - accuracy: 0.46 - ETA: 0s - loss: 0.6994 - accuracy: 0.40 - ETA: 0s - loss: 0.6984 - accuracy: 0.38 - ETA: 0s - loss: 0.6938 - accuracy: 0.41 - 0s 4ms/step - loss: 0.6934 - accuracy: 0.4371 - val_loss: 0.6886 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7439 - accuracy: 0.62 - ETA: 0s - loss: 0.6970 - accuracy: 0.69 - ETA: 0s - loss: 0.7041 - accuracy: 0.66 - ETA: 0s - loss: 0.6987 - accuracy: 0.53 - ETA: 0s - loss: 0.6970 - accuracy: 0.48 - ETA: 0s - loss: 0.6965 - accuracy: 0.49 - ETA: 0s - loss: 0.6932 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5043 - val_loss: 0.6899 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.71 - ETA: 0s - loss: 0.6970 - accuracy: 0.69 - ETA: 0s - loss: 0.6946 - accuracy: 0.69 - ETA: 0s - loss: 0.6943 - accuracy: 0.69 - ETA: 0s - loss: 0.6917 - accuracy: 0.70 - ETA: 0s - loss: 0.6893 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6933 - accuracy: 0.6991 - val_loss: 0.6906 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.68 - ETA: 0s - loss: 0.6921 - accuracy: 0.70 - ETA: 0s - loss: 0.6950 - accuracy: 0.57 - ETA: 0s - loss: 0.6931 - accuracy: 0.47 - ETA: 0s - loss: 0.6949 - accuracy: 0.43 - ETA: 0s - loss: 0.6912 - accuracy: 0.40 - ETA: 0s - loss: 0.6932 - accuracy: 0.44 - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4461 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.71 - ETA: 0s - loss: 0.6854 - accuracy: 0.71 - ETA: 0s - loss: 0.6892 - accuracy: 0.70 - ETA: 0s - loss: 0.6952 - accuracy: 0.67 - ETA: 0s - loss: 0.6954 - accuracy: 0.58 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6932 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4920 - val_loss: 0.6933 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.31 - ETA: 0s - loss: 0.6882 - accuracy: 0.60 - ETA: 0s - loss: 0.7009 - accuracy: 0.59 - ETA: 0s - loss: 0.6961 - accuracy: 0.49 - ETA: 0s - loss: 0.6936 - accuracy: 0.44 - ETA: 0s - loss: 0.6900 - accuracy: 0.46 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5185 - val_loss: 0.6913 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6801 - accuracy: 0.71 - ETA: 0s - loss: 0.6721 - accuracy: 0.73 - ETA: 0s - loss: 0.6808 - accuracy: 0.71 - ETA: 0s - loss: 0.6845 - accuracy: 0.71 - ETA: 0s - loss: 0.6869 - accuracy: 0.70 - ETA: 0s - loss: 0.6882 - accuracy: 0.70 - ETA: 0s - loss: 0.6932 - accuracy: 0.6695Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.6532 - val_loss: 0.6995 - val_accuracy: 0.3045\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.75 - ETA: 0s - loss: 1.6621 - accuracy: 0.66 - ETA: 0s - loss: 1.1647 - accuracy: 0.68 - ETA: 0s - loss: 0.9922 - accuracy: 0.70 - ETA: 0s - loss: 0.8906 - accuracy: 0.70 - ETA: 0s - loss: 0.8331 - accuracy: 0.71 - ETA: 0s - loss: 0.7983 - accuracy: 0.71 - 1s 6ms/step - loss: 0.7940 - accuracy: 0.7122 - val_loss: 0.5960 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5792 - accuracy: 0.78 - ETA: 0s - loss: 0.6274 - accuracy: 0.76 - ETA: 0s - loss: 0.5923 - accuracy: 0.75 - ETA: 0s - loss: 0.5766 - accuracy: 0.76 - ETA: 0s - loss: 0.5763 - accuracy: 0.75 - ETA: 0s - loss: 0.5691 - accuracy: 0.76 - ETA: 0s - loss: 0.5722 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5720 - accuracy: 0.7596 - val_loss: 0.5751 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.87 - ETA: 0s - loss: 0.5281 - accuracy: 0.79 - ETA: 0s - loss: 0.5138 - accuracy: 0.80 - ETA: 0s - loss: 0.5259 - accuracy: 0.80 - ETA: 0s - loss: 0.5255 - accuracy: 0.80 - ETA: 0s - loss: 0.5297 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5365 - accuracy: 0.7951 - val_loss: 0.5861 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.84 - ETA: 0s - loss: 0.5216 - accuracy: 0.80 - ETA: 0s - loss: 0.4895 - accuracy: 0.81 - ETA: 0s - loss: 0.5007 - accuracy: 0.82 - ETA: 0s - loss: 0.5001 - accuracy: 0.82 - ETA: 0s - loss: 0.5098 - accuracy: 0.81 - ETA: 0s - loss: 0.5293 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5293 - accuracy: 0.8063 - val_loss: 0.6831 - val_accuracy: 0.7313\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.81 - ETA: 0s - loss: 0.5055 - accuracy: 0.83 - ETA: 0s - loss: 0.5176 - accuracy: 0.82 - ETA: 0s - loss: 0.5054 - accuracy: 0.82 - ETA: 0s - loss: 0.5154 - accuracy: 0.83 - ETA: 0s - loss: 0.5540 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5525 - accuracy: 0.8175 - val_loss: 0.6406 - val_accuracy: 0.7015\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.81 - ETA: 0s - loss: 0.5003 - accuracy: 0.80 - ETA: 0s - loss: 0.5019 - accuracy: 0.81 - ETA: 0s - loss: 0.4924 - accuracy: 0.82 - ETA: 0s - loss: 0.4982 - accuracy: 0.82 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - ETA: 0s - loss: 0.4942 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4942 - accuracy: 0.8287 - val_loss: 1.4944 - val_accuracy: 0.6388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3932 - accuracy: 0.84 - ETA: 0s - loss: 0.5353 - accuracy: 0.84 - ETA: 0s - loss: 0.5166 - accuracy: 0.84 - ETA: 0s - loss: 0.5306 - accuracy: 0.83 - ETA: 0s - loss: 0.5246 - accuracy: 0.82 - ETA: 0s - loss: 0.5738 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5757 - accuracy: 0.8160 - val_loss: 0.5576 - val_accuracy: 0.7448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.87 - ETA: 0s - loss: 0.5423 - accuracy: 0.82 - ETA: 0s - loss: 0.5536 - accuracy: 0.81 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - ETA: 0s - loss: 0.5330 - accuracy: 0.82 - ETA: 0s - loss: 0.5359 - accuracy: 0.81 - ETA: 0s - loss: 0.5282 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5277 - accuracy: 0.8216 - val_loss: 0.6258 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.90 - ETA: 0s - loss: 0.4844 - accuracy: 0.83 - ETA: 0s - loss: 0.4842 - accuracy: 0.83 - ETA: 0s - loss: 0.4874 - accuracy: 0.83 - ETA: 0s - loss: 0.4960 - accuracy: 0.81 - ETA: 0s - loss: 0.4910 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4920 - accuracy: 0.8197 - val_loss: 0.6629 - val_accuracy: 0.7313\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.93 - ETA: 0s - loss: 0.4375 - accuracy: 0.85 - ETA: 0s - loss: 0.4119 - accuracy: 0.86 - ETA: 0s - loss: 0.4306 - accuracy: 0.84 - ETA: 0s - loss: 0.4315 - accuracy: 0.85 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - ETA: 0s - loss: 0.4484 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8447 - val_loss: 0.9830 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.71 - ETA: 0s - loss: 0.4263 - accuracy: 0.86 - ETA: 0s - loss: 0.4511 - accuracy: 0.86 - ETA: 0s - loss: 0.4378 - accuracy: 0.86 - ETA: 0s - loss: 0.4262 - accuracy: 0.86 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - ETA: 0s - loss: 0.4569 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4657 - accuracy: 0.8529 - val_loss: 0.6862 - val_accuracy: 0.7239\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.78 - ETA: 0s - loss: 0.4470 - accuracy: 0.84 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - ETA: 0s - loss: 0.4265 - accuracy: 0.86 - ETA: 0s - loss: 0.4291 - accuracy: 0.86 - ETA: 0s - loss: 0.4350 - accuracy: 0.86 - ETA: 0s - loss: 0.4359 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4340 - accuracy: 0.8611 - val_loss: 0.8355 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.87 - ETA: 0s - loss: 0.4403 - accuracy: 0.87 - ETA: 0s - loss: 0.4142 - accuracy: 0.87 - ETA: 0s - loss: 0.4002 - accuracy: 0.88 - ETA: 0s - loss: 0.4128 - accuracy: 0.87 - ETA: 0s - loss: 0.4187 - accuracy: 0.87 - ETA: 0s - loss: 0.4230 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4286 - accuracy: 0.8638 - val_loss: 0.6760 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 1.00 - ETA: 0s - loss: 0.3821 - accuracy: 0.87 - ETA: 0s - loss: 0.3906 - accuracy: 0.87 - ETA: 0s - loss: 0.3890 - accuracy: 0.88 - ETA: 0s - loss: 0.3916 - accuracy: 0.87 - ETA: 0s - loss: 0.4059 - accuracy: 0.87 - ETA: 0s - loss: 0.4180 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8679 - val_loss: 0.9400 - val_accuracy: 0.7149\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.75 - ETA: 0s - loss: 0.3944 - accuracy: 0.89 - ETA: 0s - loss: 0.3933 - accuracy: 0.88 - ETA: 0s - loss: 0.3845 - accuracy: 0.89 - ETA: 0s - loss: 0.4086 - accuracy: 0.88 - ETA: 0s - loss: 0.4315 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4428 - accuracy: 0.8649 - val_loss: 0.9610 - val_accuracy: 0.7104\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.78 - ETA: 0s - loss: 0.4601 - accuracy: 0.83 - ETA: 0s - loss: 0.4677 - accuracy: 0.84 - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4423 - accuracy: 0.85 - ETA: 0s - loss: 0.4308 - accuracy: 0.86 - ETA: 0s - loss: 0.4247 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4214 - accuracy: 0.8686 - val_loss: 0.7396 - val_accuracy: 0.7209\n",
      "Epoch 17/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4516 - accuracy: 0.87 - ETA: 0s - loss: 0.4426 - accuracy: 0.86 - ETA: 0s - loss: 0.4150 - accuracy: 0.87 - ETA: 0s - loss: 0.4049 - accuracy: 0.87 - ETA: 0s - loss: 0.4049 - accuracy: 0.88 - ETA: 0s - loss: 0.3965 - accuracy: 0.88 - ETA: 0s - loss: 0.3877 - accuracy: 0.8864Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3928 - accuracy: 0.8839 - val_loss: 0.8205 - val_accuracy: 0.7433\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 62fd94047cb0663669a3a8de55683fad</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7119402885437012</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.2874347149211607</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.65 - ETA: 0s - loss: 1.3524 - accuracy: 0.63 - ETA: 0s - loss: 1.0044 - accuracy: 0.67 - ETA: 0s - loss: 0.8697 - accuracy: 0.69 - ETA: 0s - loss: 0.8198 - accuracy: 0.70 - ETA: 0s - loss: 0.7768 - accuracy: 0.70 - 0s 6ms/step - loss: 0.7768 - accuracy: 0.7088 - val_loss: 0.5767 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.71 - ETA: 0s - loss: 0.6233 - accuracy: 0.69 - ETA: 0s - loss: 0.5934 - accuracy: 0.73 - ETA: 0s - loss: 0.5793 - accuracy: 0.74 - ETA: 0s - loss: 0.5747 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5820 - accuracy: 0.7488 - val_loss: 0.5964 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5042 - accuracy: 0.78 - ETA: 0s - loss: 0.5292 - accuracy: 0.76 - ETA: 0s - loss: 0.5134 - accuracy: 0.79 - ETA: 0s - loss: 0.5116 - accuracy: 0.79 - ETA: 0s - loss: 0.5181 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5134 - accuracy: 0.7962 - val_loss: 0.6565 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.90 - ETA: 0s - loss: 0.4565 - accuracy: 0.83 - ETA: 0s - loss: 0.4571 - accuracy: 0.82 - ETA: 0s - loss: 0.4689 - accuracy: 0.82 - ETA: 0s - loss: 0.4809 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4893 - accuracy: 0.8201 - val_loss: 0.5629 - val_accuracy: 0.7269\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8517 - accuracy: 0.65 - ETA: 0s - loss: 0.5076 - accuracy: 0.81 - ETA: 0s - loss: 0.4840 - accuracy: 0.82 - ETA: 0s - loss: 0.4807 - accuracy: 0.82 - ETA: 0s - loss: 0.4676 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4701 - accuracy: 0.8309 - val_loss: 0.5760 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.93 - ETA: 0s - loss: 0.4006 - accuracy: 0.86 - ETA: 0s - loss: 0.4043 - accuracy: 0.86 - ETA: 0s - loss: 0.4000 - accuracy: 0.86 - ETA: 0s - loss: 0.4105 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8537 - val_loss: 0.8129 - val_accuracy: 0.6896\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.87 - ETA: 0s - loss: 0.3949 - accuracy: 0.84 - ETA: 0s - loss: 0.3856 - accuracy: 0.86 - ETA: 0s - loss: 0.3941 - accuracy: 0.87 - ETA: 0s - loss: 0.4140 - accuracy: 0.86 - ETA: 0s - loss: 0.4288 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8541 - val_loss: 0.7622 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.87 - ETA: 0s - loss: 0.4168 - accuracy: 0.84 - ETA: 0s - loss: 0.5341 - accuracy: 0.81 - ETA: 0s - loss: 0.4980 - accuracy: 0.83 - ETA: 0s - loss: 0.4986 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4875 - accuracy: 0.8373 - val_loss: 0.5646 - val_accuracy: 0.7522\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.87 - ETA: 0s - loss: 0.4040 - accuracy: 0.89 - ETA: 0s - loss: 0.3997 - accuracy: 0.89 - ETA: 0s - loss: 0.4359 - accuracy: 0.87 - ETA: 0s - loss: 0.4334 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8686 - val_loss: 0.8580 - val_accuracy: 0.6896\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.90 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3296 - accuracy: 0.89 - ETA: 0s - loss: 0.3657 - accuracy: 0.87 - ETA: 0s - loss: 0.3917 - accuracy: 0.86 - ETA: 0s - loss: 0.3979 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3979 - accuracy: 0.8578 - val_loss: 0.7442 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.87 - ETA: 0s - loss: 0.3265 - accuracy: 0.90 - ETA: 0s - loss: 0.3404 - accuracy: 0.89 - ETA: 0s - loss: 0.3730 - accuracy: 0.89 - ETA: 0s - loss: 0.3670 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8847 - val_loss: 0.7933 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5921 - accuracy: 0.81 - ETA: 0s - loss: 0.3389 - accuracy: 0.88 - ETA: 0s - loss: 0.3346 - accuracy: 0.88 - ETA: 0s - loss: 0.3234 - accuracy: 0.89 - ETA: 0s - loss: 0.3246 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8955 - val_loss: 0.7512 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.96 - ETA: 0s - loss: 0.3031 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.90 - ETA: 0s - loss: 0.2984 - accuracy: 0.91 - ETA: 0s - loss: 0.2980 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2933 - accuracy: 0.9141 - val_loss: 0.8669 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.93 - ETA: 0s - loss: 0.4024 - accuracy: 0.89 - ETA: 0s - loss: 0.3908 - accuracy: 0.89 - ETA: 0s - loss: 0.3668 - accuracy: 0.89 - ETA: 0s - loss: 0.3610 - accuracy: 0.89 - ETA: 0s - loss: 0.3457 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3400 - accuracy: 0.9029 - val_loss: 1.0059 - val_accuracy: 0.7254\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.96 - ETA: 0s - loss: 0.2916 - accuracy: 0.91 - ETA: 0s - loss: 0.2933 - accuracy: 0.91 - ETA: 0s - loss: 0.3269 - accuracy: 0.90 - ETA: 0s - loss: 0.3200 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3267 - accuracy: 0.9089 - val_loss: 1.4167 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.93 - ETA: 0s - loss: 0.4248 - accuracy: 0.88 - ETA: 0s - loss: 0.3643 - accuracy: 0.90 - ETA: 0s - loss: 0.3617 - accuracy: 0.90 - ETA: 0s - loss: 0.3479 - accuracy: 0.90 - ETA: 0s - loss: 0.5871 - accuracy: 0.90 - 0s 4ms/step - loss: 0.5754 - accuracy: 0.9029 - val_loss: 0.7522 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.93 - ETA: 0s - loss: 0.4548 - accuracy: 0.86 - ETA: 0s - loss: 0.4151 - accuracy: 0.87 - ETA: 0s - loss: 0.3901 - accuracy: 0.88 - ETA: 0s - loss: 0.3956 - accuracy: 0.88 - ETA: 0s - loss: 0.3940 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3934 - accuracy: 0.8817 - val_loss: 0.7127 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.84 - ETA: 0s - loss: 0.4111 - accuracy: 0.88 - ETA: 0s - loss: 0.4013 - accuracy: 0.88 - ETA: 0s - loss: 0.4138 - accuracy: 0.87 - ETA: 0s - loss: 0.4032 - accuracy: 0.88 - ETA: 0s - loss: 0.3897 - accuracy: 0.8903Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3897 - accuracy: 0.8903 - val_loss: 0.8379 - val_accuracy: 0.7164\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.78 - ETA: 0s - loss: 1.1315 - accuracy: 0.65 - ETA: 0s - loss: 0.8964 - accuracy: 0.67 - ETA: 0s - loss: 0.8220 - accuracy: 0.68 - ETA: 0s - loss: 0.7708 - accuracy: 0.70 - ETA: 0s - loss: 0.7361 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7272 - accuracy: 0.7100 - val_loss: 0.5756 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.84 - ETA: 0s - loss: 0.5678 - accuracy: 0.75 - ETA: 0s - loss: 0.5623 - accuracy: 0.76 - ETA: 0s - loss: 0.5485 - accuracy: 0.76 - ETA: 0s - loss: 0.5578 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7600 - val_loss: 0.7579 - val_accuracy: 0.6746\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.81 - ETA: 0s - loss: 0.5303 - accuracy: 0.77 - ETA: 0s - loss: 0.5788 - accuracy: 0.76 - ETA: 0s - loss: 0.5667 - accuracy: 0.77 - ETA: 0s - loss: 0.5543 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5579 - accuracy: 0.7764 - val_loss: 0.7177 - val_accuracy: 0.6269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.71 - ETA: 0s - loss: 0.6215 - accuracy: 0.78 - ETA: 0s - loss: 0.5859 - accuracy: 0.79 - ETA: 0s - loss: 0.5664 - accuracy: 0.79 - ETA: 0s - loss: 0.5553 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5567 - accuracy: 0.7921 - val_loss: 0.7853 - val_accuracy: 0.6284\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.53 - ETA: 0s - loss: 0.5170 - accuracy: 0.76 - ETA: 0s - loss: 0.4844 - accuracy: 0.79 - ETA: 0s - loss: 0.4828 - accuracy: 0.80 - ETA: 0s - loss: 0.4893 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4911 - accuracy: 0.8014 - val_loss: 0.6699 - val_accuracy: 0.7448\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.90 - ETA: 0s - loss: 0.3718 - accuracy: 0.85 - ETA: 0s - loss: 0.4031 - accuracy: 0.84 - ETA: 0s - loss: 0.4236 - accuracy: 0.83 - ETA: 0s - loss: 0.4332 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8339 - val_loss: 0.7005 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.84 - ETA: 0s - loss: 0.3993 - accuracy: 0.85 - ETA: 0s - loss: 0.3885 - accuracy: 0.85 - ETA: 0s - loss: 0.3919 - accuracy: 0.85 - ETA: 0s - loss: 0.5467 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5423 - accuracy: 0.8279 - val_loss: 0.7542 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.84 - ETA: 0s - loss: 0.5374 - accuracy: 0.78 - ETA: 0s - loss: 0.5107 - accuracy: 0.80 - ETA: 0s - loss: 0.4696 - accuracy: 0.81 - ETA: 0s - loss: 0.4612 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8238 - val_loss: 0.7103 - val_accuracy: 0.6836\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.71 - ETA: 0s - loss: 0.3541 - accuracy: 0.87 - ETA: 0s - loss: 0.3709 - accuracy: 0.87 - ETA: 0s - loss: 0.3777 - accuracy: 0.86 - ETA: 0s - loss: 0.3795 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8600 - val_loss: 0.9051 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.90 - ETA: 0s - loss: 0.4013 - accuracy: 0.86 - ETA: 0s - loss: 0.4110 - accuracy: 0.85 - ETA: 0s - loss: 0.4167 - accuracy: 0.84 - ETA: 0s - loss: 0.4431 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8414 - val_loss: 1.1078 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4247 - accuracy: 0.81 - ETA: 0s - loss: 0.3270 - accuracy: 0.89 - ETA: 0s - loss: 0.3382 - accuracy: 0.89 - ETA: 0s - loss: 0.3447 - accuracy: 0.89 - ETA: 0s - loss: 0.3446 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8832 - val_loss: 0.8077 - val_accuracy: 0.7164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.87 - ETA: 0s - loss: 0.3557 - accuracy: 0.87 - ETA: 0s - loss: 0.3570 - accuracy: 0.88 - ETA: 0s - loss: 0.3674 - accuracy: 0.88 - ETA: 0s - loss: 0.3603 - accuracy: 0.88 - ETA: 0s - loss: 0.3558 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3574 - accuracy: 0.8854 - val_loss: 0.9920 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.90 - ETA: 0s - loss: 0.3550 - accuracy: 0.86 - ETA: 0s - loss: 0.3343 - accuracy: 0.88 - ETA: 0s - loss: 0.3166 - accuracy: 0.89 - ETA: 0s - loss: 0.3101 - accuracy: 0.90 - ETA: 0s - loss: 0.3156 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3159 - accuracy: 0.9007 - val_loss: 0.7842 - val_accuracy: 0.7403\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.87 - ETA: 0s - loss: 0.3026 - accuracy: 0.90 - ETA: 0s - loss: 0.4094 - accuracy: 0.90 - ETA: 0s - loss: 0.4123 - accuracy: 0.89 - ETA: 0s - loss: 0.4026 - accuracy: 0.88 - ETA: 0s - loss: 0.4043 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4043 - accuracy: 0.8850 - val_loss: 1.9245 - val_accuracy: 0.7075\n",
      "Epoch 15/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.87 - ETA: 0s - loss: 0.7690 - accuracy: 0.84 - ETA: 0s - loss: 0.6408 - accuracy: 0.86 - ETA: 0s - loss: 0.5605 - accuracy: 0.86 - ETA: 0s - loss: 0.4978 - accuracy: 0.87 - ETA: 0s - loss: 0.4877 - accuracy: 0.8720Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.8731 - val_loss: 1.8800 - val_accuracy: 0.7000\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.81 - ETA: 0s - loss: 1.2754 - accuracy: 0.65 - ETA: 0s - loss: 0.9408 - accuracy: 0.68 - ETA: 0s - loss: 0.8241 - accuracy: 0.70 - ETA: 0s - loss: 0.7831 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7497 - accuracy: 0.7186 - val_loss: 0.6007 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.78 - ETA: 0s - loss: 0.5479 - accuracy: 0.78 - ETA: 0s - loss: 0.5469 - accuracy: 0.78 - ETA: 0s - loss: 0.5476 - accuracy: 0.77 - ETA: 0s - loss: 0.5510 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5568 - accuracy: 0.7686 - val_loss: 0.5751 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.96 - ETA: 0s - loss: 0.4401 - accuracy: 0.83 - ETA: 0s - loss: 0.4754 - accuracy: 0.81 - ETA: 0s - loss: 0.4964 - accuracy: 0.80 - ETA: 0s - loss: 0.4988 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5189 - accuracy: 0.8007 - val_loss: 0.6714 - val_accuracy: 0.6776\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.90 - ETA: 0s - loss: 0.4817 - accuracy: 0.81 - ETA: 0s - loss: 0.5023 - accuracy: 0.80 - ETA: 0s - loss: 0.4987 - accuracy: 0.80 - ETA: 0s - loss: 0.5043 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5052 - accuracy: 0.8033 - val_loss: 0.5892 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.87 - ETA: 0s - loss: 0.4119 - accuracy: 0.85 - ETA: 0s - loss: 0.4359 - accuracy: 0.85 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - ETA: 0s - loss: 0.4613 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4626 - accuracy: 0.8294 - val_loss: 0.7381 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.78 - ETA: 0s - loss: 0.5432 - accuracy: 0.80 - ETA: 0s - loss: 0.5620 - accuracy: 0.78 - ETA: 0s - loss: 0.5306 - accuracy: 0.79 - ETA: 0s - loss: 0.5134 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5094 - accuracy: 0.8007 - val_loss: 0.5791 - val_accuracy: 0.7478\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.84 - ETA: 0s - loss: 0.4354 - accuracy: 0.83 - ETA: 0s - loss: 0.4242 - accuracy: 0.85 - ETA: 0s - loss: 0.4452 - accuracy: 0.84 - ETA: 0s - loss: 0.4464 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4431 - accuracy: 0.8432 - val_loss: 0.7981 - val_accuracy: 0.7015\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.81 - ETA: 0s - loss: 0.4905 - accuracy: 0.84 - ETA: 0s - loss: 0.5521 - accuracy: 0.80 - ETA: 0s - loss: 0.4924 - accuracy: 0.82 - ETA: 0s - loss: 0.4759 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4669 - accuracy: 0.8287 - val_loss: 0.8937 - val_accuracy: 0.6925\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.81 - ETA: 0s - loss: 0.3968 - accuracy: 0.86 - ETA: 0s - loss: 0.3883 - accuracy: 0.86 - ETA: 0s - loss: 0.3789 - accuracy: 0.86 - ETA: 0s - loss: 0.3639 - accuracy: 0.86 - 0s 3ms/step - loss: 0.5743 - accuracy: 0.8649 - val_loss: 0.7400 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.78 - ETA: 0s - loss: 0.4956 - accuracy: 0.81 - ETA: 0s - loss: 0.4648 - accuracy: 0.83 - ETA: 0s - loss: 0.4686 - accuracy: 0.83 - ETA: 0s - loss: 0.4523 - accuracy: 0.84 - ETA: 0s - loss: 0.4445 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4447 - accuracy: 0.8447 - val_loss: 0.8464 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.81 - ETA: 0s - loss: 0.3477 - accuracy: 0.88 - ETA: 0s - loss: 0.3424 - accuracy: 0.88 - ETA: 0s - loss: 0.3464 - accuracy: 0.88 - ETA: 0s - loss: 0.3684 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8847 - val_loss: 0.8053 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.93 - ETA: 0s - loss: 0.3358 - accuracy: 0.88 - ETA: 0s - loss: 0.3276 - accuracy: 0.89 - ETA: 0s - loss: 0.3292 - accuracy: 0.89 - ETA: 0s - loss: 0.3300 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8891 - val_loss: 0.8441 - val_accuracy: 0.7179\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.87 - ETA: 0s - loss: 0.3260 - accuracy: 0.88 - ETA: 0s - loss: 0.3166 - accuracy: 0.89 - ETA: 0s - loss: 0.3845 - accuracy: 0.86 - ETA: 0s - loss: 0.4356 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4586 - accuracy: 0.8518 - val_loss: 1.0232 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.78 - ETA: 0s - loss: 0.5087 - accuracy: 0.84 - ETA: 0s - loss: 0.5822 - accuracy: 0.84 - ETA: 0s - loss: 0.5752 - accuracy: 0.83 - ETA: 0s - loss: 0.5655 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5384 - accuracy: 0.8365 - val_loss: 0.6297 - val_accuracy: 0.7418\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.84 - ETA: 0s - loss: 0.3927 - accuracy: 0.86 - ETA: 0s - loss: 0.4235 - accuracy: 0.85 - ETA: 0s - loss: 0.4340 - accuracy: 0.85 - ETA: 0s - loss: 0.4250 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4182 - accuracy: 0.8578 - val_loss: 0.6826 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4892 - accuracy: 0.81 - ETA: 0s - loss: 0.3784 - accuracy: 0.86 - ETA: 0s - loss: 0.3939 - accuracy: 0.85 - ETA: 0s - loss: 0.3753 - accuracy: 0.86 - ETA: 0s - loss: 0.3754 - accuracy: 0.87 - ETA: 0s - loss: 0.3787 - accuracy: 0.8761Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3784 - accuracy: 0.8764 - val_loss: 1.2143 - val_accuracy: 0.7343\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ef39846a0f0e0215fb99ff3142d2dc38</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7482587099075317</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.41275295630066455</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 85</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9857 - accuracy: 0.71 - ETA: 0s - loss: 1.5399 - accuracy: 0.59 - ETA: 0s - loss: 1.1504 - accuracy: 0.57 - ETA: 0s - loss: 1.0057 - accuracy: 0.54 - 0s 4ms/step - loss: 0.9892 - accuracy: 0.5364 - val_loss: 0.6915 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.49 - ETA: 0s - loss: 0.6795 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.64 - 0s 3ms/step - loss: 0.6822 - accuracy: 0.6446 - val_loss: 0.6436 - val_accuracy: 0.7224\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.78 - ETA: 0s - loss: 0.6499 - accuracy: 0.73 - ETA: 0s - loss: 0.6593 - accuracy: 0.71 - ETA: 0s - loss: 0.6625 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6671 - accuracy: 0.7148 - val_loss: 0.6402 - val_accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.68 - ETA: 0s - loss: 0.6651 - accuracy: 0.69 - ETA: 0s - loss: 0.6595 - accuracy: 0.71 - ETA: 0s - loss: 0.6670 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6695 - accuracy: 0.7234 - val_loss: 0.6318 - val_accuracy: 0.7179\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.65 - ETA: 0s - loss: 0.6585 - accuracy: 0.68 - ETA: 0s - loss: 0.6660 - accuracy: 0.68 - ETA: 0s - loss: 0.6626 - accuracy: 0.70 - 0s 3ms/step - loss: 0.6617 - accuracy: 0.7055 - val_loss: 0.6255 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7275 - accuracy: 0.75 - ETA: 0s - loss: 0.6855 - accuracy: 0.72 - ETA: 0s - loss: 0.6987 - accuracy: 0.73 - ETA: 0s - loss: 0.6935 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6902 - accuracy: 0.7249 - val_loss: 0.7454 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5952 - accuracy: 0.65 - ETA: 0s - loss: 0.7501 - accuracy: 0.51 - ETA: 0s - loss: 0.7210 - accuracy: 0.55 - ETA: 0s - loss: 0.7095 - accuracy: 0.57 - 0s 2ms/step - loss: 0.7092 - accuracy: 0.5730 - val_loss: 0.6714 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.65 - ETA: 0s - loss: 0.6949 - accuracy: 0.65 - ETA: 0s - loss: 0.6932 - accuracy: 0.56 - ETA: 0s - loss: 0.6950 - accuracy: 0.51 - 0s 2ms/step - loss: 0.6953 - accuracy: 0.5103 - val_loss: 0.6974 - val_accuracy: 0.3030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.40 - ETA: 0s - loss: 0.7016 - accuracy: 0.37 - ETA: 0s - loss: 0.6973 - accuracy: 0.38 - ETA: 0s - loss: 0.6952 - accuracy: 0.47 - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4838 - val_loss: 0.6794 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.75 - ETA: 0s - loss: 0.6742 - accuracy: 0.72 - ETA: 0s - loss: 0.6873 - accuracy: 0.65 - ETA: 0s - loss: 0.6925 - accuracy: 0.56 - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5580 - val_loss: 0.7037 - val_accuracy: 0.3030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7484 - accuracy: 0.43 - ETA: 0s - loss: 0.6973 - accuracy: 0.32 - ETA: 0s - loss: 0.6923 - accuracy: 0.45 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5409 - val_loss: 0.6856 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.68 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6944 - accuracy: 0.54 - ETA: 0s - loss: 0.6946 - accuracy: 0.46 - 0s 2ms/step - loss: 0.6942 - accuracy: 0.4677 - val_loss: 0.6906 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.31 - ETA: 0s - loss: 0.6911 - accuracy: 0.64 - ETA: 0s - loss: 0.6959 - accuracy: 0.53 - ETA: 0s - loss: 0.6944 - accuracy: 0.48 - 0s 2ms/step - loss: 0.6944 - accuracy: 0.4998 - val_loss: 0.6808 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.75 - ETA: 0s - loss: 0.6933 - accuracy: 0.68 - ETA: 0s - loss: 0.6921 - accuracy: 0.67 - ETA: 0s - loss: 0.6945 - accuracy: 0.57 - 0s 2ms/step - loss: 0.6940 - accuracy: 0.5569 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.31 - ETA: 0s - loss: 0.7016 - accuracy: 0.45 - ETA: 0s - loss: 0.6956 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.57 - 0s 2ms/step - loss: 0.6940 - accuracy: 0.5838 - val_loss: 0.6846 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.7659 - accuracy: 0.59 - ETA: 0s - loss: 0.7010 - accuracy: 0.35 - ETA: 0s - loss: 0.6991 - accuracy: 0.33 - ETA: 0s - loss: 0.6906 - accuracy: 0.4345Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.4681 - val_loss: 0.6780 - val_accuracy: 0.6955\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7433 - accuracy: 0.65 - ETA: 0s - loss: 1.7009 - accuracy: 0.58 - ETA: 0s - loss: 1.2669 - accuracy: 0.55 - ETA: 0s - loss: 1.0881 - accuracy: 0.53 - 0s 4ms/step - loss: 1.0881 - accuracy: 0.5394 - val_loss: 0.6598 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.68 - ETA: 0s - loss: 0.7022 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5651 - val_loss: 0.6471 - val_accuracy: 0.7090\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.81 - ETA: 0s - loss: 0.6943 - accuracy: 0.71 - ETA: 0s - loss: 0.6869 - accuracy: 0.71 - ETA: 0s - loss: 0.7122 - accuracy: 0.70 - 0s 2ms/step - loss: 0.7130 - accuracy: 0.7070 - val_loss: 0.6556 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.65 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.7009 - accuracy: 0.47 - ETA: 0s - loss: 0.7073 - accuracy: 0.49 - 0s 2ms/step - loss: 0.7079 - accuracy: 0.4983 - val_loss: 0.6507 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.62 - ETA: 0s - loss: 0.7106 - accuracy: 0.58 - ETA: 0s - loss: 0.7019 - accuracy: 0.60 - ETA: 0s - loss: 0.6984 - accuracy: 0.57 - 0s 2ms/step - loss: 0.6984 - accuracy: 0.5707 - val_loss: 0.6734 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6832 - accuracy: 0.65 - ETA: 0s - loss: 0.7003 - accuracy: 0.41 - ETA: 0s - loss: 0.6949 - accuracy: 0.52 - ETA: 0s - loss: 0.6957 - accuracy: 0.49 - 0s 2ms/step - loss: 0.6951 - accuracy: 0.4923 - val_loss: 0.6793 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7176 - accuracy: 0.40 - ETA: 0s - loss: 0.6995 - accuracy: 0.37 - ETA: 0s - loss: 0.6970 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.53 - 0s 2ms/step - loss: 0.6949 - accuracy: 0.5383 - val_loss: 0.6777 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.68 - ETA: 0s - loss: 0.7015 - accuracy: 0.58 - ETA: 0s - loss: 0.6992 - accuracy: 0.47 - ETA: 0s - loss: 0.6947 - accuracy: 0.45 - 0s 2ms/step - loss: 0.6947 - accuracy: 0.4573 - val_loss: 0.6768 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6846 - accuracy: 0.62 - ETA: 0s - loss: 0.6880 - accuracy: 0.69 - ETA: 0s - loss: 0.6950 - accuracy: 0.63 - ETA: 0s - loss: 0.6956 - accuracy: 0.55 - 0s 2ms/step - loss: 0.6941 - accuracy: 0.5439 - val_loss: 0.6917 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.34 - ETA: 0s - loss: 0.6897 - accuracy: 0.64 - ETA: 0s - loss: 0.6876 - accuracy: 0.57 - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5819 - val_loss: 0.6923 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.31 - ETA: 0s - loss: 0.7102 - accuracy: 0.37 - ETA: 0s - loss: 0.6984 - accuracy: 0.35 - ETA: 0s - loss: 0.6939 - accuracy: 0.46 - 0s 2ms/step - loss: 0.6939 - accuracy: 0.4606 - val_loss: 0.6683 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7082 - accuracy: 0.68 - ETA: 0s - loss: 0.7103 - accuracy: 0.67 - ETA: 0s - loss: 0.7032 - accuracy: 0.50 - ETA: 0s - loss: 0.6967 - accuracy: 0.5427Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5539 - val_loss: 0.6777 - val_accuracy: 0.6955\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9493 - accuracy: 0.62 - ETA: 0s - loss: 1.6051 - accuracy: 0.62 - ETA: 0s - loss: 1.2346 - accuracy: 0.59 - ETA: 0s - loss: 1.0946 - accuracy: 0.56 - 0s 4ms/step - loss: 1.0614 - accuracy: 0.5677 - val_loss: 0.5996 - val_accuracy: 0.7030\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7598 - accuracy: 0.34 - ETA: 0s - loss: 0.7328 - accuracy: 0.51 - ETA: 0s - loss: 0.7221 - accuracy: 0.53 - 0s 2ms/step - loss: 0.7111 - accuracy: 0.5476 - val_loss: 0.6020 - val_accuracy: 0.7030\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.53 - ETA: 0s - loss: 0.6328 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.55 - 0s 2ms/step - loss: 0.6667 - accuracy: 0.5677 - val_loss: 0.6376 - val_accuracy: 0.6522\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.67 - ETA: 0s - loss: 0.6847 - accuracy: 0.65 - 0s 2ms/step - loss: 0.7403 - accuracy: 0.6409 - val_loss: 0.5874 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.84 - ETA: 0s - loss: 0.7353 - accuracy: 0.62 - ETA: 0s - loss: 0.7169 - accuracy: 0.55 - ETA: 0s - loss: 0.7018 - accuracy: 0.54 - 0s 2ms/step - loss: 0.7005 - accuracy: 0.5457 - val_loss: 0.6360 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.65 - ETA: 0s - loss: 0.7060 - accuracy: 0.47 - ETA: 0s - loss: 0.7048 - accuracy: 0.44 - ETA: 0s - loss: 0.7003 - accuracy: 0.44 - 0s 2ms/step - loss: 0.7090 - accuracy: 0.4457 - val_loss: 0.6849 - val_accuracy: 0.4970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.37 - ETA: 0s - loss: 0.7019 - accuracy: 0.32 - ETA: 0s - loss: 0.6948 - accuracy: 0.42 - ETA: 0s - loss: 0.6998 - accuracy: 0.51 - 0s 2ms/step - loss: 0.6991 - accuracy: 0.5349 - val_loss: 0.6728 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.71 - ETA: 0s - loss: 0.6665 - accuracy: 0.74 - ETA: 0s - loss: 0.6768 - accuracy: 0.73 - ETA: 0s - loss: 0.6926 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6933 - accuracy: 0.7357 - val_loss: 0.6614 - val_accuracy: 0.6940\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.78 - ETA: 0s - loss: 0.7151 - accuracy: 0.71 - ETA: 0s - loss: 0.7020 - accuracy: 0.71 - ETA: 0s - loss: 0.7007 - accuracy: 0.65 - 0s 2ms/step - loss: 0.7004 - accuracy: 0.6278 - val_loss: 0.7149 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7018 - accuracy: 0.37 - ETA: 0s - loss: 0.7195 - accuracy: 0.30 - ETA: 0s - loss: 0.7079 - accuracy: 0.35 - ETA: 0s - loss: 0.7033 - accuracy: 0.35 - 0s 2ms/step - loss: 0.7024 - accuracy: 0.3666 - val_loss: 0.6928 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.59 - ETA: 0s - loss: 0.6849 - accuracy: 0.69 - ETA: 0s - loss: 0.6772 - accuracy: 0.70 - ETA: 0s - loss: 0.6738 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6711 - accuracy: 0.7182 - val_loss: 0.6708 - val_accuracy: 0.7239\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.81 - ETA: 0s - loss: 0.6889 - accuracy: 0.75 - ETA: 0s - loss: 0.6839 - accuracy: 0.72 - ETA: 0s - loss: 0.6893 - accuracy: 0.71 - 0s 3ms/step - loss: 0.6872 - accuracy: 0.7167 - val_loss: 0.6681 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.59 - ETA: 0s - loss: 0.7030 - accuracy: 0.70 - ETA: 0s - loss: 0.6857 - accuracy: 0.71 - ETA: 0s - loss: 0.6848 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6837 - accuracy: 0.7144 - val_loss: 0.6789 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6993 - accuracy: 0.72 - ETA: 0s - loss: 0.6942 - accuracy: 0.71 - ETA: 0s - loss: 0.6886 - accuracy: 0.72 - 0s 2ms/step - loss: 0.6892 - accuracy: 0.7200 - val_loss: 0.6599 - val_accuracy: 0.7224\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.87 - ETA: 0s - loss: 0.7044 - accuracy: 0.70 - ETA: 0s - loss: 0.6904 - accuracy: 0.60 - ETA: 0s - loss: 0.6947 - accuracy: 0.61 - 0s 2ms/step - loss: 0.6942 - accuracy: 0.6040 - val_loss: 0.7011 - val_accuracy: 0.3045\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.21 - ETA: 0s - loss: 0.6882 - accuracy: 0.59 - ETA: 0s - loss: 0.6860 - accuracy: 0.61 - ETA: 0s - loss: 0.6925 - accuracy: 0.58 - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5831 - val_loss: 0.7123 - val_accuracy: 0.3045\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7349 - accuracy: 0.37 - ETA: 0s - loss: 0.7013 - accuracy: 0.31 - ETA: 0s - loss: 0.6990 - accuracy: 0.31 - ETA: 0s - loss: 0.6926 - accuracy: 0.44 - 0s 2ms/step - loss: 0.6926 - accuracy: 0.4550 - val_loss: 0.6773 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7505 - accuracy: 0.62 - ETA: 0s - loss: 0.6893 - accuracy: 0.70 - ETA: 0s - loss: 0.6943 - accuracy: 0.59 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5390 - val_loss: 0.6886 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.78 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6965 - accuracy: 0.44 - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4300 - val_loss: 0.6833 - val_accuracy: 0.6955\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7019 - accuracy: 0.68 - ETA: 0s - loss: 0.7034 - accuracy: 0.61 - ETA: 0s - loss: 0.6994 - accuracy: 0.46 - ETA: 0s - loss: 0.6940 - accuracy: 0.45 - 0s 3ms/step - loss: 0.6927 - accuracy: 0.4763 - val_loss: 0.8109 - val_accuracy: 0.6940\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6991 - accuracy: 0.69 - ETA: 0s - loss: 0.6963 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.53 - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5558 - val_loss: 0.8255 - val_accuracy: 0.6940\n",
      "Epoch 22/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.7014 - accuracy: 0.68 - ETA: 0s - loss: 0.6851 - accuracy: 0.71 - ETA: 0s - loss: 0.6920 - accuracy: 0.67 - ETA: 0s - loss: 0.6932 - accuracy: 0.5555Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5312 - val_loss: 0.8344 - val_accuracy: 0.3045\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 19c2bfb8ba50ea8610467cf89819b5a9</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7233830690383911</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9336740997346352</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 90</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.62 - ETA: 0s - loss: 2.1158 - accuracy: 0.61 - ETA: 0s - loss: 1.5050 - accuracy: 0.63 - ETA: 0s - loss: 1.2388 - accuracy: 0.65 - ETA: 0s - loss: 1.1082 - accuracy: 0.66 - ETA: 0s - loss: 1.0116 - accuracy: 0.67 - ETA: 0s - loss: 0.9497 - accuracy: 0.67 - ETA: 0s - loss: 0.9002 - accuracy: 0.68 - ETA: 0s - loss: 0.8704 - accuracy: 0.68 - 1s 8ms/step - loss: 0.8677 - accuracy: 0.6876 - val_loss: 0.6059 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.68 - ETA: 0s - loss: 0.5350 - accuracy: 0.79 - ETA: 0s - loss: 0.5515 - accuracy: 0.78 - ETA: 0s - loss: 0.5531 - accuracy: 0.77 - ETA: 0s - loss: 0.5728 - accuracy: 0.76 - ETA: 0s - loss: 0.5717 - accuracy: 0.76 - ETA: 0s - loss: 0.5734 - accuracy: 0.76 - ETA: 0s - loss: 0.5755 - accuracy: 0.75 - ETA: 0s - loss: 0.5740 - accuracy: 0.76 - 1s 7ms/step - loss: 0.5733 - accuracy: 0.7626 - val_loss: 0.5949 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.62 - ETA: 0s - loss: 0.5423 - accuracy: 0.78 - ETA: 0s - loss: 0.5387 - accuracy: 0.79 - ETA: 0s - loss: 0.5277 - accuracy: 0.80 - ETA: 0s - loss: 0.5190 - accuracy: 0.79 - ETA: 0s - loss: 0.5151 - accuracy: 0.80 - ETA: 0s - loss: 0.5212 - accuracy: 0.80 - ETA: 0s - loss: 0.5293 - accuracy: 0.79 - ETA: 0s - loss: 0.5281 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5251 - accuracy: 0.7992 - val_loss: 0.7116 - val_accuracy: 0.6940\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.84 - ETA: 0s - loss: 0.5395 - accuracy: 0.80 - ETA: 0s - loss: 0.5403 - accuracy: 0.77 - ETA: 0s - loss: 0.5281 - accuracy: 0.77 - ETA: 0s - loss: 0.5201 - accuracy: 0.79 - ETA: 0s - loss: 0.5213 - accuracy: 0.79 - ETA: 0s - loss: 0.5251 - accuracy: 0.79 - ETA: 0s - loss: 0.5232 - accuracy: 0.80 - ETA: 0s - loss: 0.5195 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5181 - accuracy: 0.8029 - val_loss: 0.7376 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.90 - ETA: 0s - loss: 0.4257 - accuracy: 0.84 - ETA: 0s - loss: 0.4514 - accuracy: 0.83 - ETA: 0s - loss: 0.4626 - accuracy: 0.83 - ETA: 0s - loss: 0.4670 - accuracy: 0.83 - ETA: 0s - loss: 0.4678 - accuracy: 0.83 - ETA: 0s - loss: 0.4642 - accuracy: 0.83 - ETA: 0s - loss: 0.4746 - accuracy: 0.82 - ETA: 0s - loss: 0.4751 - accuracy: 0.82 - 0s 6ms/step - loss: 0.4783 - accuracy: 0.8283 - val_loss: 0.5875 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.87 - ETA: 0s - loss: 0.3981 - accuracy: 0.85 - ETA: 0s - loss: 0.4172 - accuracy: 0.84 - ETA: 0s - loss: 0.4285 - accuracy: 0.83 - ETA: 0s - loss: 0.4433 - accuracy: 0.83 - ETA: 0s - loss: 0.4465 - accuracy: 0.83 - ETA: 0s - loss: 0.4587 - accuracy: 0.83 - ETA: 0s - loss: 0.4592 - accuracy: 0.82 - ETA: 0s - loss: 0.4623 - accuracy: 0.82 - 1s 6ms/step - loss: 0.4657 - accuracy: 0.8249 - val_loss: 0.7104 - val_accuracy: 0.6925\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.84 - ETA: 0s - loss: 0.4340 - accuracy: 0.84 - ETA: 0s - loss: 0.4349 - accuracy: 0.84 - ETA: 0s - loss: 0.4271 - accuracy: 0.85 - ETA: 0s - loss: 0.4346 - accuracy: 0.85 - ETA: 0s - loss: 0.4311 - accuracy: 0.84 - ETA: 0s - loss: 0.4290 - accuracy: 0.84 - ETA: 0s - loss: 0.4393 - accuracy: 0.84 - 1s 7ms/step - loss: 0.4454 - accuracy: 0.8451 - val_loss: 0.5499 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.81 - ETA: 0s - loss: 0.4371 - accuracy: 0.86 - ETA: 0s - loss: 0.4360 - accuracy: 0.86 - ETA: 0s - loss: 0.4193 - accuracy: 0.87 - ETA: 0s - loss: 0.4239 - accuracy: 0.87 - ETA: 0s - loss: 0.4263 - accuracy: 0.87 - ETA: 0s - loss: 0.4257 - accuracy: 0.86 - ETA: 0s - loss: 0.4435 - accuracy: 0.86 - ETA: 0s - loss: 0.4391 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4375 - accuracy: 0.8667 - val_loss: 0.7175 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.96 - ETA: 0s - loss: 0.3961 - accuracy: 0.88 - ETA: 0s - loss: 0.4118 - accuracy: 0.88 - ETA: 0s - loss: 0.4093 - accuracy: 0.88 - ETA: 0s - loss: 0.3890 - accuracy: 0.88 - ETA: 0s - loss: 0.3864 - accuracy: 0.88 - ETA: 0s - loss: 0.3916 - accuracy: 0.88 - ETA: 0s - loss: 0.4062 - accuracy: 0.87 - ETA: 0s - loss: 0.4161 - accuracy: 0.87 - 1s 6ms/step - loss: 0.4288 - accuracy: 0.8690 - val_loss: 0.7592 - val_accuracy: 0.6910\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.75 - ETA: 0s - loss: 0.4759 - accuracy: 0.81 - ETA: 0s - loss: 0.4236 - accuracy: 0.84 - ETA: 0s - loss: 0.4229 - accuracy: 0.84 - ETA: 0s - loss: 0.4158 - accuracy: 0.85 - ETA: 0s - loss: 0.4335 - accuracy: 0.85 - ETA: 0s - loss: 0.4336 - accuracy: 0.85 - ETA: 0s - loss: 0.4260 - accuracy: 0.85 - ETA: 0s - loss: 0.4303 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4348 - accuracy: 0.8589 - val_loss: 0.8541 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.90 - ETA: 0s - loss: 0.4057 - accuracy: 0.85 - ETA: 0s - loss: 0.4053 - accuracy: 0.87 - ETA: 0s - loss: 0.4511 - accuracy: 0.85 - ETA: 0s - loss: 0.5202 - accuracy: 0.84 - ETA: 0s - loss: 0.5205 - accuracy: 0.83 - ETA: 0s - loss: 0.5430 - accuracy: 0.82 - ETA: 0s - loss: 0.5417 - accuracy: 0.82 - ETA: 0s - loss: 0.5385 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5328 - accuracy: 0.8246 - val_loss: 0.6780 - val_accuracy: 0.6881\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.87 - ETA: 0s - loss: 0.4188 - accuracy: 0.85 - ETA: 0s - loss: 0.4160 - accuracy: 0.86 - ETA: 0s - loss: 0.3919 - accuracy: 0.86 - ETA: 0s - loss: 0.3870 - accuracy: 0.86 - ETA: 0s - loss: 0.4128 - accuracy: 0.85 - ETA: 0s - loss: 0.4225 - accuracy: 0.86 - ETA: 0s - loss: 0.4370 - accuracy: 0.85 - ETA: 0s - loss: 0.4344 - accuracy: 0.86 - ETA: 0s - loss: 0.4391 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4387 - accuracy: 0.8582 - val_loss: 0.7047 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.90 - ETA: 0s - loss: 0.4125 - accuracy: 0.85 - ETA: 0s - loss: 0.3762 - accuracy: 0.87 - ETA: 0s - loss: 0.3822 - accuracy: 0.88 - ETA: 0s - loss: 0.3788 - accuracy: 0.88 - ETA: 0s - loss: 0.3863 - accuracy: 0.87 - ETA: 0s - loss: 0.3882 - accuracy: 0.87 - ETA: 0s - loss: 0.3906 - accuracy: 0.87 - ETA: 0s - loss: 0.3938 - accuracy: 0.87 - 1s 6ms/step - loss: 0.3958 - accuracy: 0.8720 - val_loss: 0.8976 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.87 - ETA: 0s - loss: 0.4405 - accuracy: 0.86 - ETA: 0s - loss: 0.3986 - accuracy: 0.87 - ETA: 0s - loss: 0.4092 - accuracy: 0.87 - ETA: 0s - loss: 0.4061 - accuracy: 0.87 - ETA: 0s - loss: 0.3858 - accuracy: 0.88 - ETA: 0s - loss: 0.3751 - accuracy: 0.88 - ETA: 0s - loss: 0.3680 - accuracy: 0.89 - ETA: 0s - loss: 0.3725 - accuracy: 0.88 - 1s 6ms/step - loss: 0.3736 - accuracy: 0.8880 - val_loss: 0.8621 - val_accuracy: 0.6970\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7704 - accuracy: 0.93 - ETA: 0s - loss: 0.4356 - accuracy: 0.89 - ETA: 0s - loss: 0.4218 - accuracy: 0.88 - ETA: 0s - loss: 0.4076 - accuracy: 0.89 - ETA: 0s - loss: 0.4182 - accuracy: 0.89 - ETA: 0s - loss: 0.4126 - accuracy: 0.88 - ETA: 0s - loss: 0.4143 - accuracy: 0.88 - ETA: 0s - loss: 0.4157 - accuracy: 0.88 - ETA: 0s - loss: 0.4046 - accuracy: 0.88 - 0s 6ms/step - loss: 0.4033 - accuracy: 0.8854 - val_loss: 0.8948 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.87 - ETA: 0s - loss: 0.3606 - accuracy: 0.89 - ETA: 0s - loss: 0.3703 - accuracy: 0.89 - ETA: 0s - loss: 0.3538 - accuracy: 0.89 - ETA: 0s - loss: 0.3700 - accuracy: 0.89 - ETA: 0s - loss: 0.3674 - accuracy: 0.89 - ETA: 0s - loss: 0.3649 - accuracy: 0.89 - ETA: 0s - loss: 0.3603 - accuracy: 0.89 - ETA: 0s - loss: 0.3581 - accuracy: 0.89 - 0s 6ms/step - loss: 0.3588 - accuracy: 0.8985 - val_loss: 1.1864 - val_accuracy: 0.7224\n",
      "Epoch 17/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2166 - accuracy: 0.96 - ETA: 0s - loss: 0.3437 - accuracy: 0.90 - ETA: 0s - loss: 0.3655 - accuracy: 0.90 - ETA: 0s - loss: 0.3505 - accuracy: 0.90 - ETA: 0s - loss: 0.3433 - accuracy: 0.90 - ETA: 0s - loss: 0.3610 - accuracy: 0.90 - ETA: 0s - loss: 0.3597 - accuracy: 0.90 - ETA: 0s - loss: 0.3560 - accuracy: 0.90 - ETA: 0s - loss: 0.3564 - accuracy: 0.9059Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.3606 - accuracy: 0.9041 - val_loss: 0.9281 - val_accuracy: 0.7433\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7695 - accuracy: 0.56 - ETA: 0s - loss: 1.7995 - accuracy: 0.67 - ETA: 0s - loss: 1.3159 - accuracy: 0.69 - ETA: 0s - loss: 1.0843 - accuracy: 0.71 - ETA: 0s - loss: 0.9743 - accuracy: 0.71 - ETA: 0s - loss: 0.9121 - accuracy: 0.72 - ETA: 0s - loss: 0.8652 - accuracy: 0.72 - ETA: 0s - loss: 0.8351 - accuracy: 0.72 - ETA: 0s - loss: 0.8154 - accuracy: 0.72 - 1s 7ms/step - loss: 0.8154 - accuracy: 0.7223 - val_loss: 0.5826 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7892 - accuracy: 0.68 - ETA: 0s - loss: 0.5759 - accuracy: 0.77 - ETA: 0s - loss: 0.5712 - accuracy: 0.78 - ETA: 0s - loss: 0.6115 - accuracy: 0.78 - ETA: 0s - loss: 0.5993 - accuracy: 0.77 - ETA: 0s - loss: 0.5971 - accuracy: 0.77 - ETA: 0s - loss: 0.6000 - accuracy: 0.77 - ETA: 0s - loss: 0.5974 - accuracy: 0.77 - ETA: 0s - loss: 0.5981 - accuracy: 0.77 - 0s 6ms/step - loss: 0.5957 - accuracy: 0.7712 - val_loss: 0.6132 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5244 - accuracy: 0.81 - ETA: 0s - loss: 0.5789 - accuracy: 0.77 - ETA: 0s - loss: 0.5350 - accuracy: 0.81 - ETA: 0s - loss: 0.5642 - accuracy: 0.79 - ETA: 0s - loss: 0.5787 - accuracy: 0.77 - ETA: 0s - loss: 0.5729 - accuracy: 0.78 - ETA: 0s - loss: 0.5612 - accuracy: 0.78 - ETA: 0s - loss: 0.5509 - accuracy: 0.79 - ETA: 0s - loss: 0.5567 - accuracy: 0.79 - ETA: 0s - loss: 0.5479 - accuracy: 0.79 - ETA: 0s - loss: 0.5468 - accuracy: 0.79 - 1s 7ms/step - loss: 0.5486 - accuracy: 0.7988 - val_loss: 0.7143 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.71 - ETA: 0s - loss: 0.5476 - accuracy: 0.80 - ETA: 0s - loss: 0.5232 - accuracy: 0.81 - ETA: 0s - loss: 0.5145 - accuracy: 0.81 - ETA: 0s - loss: 0.5189 - accuracy: 0.80 - ETA: 0s - loss: 0.5249 - accuracy: 0.80 - ETA: 0s - loss: 0.5256 - accuracy: 0.80 - ETA: 0s - loss: 0.5315 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5345 - accuracy: 0.8037 - val_loss: 0.7432 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.78 - ETA: 0s - loss: 0.5823 - accuracy: 0.78 - ETA: 0s - loss: 0.5519 - accuracy: 0.80 - ETA: 0s - loss: 0.5322 - accuracy: 0.81 - ETA: 0s - loss: 0.5163 - accuracy: 0.81 - ETA: 0s - loss: 0.5201 - accuracy: 0.81 - ETA: 0s - loss: 0.5093 - accuracy: 0.82 - ETA: 0s - loss: 0.5121 - accuracy: 0.82 - ETA: 0s - loss: 0.5067 - accuracy: 0.82 - 0s 6ms/step - loss: 0.5058 - accuracy: 0.8242 - val_loss: 0.6036 - val_accuracy: 0.7179\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.84 - ETA: 0s - loss: 0.4957 - accuracy: 0.82 - ETA: 0s - loss: 0.4843 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - ETA: 0s - loss: 0.4937 - accuracy: 0.83 - ETA: 0s - loss: 0.4917 - accuracy: 0.83 - ETA: 0s - loss: 0.5169 - accuracy: 0.83 - ETA: 0s - loss: 0.5073 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5106 - accuracy: 0.8354 - val_loss: 0.6110 - val_accuracy: 0.7358\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.90 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - ETA: 0s - loss: 0.4601 - accuracy: 0.86 - ETA: 0s - loss: 0.4628 - accuracy: 0.85 - ETA: 0s - loss: 0.4791 - accuracy: 0.84 - ETA: 0s - loss: 0.4906 - accuracy: 0.83 - ETA: 0s - loss: 0.4894 - accuracy: 0.83 - ETA: 0s - loss: 0.4983 - accuracy: 0.83 - ETA: 0s - loss: 0.5001 - accuracy: 0.83 - 1s 6ms/step - loss: 0.4953 - accuracy: 0.8384 - val_loss: 0.8338 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.87 - ETA: 0s - loss: 0.5391 - accuracy: 0.81 - ETA: 0s - loss: 0.5197 - accuracy: 0.81 - ETA: 0s - loss: 0.5028 - accuracy: 0.82 - ETA: 0s - loss: 0.5045 - accuracy: 0.82 - ETA: 0s - loss: 0.5017 - accuracy: 0.82 - ETA: 0s - loss: 0.4899 - accuracy: 0.83 - ETA: 0s - loss: 0.4880 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - 1s 7ms/step - loss: 0.4882 - accuracy: 0.8361 - val_loss: 0.6531 - val_accuracy: 0.7313\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6568 - accuracy: 0.78 - ETA: 0s - loss: 0.4733 - accuracy: 0.83 - ETA: 0s - loss: 0.5042 - accuracy: 0.82 - ETA: 0s - loss: 0.5114 - accuracy: 0.82 - ETA: 0s - loss: 0.4939 - accuracy: 0.83 - ETA: 0s - loss: 0.5273 - accuracy: 0.83 - ETA: 0s - loss: 0.5263 - accuracy: 0.83 - ETA: 0s - loss: 0.5306 - accuracy: 0.83 - ETA: 0s - loss: 0.5270 - accuracy: 0.83 - 1s 6ms/step - loss: 0.5274 - accuracy: 0.8343 - val_loss: 0.7305 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.84 - ETA: 0s - loss: 0.4798 - accuracy: 0.82 - ETA: 0s - loss: 0.4804 - accuracy: 0.83 - ETA: 0s - loss: 0.5139 - accuracy: 0.81 - ETA: 0s - loss: 0.5239 - accuracy: 0.81 - ETA: 0s - loss: 0.5403 - accuracy: 0.80 - ETA: 0s - loss: 0.5416 - accuracy: 0.81 - ETA: 0s - loss: 0.5559 - accuracy: 0.81 - ETA: 0s - loss: 0.5520 - accuracy: 0.81 - 1s 7ms/step - loss: 0.5463 - accuracy: 0.8175 - val_loss: 0.5947 - val_accuracy: 0.7507\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.78 - ETA: 0s - loss: 0.5355 - accuracy: 0.80 - ETA: 0s - loss: 0.5533 - accuracy: 0.79 - ETA: 0s - loss: 0.5466 - accuracy: 0.80 - ETA: 0s - loss: 0.5388 - accuracy: 0.81 - ETA: 0s - loss: 0.5384 - accuracy: 0.81 - ETA: 0s - loss: 0.5429 - accuracy: 0.81 - ETA: 0s - loss: 0.5461 - accuracy: 0.81 - ETA: 0s - loss: 0.5456 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5466 - accuracy: 0.8100 - val_loss: 0.7236 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.71 - ETA: 0s - loss: 0.4845 - accuracy: 0.84 - ETA: 0s - loss: 0.4845 - accuracy: 0.84 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.5269 - accuracy: 0.83 - ETA: 0s - loss: 0.5343 - accuracy: 0.82 - ETA: 0s - loss: 0.5409 - accuracy: 0.81 - ETA: 0s - loss: 0.5455 - accuracy: 0.81 - ETA: 0s - loss: 0.5478 - accuracy: 0.81 - ETA: 0s - loss: 0.5457 - accuracy: 0.81 - 1s 7ms/step - loss: 0.5445 - accuracy: 0.8186 - val_loss: 0.6787 - val_accuracy: 0.7328\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.90 - ETA: 0s - loss: 0.5170 - accuracy: 0.82 - ETA: 0s - loss: 0.5332 - accuracy: 0.81 - ETA: 0s - loss: 0.5270 - accuracy: 0.82 - ETA: 0s - loss: 0.5172 - accuracy: 0.82 - ETA: 0s - loss: 0.5261 - accuracy: 0.82 - ETA: 0s - loss: 0.5227 - accuracy: 0.82 - ETA: 0s - loss: 0.5293 - accuracy: 0.82 - ETA: 0s - loss: 0.5185 - accuracy: 0.82 - ETA: 0s - loss: 0.5155 - accuracy: 0.82 - 1s 8ms/step - loss: 0.5191 - accuracy: 0.8279 - val_loss: 0.5977 - val_accuracy: 0.7522\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.87 - ETA: 0s - loss: 0.4735 - accuracy: 0.86 - ETA: 0s - loss: 0.4849 - accuracy: 0.84 - ETA: 0s - loss: 0.4802 - accuracy: 0.85 - ETA: 0s - loss: 0.4644 - accuracy: 0.85 - ETA: 0s - loss: 0.4697 - accuracy: 0.85 - ETA: 0s - loss: 0.4633 - accuracy: 0.85 - ETA: 0s - loss: 0.4615 - accuracy: 0.85 - ETA: 0s - loss: 0.4534 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4497 - accuracy: 0.8615 - val_loss: 0.7187 - val_accuracy: 0.7179\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.78 - ETA: 0s - loss: 0.3914 - accuracy: 0.89 - ETA: 0s - loss: 0.3897 - accuracy: 0.88 - ETA: 0s - loss: 0.4113 - accuracy: 0.87 - ETA: 0s - loss: 0.4174 - accuracy: 0.87 - ETA: 0s - loss: 0.4318 - accuracy: 0.86 - ETA: 0s - loss: 0.4353 - accuracy: 0.86 - ETA: 0s - loss: 0.4363 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4349 - accuracy: 0.8667 - val_loss: 0.6526 - val_accuracy: 0.7313\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.93 - ETA: 0s - loss: 0.3820 - accuracy: 0.89 - ETA: 0s - loss: 0.3961 - accuracy: 0.88 - ETA: 0s - loss: 0.4206 - accuracy: 0.87 - ETA: 0s - loss: 0.4351 - accuracy: 0.87 - ETA: 0s - loss: 0.4382 - accuracy: 0.86 - ETA: 0s - loss: 0.4257 - accuracy: 0.87 - ETA: 0s - loss: 0.4196 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4206 - accuracy: 0.8791 - val_loss: 0.8825 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.84 - ETA: 0s - loss: 0.3694 - accuracy: 0.89 - ETA: 0s - loss: 0.4023 - accuracy: 0.87 - ETA: 0s - loss: 0.4067 - accuracy: 0.88 - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.4315 - accuracy: 0.87 - ETA: 0s - loss: 0.4338 - accuracy: 0.87 - ETA: 0s - loss: 0.4319 - accuracy: 0.87 - ETA: 0s - loss: 0.4318 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4333 - accuracy: 0.8694 - val_loss: 0.9483 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.84 - ETA: 0s - loss: 0.4789 - accuracy: 0.84 - ETA: 0s - loss: 0.4508 - accuracy: 0.86 - ETA: 0s - loss: 0.4138 - accuracy: 0.88 - ETA: 0s - loss: 0.4205 - accuracy: 0.87 - ETA: 0s - loss: 0.4166 - accuracy: 0.88 - ETA: 0s - loss: 0.4357 - accuracy: 0.87 - ETA: 0s - loss: 0.4234 - accuracy: 0.87 - ETA: 0s - loss: 0.4248 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4241 - accuracy: 0.8772 - val_loss: 0.8631 - val_accuracy: 0.7343\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4962 - accuracy: 0.84 - ETA: 0s - loss: 0.4203 - accuracy: 0.88 - ETA: 0s - loss: 0.4288 - accuracy: 0.87 - ETA: 0s - loss: 0.4426 - accuracy: 0.86 - ETA: 0s - loss: 0.4398 - accuracy: 0.86 - ETA: 0s - loss: 0.4352 - accuracy: 0.87 - ETA: 0s - loss: 0.4379 - accuracy: 0.87 - ETA: 0s - loss: 0.4330 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8742 - val_loss: 0.9799 - val_accuracy: 0.7254\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.84 - ETA: 0s - loss: 0.4161 - accuracy: 0.86 - ETA: 0s - loss: 0.4404 - accuracy: 0.86 - ETA: 0s - loss: 0.4380 - accuracy: 0.86 - ETA: 0s - loss: 0.4151 - accuracy: 0.87 - ETA: 0s - loss: 0.4060 - accuracy: 0.88 - ETA: 0s - loss: 0.4184 - accuracy: 0.87 - ETA: 0s - loss: 0.4178 - accuracy: 0.87 - ETA: 0s - loss: 0.4265 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4264 - accuracy: 0.8735 - val_loss: 0.7844 - val_accuracy: 0.7493\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.84 - ETA: 0s - loss: 0.4713 - accuracy: 0.84 - ETA: 0s - loss: 0.4377 - accuracy: 0.86 - ETA: 0s - loss: 0.4632 - accuracy: 0.87 - ETA: 0s - loss: 0.4661 - accuracy: 0.87 - ETA: 0s - loss: 0.4661 - accuracy: 0.86 - ETA: 0s - loss: 0.4591 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.87 - ETA: 0s - loss: 0.4357 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4438 - accuracy: 0.8735 - val_loss: 1.2675 - val_accuracy: 0.7134\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.87 - ETA: 0s - loss: 0.4500 - accuracy: 0.85 - ETA: 0s - loss: 0.4448 - accuracy: 0.86 - ETA: 0s - loss: 0.4409 - accuracy: 0.86 - ETA: 0s - loss: 0.4390 - accuracy: 0.86 - ETA: 0s - loss: 0.4371 - accuracy: 0.86 - ETA: 0s - loss: 0.4368 - accuracy: 0.86 - ETA: 0s - loss: 0.4461 - accuracy: 0.86 - ETA: 0s - loss: 0.4423 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4424 - accuracy: 0.8690 - val_loss: 0.7767 - val_accuracy: 0.7388\n",
      "Epoch 23/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4953 - accuracy: 0.84 - ETA: 0s - loss: 0.4654 - accuracy: 0.85 - ETA: 0s - loss: 0.4600 - accuracy: 0.86 - ETA: 0s - loss: 0.4620 - accuracy: 0.86 - ETA: 0s - loss: 0.4528 - accuracy: 0.87 - ETA: 0s - loss: 0.4702 - accuracy: 0.86 - ETA: 0s - loss: 0.4935 - accuracy: 0.85 - ETA: 0s - loss: 0.5035 - accuracy: 0.8474Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5039 - accuracy: 0.8481 - val_loss: 0.6474 - val_accuracy: 0.7433\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.68 - ETA: 0s - loss: 3.6558 - accuracy: 0.59 - ETA: 0s - loss: 2.1131 - accuracy: 0.65 - ETA: 0s - loss: 1.6047 - accuracy: 0.65 - ETA: 0s - loss: 1.3628 - accuracy: 0.67 - ETA: 0s - loss: 1.2161 - accuracy: 0.68 - ETA: 0s - loss: 1.1291 - accuracy: 0.68 - ETA: 0s - loss: 1.0564 - accuracy: 0.68 - 1s 7ms/step - loss: 1.0221 - accuracy: 0.6883 - val_loss: 0.6597 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8744 - accuracy: 0.71 - ETA: 0s - loss: 0.6423 - accuracy: 0.73 - ETA: 0s - loss: 0.6050 - accuracy: 0.74 - ETA: 0s - loss: 0.5855 - accuracy: 0.76 - ETA: 0s - loss: 0.5746 - accuracy: 0.76 - ETA: 0s - loss: 0.5684 - accuracy: 0.77 - ETA: 0s - loss: 0.5721 - accuracy: 0.76 - ETA: 0s - loss: 0.5773 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5782 - accuracy: 0.7619 - val_loss: 0.6312 - val_accuracy: 0.6836\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.71 - ETA: 0s - loss: 0.5249 - accuracy: 0.78 - ETA: 0s - loss: 0.5264 - accuracy: 0.78 - ETA: 0s - loss: 0.5014 - accuracy: 0.80 - ETA: 0s - loss: 0.4948 - accuracy: 0.81 - ETA: 0s - loss: 0.4846 - accuracy: 0.81 - ETA: 0s - loss: 0.5147 - accuracy: 0.80 - ETA: 0s - loss: 0.5560 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5576 - accuracy: 0.7913 - val_loss: 0.6144 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6243 - accuracy: 0.65 - ETA: 0s - loss: 0.5988 - accuracy: 0.74 - ETA: 0s - loss: 0.5493 - accuracy: 0.79 - ETA: 0s - loss: 0.5142 - accuracy: 0.81 - ETA: 0s - loss: 0.5217 - accuracy: 0.80 - ETA: 0s - loss: 0.5323 - accuracy: 0.80 - ETA: 0s - loss: 0.5382 - accuracy: 0.80 - ETA: 0s - loss: 0.5409 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5405 - accuracy: 0.8044 - val_loss: 0.5902 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.78 - ETA: 0s - loss: 0.5325 - accuracy: 0.78 - ETA: 0s - loss: 0.5327 - accuracy: 0.77 - ETA: 0s - loss: 0.5034 - accuracy: 0.79 - ETA: 0s - loss: 0.5111 - accuracy: 0.80 - ETA: 0s - loss: 0.5796 - accuracy: 0.80 - ETA: 0s - loss: 0.5634 - accuracy: 0.80 - ETA: 0s - loss: 0.5561 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5556 - accuracy: 0.8089 - val_loss: 0.5635 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.96 - ETA: 0s - loss: 0.6303 - accuracy: 0.83 - ETA: 0s - loss: 0.5927 - accuracy: 0.82 - ETA: 0s - loss: 0.5766 - accuracy: 0.83 - ETA: 0s - loss: 0.5760 - accuracy: 0.82 - ETA: 0s - loss: 0.5682 - accuracy: 0.81 - ETA: 0s - loss: 0.5577 - accuracy: 0.82 - ETA: 0s - loss: 0.5444 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5403 - accuracy: 0.8257 - val_loss: 0.9268 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7580 - accuracy: 0.78 - ETA: 0s - loss: 0.4730 - accuracy: 0.84 - ETA: 0s - loss: 0.4670 - accuracy: 0.84 - ETA: 0s - loss: 0.4777 - accuracy: 0.83 - ETA: 0s - loss: 0.4780 - accuracy: 0.83 - ETA: 0s - loss: 0.4748 - accuracy: 0.83 - ETA: 0s - loss: 0.4877 - accuracy: 0.82 - ETA: 0s - loss: 0.4911 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4934 - accuracy: 0.8249 - val_loss: 0.5604 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.93 - ETA: 0s - loss: 0.5088 - accuracy: 0.84 - ETA: 0s - loss: 0.5156 - accuracy: 0.83 - ETA: 0s - loss: 0.4953 - accuracy: 0.83 - ETA: 0s - loss: 0.4814 - accuracy: 0.84 - ETA: 0s - loss: 0.4739 - accuracy: 0.84 - ETA: 0s - loss: 0.4740 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4759 - accuracy: 0.8421 - val_loss: 0.6334 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.84 - ETA: 0s - loss: 0.4774 - accuracy: 0.84 - ETA: 0s - loss: 0.4587 - accuracy: 0.85 - ETA: 0s - loss: 0.4560 - accuracy: 0.85 - ETA: 0s - loss: 0.4674 - accuracy: 0.85 - ETA: 0s - loss: 0.4595 - accuracy: 0.84 - ETA: 0s - loss: 0.4717 - accuracy: 0.84 - ETA: 0s - loss: 0.4759 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4753 - accuracy: 0.8417 - val_loss: 0.7831 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.81 - ETA: 0s - loss: 0.4517 - accuracy: 0.84 - ETA: 0s - loss: 0.4560 - accuracy: 0.84 - ETA: 0s - loss: 0.4782 - accuracy: 0.83 - ETA: 0s - loss: 0.4876 - accuracy: 0.84 - ETA: 0s - loss: 0.4891 - accuracy: 0.84 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4850 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4857 - accuracy: 0.8425 - val_loss: 0.7165 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.78 - ETA: 0s - loss: 0.5527 - accuracy: 0.79 - ETA: 0s - loss: 0.5501 - accuracy: 0.79 - ETA: 0s - loss: 0.5352 - accuracy: 0.80 - ETA: 0s - loss: 0.5095 - accuracy: 0.81 - ETA: 0s - loss: 0.4927 - accuracy: 0.82 - ETA: 0s - loss: 0.4813 - accuracy: 0.83 - ETA: 0s - loss: 0.4802 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4853 - accuracy: 0.8373 - val_loss: 0.7942 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.81 - ETA: 0s - loss: 0.4907 - accuracy: 0.87 - ETA: 0s - loss: 0.4780 - accuracy: 0.86 - ETA: 0s - loss: 0.5457 - accuracy: 0.85 - ETA: 0s - loss: 0.5268 - accuracy: 0.85 - ETA: 0s - loss: 0.5028 - accuracy: 0.85 - ETA: 0s - loss: 0.4958 - accuracy: 0.85 - ETA: 0s - loss: 0.4836 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4761 - accuracy: 0.8555 - val_loss: 0.7857 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.90 - ETA: 0s - loss: 0.4914 - accuracy: 0.84 - ETA: 0s - loss: 0.4942 - accuracy: 0.84 - ETA: 0s - loss: 0.4956 - accuracy: 0.84 - ETA: 0s - loss: 0.4825 - accuracy: 0.84 - ETA: 0s - loss: 0.4873 - accuracy: 0.84 - ETA: 0s - loss: 0.4875 - accuracy: 0.84 - ETA: 0s - loss: 0.4829 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4737 - accuracy: 0.8481 - val_loss: 0.6260 - val_accuracy: 0.7403\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.87 - ETA: 0s - loss: 0.4255 - accuracy: 0.87 - ETA: 0s - loss: 0.4809 - accuracy: 0.83 - ETA: 0s - loss: 0.4708 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.85 - ETA: 0s - loss: 0.4748 - accuracy: 0.85 - ETA: 0s - loss: 0.4651 - accuracy: 0.85 - ETA: 0s - loss: 0.4679 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4686 - accuracy: 0.8529 - val_loss: 0.7047 - val_accuracy: 0.7448\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.96 - ETA: 0s - loss: 0.3965 - accuracy: 0.89 - ETA: 0s - loss: 0.4128 - accuracy: 0.88 - ETA: 0s - loss: 0.4164 - accuracy: 0.87 - ETA: 0s - loss: 0.4336 - accuracy: 0.86 - ETA: 0s - loss: 0.4433 - accuracy: 0.86 - ETA: 0s - loss: 0.4407 - accuracy: 0.86 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4436 - accuracy: 0.8600 - val_loss: 0.7554 - val_accuracy: 0.7149\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.87 - ETA: 0s - loss: 0.4924 - accuracy: 0.85 - ETA: 0s - loss: 0.4543 - accuracy: 0.86 - ETA: 0s - loss: 0.4455 - accuracy: 0.87 - ETA: 0s - loss: 0.4541 - accuracy: 0.85 - ETA: 0s - loss: 0.4351 - accuracy: 0.86 - ETA: 0s - loss: 0.4985 - accuracy: 0.85 - ETA: 0s - loss: 0.5419 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5574 - accuracy: 0.8373 - val_loss: 0.6231 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.84 - ETA: 0s - loss: 0.5330 - accuracy: 0.82 - ETA: 0s - loss: 0.5009 - accuracy: 0.84 - ETA: 0s - loss: 0.5347 - accuracy: 0.84 - ETA: 0s - loss: 0.5659 - accuracy: 0.83 - ETA: 0s - loss: 0.5702 - accuracy: 0.82 - ETA: 0s - loss: 0.5786 - accuracy: 0.81 - ETA: 0s - loss: 0.5817 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5807 - accuracy: 0.8137 - val_loss: 0.6170 - val_accuracy: 0.7493\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.84 - ETA: 0s - loss: 0.5557 - accuracy: 0.82 - ETA: 0s - loss: 0.5916 - accuracy: 0.79 - ETA: 0s - loss: 0.5771 - accuracy: 0.80 - ETA: 0s - loss: 0.5612 - accuracy: 0.80 - ETA: 0s - loss: 0.5538 - accuracy: 0.81 - ETA: 0s - loss: 0.5435 - accuracy: 0.82 - ETA: 0s - loss: 0.5465 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5610 - accuracy: 0.8178 - val_loss: 0.7732 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.78 - ETA: 0s - loss: 0.5479 - accuracy: 0.82 - ETA: 0s - loss: 0.5240 - accuracy: 0.82 - ETA: 0s - loss: 0.5201 - accuracy: 0.83 - ETA: 0s - loss: 0.5140 - accuracy: 0.83 - ETA: 0s - loss: 0.5294 - accuracy: 0.83 - ETA: 0s - loss: 0.5250 - accuracy: 0.83 - ETA: 0s - loss: 0.5168 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5170 - accuracy: 0.8414 - val_loss: 0.8376 - val_accuracy: 0.7299\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.90 - ETA: 0s - loss: 0.4918 - accuracy: 0.83 - ETA: 0s - loss: 0.4802 - accuracy: 0.84 - ETA: 0s - loss: 0.4680 - accuracy: 0.85 - ETA: 0s - loss: 0.4755 - accuracy: 0.85 - ETA: 0s - loss: 0.4771 - accuracy: 0.85 - ETA: 0s - loss: 0.4707 - accuracy: 0.86 - ETA: 0s - loss: 0.4672 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4992 - accuracy: 0.8589 - val_loss: 1.7782 - val_accuracy: 0.7493\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.87 - ETA: 0s - loss: 0.6520 - accuracy: 0.80 - ETA: 0s - loss: 0.6274 - accuracy: 0.80 - ETA: 0s - loss: 0.6230 - accuracy: 0.79 - ETA: 0s - loss: 0.6190 - accuracy: 0.78 - ETA: 0s - loss: 0.6281 - accuracy: 0.79 - ETA: 0s - loss: 0.6314 - accuracy: 0.79 - ETA: 0s - loss: 0.6280 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6274 - accuracy: 0.7895 - val_loss: 0.6599 - val_accuracy: 0.7269\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.75 - ETA: 0s - loss: 0.5762 - accuracy: 0.79 - ETA: 0s - loss: 0.5524 - accuracy: 0.81 - ETA: 0s - loss: 0.5619 - accuracy: 0.81 - ETA: 0s - loss: 0.5688 - accuracy: 0.80 - ETA: 0s - loss: 0.5680 - accuracy: 0.80 - ETA: 0s - loss: 0.5647 - accuracy: 0.80 - ETA: 0s - loss: 0.5712 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5737 - accuracy: 0.8010 - val_loss: 0.6112 - val_accuracy: 0.7507\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.81 - ETA: 0s - loss: 0.5738 - accuracy: 0.80 - ETA: 0s - loss: 0.5901 - accuracy: 0.79 - ETA: 0s - loss: 0.5841 - accuracy: 0.79 - ETA: 0s - loss: 0.5796 - accuracy: 0.80 - ETA: 0s - loss: 0.5794 - accuracy: 0.79 - ETA: 0s - loss: 0.5716 - accuracy: 0.80 - ETA: 0s - loss: 0.5667 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5722 - accuracy: 0.8104 - val_loss: 0.6071 - val_accuracy: 0.7433\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7354 - accuracy: 0.68 - ETA: 0s - loss: 0.5241 - accuracy: 0.82 - ETA: 0s - loss: 0.5149 - accuracy: 0.82 - ETA: 0s - loss: 0.5336 - accuracy: 0.82 - ETA: 0s - loss: 0.5324 - accuracy: 0.82 - ETA: 0s - loss: 0.5388 - accuracy: 0.82 - ETA: 0s - loss: 0.5410 - accuracy: 0.82 - ETA: 0s - loss: 0.5449 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5477 - accuracy: 0.8160 - val_loss: 0.6752 - val_accuracy: 0.7642\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.84 - ETA: 0s - loss: 0.5749 - accuracy: 0.78 - ETA: 0s - loss: 0.5640 - accuracy: 0.80 - ETA: 0s - loss: 0.5666 - accuracy: 0.79 - ETA: 0s - loss: 0.5583 - accuracy: 0.80 - ETA: 0s - loss: 0.5747 - accuracy: 0.81 - ETA: 0s - loss: 0.5707 - accuracy: 0.81 - ETA: 0s - loss: 0.5751 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5696 - accuracy: 0.8115 - val_loss: 0.9071 - val_accuracy: 0.7358\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.87 - ETA: 0s - loss: 0.5089 - accuracy: 0.83 - ETA: 0s - loss: 0.5336 - accuracy: 0.82 - ETA: 0s - loss: 0.5300 - accuracy: 0.82 - ETA: 0s - loss: 0.5340 - accuracy: 0.82 - ETA: 0s - loss: 0.5290 - accuracy: 0.82 - ETA: 0s - loss: 0.5357 - accuracy: 0.83 - ETA: 0s - loss: 0.5465 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5472 - accuracy: 0.8294 - val_loss: 0.5680 - val_accuracy: 0.7507\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.81 - ETA: 0s - loss: 0.5139 - accuracy: 0.84 - ETA: 0s - loss: 0.5428 - accuracy: 0.82 - ETA: 0s - loss: 0.5281 - accuracy: 0.82 - ETA: 0s - loss: 0.5142 - accuracy: 0.83 - ETA: 0s - loss: 0.5140 - accuracy: 0.83 - ETA: 0s - loss: 0.5024 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5170 - accuracy: 0.8317 - val_loss: 0.6232 - val_accuracy: 0.7373\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6400 - accuracy: 0.75 - ETA: 0s - loss: 0.5787 - accuracy: 0.79 - ETA: 0s - loss: 0.5954 - accuracy: 0.81 - ETA: 0s - loss: 0.5771 - accuracy: 0.80 - ETA: 0s - loss: 0.5461 - accuracy: 0.82 - ETA: 0s - loss: 0.5320 - accuracy: 0.83 - ETA: 0s - loss: 0.5211 - accuracy: 0.83 - ETA: 0s - loss: 0.5143 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5134 - accuracy: 0.8387 - val_loss: 0.6609 - val_accuracy: 0.7567\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.90 - ETA: 0s - loss: 0.4666 - accuracy: 0.86 - ETA: 0s - loss: 0.4715 - accuracy: 0.85 - ETA: 0s - loss: 0.4775 - accuracy: 0.84 - ETA: 0s - loss: 0.4673 - accuracy: 0.85 - ETA: 0s - loss: 0.4753 - accuracy: 0.85 - ETA: 0s - loss: 0.4746 - accuracy: 0.85 - ETA: 0s - loss: 0.4760 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4773 - accuracy: 0.8522 - val_loss: 0.7179 - val_accuracy: 0.7522\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.84 - ETA: 0s - loss: 0.4082 - accuracy: 0.88 - ETA: 0s - loss: 0.4055 - accuracy: 0.88 - ETA: 0s - loss: 0.4316 - accuracy: 0.87 - ETA: 0s - loss: 0.4489 - accuracy: 0.86 - ETA: 0s - loss: 0.4574 - accuracy: 0.85 - ETA: 0s - loss: 0.4566 - accuracy: 0.86 - ETA: 0s - loss: 0.4612 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4674 - accuracy: 0.8604 - val_loss: 0.6611 - val_accuracy: 0.7478\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.84 - ETA: 0s - loss: 0.4529 - accuracy: 0.86 - ETA: 0s - loss: 0.4393 - accuracy: 0.86 - ETA: 0s - loss: 0.4474 - accuracy: 0.86 - ETA: 0s - loss: 0.4379 - accuracy: 0.87 - ETA: 0s - loss: 0.4429 - accuracy: 0.86 - ETA: 0s - loss: 0.4417 - accuracy: 0.86 - ETA: 0s - loss: 0.4343 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4420 - accuracy: 0.8679 - val_loss: 1.1544 - val_accuracy: 0.7463\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.81 - ETA: 0s - loss: 0.6790 - accuracy: 0.85 - ETA: 0s - loss: 0.5744 - accuracy: 0.86 - ETA: 0s - loss: 0.5534 - accuracy: 0.85 - ETA: 0s - loss: 0.5338 - accuracy: 0.84 - ETA: 0s - loss: 0.5350 - accuracy: 0.84 - ETA: 0s - loss: 0.5146 - accuracy: 0.85 - ETA: 0s - loss: 0.5075 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5126 - accuracy: 0.8537 - val_loss: 0.8099 - val_accuracy: 0.7388\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.93 - ETA: 0s - loss: 0.4213 - accuracy: 0.88 - ETA: 0s - loss: 0.4362 - accuracy: 0.86 - ETA: 0s - loss: 0.4392 - accuracy: 0.86 - ETA: 0s - loss: 0.4512 - accuracy: 0.86 - ETA: 0s - loss: 0.4476 - accuracy: 0.86 - ETA: 0s - loss: 0.4459 - accuracy: 0.86 - ETA: 0s - loss: 0.4590 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4701 - accuracy: 0.8589 - val_loss: 0.8220 - val_accuracy: 0.7552\n",
      "Epoch 34/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5784 - accuracy: 0.78 - ETA: 0s - loss: 0.5888 - accuracy: 0.82 - ETA: 0s - loss: 0.5960 - accuracy: 0.82 - ETA: 0s - loss: 0.5466 - accuracy: 0.83 - ETA: 0s - loss: 0.5345 - accuracy: 0.83 - ETA: 0s - loss: 0.6026 - accuracy: 0.83 - ETA: 0s - loss: 0.6004 - accuracy: 0.82 - ETA: 0s - loss: 0.5981 - accuracy: 0.8229Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.8227 - val_loss: 1.5404 - val_accuracy: 0.7358\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 780ceda1c25875f7ca9ae75477d63495</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7547263701756796</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5881450463090249</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 500</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 175</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8941 - accuracy: 0.59 - ETA: 0s - loss: 2.0220 - accuracy: 0.63 - ETA: 0s - loss: 1.2915 - accuracy: 0.70 - ETA: 0s - loss: 1.1073 - accuracy: 0.70 - ETA: 0s - loss: 0.9932 - accuracy: 0.70 - ETA: 0s - loss: 0.9320 - accuracy: 0.70 - 0s 6ms/step - loss: 0.9007 - accuracy: 0.7055 - val_loss: 0.5707 - val_accuracy: 0.7104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.84 - ETA: 0s - loss: 0.6767 - accuracy: 0.72 - ETA: 0s - loss: 0.6151 - accuracy: 0.73 - ETA: 0s - loss: 0.6128 - accuracy: 0.73 - ETA: 0s - loss: 0.6120 - accuracy: 0.73 - ETA: 0s - loss: 0.6108 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6071 - accuracy: 0.7368 - val_loss: 0.5887 - val_accuracy: 0.7209\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.78 - ETA: 0s - loss: 0.5971 - accuracy: 0.77 - ETA: 0s - loss: 0.5687 - accuracy: 0.77 - ETA: 0s - loss: 0.5721 - accuracy: 0.77 - ETA: 0s - loss: 0.5726 - accuracy: 0.76 - ETA: 0s - loss: 0.5731 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5735 - accuracy: 0.7641 - val_loss: 0.5839 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.81 - ETA: 0s - loss: 0.5089 - accuracy: 0.80 - ETA: 0s - loss: 0.5324 - accuracy: 0.79 - ETA: 0s - loss: 0.5587 - accuracy: 0.80 - ETA: 0s - loss: 0.5620 - accuracy: 0.80 - ETA: 0s - loss: 0.5574 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5612 - accuracy: 0.7977 - val_loss: 0.5700 - val_accuracy: 0.7358\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.71 - ETA: 0s - loss: 0.5389 - accuracy: 0.80 - ETA: 0s - loss: 0.5260 - accuracy: 0.81 - ETA: 0s - loss: 0.5399 - accuracy: 0.80 - ETA: 0s - loss: 0.5280 - accuracy: 0.81 - ETA: 0s - loss: 0.5317 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5378 - accuracy: 0.8070 - val_loss: 0.6358 - val_accuracy: 0.7328\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4460 - accuracy: 0.84 - ETA: 0s - loss: 0.5296 - accuracy: 0.78 - ETA: 0s - loss: 0.5055 - accuracy: 0.80 - ETA: 0s - loss: 0.5303 - accuracy: 0.81 - ETA: 0s - loss: 0.5351 - accuracy: 0.80 - ETA: 0s - loss: 0.5209 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5167 - accuracy: 0.8197 - val_loss: 0.5527 - val_accuracy: 0.7448\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.90 - ETA: 0s - loss: 0.4843 - accuracy: 0.83 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.5161 - accuracy: 0.82 - ETA: 0s - loss: 0.5274 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5305 - accuracy: 0.8234 - val_loss: 0.6083 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.81 - ETA: 0s - loss: 0.5128 - accuracy: 0.81 - ETA: 0s - loss: 0.5292 - accuracy: 0.81 - ETA: 0s - loss: 0.5154 - accuracy: 0.81 - ETA: 0s - loss: 0.5011 - accuracy: 0.82 - ETA: 0s - loss: 0.4861 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4938 - accuracy: 0.8317 - val_loss: 0.5378 - val_accuracy: 0.7627\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.87 - ETA: 0s - loss: 0.4648 - accuracy: 0.83 - ETA: 0s - loss: 0.4827 - accuracy: 0.83 - ETA: 0s - loss: 0.5037 - accuracy: 0.82 - ETA: 0s - loss: 0.4980 - accuracy: 0.83 - ETA: 0s - loss: 0.4991 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4967 - accuracy: 0.8354 - val_loss: 0.7277 - val_accuracy: 0.7478\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.87 - ETA: 0s - loss: 0.4112 - accuracy: 0.87 - ETA: 0s - loss: 0.4393 - accuracy: 0.87 - ETA: 0s - loss: 0.4590 - accuracy: 0.85 - ETA: 0s - loss: 0.4826 - accuracy: 0.84 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4786 - accuracy: 0.8365 - val_loss: 0.7120 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4253 - accuracy: 0.90 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - ETA: 0s - loss: 0.4237 - accuracy: 0.86 - ETA: 0s - loss: 0.4167 - accuracy: 0.87 - ETA: 0s - loss: 0.4363 - accuracy: 0.86 - ETA: 0s - loss: 0.4550 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4592 - accuracy: 0.8552 - val_loss: 0.6277 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.87 - ETA: 0s - loss: 0.3991 - accuracy: 0.88 - ETA: 0s - loss: 0.4348 - accuracy: 0.85 - ETA: 0s - loss: 0.4332 - accuracy: 0.85 - ETA: 0s - loss: 0.4415 - accuracy: 0.85 - ETA: 0s - loss: 0.4566 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4644 - accuracy: 0.8514 - val_loss: 0.6404 - val_accuracy: 0.7582\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7883 - accuracy: 0.65 - ETA: 0s - loss: 0.5060 - accuracy: 0.84 - ETA: 0s - loss: 0.5024 - accuracy: 0.84 - ETA: 0s - loss: 0.5217 - accuracy: 0.83 - ETA: 0s - loss: 0.5322 - accuracy: 0.82 - ETA: 0s - loss: 0.5292 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5242 - accuracy: 0.8272 - val_loss: 0.8487 - val_accuracy: 0.7448\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.81 - ETA: 0s - loss: 0.5343 - accuracy: 0.84 - ETA: 0s - loss: 0.5317 - accuracy: 0.85 - ETA: 0s - loss: 0.6592 - accuracy: 0.85 - ETA: 0s - loss: 0.6213 - accuracy: 0.85 - ETA: 0s - loss: 0.6127 - accuracy: 0.84 - 0s 4ms/step - loss: 0.6087 - accuracy: 0.8387 - val_loss: 0.5854 - val_accuracy: 0.7493\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.87 - ETA: 0s - loss: 0.4999 - accuracy: 0.82 - ETA: 0s - loss: 0.5318 - accuracy: 0.80 - ETA: 0s - loss: 0.5381 - accuracy: 0.82 - ETA: 0s - loss: 0.5224 - accuracy: 0.82 - ETA: 0s - loss: 0.5211 - accuracy: 0.82 - 0s 4ms/step - loss: 0.6237 - accuracy: 0.8287 - val_loss: 0.5813 - val_accuracy: 0.7418\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6558 - accuracy: 0.75 - ETA: 0s - loss: 0.5139 - accuracy: 0.82 - ETA: 0s - loss: 0.4842 - accuracy: 0.83 - ETA: 0s - loss: 0.4719 - accuracy: 0.83 - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4530 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8499 - val_loss: 0.6712 - val_accuracy: 0.7343\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.96 - ETA: 0s - loss: 0.5009 - accuracy: 0.83 - ETA: 0s - loss: 0.4687 - accuracy: 0.84 - ETA: 0s - loss: 0.4396 - accuracy: 0.86 - ETA: 0s - loss: 0.4460 - accuracy: 0.85 - ETA: 0s - loss: 0.4450 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4449 - accuracy: 0.8596 - val_loss: 0.6844 - val_accuracy: 0.7537\n",
      "Epoch 18/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.2547 - accuracy: 0.93 - ETA: 0s - loss: 0.5361 - accuracy: 0.86 - ETA: 0s - loss: 0.5141 - accuracy: 0.85 - ETA: 0s - loss: 0.4788 - accuracy: 0.86 - ETA: 0s - loss: 0.4715 - accuracy: 0.86 - ETA: 0s - loss: 0.4632 - accuracy: 0.8629Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4660 - accuracy: 0.8604 - val_loss: 0.6317 - val_accuracy: 0.7448\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.75 - ETA: 0s - loss: 2.0222 - accuracy: 0.58 - ETA: 0s - loss: 1.3982 - accuracy: 0.63 - ETA: 0s - loss: 1.1233 - accuracy: 0.67 - ETA: 0s - loss: 0.9899 - accuracy: 0.68 - ETA: 0s - loss: 0.9360 - accuracy: 0.69 - 0s 5ms/step - loss: 0.9017 - accuracy: 0.6928 - val_loss: 0.5925 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.59 - ETA: 0s - loss: 0.6139 - accuracy: 0.74 - ETA: 0s - loss: 0.6091 - accuracy: 0.74 - ETA: 0s - loss: 0.5960 - accuracy: 0.73 - ETA: 0s - loss: 0.6008 - accuracy: 0.73 - ETA: 0s - loss: 0.6006 - accuracy: 0.72 - 0s 4ms/step - loss: 0.5975 - accuracy: 0.7331 - val_loss: 0.6362 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.75 - ETA: 0s - loss: 0.5602 - accuracy: 0.77 - ETA: 0s - loss: 0.5313 - accuracy: 0.76 - ETA: 0s - loss: 0.5879 - accuracy: 0.78 - ETA: 0s - loss: 0.5955 - accuracy: 0.76 - ETA: 0s - loss: 0.5789 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5765 - accuracy: 0.7753 - val_loss: 0.5957 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.90 - ETA: 0s - loss: 0.5369 - accuracy: 0.77 - ETA: 0s - loss: 0.5320 - accuracy: 0.77 - ETA: 0s - loss: 0.5192 - accuracy: 0.78 - ETA: 0s - loss: 0.5040 - accuracy: 0.79 - ETA: 0s - loss: 0.5039 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5026 - accuracy: 0.7977 - val_loss: 0.7628 - val_accuracy: 0.6701\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.78 - ETA: 0s - loss: 0.4599 - accuracy: 0.80 - ETA: 0s - loss: 0.4555 - accuracy: 0.81 - ETA: 0s - loss: 0.4627 - accuracy: 0.82 - ETA: 0s - loss: 0.4815 - accuracy: 0.81 - ETA: 0s - loss: 0.4850 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4869 - accuracy: 0.8119 - val_loss: 1.0618 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4040 - accuracy: 0.87 - ETA: 0s - loss: 0.5332 - accuracy: 0.85 - ETA: 0s - loss: 0.5602 - accuracy: 0.82 - ETA: 0s - loss: 0.5456 - accuracy: 0.81 - ETA: 0s - loss: 0.5396 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5412 - accuracy: 0.8066 - val_loss: 0.6807 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.87 - ETA: 0s - loss: 0.4518 - accuracy: 0.82 - ETA: 0s - loss: 0.5007 - accuracy: 0.81 - ETA: 0s - loss: 0.4748 - accuracy: 0.82 - ETA: 0s - loss: 0.4745 - accuracy: 0.82 - ETA: 0s - loss: 0.4757 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4780 - accuracy: 0.8238 - val_loss: 0.6635 - val_accuracy: 0.6940\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.84 - ETA: 0s - loss: 0.4010 - accuracy: 0.84 - ETA: 0s - loss: 0.4452 - accuracy: 0.83 - ETA: 0s - loss: 0.4151 - accuracy: 0.84 - ETA: 0s - loss: 0.4288 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8354 - val_loss: 0.5539 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.90 - ETA: 0s - loss: 0.4353 - accuracy: 0.86 - ETA: 0s - loss: 0.4910 - accuracy: 0.83 - ETA: 0s - loss: 0.4755 - accuracy: 0.83 - ETA: 0s - loss: 0.4700 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4593 - accuracy: 0.8365 - val_loss: 0.6594 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.93 - ETA: 0s - loss: 0.4290 - accuracy: 0.85 - ETA: 0s - loss: 0.4234 - accuracy: 0.85 - ETA: 0s - loss: 0.4109 - accuracy: 0.86 - ETA: 0s - loss: 0.4062 - accuracy: 0.86 - ETA: 0s - loss: 0.4137 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4114 - accuracy: 0.8623 - val_loss: 0.7561 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.84 - ETA: 0s - loss: 0.4305 - accuracy: 0.84 - ETA: 0s - loss: 0.4089 - accuracy: 0.85 - ETA: 0s - loss: 0.4089 - accuracy: 0.85 - ETA: 0s - loss: 0.4091 - accuracy: 0.85 - ETA: 0s - loss: 0.4104 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4020 - accuracy: 0.8649 - val_loss: 1.0621 - val_accuracy: 0.7209\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4319 - accuracy: 0.84 - ETA: 0s - loss: 0.3022 - accuracy: 0.90 - ETA: 0s - loss: 0.3564 - accuracy: 0.88 - ETA: 0s - loss: 0.3707 - accuracy: 0.88 - ETA: 0s - loss: 0.3698 - accuracy: 0.87 - ETA: 0s - loss: 0.3869 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8705 - val_loss: 0.7259 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.90 - ETA: 0s - loss: 0.4332 - accuracy: 0.87 - ETA: 0s - loss: 0.3897 - accuracy: 0.88 - ETA: 0s - loss: 0.3869 - accuracy: 0.88 - ETA: 0s - loss: 0.3956 - accuracy: 0.87 - ETA: 0s - loss: 0.4073 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4131 - accuracy: 0.8708 - val_loss: 0.8634 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.84 - ETA: 0s - loss: 0.4815 - accuracy: 0.85 - ETA: 0s - loss: 0.4461 - accuracy: 0.85 - ETA: 0s - loss: 0.3938 - accuracy: 0.87 - ETA: 0s - loss: 0.3870 - accuracy: 0.88 - ETA: 0s - loss: 0.4160 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4115 - accuracy: 0.8802 - val_loss: 1.3385 - val_accuracy: 0.7224\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.93 - ETA: 0s - loss: 0.3873 - accuracy: 0.86 - ETA: 0s - loss: 0.3675 - accuracy: 0.88 - ETA: 0s - loss: 0.3578 - accuracy: 0.88 - ETA: 0s - loss: 0.3817 - accuracy: 0.89 - ETA: 0s - loss: 0.3820 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3914 - accuracy: 0.8858 - val_loss: 0.6532 - val_accuracy: 0.7537\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.96 - ETA: 0s - loss: 0.5679 - accuracy: 0.89 - ETA: 0s - loss: 0.4789 - accuracy: 0.88 - ETA: 0s - loss: 0.4395 - accuracy: 0.88 - ETA: 0s - loss: 0.4377 - accuracy: 0.88 - ETA: 0s - loss: 0.5361 - accuracy: 0.88 - 0s 4ms/step - loss: 0.5320 - accuracy: 0.8809 - val_loss: 0.6754 - val_accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.84 - ETA: 0s - loss: 0.3957 - accuracy: 0.87 - ETA: 0s - loss: 0.3859 - accuracy: 0.88 - ETA: 0s - loss: 0.3967 - accuracy: 0.88 - ETA: 0s - loss: 0.4189 - accuracy: 0.87 - ETA: 0s - loss: 0.4107 - accuracy: 0.87 - ETA: 0s - loss: 0.4108 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4113 - accuracy: 0.8735 - val_loss: 0.6400 - val_accuracy: 0.7493\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.87 - ETA: 0s - loss: 0.3301 - accuracy: 0.91 - ETA: 0s - loss: 0.3692 - accuracy: 0.89 - ETA: 0s - loss: 0.3889 - accuracy: 0.88 - ETA: 0s - loss: 0.3969 - accuracy: 0.88 - ETA: 0s - loss: 0.3915 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3786 - accuracy: 0.8880 - val_loss: 1.0464 - val_accuracy: 0.7358\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.93 - ETA: 0s - loss: 0.3799 - accuracy: 0.89 - ETA: 0s - loss: 0.4052 - accuracy: 0.88 - ETA: 0s - loss: 0.3956 - accuracy: 0.88 - ETA: 0s - loss: 0.4429 - accuracy: 0.88 - ETA: 0s - loss: 0.4236 - accuracy: 0.89 - 0s 4ms/step - loss: 0.4117 - accuracy: 0.8914 - val_loss: 0.6870 - val_accuracy: 0.7493\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.90 - ETA: 0s - loss: 0.3772 - accuracy: 0.89 - ETA: 0s - loss: 0.3482 - accuracy: 0.90 - ETA: 0s - loss: 0.3879 - accuracy: 0.88 - ETA: 0s - loss: 0.3824 - accuracy: 0.89 - ETA: 0s - loss: 0.4086 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8944 - val_loss: 1.0767 - val_accuracy: 0.7537\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.90 - ETA: 0s - loss: 0.3065 - accuracy: 0.91 - ETA: 0s - loss: 0.4964 - accuracy: 0.90 - ETA: 0s - loss: 0.4518 - accuracy: 0.90 - ETA: 0s - loss: 0.5201 - accuracy: 0.89 - ETA: 0s - loss: 0.5378 - accuracy: 0.89 - 0s 4ms/step - loss: 0.5382 - accuracy: 0.8888 - val_loss: 0.7259 - val_accuracy: 0.7388\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.87 - ETA: 0s - loss: 0.4803 - accuracy: 0.87 - ETA: 0s - loss: 0.4439 - accuracy: 0.87 - ETA: 0s - loss: 0.4348 - accuracy: 0.87 - ETA: 0s - loss: 0.8341 - accuracy: 0.88 - ETA: 0s - loss: 0.8724 - accuracy: 0.88 - 0s 4ms/step - loss: 0.8605 - accuracy: 0.8768 - val_loss: 1.7887 - val_accuracy: 0.6806\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.75 - ETA: 0s - loss: 0.4736 - accuracy: 0.88 - ETA: 0s - loss: 0.4086 - accuracy: 0.89 - ETA: 0s - loss: 0.3983 - accuracy: 0.89 - ETA: 0s - loss: 0.4972 - accuracy: 0.89 - ETA: 0s - loss: 0.4820 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4765 - accuracy: 0.8880 - val_loss: 1.5093 - val_accuracy: 0.7299\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.90 - ETA: 0s - loss: 0.3350 - accuracy: 0.91 - ETA: 0s - loss: 0.3133 - accuracy: 0.91 - ETA: 0s - loss: 0.3363 - accuracy: 0.90 - ETA: 0s - loss: 0.3425 - accuracy: 0.90 - ETA: 0s - loss: 0.3534 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3561 - accuracy: 0.9052 - val_loss: 1.2933 - val_accuracy: 0.7418\n",
      "Epoch 25/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.3482 - accuracy: 0.90 - ETA: 0s - loss: 0.5089 - accuracy: 0.88 - ETA: 0s - loss: 0.4536 - accuracy: 0.88 - ETA: 0s - loss: 0.4389 - accuracy: 0.88 - ETA: 0s - loss: 0.4138 - accuracy: 0.89 - ETA: 0s - loss: 0.4000 - accuracy: 0.8983Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4136 - accuracy: 0.8977 - val_loss: 1.6214 - val_accuracy: 0.7194\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1866 - accuracy: 0.59 - ETA: 0s - loss: 1.7308 - accuracy: 0.55 - ETA: 0s - loss: 1.2134 - accuracy: 0.63 - ETA: 0s - loss: 1.0945 - accuracy: 0.64 - ETA: 0s - loss: 0.9844 - accuracy: 0.67 - ETA: 0s - loss: 0.9200 - accuracy: 0.68 - 0s 5ms/step - loss: 0.8807 - accuracy: 0.6894 - val_loss: 0.6145 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.71 - ETA: 0s - loss: 0.5723 - accuracy: 0.76 - ETA: 0s - loss: 0.5790 - accuracy: 0.75 - ETA: 0s - loss: 0.5684 - accuracy: 0.75 - ETA: 0s - loss: 0.5954 - accuracy: 0.74 - ETA: 0s - loss: 0.5898 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5898 - accuracy: 0.7436 - val_loss: 0.5769 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.75 - ETA: 0s - loss: 0.5561 - accuracy: 0.79 - ETA: 0s - loss: 0.5452 - accuracy: 0.79 - ETA: 0s - loss: 0.5599 - accuracy: 0.77 - ETA: 0s - loss: 0.5577 - accuracy: 0.76 - ETA: 0s - loss: 0.5580 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5522 - accuracy: 0.7738 - val_loss: 0.6534 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.75 - ETA: 0s - loss: 0.5364 - accuracy: 0.80 - ETA: 0s - loss: 0.5081 - accuracy: 0.80 - ETA: 0s - loss: 0.4981 - accuracy: 0.81 - ETA: 0s - loss: 0.5073 - accuracy: 0.80 - ETA: 0s - loss: 0.5173 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5148 - accuracy: 0.7932 - val_loss: 0.5832 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.78 - ETA: 0s - loss: 0.4470 - accuracy: 0.84 - ETA: 0s - loss: 0.4390 - accuracy: 0.84 - ETA: 0s - loss: 0.4617 - accuracy: 0.82 - ETA: 0s - loss: 0.4767 - accuracy: 0.82 - ETA: 0s - loss: 0.4988 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5016 - accuracy: 0.8093 - val_loss: 0.7887 - val_accuracy: 0.6612\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.78 - ETA: 0s - loss: 0.4585 - accuracy: 0.81 - ETA: 0s - loss: 0.4492 - accuracy: 0.83 - ETA: 0s - loss: 0.4447 - accuracy: 0.84 - ETA: 0s - loss: 0.4516 - accuracy: 0.83 - ETA: 0s - loss: 0.4568 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4598 - accuracy: 0.8283 - val_loss: 0.9155 - val_accuracy: 0.6925\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.75 - ETA: 0s - loss: 0.4731 - accuracy: 0.82 - ETA: 0s - loss: 0.4654 - accuracy: 0.84 - ETA: 0s - loss: 0.4646 - accuracy: 0.83 - ETA: 0s - loss: 0.4620 - accuracy: 0.83 - ETA: 0s - loss: 0.4669 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4679 - accuracy: 0.8317 - val_loss: 0.5503 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.81 - ETA: 0s - loss: 0.4683 - accuracy: 0.83 - ETA: 0s - loss: 0.4520 - accuracy: 0.83 - ETA: 0s - loss: 0.4583 - accuracy: 0.84 - ETA: 0s - loss: 0.4600 - accuracy: 0.84 - ETA: 0s - loss: 0.4541 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4530 - accuracy: 0.8443 - val_loss: 1.0009 - val_accuracy: 0.6910\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3760 - accuracy: 0.87 - ETA: 0s - loss: 0.4802 - accuracy: 0.81 - ETA: 0s - loss: 0.4703 - accuracy: 0.83 - ETA: 0s - loss: 0.4620 - accuracy: 0.83 - ETA: 0s - loss: 0.4405 - accuracy: 0.84 - ETA: 0s - loss: 0.4356 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8458 - val_loss: 0.8066 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6267 - accuracy: 0.78 - ETA: 0s - loss: 0.4142 - accuracy: 0.84 - ETA: 0s - loss: 0.3877 - accuracy: 0.86 - ETA: 0s - loss: 0.3977 - accuracy: 0.86 - ETA: 0s - loss: 0.4025 - accuracy: 0.86 - ETA: 0s - loss: 0.4106 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4105 - accuracy: 0.8660 - val_loss: 0.8982 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.93 - ETA: 0s - loss: 0.3052 - accuracy: 0.89 - ETA: 0s - loss: 0.8532 - accuracy: 0.86 - ETA: 0s - loss: 0.7957 - accuracy: 0.83 - ETA: 0s - loss: 0.7453 - accuracy: 0.82 - ETA: 0s - loss: 0.6943 - accuracy: 0.82 - 0s 4ms/step - loss: 0.6842 - accuracy: 0.8261 - val_loss: 0.5890 - val_accuracy: 0.7448\n",
      "Epoch 12/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 1.2368 - accuracy: 0.65 - ETA: 0s - loss: 0.4565 - accuracy: 0.86 - ETA: 0s - loss: 0.4827 - accuracy: 0.84 - ETA: 0s - loss: 0.4654 - accuracy: 0.85 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4458 - accuracy: 0.8600Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4459 - accuracy: 0.8593 - val_loss: 0.8768 - val_accuracy: 0.6881\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 97041439c1f469f0335bc42d14dd0494</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.753731350104014</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5347505163307649</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 90</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3391 - accuracy: 0.65 - ETA: 0s - loss: 9.2877 - accuracy: 0.54 - ETA: 0s - loss: 6.8774 - accuracy: 0.54 - ETA: 0s - loss: 4.8118 - accuracy: 0.60 - ETA: 0s - loss: 3.7109 - accuracy: 0.63 - ETA: 0s - loss: 3.0141 - accuracy: 0.65 - ETA: 0s - loss: 2.5768 - accuracy: 0.66 - ETA: 0s - loss: 2.2950 - accuracy: 0.67 - 1s 7ms/step - loss: 2.2616 - accuracy: 0.6775 - val_loss: 0.6139 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.71 - ETA: 0s - loss: 0.6462 - accuracy: 0.77 - ETA: 0s - loss: 0.6454 - accuracy: 0.76 - ETA: 0s - loss: 0.6963 - accuracy: 0.75 - ETA: 0s - loss: 0.6932 - accuracy: 0.74 - ETA: 0s - loss: 0.6987 - accuracy: 0.74 - ETA: 0s - loss: 0.6932 - accuracy: 0.73 - ETA: 0s - loss: 0.6931 - accuracy: 0.73 - 0s 6ms/step - loss: 0.6921 - accuracy: 0.7327 - val_loss: 0.6053 - val_accuracy: 0.7313\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9175 - accuracy: 0.65 - ETA: 0s - loss: 0.6967 - accuracy: 0.73 - ETA: 0s - loss: 0.7112 - accuracy: 0.72 - ETA: 0s - loss: 0.6913 - accuracy: 0.72 - ETA: 0s - loss: 0.6830 - accuracy: 0.72 - ETA: 0s - loss: 0.6727 - accuracy: 0.73 - ETA: 0s - loss: 0.6615 - accuracy: 0.74 - ETA: 0s - loss: 0.6734 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6713 - accuracy: 0.7353 - val_loss: 0.6108 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.68 - ETA: 0s - loss: 0.7229 - accuracy: 0.71 - ETA: 0s - loss: 0.7653 - accuracy: 0.70 - ETA: 0s - loss: 0.7365 - accuracy: 0.71 - ETA: 0s - loss: 0.7111 - accuracy: 0.72 - ETA: 0s - loss: 0.7070 - accuracy: 0.72 - ETA: 0s - loss: 0.6991 - accuracy: 0.73 - ETA: 0s - loss: 0.6907 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6898 - accuracy: 0.7339 - val_loss: 0.6129 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.71 - ETA: 0s - loss: 0.6030 - accuracy: 0.76 - ETA: 0s - loss: 0.6424 - accuracy: 0.74 - ETA: 0s - loss: 0.6591 - accuracy: 0.74 - ETA: 0s - loss: 0.6783 - accuracy: 0.73 - ETA: 0s - loss: 0.6753 - accuracy: 0.73 - ETA: 0s - loss: 0.6733 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6677 - accuracy: 0.7361 - val_loss: 0.6150 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.68 - ETA: 0s - loss: 0.5974 - accuracy: 0.77 - ETA: 0s - loss: 0.6312 - accuracy: 0.75 - ETA: 0s - loss: 0.6423 - accuracy: 0.74 - ETA: 0s - loss: 0.6517 - accuracy: 0.74 - ETA: 0s - loss: 0.6534 - accuracy: 0.74 - ETA: 0s - loss: 0.6510 - accuracy: 0.74 - ETA: 0s - loss: 0.6507 - accuracy: 0.74 - ETA: 0s - loss: 0.6814 - accuracy: 0.74 - 0s 6ms/step - loss: 0.6899 - accuracy: 0.7398 - val_loss: 0.6242 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.71 - ETA: 0s - loss: 0.6651 - accuracy: 0.75 - ETA: 0s - loss: 0.7190 - accuracy: 0.74 - ETA: 0s - loss: 0.6962 - accuracy: 0.74 - ETA: 0s - loss: 0.6881 - accuracy: 0.73 - ETA: 0s - loss: 0.6798 - accuracy: 0.74 - ETA: 0s - loss: 0.6752 - accuracy: 0.73 - ETA: 0s - loss: 0.7002 - accuracy: 0.73 - ETA: 0s - loss: 0.6969 - accuracy: 0.73 - 1s 6ms/step - loss: 0.6992 - accuracy: 0.7264 - val_loss: 0.6419 - val_accuracy: 0.7060\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.68 - ETA: 0s - loss: 0.6965 - accuracy: 0.67 - ETA: 0s - loss: 0.6870 - accuracy: 0.67 - ETA: 0s - loss: 0.6648 - accuracy: 0.68 - ETA: 0s - loss: 0.6627 - accuracy: 0.68 - ETA: 0s - loss: 0.6607 - accuracy: 0.69 - ETA: 0s - loss: 0.6520 - accuracy: 0.70 - ETA: 0s - loss: 0.6439 - accuracy: 0.71 - ETA: 0s - loss: 0.6536 - accuracy: 0.71 - 0s 6ms/step - loss: 0.6544 - accuracy: 0.7159 - val_loss: 0.6290 - val_accuracy: 0.7269\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6076 - accuracy: 0.81 - ETA: 0s - loss: 0.6507 - accuracy: 0.75 - ETA: 0s - loss: 0.6322 - accuracy: 0.76 - ETA: 0s - loss: 0.6235 - accuracy: 0.76 - ETA: 0s - loss: 0.6346 - accuracy: 0.76 - ETA: 0s - loss: 0.6736 - accuracy: 0.75 - ETA: 0s - loss: 0.6650 - accuracy: 0.75 - ETA: 0s - loss: 0.6675 - accuracy: 0.74 - ETA: 0s - loss: 0.6655 - accuracy: 0.74 - 1s 8ms/step - loss: 0.6621 - accuracy: 0.7462 - val_loss: 0.6101 - val_accuracy: 0.7403\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.5187 - accuracy: 0.65 - ETA: 0s - loss: 1.2031 - accuracy: 0.77 - ETA: 0s - loss: 1.0856 - accuracy: 0.74 - ETA: 0s - loss: 1.0220 - accuracy: 0.75 - ETA: 0s - loss: 0.9506 - accuracy: 0.73 - ETA: 0s - loss: 0.8994 - accuracy: 0.73 - ETA: 0s - loss: 0.8964 - accuracy: 0.73 - ETA: 0s - loss: 0.9079 - accuracy: 0.72 - ETA: 0s - loss: 0.8731 - accuracy: 0.72 - 0s 6ms/step - loss: 0.8677 - accuracy: 0.7264 - val_loss: 0.6259 - val_accuracy: 0.7090\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.78 - ETA: 0s - loss: 0.7945 - accuracy: 0.73 - ETA: 0s - loss: 0.7455 - accuracy: 0.72 - ETA: 0s - loss: 0.7295 - accuracy: 0.71 - ETA: 0s - loss: 0.7210 - accuracy: 0.68 - ETA: 0s - loss: 0.7162 - accuracy: 0.66 - ETA: 0s - loss: 0.7149 - accuracy: 0.65 - ETA: 0s - loss: 0.7088 - accuracy: 0.63 - ETA: 0s - loss: 0.7129 - accuracy: 0.63 - 0s 6ms/step - loss: 0.7131 - accuracy: 0.6357 - val_loss: 0.6481 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.62 - ETA: 0s - loss: 0.6750 - accuracy: 0.62 - ETA: 0s - loss: 0.6950 - accuracy: 0.57 - ETA: 0s - loss: 0.6938 - accuracy: 0.57 - ETA: 0s - loss: 0.6913 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.59 - ETA: 0s - loss: 0.7152 - accuracy: 0.60 - ETA: 0s - loss: 0.7240 - accuracy: 0.62 - ETA: 0s - loss: 0.7206 - accuracy: 0.63 - 0s 6ms/step - loss: 0.7170 - accuracy: 0.6394 - val_loss: 0.6500 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7400 - accuracy: 0.62 - ETA: 0s - loss: 0.6826 - accuracy: 0.61 - ETA: 0s - loss: 0.6839 - accuracy: 0.62 - ETA: 0s - loss: 0.6786 - accuracy: 0.61 - ETA: 0s - loss: 0.6741 - accuracy: 0.62 - ETA: 0s - loss: 0.6663 - accuracy: 0.63 - ETA: 0s - loss: 0.6739 - accuracy: 0.64 - ETA: 0s - loss: 0.6700 - accuracy: 0.65 - ETA: 0s - loss: 0.6772 - accuracy: 0.65 - 0s 6ms/step - loss: 0.6770 - accuracy: 0.6603 - val_loss: 0.6623 - val_accuracy: 0.7015\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.56 - ETA: 0s - loss: 0.8097 - accuracy: 0.69 - ETA: 0s - loss: 1.7523 - accuracy: 0.67 - ETA: 0s - loss: 1.4199 - accuracy: 0.67 - ETA: 0s - loss: 1.2467 - accuracy: 0.67 - ETA: 0s - loss: 1.1356 - accuracy: 0.65 - ETA: 0s - loss: 1.0700 - accuracy: 0.63 - ETA: 0s - loss: 1.0141 - accuracy: 0.60 - ETA: 0s - loss: 0.9707 - accuracy: 0.59 - 0s 6ms/step - loss: 0.9480 - accuracy: 0.5984 - val_loss: 0.6688 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7881 - accuracy: 0.46 - ETA: 0s - loss: 0.6941 - accuracy: 0.55 - ETA: 0s - loss: 0.6959 - accuracy: 0.57 - ETA: 0s - loss: 0.7018 - accuracy: 0.57 - ETA: 0s - loss: 0.7059 - accuracy: 0.56 - ETA: 0s - loss: 0.6999 - accuracy: 0.55 - ETA: 0s - loss: 0.6938 - accuracy: 0.55 - ETA: 0s - loss: 0.6963 - accuracy: 0.55 - ETA: 0s - loss: 0.6964 - accuracy: 0.56 - 0s 6ms/step - loss: 0.6961 - accuracy: 0.5633 - val_loss: 0.6742 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.59 - ETA: 0s - loss: 0.6991 - accuracy: 0.61 - ETA: 0s - loss: 0.6877 - accuracy: 0.62 - ETA: 0s - loss: 0.6905 - accuracy: 0.62 - ETA: 0s - loss: 0.6913 - accuracy: 0.61 - ETA: 0s - loss: 0.6955 - accuracy: 0.60 - ETA: 0s - loss: 0.6954 - accuracy: 0.59 - ETA: 0s - loss: 0.6940 - accuracy: 0.58 - ETA: 0s - loss: 0.6927 - accuracy: 0.58 - 0s 6ms/step - loss: 0.6937 - accuracy: 0.5801 - val_loss: 0.6800 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6389 - accuracy: 0.65 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6926 - accuracy: 0.60 - ETA: 0s - loss: 0.6875 - accuracy: 0.60 - ETA: 0s - loss: 0.6891 - accuracy: 0.60 - ETA: 0s - loss: 0.6887 - accuracy: 0.60 - ETA: 0s - loss: 0.6888 - accuracy: 0.59 - ETA: 0s - loss: 0.6915 - accuracy: 0.58 - ETA: 0s - loss: 0.6915 - accuracy: 0.58 - ETA: 0s - loss: 0.6899 - accuracy: 0.58 - ETA: 0s - loss: 0.6937 - accuracy: 0.58 - 1s 8ms/step - loss: 0.6930 - accuracy: 0.5793 - val_loss: 0.6897 - val_accuracy: 0.6985\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.40 - ETA: 0s - loss: 0.6688 - accuracy: 0.52 - ETA: 0s - loss: 0.6748 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6868 - accuracy: 0.58 - ETA: 0s - loss: 0.6900 - accuracy: 0.58 - ETA: 0s - loss: 0.6929 - accuracy: 0.57 - 1s 7ms/step - loss: 0.6929 - accuracy: 0.5763 - val_loss: 0.6950 - val_accuracy: 0.3045\n",
      "Epoch 19/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6473 - accuracy: 0.40 - ETA: 0s - loss: 0.7849 - accuracy: 0.40 - ETA: 0s - loss: 0.7445 - accuracy: 0.34 - ETA: 0s - loss: 0.7156 - accuracy: 0.33 - ETA: 0s - loss: 0.7026 - accuracy: 0.34 - ETA: 0s - loss: 0.7028 - accuracy: 0.39 - ETA: 0s - loss: 0.6938 - accuracy: 0.42 - ETA: 0s - loss: 0.6965 - accuracy: 0.46 - ETA: 0s - loss: 0.7002 - accuracy: 0.47 - ETA: 0s - loss: 0.7016 - accuracy: 0.4755Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.7012 - accuracy: 0.4748 - val_loss: 0.6921 - val_accuracy: 0.6985\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9303 - accuracy: 0.62 - ETA: 0s - loss: 8.5703 - accuracy: 0.60 - ETA: 0s - loss: 5.6017 - accuracy: 0.61 - ETA: 0s - loss: 4.3239 - accuracy: 0.62 - ETA: 0s - loss: 3.5037 - accuracy: 0.63 - ETA: 0s - loss: 2.9943 - accuracy: 0.64 - ETA: 0s - loss: 2.6322 - accuracy: 0.66 - ETA: 0s - loss: 2.3778 - accuracy: 0.66 - ETA: 0s - loss: 2.1983 - accuracy: 0.67 - ETA: 0s - loss: 2.0494 - accuracy: 0.67 - 1s 8ms/step - loss: 2.0409 - accuracy: 0.6790 - val_loss: 0.6225 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7533 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.75 - ETA: 0s - loss: 0.6362 - accuracy: 0.75 - ETA: 0s - loss: 0.6570 - accuracy: 0.73 - ETA: 0s - loss: 0.6621 - accuracy: 0.73 - ETA: 0s - loss: 0.6650 - accuracy: 0.73 - ETA: 0s - loss: 0.6662 - accuracy: 0.73 - ETA: 0s - loss: 0.6603 - accuracy: 0.74 - ETA: 0s - loss: 0.6567 - accuracy: 0.74 - 0s 6ms/step - loss: 0.6567 - accuracy: 0.7432 - val_loss: 0.6065 - val_accuracy: 0.7373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8551 - accuracy: 0.65 - ETA: 0s - loss: 0.6923 - accuracy: 0.73 - ETA: 0s - loss: 0.6592 - accuracy: 0.75 - ETA: 0s - loss: 0.6506 - accuracy: 0.74 - ETA: 0s - loss: 0.6575 - accuracy: 0.75 - ETA: 0s - loss: 0.6645 - accuracy: 0.74 - ETA: 0s - loss: 0.6623 - accuracy: 0.74 - ETA: 0s - loss: 0.6527 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6494 - accuracy: 0.7503 - val_loss: 0.5993 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7678 - accuracy: 0.65 - ETA: 0s - loss: 0.6562 - accuracy: 0.75 - ETA: 0s - loss: 0.6443 - accuracy: 0.74 - ETA: 0s - loss: 0.6310 - accuracy: 0.75 - ETA: 0s - loss: 0.6324 - accuracy: 0.75 - ETA: 0s - loss: 0.6341 - accuracy: 0.75 - ETA: 0s - loss: 0.6391 - accuracy: 0.75 - ETA: 0s - loss: 0.6320 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6308 - accuracy: 0.7615 - val_loss: 0.6123 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.71 - ETA: 0s - loss: 0.6382 - accuracy: 0.75 - ETA: 0s - loss: 0.6122 - accuracy: 0.77 - ETA: 0s - loss: 0.6241 - accuracy: 0.76 - ETA: 0s - loss: 0.6317 - accuracy: 0.76 - ETA: 0s - loss: 0.6274 - accuracy: 0.76 - ETA: 0s - loss: 0.6250 - accuracy: 0.76 - ETA: 0s - loss: 0.6263 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6263 - accuracy: 0.7615 - val_loss: 0.6003 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.87 - ETA: 0s - loss: 0.6191 - accuracy: 0.76 - ETA: 0s - loss: 0.6198 - accuracy: 0.76 - ETA: 0s - loss: 0.6130 - accuracy: 0.77 - ETA: 0s - loss: 0.6238 - accuracy: 0.76 - ETA: 0s - loss: 0.6303 - accuracy: 0.75 - ETA: 0s - loss: 0.6239 - accuracy: 0.76 - 0s 6ms/step - loss: 0.6321 - accuracy: 0.7600 - val_loss: 0.5952 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.84 - ETA: 0s - loss: 0.6739 - accuracy: 0.77 - ETA: 0s - loss: 0.6662 - accuracy: 0.76 - ETA: 0s - loss: 0.6817 - accuracy: 0.76 - ETA: 0s - loss: 0.6673 - accuracy: 0.76 - ETA: 0s - loss: 0.6449 - accuracy: 0.77 - ETA: 0s - loss: 0.6416 - accuracy: 0.77 - ETA: 0s - loss: 0.6506 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6506 - accuracy: 0.7693 - val_loss: 0.6247 - val_accuracy: 0.6985\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.81 - ETA: 0s - loss: 0.7964 - accuracy: 0.73 - ETA: 0s - loss: 0.7272 - accuracy: 0.75 - ETA: 0s - loss: 0.7795 - accuracy: 0.74 - ETA: 0s - loss: 0.7609 - accuracy: 0.73 - ETA: 0s - loss: 0.7749 - accuracy: 0.74 - ETA: 0s - loss: 0.7631 - accuracy: 0.73 - ETA: 0s - loss: 0.7465 - accuracy: 0.73 - 0s 5ms/step - loss: 0.7398 - accuracy: 0.7353 - val_loss: 0.6269 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7675 - accuracy: 0.62 - ETA: 0s - loss: 0.7973 - accuracy: 0.68 - ETA: 0s - loss: 0.7686 - accuracy: 0.66 - ETA: 0s - loss: 0.7325 - accuracy: 0.69 - ETA: 0s - loss: 0.7354 - accuracy: 0.69 - ETA: 0s - loss: 0.7340 - accuracy: 0.66 - ETA: 0s - loss: 0.7222 - accuracy: 0.64 - ETA: 0s - loss: 0.7151 - accuracy: 0.64 - 0s 5ms/step - loss: 0.7151 - accuracy: 0.6480 - val_loss: 0.6485 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.78 - ETA: 0s - loss: 0.6916 - accuracy: 0.64 - ETA: 0s - loss: 0.6924 - accuracy: 0.64 - ETA: 0s - loss: 0.6949 - accuracy: 0.62 - ETA: 0s - loss: 0.6825 - accuracy: 0.61 - ETA: 0s - loss: 0.6890 - accuracy: 0.61 - ETA: 0s - loss: 0.6918 - accuracy: 0.62 - ETA: 0s - loss: 0.6943 - accuracy: 0.61 - 0s 5ms/step - loss: 0.6933 - accuracy: 0.6054 - val_loss: 0.6653 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.59 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6992 - accuracy: 0.57 - ETA: 0s - loss: 0.6957 - accuracy: 0.56 - ETA: 0s - loss: 0.6901 - accuracy: 0.57 - ETA: 0s - loss: 0.6890 - accuracy: 0.58 - ETA: 0s - loss: 0.6900 - accuracy: 0.59 - ETA: 0s - loss: 0.6930 - accuracy: 0.59 - 0s 5ms/step - loss: 0.6916 - accuracy: 0.6021 - val_loss: 0.6695 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6621 - accuracy: 0.56 - ETA: 0s - loss: 0.6905 - accuracy: 0.57 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.56 - ETA: 0s - loss: 0.6864 - accuracy: 0.57 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5763 - val_loss: 0.6723 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.56 - ETA: 0s - loss: 0.7066 - accuracy: 0.53 - ETA: 0s - loss: 0.7030 - accuracy: 0.51 - ETA: 0s - loss: 0.6964 - accuracy: 0.49 - ETA: 0s - loss: 0.6993 - accuracy: 0.49 - ETA: 0s - loss: 0.6951 - accuracy: 0.49 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5409 - val_loss: 0.6687 - val_accuracy: 0.6985\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7327 - accuracy: 0.62 - ETA: 0s - loss: 0.6956 - accuracy: 0.66 - ETA: 0s - loss: 0.7091 - accuracy: 0.64 - ETA: 0s - loss: 0.6984 - accuracy: 0.63 - ETA: 0s - loss: 0.6927 - accuracy: 0.62 - ETA: 0s - loss: 0.6954 - accuracy: 0.62 - ETA: 0s - loss: 0.6904 - accuracy: 0.62 - ETA: 0s - loss: 0.6929 - accuracy: 0.63 - 0s 5ms/step - loss: 0.6926 - accuracy: 0.6323 - val_loss: 0.6785 - val_accuracy: 0.6985\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.68 - ETA: 0s - loss: 0.6872 - accuracy: 0.60 - ETA: 0s - loss: 0.6879 - accuracy: 0.59 - ETA: 0s - loss: 0.6852 - accuracy: 0.60 - ETA: 0s - loss: 0.6894 - accuracy: 0.61 - ETA: 0s - loss: 0.6893 - accuracy: 0.61 - ETA: 0s - loss: 0.6899 - accuracy: 0.60 - ETA: 0s - loss: 0.6906 - accuracy: 0.59 - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5939 - val_loss: 0.6795 - val_accuracy: 0.6985\n",
      "Epoch 16/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.7176 - accuracy: 0.65 - ETA: 0s - loss: 0.6938 - accuracy: 0.60 - ETA: 0s - loss: 0.6878 - accuracy: 0.61 - ETA: 0s - loss: 0.6954 - accuracy: 0.58 - ETA: 0s - loss: 0.6893 - accuracy: 0.57 - ETA: 0s - loss: 0.6920 - accuracy: 0.57 - ETA: 0s - loss: 0.6908 - accuracy: 0.58 - ETA: 0s - loss: 0.6902 - accuracy: 0.5867Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5939 - val_loss: 0.6766 - val_accuracy: 0.6985\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0087 - accuracy: 0.62 - ETA: 0s - loss: 7.9005 - accuracy: 0.60 - ETA: 0s - loss: 5.0458 - accuracy: 0.61 - ETA: 0s - loss: 3.7405 - accuracy: 0.63 - ETA: 0s - loss: 3.0249 - accuracy: 0.64 - ETA: 0s - loss: 2.5603 - accuracy: 0.66 - ETA: 0s - loss: 2.2221 - accuracy: 0.67 - ETA: 0s - loss: 2.0045 - accuracy: 0.68 - 1s 7ms/step - loss: 1.9276 - accuracy: 0.6894 - val_loss: 0.5906 - val_accuracy: 0.7433\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.68 - ETA: 0s - loss: 0.6254 - accuracy: 0.75 - ETA: 0s - loss: 0.6594 - accuracy: 0.73 - ETA: 0s - loss: 0.6589 - accuracy: 0.73 - ETA: 0s - loss: 0.6533 - accuracy: 0.73 - ETA: 0s - loss: 0.6701 - accuracy: 0.72 - ETA: 0s - loss: 0.6587 - accuracy: 0.72 - ETA: 0s - loss: 0.6655 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6652 - accuracy: 0.7301 - val_loss: 0.6017 - val_accuracy: 0.7418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7592 - accuracy: 0.59 - ETA: 0s - loss: 0.6416 - accuracy: 0.75 - ETA: 0s - loss: 0.6246 - accuracy: 0.75 - ETA: 0s - loss: 0.6539 - accuracy: 0.74 - ETA: 0s - loss: 0.6750 - accuracy: 0.74 - ETA: 0s - loss: 0.6668 - accuracy: 0.74 - ETA: 0s - loss: 0.6648 - accuracy: 0.74 - ETA: 0s - loss: 0.6549 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6545 - accuracy: 0.7477 - val_loss: 0.5912 - val_accuracy: 0.7358\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.62 - ETA: 0s - loss: 0.6697 - accuracy: 0.71 - ETA: 0s - loss: 0.6573 - accuracy: 0.73 - ETA: 0s - loss: 0.6590 - accuracy: 0.73 - ETA: 0s - loss: 0.6657 - accuracy: 0.73 - ETA: 0s - loss: 0.6664 - accuracy: 0.73 - ETA: 0s - loss: 0.6708 - accuracy: 0.73 - ETA: 0s - loss: 0.6653 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6643 - accuracy: 0.7361 - val_loss: 0.6274 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.87 - ETA: 0s - loss: 0.6299 - accuracy: 0.79 - ETA: 0s - loss: 0.6735 - accuracy: 0.76 - ETA: 0s - loss: 0.6575 - accuracy: 0.76 - ETA: 0s - loss: 0.6626 - accuracy: 0.76 - ETA: 0s - loss: 0.6646 - accuracy: 0.75 - ETA: 0s - loss: 0.6668 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6730 - accuracy: 0.7521 - val_loss: 0.6372 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.75 - ETA: 0s - loss: 0.6696 - accuracy: 0.76 - ETA: 0s - loss: 0.7509 - accuracy: 0.75 - ETA: 0s - loss: 0.7275 - accuracy: 0.74 - ETA: 0s - loss: 0.7224 - accuracy: 0.73 - ETA: 0s - loss: 0.7188 - accuracy: 0.74 - ETA: 0s - loss: 0.7132 - accuracy: 0.73 - ETA: 0s - loss: 0.7104 - accuracy: 0.73 - ETA: 0s - loss: 0.7173 - accuracy: 0.72 - ETA: 0s - loss: 0.7039 - accuracy: 0.73 - ETA: 0s - loss: 0.7016 - accuracy: 0.73 - 1s 8ms/step - loss: 0.6991 - accuracy: 0.7368 - val_loss: 0.5968 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.81 - ETA: 0s - loss: 0.7282 - accuracy: 0.73 - ETA: 0s - loss: 0.7531 - accuracy: 0.73 - ETA: 0s - loss: 0.7304 - accuracy: 0.74 - ETA: 0s - loss: 0.7201 - accuracy: 0.74 - ETA: 0s - loss: 0.7108 - accuracy: 0.74 - ETA: 0s - loss: 0.7073 - accuracy: 0.74 - ETA: 0s - loss: 0.7167 - accuracy: 0.73 - ETA: 0s - loss: 0.7236 - accuracy: 0.72 - 0s 6ms/step - loss: 0.7236 - accuracy: 0.7256 - val_loss: 0.6273 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.1550 - accuracy: 0.37 - ETA: 0s - loss: 0.7717 - accuracy: 0.62 - ETA: 0s - loss: 0.7926 - accuracy: 0.65 - ETA: 0s - loss: 0.7795 - accuracy: 0.65 - ETA: 0s - loss: 0.7623 - accuracy: 0.66 - ETA: 0s - loss: 0.7446 - accuracy: 0.67 - ETA: 0s - loss: 0.7545 - accuracy: 0.68 - ETA: 0s - loss: 0.7505 - accuracy: 0.68 - ETA: 0s - loss: 0.7419 - accuracy: 0.67 - 1s 6ms/step - loss: 0.7464 - accuracy: 0.6626 - val_loss: 0.6628 - val_accuracy: 0.6970\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6220 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.64 - ETA: 0s - loss: 0.6593 - accuracy: 0.65 - ETA: 0s - loss: 0.6763 - accuracy: 0.66 - ETA: 0s - loss: 0.6730 - accuracy: 0.67 - ETA: 0s - loss: 0.6769 - accuracy: 0.67 - ETA: 0s - loss: 0.6747 - accuracy: 0.67 - ETA: 0s - loss: 0.6808 - accuracy: 0.67 - ETA: 0s - loss: 0.6835 - accuracy: 0.67 - 1s 7ms/step - loss: 0.6836 - accuracy: 0.6667 - val_loss: 0.6558 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.65 - ETA: 0s - loss: 0.7225 - accuracy: 0.63 - ETA: 0s - loss: 0.6845 - accuracy: 0.63 - ETA: 0s - loss: 0.6755 - accuracy: 0.63 - ETA: 0s - loss: 0.6838 - accuracy: 0.62 - ETA: 0s - loss: 0.6822 - accuracy: 0.63 - ETA: 0s - loss: 0.6811 - accuracy: 0.63 - ETA: 0s - loss: 0.6727 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.64 - 1s 6ms/step - loss: 0.6733 - accuracy: 0.6469 - val_loss: 0.6489 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6908 - accuracy: 0.68 - ETA: 0s - loss: 0.7039 - accuracy: 0.68 - ETA: 0s - loss: 0.6962 - accuracy: 0.69 - ETA: 0s - loss: 0.7093 - accuracy: 0.67 - ETA: 0s - loss: 0.7047 - accuracy: 0.66 - ETA: 0s - loss: 0.6989 - accuracy: 0.65 - ETA: 0s - loss: 0.7030 - accuracy: 0.64 - ETA: 0s - loss: 0.6991 - accuracy: 0.61 - ETA: 0s - loss: 0.6990 - accuracy: 0.61 - ETA: 0s - loss: 0.7001 - accuracy: 0.61 - ETA: 0s - loss: 0.6986 - accuracy: 0.61 - ETA: 0s - loss: 0.6976 - accuracy: 0.6102Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6949 - accuracy: 0.6125 - val_loss: 0.6626 - val_accuracy: 0.6985\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 2df3e5c9d56f6f71b555ef809747fdc8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7417910496393839</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9332903414679479</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 70</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9009 - accuracy: 0.81 - ETA: 0s - loss: 6.0638 - accuracy: 0.59 - ETA: 0s - loss: 5.2231 - accuracy: 0.60 - ETA: 0s - loss: 4.3955 - accuracy: 0.56 - ETA: 0s - loss: 3.3777 - accuracy: 0.59 - ETA: 0s - loss: 2.9137 - accuracy: 0.60 - ETA: 0s - loss: 2.6247 - accuracy: 0.62 - ETA: 0s - loss: 2.4929 - accuracy: 0.63 - ETA: 0s - loss: 2.4161 - accuracy: 0.63 - ETA: 0s - loss: 2.3336 - accuracy: 0.62 - ETA: 0s - loss: 2.2233 - accuracy: 0.62 - 1s 12ms/step - loss: 2.1252 - accuracy: 0.6312 - val_loss: 0.6060 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.78 - ETA: 0s - loss: 0.9080 - accuracy: 0.70 - ETA: 0s - loss: 0.9028 - accuracy: 0.68 - ETA: 0s - loss: 0.8568 - accuracy: 0.67 - ETA: 0s - loss: 0.8535 - accuracy: 0.67 - ETA: 0s - loss: 0.8480 - accuracy: 0.67 - ETA: 0s - loss: 0.8453 - accuracy: 0.67 - ETA: 0s - loss: 0.8299 - accuracy: 0.66 - ETA: 0s - loss: 0.8137 - accuracy: 0.66 - ETA: 0s - loss: 0.7952 - accuracy: 0.68 - ETA: 0s - loss: 0.7755 - accuracy: 0.68 - 1s 8ms/step - loss: 0.7782 - accuracy: 0.6842 - val_loss: 0.6169 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7958 - accuracy: 0.65 - ETA: 0s - loss: 0.6172 - accuracy: 0.71 - ETA: 0s - loss: 0.6370 - accuracy: 0.71 - ETA: 0s - loss: 0.6520 - accuracy: 0.71 - ETA: 0s - loss: 0.6611 - accuracy: 0.71 - ETA: 0s - loss: 0.6542 - accuracy: 0.71 - ETA: 0s - loss: 0.7099 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7109 - accuracy: 0.7107 - val_loss: 0.6167 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7757 - accuracy: 0.59 - ETA: 0s - loss: 0.8593 - accuracy: 0.69 - ETA: 0s - loss: 0.8449 - accuracy: 0.69 - ETA: 0s - loss: 0.8315 - accuracy: 0.69 - ETA: 0s - loss: 0.8182 - accuracy: 0.69 - ETA: 0s - loss: 0.7973 - accuracy: 0.69 - ETA: 0s - loss: 0.8135 - accuracy: 0.69 - 0s 5ms/step - loss: 0.8113 - accuracy: 0.6935 - val_loss: 0.6112 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1415 - accuracy: 0.71 - ETA: 0s - loss: 0.7937 - accuracy: 0.67 - ETA: 0s - loss: 0.7657 - accuracy: 0.70 - ETA: 0s - loss: 1.0029 - accuracy: 0.72 - ETA: 0s - loss: 0.8842 - accuracy: 0.73 - ETA: 0s - loss: 0.8045 - accuracy: 0.74 - ETA: 0s - loss: 0.9152 - accuracy: 0.72 - ETA: 0s - loss: 0.9103 - accuracy: 0.72 - ETA: 0s - loss: 0.8939 - accuracy: 0.72 - 1s 6ms/step - loss: 0.9073 - accuracy: 0.7256 - val_loss: 0.6230 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6605 - accuracy: 0.65 - ETA: 0s - loss: 0.7620 - accuracy: 0.74 - ETA: 0s - loss: 0.9131 - accuracy: 0.73 - ETA: 0s - loss: 0.9405 - accuracy: 0.71 - ETA: 0s - loss: 0.9436 - accuracy: 0.72 - ETA: 0s - loss: 0.8747 - accuracy: 0.72 - ETA: 0s - loss: 0.8512 - accuracy: 0.72 - 0s 5ms/step - loss: 0.8309 - accuracy: 0.7260 - val_loss: 0.7224 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.68 - ETA: 0s - loss: 0.6619 - accuracy: 0.74 - ETA: 0s - loss: 0.7132 - accuracy: 0.76 - ETA: 0s - loss: 0.7370 - accuracy: 0.76 - ETA: 0s - loss: 0.7733 - accuracy: 0.75 - ETA: 0s - loss: 0.7559 - accuracy: 0.75 - ETA: 0s - loss: 0.7612 - accuracy: 0.75 - 1s 6ms/step - loss: 0.7581 - accuracy: 0.7622 - val_loss: 0.6254 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.81 - ETA: 0s - loss: 0.6979 - accuracy: 0.78 - ETA: 0s - loss: 0.6710 - accuracy: 0.76 - ETA: 0s - loss: 0.6498 - accuracy: 0.76 - ETA: 0s - loss: 0.6456 - accuracy: 0.76 - ETA: 0s - loss: 0.6429 - accuracy: 0.76 - ETA: 0s - loss: 0.6480 - accuracy: 0.76 - ETA: 0s - loss: 0.6752 - accuracy: 0.76 - ETA: 0s - loss: 0.6875 - accuracy: 0.76 - 1s 7ms/step - loss: 0.6854 - accuracy: 0.7648 - val_loss: 0.6363 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.84 - ETA: 0s - loss: 0.5748 - accuracy: 0.80 - ETA: 0s - loss: 0.6209 - accuracy: 0.80 - ETA: 0s - loss: 0.6182 - accuracy: 0.80 - ETA: 0s - loss: 0.6561 - accuracy: 0.79 - ETA: 0s - loss: 0.6905 - accuracy: 0.78 - ETA: 0s - loss: 0.6957 - accuracy: 0.77 - ETA: 0s - loss: 0.7008 - accuracy: 0.77 - ETA: 0s - loss: 0.7116 - accuracy: 0.77 - ETA: 0s - loss: 0.7003 - accuracy: 0.76 - ETA: 0s - loss: 0.6929 - accuracy: 0.77 - 1s 8ms/step - loss: 0.6929 - accuracy: 0.7704 - val_loss: 0.6335 - val_accuracy: 0.7418\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.81 - ETA: 0s - loss: 0.7145 - accuracy: 0.78 - ETA: 0s - loss: 0.6869 - accuracy: 0.78 - ETA: 0s - loss: 0.6795 - accuracy: 0.78 - ETA: 0s - loss: 0.6832 - accuracy: 0.78 - ETA: 0s - loss: 0.6780 - accuracy: 0.77 - ETA: 0s - loss: 0.6504 - accuracy: 0.78 - ETA: 0s - loss: 0.6419 - accuracy: 0.78 - 0s 6ms/step - loss: 0.6349 - accuracy: 0.7898 - val_loss: 0.7809 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.75 - ETA: 0s - loss: 0.5394 - accuracy: 0.80 - ETA: 0s - loss: 0.5617 - accuracy: 0.80 - ETA: 0s - loss: 0.6003 - accuracy: 0.81 - ETA: 0s - loss: 0.5957 - accuracy: 0.80 - ETA: 0s - loss: 0.5929 - accuracy: 0.80 - ETA: 0s - loss: 0.6153 - accuracy: 0.80 - ETA: 0s - loss: 0.6081 - accuracy: 0.80 - ETA: 0s - loss: 0.6464 - accuracy: 0.80 - 1s 6ms/step - loss: 0.6464 - accuracy: 0.8037 - val_loss: 0.7366 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4820 - accuracy: 0.84 - ETA: 0s - loss: 0.6405 - accuracy: 0.77 - ETA: 0s - loss: 0.7581 - accuracy: 0.77 - ETA: 0s - loss: 0.7456 - accuracy: 0.77 - ETA: 0s - loss: 0.7049 - accuracy: 0.78 - ETA: 0s - loss: 0.6862 - accuracy: 0.78 - ETA: 0s - loss: 0.6769 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6710 - accuracy: 0.7857 - val_loss: 0.8605 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1986 - accuracy: 0.68 - ETA: 0s - loss: 0.6382 - accuracy: 0.80 - ETA: 0s - loss: 0.8051 - accuracy: 0.80 - ETA: 0s - loss: 0.7581 - accuracy: 0.80 - ETA: 0s - loss: 0.7191 - accuracy: 0.80 - ETA: 0s - loss: 0.6805 - accuracy: 0.80 - ETA: 0s - loss: 0.7620 - accuracy: 0.79 - 0s 6ms/step - loss: 0.7442 - accuracy: 0.7928 - val_loss: 0.8280 - val_accuracy: 0.7522\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5349 - accuracy: 0.81 - ETA: 0s - loss: 0.6229 - accuracy: 0.78 - ETA: 0s - loss: 0.5885 - accuracy: 0.80 - ETA: 0s - loss: 0.5867 - accuracy: 0.80 - ETA: 0s - loss: 0.6727 - accuracy: 0.79 - ETA: 0s - loss: 0.6531 - accuracy: 0.79 - ETA: 0s - loss: 0.6884 - accuracy: 0.79 - ETA: 0s - loss: 0.8901 - accuracy: 0.79 - 0s 6ms/step - loss: 0.8850 - accuracy: 0.7887 - val_loss: 0.7920 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3249 - accuracy: 0.78 - ETA: 0s - loss: 1.4778 - accuracy: 0.77 - ETA: 0s - loss: 2.0484 - accuracy: 0.75 - ETA: 0s - loss: 1.8863 - accuracy: 0.75 - ETA: 0s - loss: 1.8015 - accuracy: 0.75 - ETA: 0s - loss: 1.6596 - accuracy: 0.75 - ETA: 0s - loss: 1.6485 - accuracy: 0.75 - ETA: 0s - loss: 1.5536 - accuracy: 0.75 - ETA: 0s - loss: 1.4516 - accuracy: 0.75 - ETA: 0s - loss: 1.4166 - accuracy: 0.76 - ETA: 0s - loss: 1.3975 - accuracy: 0.75 - ETA: 0s - loss: 1.3191 - accuracy: 0.75 - ETA: 0s - loss: 1.3019 - accuracy: 0.75 - 1s 9ms/step - loss: 1.2887 - accuracy: 0.7555 - val_loss: 0.6792 - val_accuracy: 0.7388\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.81 - ETA: 0s - loss: 1.1145 - accuracy: 0.77 - ETA: 0s - loss: 1.0619 - accuracy: 0.78 - ETA: 0s - loss: 1.0178 - accuracy: 0.75 - ETA: 0s - loss: 0.8764 - accuracy: 0.76 - ETA: 0s - loss: 0.8154 - accuracy: 0.77 - ETA: 0s - loss: 1.0649 - accuracy: 0.77 - ETA: 0s - loss: 0.9797 - accuracy: 0.78 - ETA: 0s - loss: 0.9599 - accuracy: 0.77 - ETA: 0s - loss: 0.9574 - accuracy: 0.77 - ETA: 0s - loss: 0.9757 - accuracy: 0.77 - 1s 8ms/step - loss: 0.9712 - accuracy: 0.7749 - val_loss: 0.6834 - val_accuracy: 0.7463\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8978 - accuracy: 0.68 - ETA: 0s - loss: 1.5683 - accuracy: 0.70 - ETA: 0s - loss: 1.5461 - accuracy: 0.72 - ETA: 0s - loss: 1.2448 - accuracy: 0.73 - ETA: 0s - loss: 1.2524 - accuracy: 0.73 - ETA: 0s - loss: 1.1070 - accuracy: 0.74 - ETA: 0s - loss: 1.1919 - accuracy: 0.75 - ETA: 0s - loss: 1.1843 - accuracy: 0.74 - ETA: 0s - loss: 1.1556 - accuracy: 0.75 - 1s 6ms/step - loss: 1.1312 - accuracy: 0.7469 - val_loss: 0.6930 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2050 - accuracy: 0.75 - ETA: 0s - loss: 0.7452 - accuracy: 0.72 - ETA: 0s - loss: 0.8948 - accuracy: 0.70 - ETA: 0s - loss: 0.9736 - accuracy: 0.72 - ETA: 0s - loss: 1.1301 - accuracy: 0.73 - ETA: 0s - loss: 1.1175 - accuracy: 0.72 - ETA: 0s - loss: 1.1352 - accuracy: 0.72 - ETA: 0s - loss: 1.0807 - accuracy: 0.73 - ETA: 0s - loss: 1.0755 - accuracy: 0.73 - ETA: 0s - loss: 1.0775 - accuracy: 0.73 - ETA: 0s - loss: 1.0697 - accuracy: 0.73 - 1s 8ms/step - loss: 1.0734 - accuracy: 0.7339 - val_loss: 0.7008 - val_accuracy: 0.7388\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.78 - ETA: 0s - loss: 0.7025 - accuracy: 0.75 - ETA: 0s - loss: 0.6687 - accuracy: 0.75 - ETA: 0s - loss: 1.3680 - accuracy: 0.75 - ETA: 0s - loss: 1.1747 - accuracy: 0.75 - ETA: 0s - loss: 1.2154 - accuracy: 0.75 - ETA: 0s - loss: 1.2524 - accuracy: 0.76 - ETA: 0s - loss: 1.2376 - accuracy: 0.76 - ETA: 0s - loss: 1.2307 - accuracy: 0.76 - ETA: 0s - loss: 1.2598 - accuracy: 0.75 - ETA: 0s - loss: 1.2745 - accuracy: 0.75 - ETA: 0s - loss: 1.2927 - accuracy: 0.75 - ETA: 0s - loss: 1.4486 - accuracy: 0.75 - 1s 9ms/step - loss: 1.4523 - accuracy: 0.7563 - val_loss: 0.8917 - val_accuracy: 0.7418\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.78 - ETA: 0s - loss: 2.4725 - accuracy: 0.77 - ETA: 0s - loss: 1.9966 - accuracy: 0.75 - ETA: 0s - loss: 1.6415 - accuracy: 0.74 - ETA: 0s - loss: 1.4587 - accuracy: 0.74 - ETA: 0s - loss: 1.3142 - accuracy: 0.74 - ETA: 0s - loss: 1.5215 - accuracy: 0.73 - 0s 5ms/step - loss: 1.4493 - accuracy: 0.7294 - val_loss: 0.7242 - val_accuracy: 0.6955\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6971 - accuracy: 0.70 - ETA: 0s - loss: 1.4227 - accuracy: 0.72 - ETA: 0s - loss: 1.4860 - accuracy: 0.71 - ETA: 0s - loss: 1.3420 - accuracy: 0.72 - ETA: 0s - loss: 1.2530 - accuracy: 0.72 - ETA: 0s - loss: 1.2419 - accuracy: 0.71 - ETA: 0s - loss: 1.2809 - accuracy: 0.71 - ETA: 0s - loss: 1.2212 - accuracy: 0.70 - ETA: 0s - loss: 1.1774 - accuracy: 0.71 - ETA: 0s - loss: 1.1448 - accuracy: 0.71 - 1s 8ms/step - loss: 1.1448 - accuracy: 0.7100 - val_loss: 0.7073 - val_accuracy: 0.6985\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7013 - accuracy: 0.68 - ETA: 0s - loss: 0.8502 - accuracy: 0.66 - ETA: 0s - loss: 1.7955 - accuracy: 0.65 - ETA: 0s - loss: 1.6672 - accuracy: 0.55 - ETA: 0s - loss: 1.4571 - accuracy: 0.48 - ETA: 0s - loss: 1.3282 - accuracy: 0.44 - ETA: 0s - loss: 2.1527 - accuracy: 0.42 - ETA: 0s - loss: 1.9871 - accuracy: 0.41 - ETA: 0s - loss: 1.8787 - accuracy: 0.41 - ETA: 0s - loss: 1.6940 - accuracy: 0.46 - ETA: 0s - loss: 1.5558 - accuracy: 0.51 - ETA: 0s - loss: 1.4754 - accuracy: 0.53 - 1s 8ms/step - loss: 1.4567 - accuracy: 0.5431 - val_loss: 0.7229 - val_accuracy: 0.7134\n",
      "Epoch 23/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.7140 - accuracy: 0.65 - ETA: 0s - loss: 1.2360 - accuracy: 0.72 - ETA: 0s - loss: 0.9789 - accuracy: 0.73 - ETA: 0s - loss: 1.0303 - accuracy: 0.73 - ETA: 0s - loss: 0.9645 - accuracy: 0.73 - ETA: 0s - loss: 0.9192 - accuracy: 0.73 - ETA: 0s - loss: 0.8920 - accuracy: 0.73 - ETA: 0s - loss: 0.8878 - accuracy: 0.73 - ETA: 0s - loss: 0.8691 - accuracy: 0.7361Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.8671 - accuracy: 0.7361 - val_loss: 0.6781 - val_accuracy: 0.7313\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7818 - accuracy: 0.62 - ETA: 0s - loss: 5.0250 - accuracy: 0.63 - ETA: 0s - loss: 4.7388 - accuracy: 0.59 - ETA: 0s - loss: 3.7782 - accuracy: 0.60 - ETA: 0s - loss: 3.1397 - accuracy: 0.60 - ETA: 0s - loss: 2.6797 - accuracy: 0.61 - ETA: 0s - loss: 2.3741 - accuracy: 0.62 - ETA: 0s - loss: 2.1093 - accuracy: 0.63 - 1s 7ms/step - loss: 2.0173 - accuracy: 0.6290 - val_loss: 0.6085 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.59 - ETA: 0s - loss: 0.8137 - accuracy: 0.73 - ETA: 0s - loss: 0.7651 - accuracy: 0.70 - ETA: 0s - loss: 0.7902 - accuracy: 0.68 - ETA: 0s - loss: 0.7632 - accuracy: 0.68 - ETA: 0s - loss: 0.7645 - accuracy: 0.68 - ETA: 0s - loss: 0.7526 - accuracy: 0.68 - ETA: 0s - loss: 0.7390 - accuracy: 0.68 - 0s 5ms/step - loss: 0.7545 - accuracy: 0.6861 - val_loss: 0.6305 - val_accuracy: 0.7149\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6493 - accuracy: 0.59 - ETA: 0s - loss: 0.8273 - accuracy: 0.68 - ETA: 0s - loss: 0.8520 - accuracy: 0.70 - ETA: 0s - loss: 0.8480 - accuracy: 0.68 - ETA: 0s - loss: 0.8320 - accuracy: 0.69 - ETA: 0s - loss: 0.7998 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7896 - accuracy: 0.6973 - val_loss: 0.6823 - val_accuracy: 0.6284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5057 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.73 - ETA: 0s - loss: 0.7233 - accuracy: 0.72 - ETA: 0s - loss: 0.7531 - accuracy: 0.70 - ETA: 0s - loss: 0.7792 - accuracy: 0.71 - ETA: 0s - loss: 0.7740 - accuracy: 0.70 - ETA: 0s - loss: 0.7899 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7845 - accuracy: 0.7100 - val_loss: 0.6714 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7924 - accuracy: 0.71 - ETA: 0s - loss: 0.7687 - accuracy: 0.72 - ETA: 0s - loss: 0.8047 - accuracy: 0.70 - ETA: 0s - loss: 0.8096 - accuracy: 0.70 - ETA: 0s - loss: 0.7793 - accuracy: 0.72 - ETA: 0s - loss: 0.8204 - accuracy: 0.73 - ETA: 0s - loss: 0.8619 - accuracy: 0.71 - 0s 5ms/step - loss: 0.8621 - accuracy: 0.7193 - val_loss: 0.6493 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.68 - ETA: 0s - loss: 0.8797 - accuracy: 0.70 - ETA: 0s - loss: 0.9411 - accuracy: 0.69 - ETA: 0s - loss: 0.8671 - accuracy: 0.69 - ETA: 0s - loss: 0.9062 - accuracy: 0.70 - ETA: 0s - loss: 0.8807 - accuracy: 0.70 - ETA: 0s - loss: 0.8722 - accuracy: 0.70 - ETA: 0s - loss: 0.8553 - accuracy: 0.71 - ETA: 0s - loss: 0.8321 - accuracy: 0.71 - 1s 6ms/step - loss: 0.8097 - accuracy: 0.7204 - val_loss: 0.6325 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.71 - ETA: 0s - loss: 1.0681 - accuracy: 0.71 - ETA: 0s - loss: 1.1060 - accuracy: 0.71 - ETA: 0s - loss: 0.9887 - accuracy: 0.73 - ETA: 0s - loss: 0.9603 - accuracy: 0.71 - ETA: 0s - loss: 0.9441 - accuracy: 0.71 - ETA: 0s - loss: 0.9406 - accuracy: 0.71 - 0s 5ms/step - loss: 0.9224 - accuracy: 0.7219 - val_loss: 0.6214 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0725 - accuracy: 0.65 - ETA: 0s - loss: 0.6665 - accuracy: 0.77 - ETA: 0s - loss: 0.7573 - accuracy: 0.77 - ETA: 0s - loss: 0.7382 - accuracy: 0.76 - ETA: 0s - loss: 0.7163 - accuracy: 0.76 - ETA: 0s - loss: 0.7143 - accuracy: 0.76 - ETA: 0s - loss: 0.7082 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7071 - accuracy: 0.7645 - val_loss: 0.6235 - val_accuracy: 0.7269\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.65 - ETA: 0s - loss: 0.6692 - accuracy: 0.74 - ETA: 0s - loss: 0.8647 - accuracy: 0.76 - ETA: 0s - loss: 0.8007 - accuracy: 0.77 - ETA: 0s - loss: 0.7697 - accuracy: 0.77 - ETA: 0s - loss: 0.7600 - accuracy: 0.77 - ETA: 0s - loss: 0.7999 - accuracy: 0.76 - 0s 5ms/step - loss: 0.7987 - accuracy: 0.7630 - val_loss: 0.5912 - val_accuracy: 0.7552\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.87 - ETA: 0s - loss: 0.7643 - accuracy: 0.75 - ETA: 0s - loss: 0.7256 - accuracy: 0.76 - ETA: 0s - loss: 0.7065 - accuracy: 0.76 - ETA: 0s - loss: 0.7052 - accuracy: 0.76 - ETA: 0s - loss: 0.7329 - accuracy: 0.76 - ETA: 0s - loss: 0.7199 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7165 - accuracy: 0.7648 - val_loss: 0.6466 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.78 - ETA: 0s - loss: 0.6068 - accuracy: 0.79 - ETA: 0s - loss: 0.6137 - accuracy: 0.78 - ETA: 0s - loss: 0.6278 - accuracy: 0.77 - ETA: 0s - loss: 0.6525 - accuracy: 0.77 - ETA: 0s - loss: 0.6774 - accuracy: 0.77 - ETA: 0s - loss: 0.6972 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6988 - accuracy: 0.7723 - val_loss: 0.6879 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1496 - accuracy: 0.81 - ETA: 0s - loss: 0.6809 - accuracy: 0.78 - ETA: 0s - loss: 0.7217 - accuracy: 0.77 - ETA: 0s - loss: 0.7244 - accuracy: 0.77 - ETA: 0s - loss: 0.8279 - accuracy: 0.76 - ETA: 0s - loss: 0.8526 - accuracy: 0.74 - ETA: 0s - loss: 0.8197 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8197 - accuracy: 0.7514 - val_loss: 0.6534 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.87 - ETA: 0s - loss: 0.7264 - accuracy: 0.78 - ETA: 0s - loss: 0.7946 - accuracy: 0.76 - ETA: 0s - loss: 0.8118 - accuracy: 0.75 - ETA: 0s - loss: 0.7630 - accuracy: 0.75 - ETA: 0s - loss: 0.8212 - accuracy: 0.74 - ETA: 0s - loss: 0.8091 - accuracy: 0.75 - 0s 5ms/step - loss: 0.8026 - accuracy: 0.7507 - val_loss: 0.6422 - val_accuracy: 0.7224\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1454 - accuracy: 0.87 - ETA: 0s - loss: 0.9803 - accuracy: 0.73 - ETA: 0s - loss: 1.0422 - accuracy: 0.74 - ETA: 0s - loss: 0.9498 - accuracy: 0.74 - ETA: 0s - loss: 0.9235 - accuracy: 0.75 - ETA: 0s - loss: 0.9165 - accuracy: 0.75 - 0s 4ms/step - loss: 1.0037 - accuracy: 0.7529 - val_loss: 0.9044 - val_accuracy: 0.7507\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 11.6618 - accuracy: 0.718 - ETA: 0s - loss: 1.6342 - accuracy: 0.687 - ETA: 0s - loss: 1.1722 - accuracy: 0.70 - ETA: 0s - loss: 1.0270 - accuracy: 0.70 - ETA: 0s - loss: 1.0184 - accuracy: 0.69 - ETA: 0s - loss: 0.9588 - accuracy: 0.70 - ETA: 0s - loss: 0.9178 - accuracy: 0.70 - 0s 5ms/step - loss: 0.8944 - accuracy: 0.7074 - val_loss: 0.6898 - val_accuracy: 0.6985\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.81 - ETA: 0s - loss: 0.6953 - accuracy: 0.70 - ETA: 0s - loss: 0.7756 - accuracy: 0.69 - ETA: 0s - loss: 0.8060 - accuracy: 0.69 - ETA: 0s - loss: 0.7984 - accuracy: 0.70 - ETA: 0s - loss: 0.7730 - accuracy: 0.70 - ETA: 0s - loss: 0.8010 - accuracy: 0.70 - 0s 4ms/step - loss: 0.8023 - accuracy: 0.7055 - val_loss: 0.6830 - val_accuracy: 0.7075\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.78 - ETA: 0s - loss: 0.6875 - accuracy: 0.70 - ETA: 0s - loss: 0.6986 - accuracy: 0.69 - ETA: 0s - loss: 0.7138 - accuracy: 0.60 - ETA: 0s - loss: 0.7280 - accuracy: 0.53 - ETA: 0s - loss: 0.7215 - accuracy: 0.55 - ETA: 0s - loss: 0.7167 - accuracy: 0.57 - ETA: 0s - loss: 0.7136 - accuracy: 0.58 - 0s 5ms/step - loss: 0.7126 - accuracy: 0.5905 - val_loss: 0.6826 - val_accuracy: 0.6985\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6130 - accuracy: 0.81 - ETA: 0s - loss: 0.6957 - accuracy: 0.70 - ETA: 0s - loss: 0.7095 - accuracy: 0.69 - ETA: 0s - loss: 0.7131 - accuracy: 0.68 - ETA: 0s - loss: 0.7228 - accuracy: 0.59 - ETA: 0s - loss: 0.7106 - accuracy: 0.55 - ETA: 0s - loss: 0.7065 - accuracy: 0.57 - 0s 4ms/step - loss: 0.7065 - accuracy: 0.5778 - val_loss: 0.6890 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.7858 - accuracy: 0.75 - ETA: 0s - loss: 0.9231 - accuracy: 0.67 - ETA: 0s - loss: 0.8305 - accuracy: 0.54 - ETA: 0s - loss: 0.7859 - accuracy: 0.46 - ETA: 0s - loss: 0.7863 - accuracy: 0.47 - ETA: 0s - loss: 0.7709 - accuracy: 0.51 - ETA: 0s - loss: 0.7901 - accuracy: 0.53 - ETA: 0s - loss: 0.7824 - accuracy: 0.5663Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7812 - accuracy: 0.5685 - val_loss: 0.6864 - val_accuracy: 0.7015\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.84 - ETA: 0s - loss: 4.6963 - accuracy: 0.60 - ETA: 0s - loss: 3.4789 - accuracy: 0.62 - ETA: 0s - loss: 2.9078 - accuracy: 0.63 - ETA: 0s - loss: 2.4618 - accuracy: 0.63 - ETA: 0s - loss: 2.1296 - accuracy: 0.63 - ETA: 0s - loss: 1.9308 - accuracy: 0.64 - 1s 6ms/step - loss: 1.8546 - accuracy: 0.6394 - val_loss: 0.6088 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6986 - accuracy: 0.50 - ETA: 0s - loss: 0.7374 - accuracy: 0.62 - ETA: 0s - loss: 0.7484 - accuracy: 0.65 - ETA: 0s - loss: 0.7320 - accuracy: 0.67 - ETA: 0s - loss: 0.7127 - accuracy: 0.68 - ETA: 0s - loss: 0.7002 - accuracy: 0.68 - ETA: 0s - loss: 0.6951 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6930 - accuracy: 0.6980 - val_loss: 0.6144 - val_accuracy: 0.7209\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.65 - ETA: 0s - loss: 0.6058 - accuracy: 0.75 - ETA: 0s - loss: 0.5942 - accuracy: 0.74 - ETA: 0s - loss: 0.6160 - accuracy: 0.74 - ETA: 0s - loss: 0.6321 - accuracy: 0.73 - ETA: 0s - loss: 0.6620 - accuracy: 0.73 - ETA: 0s - loss: 0.7061 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7163 - accuracy: 0.7144 - val_loss: 0.6082 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.75 - ETA: 0s - loss: 0.6643 - accuracy: 0.71 - ETA: 0s - loss: 0.6925 - accuracy: 0.73 - ETA: 0s - loss: 0.7316 - accuracy: 0.73 - ETA: 0s - loss: 0.7836 - accuracy: 0.73 - ETA: 0s - loss: 0.7709 - accuracy: 0.73 - ETA: 0s - loss: 0.7582 - accuracy: 0.72 - 0s 4ms/step - loss: 0.7582 - accuracy: 0.7260 - val_loss: 0.6147 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.65 - ETA: 0s - loss: 0.7793 - accuracy: 0.73 - ETA: 0s - loss: 0.7005 - accuracy: 0.75 - ETA: 0s - loss: 0.6938 - accuracy: 0.76 - ETA: 0s - loss: 0.7118 - accuracy: 0.76 - ETA: 0s - loss: 0.7057 - accuracy: 0.75 - ETA: 0s - loss: 0.6954 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6954 - accuracy: 0.7581 - val_loss: 0.6079 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.71 - ETA: 0s - loss: 0.6107 - accuracy: 0.78 - ETA: 0s - loss: 0.5987 - accuracy: 0.77 - ETA: 0s - loss: 0.6322 - accuracy: 0.77 - ETA: 0s - loss: 0.8262 - accuracy: 0.76 - ETA: 0s - loss: 0.8501 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8465 - accuracy: 0.7533 - val_loss: 0.6413 - val_accuracy: 0.6836\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.71 - ETA: 0s - loss: 0.8829 - accuracy: 0.70 - ETA: 0s - loss: 0.8662 - accuracy: 0.72 - ETA: 0s - loss: 0.8823 - accuracy: 0.73 - ETA: 0s - loss: 0.9202 - accuracy: 0.73 - ETA: 0s - loss: 0.9030 - accuracy: 0.74 - ETA: 0s - loss: 0.8614 - accuracy: 0.74 - 0s 4ms/step - loss: 0.8529 - accuracy: 0.7480 - val_loss: 0.6965 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8708 - accuracy: 0.81 - ETA: 0s - loss: 0.6729 - accuracy: 0.75 - ETA: 0s - loss: 0.7069 - accuracy: 0.76 - ETA: 0s - loss: 0.7436 - accuracy: 0.76 - ETA: 0s - loss: 0.7226 - accuracy: 0.76 - ETA: 0s - loss: 0.7541 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7407 - accuracy: 0.7637 - val_loss: 0.6688 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.84 - ETA: 0s - loss: 0.7835 - accuracy: 0.76 - ETA: 0s - loss: 0.7721 - accuracy: 0.75 - ETA: 0s - loss: 0.7238 - accuracy: 0.77 - ETA: 0s - loss: 0.6966 - accuracy: 0.77 - ETA: 0s - loss: 0.6944 - accuracy: 0.78 - ETA: 0s - loss: 0.7043 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7011 - accuracy: 0.7786 - val_loss: 0.6150 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8079 - accuracy: 0.84 - ETA: 0s - loss: 0.6520 - accuracy: 0.79 - ETA: 0s - loss: 0.7126 - accuracy: 0.76 - ETA: 0s - loss: 0.6820 - accuracy: 0.78 - ETA: 0s - loss: 0.6666 - accuracy: 0.78 - ETA: 0s - loss: 0.6473 - accuracy: 0.79 - ETA: 0s - loss: 0.6471 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6430 - accuracy: 0.7947 - val_loss: 0.9593 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8904 - accuracy: 0.71 - ETA: 0s - loss: 0.6219 - accuracy: 0.77 - ETA: 0s - loss: 0.6132 - accuracy: 0.77 - ETA: 0s - loss: 0.6366 - accuracy: 0.78 - ETA: 0s - loss: 0.6962 - accuracy: 0.78 - ETA: 0s - loss: 0.6873 - accuracy: 0.78 - ETA: 0s - loss: 0.6878 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6872 - accuracy: 0.7775 - val_loss: 0.6064 - val_accuracy: 0.7433\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8278 - accuracy: 0.59 - ETA: 0s - loss: 0.6881 - accuracy: 0.79 - ETA: 0s - loss: 0.7481 - accuracy: 0.79 - ETA: 0s - loss: 0.7548 - accuracy: 0.78 - ETA: 0s - loss: 0.7288 - accuracy: 0.78 - ETA: 0s - loss: 0.7088 - accuracy: 0.78 - ETA: 0s - loss: 0.7061 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6973 - accuracy: 0.7846 - val_loss: 0.8035 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4799 - accuracy: 0.84 - ETA: 0s - loss: 0.6619 - accuracy: 0.81 - ETA: 0s - loss: 0.6522 - accuracy: 0.79 - ETA: 0s - loss: 0.6998 - accuracy: 0.78 - ETA: 0s - loss: 0.8243 - accuracy: 0.78 - ETA: 0s - loss: 0.7916 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7990 - accuracy: 0.7708 - val_loss: 0.7287 - val_accuracy: 0.7284\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.81 - ETA: 0s - loss: 0.8239 - accuracy: 0.79 - ETA: 0s - loss: 0.7293 - accuracy: 0.78 - ETA: 0s - loss: 0.6883 - accuracy: 0.78 - ETA: 0s - loss: 0.6757 - accuracy: 0.79 - ETA: 0s - loss: 0.6641 - accuracy: 0.78 - ETA: 0s - loss: 0.6579 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6562 - accuracy: 0.7857 - val_loss: 0.8022 - val_accuracy: 0.7269\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.84 - ETA: 0s - loss: 0.5868 - accuracy: 0.81 - ETA: 0s - loss: 0.6142 - accuracy: 0.80 - ETA: 0s - loss: 0.7182 - accuracy: 0.79 - ETA: 0s - loss: 0.8067 - accuracy: 0.79 - ETA: 0s - loss: 0.8274 - accuracy: 0.78 - ETA: 0s - loss: 0.8371 - accuracy: 0.78 - 0s 4ms/step - loss: 0.8376 - accuracy: 0.7865 - val_loss: 0.6896 - val_accuracy: 0.7448\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.75 - ETA: 0s - loss: 0.5963 - accuracy: 0.80 - ETA: 0s - loss: 0.6846 - accuracy: 0.79 - ETA: 0s - loss: 0.7755 - accuracy: 0.78 - ETA: 0s - loss: 0.7357 - accuracy: 0.77 - ETA: 0s - loss: 0.7212 - accuracy: 0.77 - ETA: 0s - loss: 0.7302 - accuracy: 0.77 - 0s 4ms/step - loss: 0.7215 - accuracy: 0.7757 - val_loss: 0.6856 - val_accuracy: 0.7254\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.81 - ETA: 0s - loss: 0.8317 - accuracy: 0.78 - ETA: 0s - loss: 0.8955 - accuracy: 0.76 - ETA: 0s - loss: 0.7909 - accuracy: 0.77 - ETA: 0s - loss: 0.7434 - accuracy: 0.77 - ETA: 0s - loss: 0.7621 - accuracy: 0.77 - ETA: 0s - loss: 0.7457 - accuracy: 0.77 - 0s 5ms/step - loss: 0.7797 - accuracy: 0.7764 - val_loss: 0.7011 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.81 - ETA: 0s - loss: 0.7244 - accuracy: 0.77 - ETA: 0s - loss: 0.6703 - accuracy: 0.80 - ETA: 0s - loss: 0.8176 - accuracy: 0.80 - ETA: 0s - loss: 0.7730 - accuracy: 0.80 - ETA: 0s - loss: 0.9641 - accuracy: 0.78 - ETA: 0s - loss: 0.9243 - accuracy: 0.78 - 0s 4ms/step - loss: 0.9086 - accuracy: 0.7831 - val_loss: 1.0741 - val_accuracy: 0.7478\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9220 - accuracy: 0.68 - ETA: 0s - loss: 0.8873 - accuracy: 0.73 - ETA: 0s - loss: 0.9450 - accuracy: 0.73 - ETA: 0s - loss: 0.9711 - accuracy: 0.74 - ETA: 0s - loss: 0.8839 - accuracy: 0.75 - ETA: 0s - loss: 0.8623 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8443 - accuracy: 0.7615 - val_loss: 0.6229 - val_accuracy: 0.7313\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7772 - accuracy: 0.62 - ETA: 0s - loss: 0.7633 - accuracy: 0.76 - ETA: 0s - loss: 0.7230 - accuracy: 0.75 - ETA: 0s - loss: 0.7572 - accuracy: 0.76 - ETA: 0s - loss: 0.8309 - accuracy: 0.76 - ETA: 0s - loss: 0.7946 - accuracy: 0.77 - ETA: 0s - loss: 0.7593 - accuracy: 0.77 - 0s 5ms/step - loss: 0.7414 - accuracy: 0.7730 - val_loss: 0.7750 - val_accuracy: 0.7448\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.65 - ETA: 0s - loss: 0.6862 - accuracy: 0.77 - ETA: 0s - loss: 1.2069 - accuracy: 0.79 - ETA: 0s - loss: 1.0042 - accuracy: 0.79 - ETA: 0s - loss: 0.9272 - accuracy: 0.78 - ETA: 0s - loss: 0.8912 - accuracy: 0.77 - 0s 4ms/step - loss: 0.8687 - accuracy: 0.7719 - val_loss: 0.7302 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.71 - ETA: 0s - loss: 0.6567 - accuracy: 0.74 - ETA: 0s - loss: 0.8763 - accuracy: 0.74 - ETA: 0s - loss: 0.8189 - accuracy: 0.76 - ETA: 0s - loss: 0.7733 - accuracy: 0.76 - ETA: 0s - loss: 0.7634 - accuracy: 0.76 - ETA: 0s - loss: 0.7453 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7420 - accuracy: 0.7585 - val_loss: 0.6735 - val_accuracy: 0.7358\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.71 - ETA: 0s - loss: 0.7628 - accuracy: 0.75 - ETA: 0s - loss: 0.6961 - accuracy: 0.75 - ETA: 0s - loss: 0.7121 - accuracy: 0.75 - ETA: 0s - loss: 0.7005 - accuracy: 0.75 - ETA: 0s - loss: 0.6925 - accuracy: 0.74 - ETA: 0s - loss: 0.6817 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6794 - accuracy: 0.7492 - val_loss: 0.7672 - val_accuracy: 0.7284\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.65 - ETA: 0s - loss: 0.6292 - accuracy: 0.74 - ETA: 0s - loss: 0.6455 - accuracy: 0.76 - ETA: 0s - loss: 0.6670 - accuracy: 0.77 - ETA: 0s - loss: 0.6740 - accuracy: 0.76 - ETA: 0s - loss: 0.6623 - accuracy: 0.76 - ETA: 0s - loss: 0.6962 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6945 - accuracy: 0.7660 - val_loss: 0.7844 - val_accuracy: 0.7358\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.90 - ETA: 0s - loss: 0.6619 - accuracy: 0.74 - ETA: 0s - loss: 0.7209 - accuracy: 0.77 - ETA: 0s - loss: 0.7104 - accuracy: 0.77 - ETA: 0s - loss: 0.6866 - accuracy: 0.77 - ETA: 0s - loss: 0.6787 - accuracy: 0.77 - ETA: 0s - loss: 0.6873 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6965 - accuracy: 0.7689 - val_loss: 0.6495 - val_accuracy: 0.7239\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5723 - accuracy: 0.75 - ETA: 0s - loss: 0.7021 - accuracy: 0.75 - ETA: 0s - loss: 0.6941 - accuracy: 0.73 - ETA: 0s - loss: 0.9290 - accuracy: 0.72 - ETA: 0s - loss: 0.9720 - accuracy: 0.72 - ETA: 0s - loss: 0.9084 - accuracy: 0.72 - ETA: 0s - loss: 0.8728 - accuracy: 0.72 - 0s 4ms/step - loss: 0.8678 - accuracy: 0.7301 - val_loss: 0.7323 - val_accuracy: 0.7224\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.65 - ETA: 0s - loss: 0.7613 - accuracy: 0.71 - ETA: 0s - loss: 0.7288 - accuracy: 0.72 - ETA: 0s - loss: 0.7946 - accuracy: 0.71 - ETA: 0s - loss: 0.8112 - accuracy: 0.71 - ETA: 0s - loss: 0.7964 - accuracy: 0.71 - 0s 4ms/step - loss: 0.8117 - accuracy: 0.7178 - val_loss: 0.7227 - val_accuracy: 0.7090\n",
      "Epoch 28/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.68 - ETA: 0s - loss: 0.7204 - accuracy: 0.66 - ETA: 0s - loss: 0.6863 - accuracy: 0.70 - ETA: 0s - loss: 0.7272 - accuracy: 0.71 - ETA: 0s - loss: 0.7066 - accuracy: 0.72 - ETA: 0s - loss: 0.6950 - accuracy: 0.72 - ETA: 0s - loss: 0.6967 - accuracy: 0.7306Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6960 - accuracy: 0.7301 - val_loss: 0.8180 - val_accuracy: 0.7269\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ac3af3fde9f9be1bcf64f4cd04531901</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7517412900924683</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7669414364584254</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 145</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6441 - accuracy: 0.78 - ETA: 0s - loss: 0.9929 - accuracy: 0.62 - ETA: 0s - loss: 0.8843 - accuracy: 0.65 - ETA: 0s - loss: 0.8130 - accuracy: 0.67 - 0s 4ms/step - loss: 0.8031 - accuracy: 0.6771 - val_loss: 0.6230 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5657 - accuracy: 0.84 - ETA: 0s - loss: 0.6377 - accuracy: 0.73 - ETA: 0s - loss: 0.6456 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6309 - accuracy: 0.7447 - val_loss: 0.5893 - val_accuracy: 0.7567\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.87 - ETA: 0s - loss: 0.6203 - accuracy: 0.76 - ETA: 0s - loss: 0.6159 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6171 - accuracy: 0.7686 - val_loss: 0.5945 - val_accuracy: 0.7493\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.84 - ETA: 0s - loss: 0.5955 - accuracy: 0.77 - ETA: 0s - loss: 0.6231 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7626 - val_loss: 0.6014 - val_accuracy: 0.7597\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.71 - ETA: 0s - loss: 0.6364 - accuracy: 0.75 - ETA: 0s - loss: 0.6389 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6416 - accuracy: 0.7563 - val_loss: 0.6107 - val_accuracy: 0.7657\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.81 - ETA: 0s - loss: 0.5968 - accuracy: 0.78 - ETA: 0s - loss: 0.5973 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6186 - accuracy: 0.7753 - val_loss: 0.6982 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9064 - accuracy: 0.78 - ETA: 0s - loss: 0.6669 - accuracy: 0.79 - ETA: 0s - loss: 0.6388 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7794 - val_loss: 0.6026 - val_accuracy: 0.7582\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.78 - ETA: 0s - loss: 0.5960 - accuracy: 0.78 - ETA: 0s - loss: 0.6053 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5982 - accuracy: 0.7857 - val_loss: 0.5906 - val_accuracy: 0.7478\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.81 - ETA: 0s - loss: 0.5708 - accuracy: 0.80 - ETA: 0s - loss: 0.5996 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5967 - accuracy: 0.7887 - val_loss: 0.5974 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.81 - ETA: 0s - loss: 0.6048 - accuracy: 0.77 - ETA: 0s - loss: 0.5785 - accuracy: 0.79 - ETA: 0s - loss: 0.5826 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7917 - val_loss: 0.5853 - val_accuracy: 0.7418\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.78 - ETA: 0s - loss: 0.5798 - accuracy: 0.79 - ETA: 0s - loss: 0.6058 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6198 - accuracy: 0.7910 - val_loss: 0.8578 - val_accuracy: 0.7194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9970 - accuracy: 0.75 - ETA: 0s - loss: 0.5754 - accuracy: 0.80 - ETA: 0s - loss: 0.5725 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5776 - accuracy: 0.7981 - val_loss: 0.5950 - val_accuracy: 0.7478\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5575 - accuracy: 0.81 - ETA: 0s - loss: 0.6008 - accuracy: 0.77 - ETA: 0s - loss: 0.5730 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7992 - val_loss: 0.5934 - val_accuracy: 0.7687\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.87 - ETA: 0s - loss: 0.5389 - accuracy: 0.81 - ETA: 0s - loss: 0.5640 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5694 - accuracy: 0.8040 - val_loss: 0.5915 - val_accuracy: 0.7448\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.78 - ETA: 0s - loss: 0.5678 - accuracy: 0.81 - ETA: 0s - loss: 0.5876 - accuracy: 0.79 - ETA: 0s - loss: 0.5964 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5939 - accuracy: 0.7842 - val_loss: 0.6035 - val_accuracy: 0.7418\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7323 - accuracy: 0.68 - ETA: 0s - loss: 0.5718 - accuracy: 0.79 - ETA: 0s - loss: 0.5774 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5757 - accuracy: 0.7969 - val_loss: 0.6257 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.87 - ETA: 0s - loss: 0.5600 - accuracy: 0.80 - ETA: 0s - loss: 0.5607 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5581 - accuracy: 0.8055 - val_loss: 0.6330 - val_accuracy: 0.7478\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.84 - ETA: 0s - loss: 0.5714 - accuracy: 0.80 - ETA: 0s - loss: 0.5651 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5940 - accuracy: 0.8052 - val_loss: 0.5987 - val_accuracy: 0.7522\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.81 - ETA: 0s - loss: 0.5984 - accuracy: 0.78 - ETA: 0s - loss: 0.5979 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7865 - val_loss: 0.5950 - val_accuracy: 0.7418\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.87 - ETA: 0s - loss: 0.5612 - accuracy: 0.80 - ETA: 0s - loss: 0.5789 - accuracy: 0.79 - ETA: 0s - loss: 0.6062 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6109 - accuracy: 0.7824 - val_loss: 0.6092 - val_accuracy: 0.7493\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.81 - ETA: 0s - loss: 0.6052 - accuracy: 0.77 - ETA: 0s - loss: 0.5975 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5897 - accuracy: 0.7880 - val_loss: 0.6649 - val_accuracy: 0.7657\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7415 - accuracy: 0.68 - ETA: 0s - loss: 0.6025 - accuracy: 0.77 - ETA: 0s - loss: 0.5931 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7869 - val_loss: 0.6232 - val_accuracy: 0.7478\n",
      "Epoch 23/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.6071 - accuracy: 0.78 - ETA: 0s - loss: 0.7018 - accuracy: 0.77 - ETA: 0s - loss: 0.6597 - accuracy: 0.7673Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.7563 - val_loss: 0.6771 - val_accuracy: 0.7075\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8870 - accuracy: 0.68 - ETA: 0s - loss: 1.1653 - accuracy: 0.60 - ETA: 0s - loss: 0.9323 - accuracy: 0.63 - 0s 3ms/step - loss: 0.8823 - accuracy: 0.6387 - val_loss: 0.5910 - val_accuracy: 0.6881\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.68 - ETA: 0s - loss: 0.6082 - accuracy: 0.69 - ETA: 0s - loss: 0.6263 - accuracy: 0.70 - 0s 2ms/step - loss: 0.6208 - accuracy: 0.7137 - val_loss: 0.5844 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.78 - ETA: 0s - loss: 0.5526 - accuracy: 0.78 - ETA: 0s - loss: 0.5548 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7633 - val_loss: 0.5674 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.84 - ETA: 0s - loss: 0.5407 - accuracy: 0.78 - ETA: 0s - loss: 0.5213 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7809 - val_loss: 0.5694 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.78 - ETA: 0s - loss: 0.4823 - accuracy: 0.82 - ETA: 0s - loss: 0.4687 - accuracy: 0.82 - ETA: 0s - loss: 0.4718 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4727 - accuracy: 0.8186 - val_loss: 0.5404 - val_accuracy: 0.7522\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.81 - ETA: 0s - loss: 0.5264 - accuracy: 0.80 - ETA: 0s - loss: 0.4881 - accuracy: 0.81 - ETA: 0s - loss: 0.5172 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5167 - accuracy: 0.8078 - val_loss: 0.5497 - val_accuracy: 0.7448\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.81 - ETA: 0s - loss: 0.4820 - accuracy: 0.80 - ETA: 0s - loss: 0.4859 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4878 - accuracy: 0.8096 - val_loss: 0.5686 - val_accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.81 - ETA: 0s - loss: 0.3872 - accuracy: 0.85 - ETA: 0s - loss: 0.4161 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8421 - val_loss: 0.6175 - val_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.78 - ETA: 0s - loss: 0.3939 - accuracy: 0.86 - ETA: 0s - loss: 0.4226 - accuracy: 0.85 - ETA: 0s - loss: 0.4193 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4193 - accuracy: 0.8485 - val_loss: 0.5641 - val_accuracy: 0.7343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.90 - ETA: 0s - loss: 0.3827 - accuracy: 0.86 - ETA: 0s - loss: 0.3736 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8626 - val_loss: 0.6175 - val_accuracy: 0.7403\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.87 - ETA: 0s - loss: 0.3379 - accuracy: 0.88 - ETA: 0s - loss: 0.4477 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8410 - val_loss: 0.5919 - val_accuracy: 0.7463\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.81 - ETA: 0s - loss: 0.3973 - accuracy: 0.87 - ETA: 0s - loss: 0.4434 - accuracy: 0.86 - ETA: 0s - loss: 0.4471 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4513 - accuracy: 0.8541 - val_loss: 1.2591 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3106 - accuracy: 0.84 - ETA: 0s - loss: 0.6375 - accuracy: 0.80 - ETA: 0s - loss: 0.6110 - accuracy: 0.79 - ETA: 0s - loss: 0.5496 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5464 - accuracy: 0.8160 - val_loss: 0.6067 - val_accuracy: 0.7478\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.90 - ETA: 0s - loss: 0.3862 - accuracy: 0.86 - ETA: 0s - loss: 0.4914 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8470 - val_loss: 0.5589 - val_accuracy: 0.7522\n",
      "Epoch 15/50\n",
      "65/84 [======================>.......] - ETA: 0s - loss: 0.5055 - accuracy: 0.81 - ETA: 0s - loss: 0.3905 - accuracy: 0.87 - ETA: 0s - loss: 0.4349 - accuracy: 0.8577Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8630 - val_loss: 0.6625 - val_accuracy: 0.7224\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.65 - ETA: 0s - loss: 0.9840 - accuracy: 0.64 - ETA: 0s - loss: 0.8699 - accuracy: 0.64 - 0s 3ms/step - loss: 0.8200 - accuracy: 0.6577 - val_loss: 0.6130 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.81 - ETA: 0s - loss: 0.6095 - accuracy: 0.72 - ETA: 0s - loss: 0.6333 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6325 - accuracy: 0.7081 - val_loss: 0.5559 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4832 - accuracy: 0.78 - ETA: 0s - loss: 0.5435 - accuracy: 0.74 - ETA: 0s - loss: 0.5578 - accuracy: 0.73 - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7309 - val_loss: 0.6037 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 1.00 - ETA: 0s - loss: 0.5469 - accuracy: 0.76 - ETA: 0s - loss: 0.5770 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5746 - accuracy: 0.7484 - val_loss: 0.5905 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.81 - ETA: 0s - loss: 0.5555 - accuracy: 0.75 - ETA: 0s - loss: 0.5364 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7563 - val_loss: 0.5716 - val_accuracy: 0.7119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.71 - ETA: 0s - loss: 0.5913 - accuracy: 0.74 - ETA: 0s - loss: 0.5801 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6231 - accuracy: 0.7320 - val_loss: 0.5723 - val_accuracy: 0.7119\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.78 - ETA: 0s - loss: 0.5259 - accuracy: 0.77 - ETA: 0s - loss: 0.5115 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7969 - val_loss: 0.5805 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.90 - ETA: 0s - loss: 0.5405 - accuracy: 0.81 - ETA: 0s - loss: 0.5273 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5180 - accuracy: 0.8037 - val_loss: 0.5934 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.87 - ETA: 0s - loss: 0.4209 - accuracy: 0.84 - ETA: 0s - loss: 0.5030 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5370 - accuracy: 0.8137 - val_loss: 0.6144 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.84 - ETA: 0s - loss: 0.4984 - accuracy: 0.82 - ETA: 0s - loss: 0.4798 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8186 - val_loss: 0.5420 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5879 - accuracy: 0.71 - ETA: 0s - loss: 0.5005 - accuracy: 0.81 - ETA: 0s - loss: 0.4993 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5129 - accuracy: 0.8223 - val_loss: 0.5653 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.90 - ETA: 0s - loss: 0.4498 - accuracy: 0.82 - ETA: 0s - loss: 0.4544 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8261 - val_loss: 0.5979 - val_accuracy: 0.7299\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.81 - ETA: 0s - loss: 0.3901 - accuracy: 0.86 - ETA: 0s - loss: 0.3927 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8563 - val_loss: 0.5774 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.90 - ETA: 0s - loss: 0.3887 - accuracy: 0.86 - ETA: 0s - loss: 0.3966 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8671 - val_loss: 0.5770 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.81 - ETA: 0s - loss: 0.3714 - accuracy: 0.87 - ETA: 0s - loss: 0.3652 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8798 - val_loss: 0.7694 - val_accuracy: 0.7134\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.71 - ETA: 0s - loss: 0.4173 - accuracy: 0.85 - ETA: 0s - loss: 0.4207 - accuracy: 0.85 - ETA: 0s - loss: 0.4119 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8596 - val_loss: 0.8338 - val_accuracy: 0.7030\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7016 - accuracy: 0.84 - ETA: 0s - loss: 0.3953 - accuracy: 0.87 - ETA: 0s - loss: 0.4025 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8690 - val_loss: 0.9527 - val_accuracy: 0.7060\n",
      "Epoch 18/50\n",
      "67/84 [======================>.......] - ETA: 0s - loss: 0.1556 - accuracy: 0.93 - ETA: 0s - loss: 0.3348 - accuracy: 0.89 - ETA: 0s - loss: 0.3525 - accuracy: 0.8895Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8903 - val_loss: 0.8691 - val_accuracy: 0.7239\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: e6e3780491d1bbde9b5fd8e4863d230d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7532338301340739</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7611846640474628</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.59 - ETA: 0s - loss: 1.1361 - accuracy: 0.64 - ETA: 0s - loss: 0.8923 - accuracy: 0.67 - ETA: 0s - loss: 0.8052 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7715 - accuracy: 0.6954 - val_loss: 0.6190 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.81 - ETA: 0s - loss: 0.5597 - accuracy: 0.73 - ETA: 0s - loss: 0.5629 - accuracy: 0.75 - ETA: 0s - loss: 0.5820 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5926 - accuracy: 0.7439 - val_loss: 0.6711 - val_accuracy: 0.6104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.65 - ETA: 0s - loss: 0.5874 - accuracy: 0.76 - ETA: 0s - loss: 0.5629 - accuracy: 0.77 - ETA: 0s - loss: 0.5575 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5617 - accuracy: 0.7716 - val_loss: 0.5337 - val_accuracy: 0.7522\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.75 - ETA: 0s - loss: 0.5413 - accuracy: 0.78 - ETA: 0s - loss: 0.5826 - accuracy: 0.80 - ETA: 0s - loss: 0.5780 - accuracy: 0.79 - ETA: 0s - loss: 0.5785 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5787 - accuracy: 0.7880 - val_loss: 0.5660 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.81 - ETA: 0s - loss: 0.5072 - accuracy: 0.81 - ETA: 0s - loss: 0.4977 - accuracy: 0.82 - ETA: 0s - loss: 0.4896 - accuracy: 0.82 - ETA: 0s - loss: 0.5029 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5024 - accuracy: 0.8216 - val_loss: 0.5689 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.81 - ETA: 0s - loss: 0.5302 - accuracy: 0.82 - ETA: 0s - loss: 0.5338 - accuracy: 0.81 - ETA: 0s - loss: 0.5043 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5017 - accuracy: 0.8205 - val_loss: 0.7581 - val_accuracy: 0.6687\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.84 - ETA: 0s - loss: 0.4805 - accuracy: 0.83 - ETA: 0s - loss: 0.5258 - accuracy: 0.81 - ETA: 0s - loss: 0.5145 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5073 - accuracy: 0.8231 - val_loss: 0.7057 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.84 - ETA: 0s - loss: 0.5632 - accuracy: 0.81 - ETA: 0s - loss: 0.5303 - accuracy: 0.81 - ETA: 0s - loss: 0.5100 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4957 - accuracy: 0.8279 - val_loss: 0.8400 - val_accuracy: 0.6970\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.84 - ETA: 0s - loss: 0.4386 - accuracy: 0.85 - ETA: 0s - loss: 0.5541 - accuracy: 0.82 - ETA: 0s - loss: 0.5378 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5247 - accuracy: 0.8201 - val_loss: 0.6289 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.4645 - accuracy: 0.82 - ETA: 0s - loss: 0.4494 - accuracy: 0.84 - ETA: 0s - loss: 0.4640 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4607 - accuracy: 0.8399 - val_loss: 0.7802 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.90 - ETA: 0s - loss: 0.3629 - accuracy: 0.89 - ETA: 0s - loss: 0.4033 - accuracy: 0.87 - ETA: 0s - loss: 0.4051 - accuracy: 0.87 - ETA: 0s - loss: 0.4108 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8686 - val_loss: 0.8182 - val_accuracy: 0.7045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.93 - ETA: 0s - loss: 0.3586 - accuracy: 0.87 - ETA: 0s - loss: 0.3655 - accuracy: 0.87 - ETA: 0s - loss: 0.3752 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8742 - val_loss: 0.8222 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.5648 - accuracy: 0.81 - ETA: 0s - loss: 0.3523 - accuracy: 0.89 - ETA: 0s - loss: 0.3627 - accuracy: 0.88 - ETA: 0s - loss: 0.3725 - accuracy: 0.8794Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8787 - val_loss: 0.9084 - val_accuracy: 0.6985\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7472 - accuracy: 0.65 - ETA: 0s - loss: 1.0297 - accuracy: 0.63 - ETA: 0s - loss: 0.8335 - accuracy: 0.67 - ETA: 0s - loss: 0.7714 - accuracy: 0.69 - ETA: 0s - loss: 0.7386 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7357 - accuracy: 0.6976 - val_loss: 0.5845 - val_accuracy: 0.6836\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.75 - ETA: 0s - loss: 0.5584 - accuracy: 0.73 - ETA: 0s - loss: 0.5610 - accuracy: 0.74 - ETA: 0s - loss: 0.5920 - accuracy: 0.74 - ETA: 0s - loss: 0.5909 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5909 - accuracy: 0.7507 - val_loss: 0.5654 - val_accuracy: 0.7239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5368 - accuracy: 0.68 - ETA: 0s - loss: 0.5635 - accuracy: 0.76 - ETA: 0s - loss: 0.5518 - accuracy: 0.76 - ETA: 0s - loss: 0.5598 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5541 - accuracy: 0.7712 - val_loss: 0.7634 - val_accuracy: 0.6373\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.78 - ETA: 0s - loss: 0.5632 - accuracy: 0.77 - ETA: 0s - loss: 0.5268 - accuracy: 0.79 - ETA: 0s - loss: 0.5301 - accuracy: 0.78 - ETA: 0s - loss: 0.5229 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5233 - accuracy: 0.7928 - val_loss: 0.5727 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.96 - ETA: 0s - loss: 0.4933 - accuracy: 0.83 - ETA: 0s - loss: 0.4810 - accuracy: 0.81 - ETA: 0s - loss: 0.5300 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5265 - accuracy: 0.8141 - val_loss: 0.6794 - val_accuracy: 0.7030\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5543 - accuracy: 0.81 - ETA: 0s - loss: 0.4729 - accuracy: 0.83 - ETA: 0s - loss: 0.4993 - accuracy: 0.81 - ETA: 0s - loss: 0.4981 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5018 - accuracy: 0.8163 - val_loss: 0.5976 - val_accuracy: 0.7239\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.87 - ETA: 0s - loss: 0.5031 - accuracy: 0.81 - ETA: 0s - loss: 0.4788 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4878 - accuracy: 0.8294 - val_loss: 0.7326 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.78 - ETA: 0s - loss: 0.4360 - accuracy: 0.81 - ETA: 0s - loss: 0.4335 - accuracy: 0.82 - ETA: 0s - loss: 0.4723 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4800 - accuracy: 0.8205 - val_loss: 0.7233 - val_accuracy: 0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.90 - ETA: 0s - loss: 0.4742 - accuracy: 0.82 - ETA: 0s - loss: 0.5092 - accuracy: 0.81 - ETA: 0s - loss: 0.4922 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4803 - accuracy: 0.8268 - val_loss: 0.7881 - val_accuracy: 0.7343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.84 - ETA: 0s - loss: 0.4719 - accuracy: 0.85 - ETA: 0s - loss: 0.4710 - accuracy: 0.84 - ETA: 0s - loss: 0.4593 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4567 - accuracy: 0.8477 - val_loss: 0.7159 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.81 - ETA: 0s - loss: 0.4401 - accuracy: 0.85 - ETA: 0s - loss: 0.4387 - accuracy: 0.84 - ETA: 0s - loss: 0.4421 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4440 - accuracy: 0.8470 - val_loss: 0.5973 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.84 - ETA: 0s - loss: 0.3984 - accuracy: 0.85 - ETA: 0s - loss: 0.4088 - accuracy: 0.85 - ETA: 0s - loss: 0.4760 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4848 - accuracy: 0.8559 - val_loss: 1.0536 - val_accuracy: 0.7134\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.90 - ETA: 0s - loss: 0.3985 - accuracy: 0.88 - ETA: 0s - loss: 0.3930 - accuracy: 0.88 - ETA: 0s - loss: 0.4015 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8694 - val_loss: 0.7521 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "70/84 [========================>.....] - ETA: 0s - loss: 0.3953 - accuracy: 0.78 - ETA: 0s - loss: 0.3694 - accuracy: 0.88 - ETA: 0s - loss: 0.3851 - accuracy: 0.88 - ETA: 0s - loss: 0.3929 - accuracy: 0.8781Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.8764 - val_loss: 0.6410 - val_accuracy: 0.7269\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0238 - accuracy: 0.46 - ETA: 0s - loss: 0.9209 - accuracy: 0.66 - ETA: 0s - loss: 0.7869 - accuracy: 0.68 - ETA: 0s - loss: 0.7431 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7172 - accuracy: 0.7044 - val_loss: 0.5918 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.81 - ETA: 0s - loss: 0.5752 - accuracy: 0.75 - ETA: 0s - loss: 0.6117 - accuracy: 0.75 - ETA: 0s - loss: 0.6019 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5962 - accuracy: 0.7510 - val_loss: 0.5826 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.84 - ETA: 0s - loss: 0.5321 - accuracy: 0.77 - ETA: 0s - loss: 0.5174 - accuracy: 0.78 - ETA: 0s - loss: 0.5719 - accuracy: 0.78 - ETA: 0s - loss: 0.5908 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5908 - accuracy: 0.7742 - val_loss: 0.6119 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.90 - ETA: 0s - loss: 0.6033 - accuracy: 0.78 - ETA: 0s - loss: 0.5818 - accuracy: 0.78 - ETA: 0s - loss: 0.5738 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5645 - accuracy: 0.7846 - val_loss: 0.5826 - val_accuracy: 0.7239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.96 - ETA: 0s - loss: 0.5083 - accuracy: 0.81 - ETA: 0s - loss: 0.5002 - accuracy: 0.83 - ETA: 0s - loss: 0.5116 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5054 - accuracy: 0.8223 - val_loss: 0.5714 - val_accuracy: 0.7328\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6408 - accuracy: 0.75 - ETA: 0s - loss: 0.4316 - accuracy: 0.84 - ETA: 0s - loss: 0.4220 - accuracy: 0.85 - ETA: 0s - loss: 0.4534 - accuracy: 0.84 - ETA: 0s - loss: 0.4691 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4683 - accuracy: 0.8331 - val_loss: 0.5923 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.81 - ETA: 0s - loss: 0.4284 - accuracy: 0.86 - ETA: 0s - loss: 0.4266 - accuracy: 0.85 - ETA: 0s - loss: 0.4395 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4425 - accuracy: 0.8485 - val_loss: 0.5881 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.87 - ETA: 0s - loss: 0.3586 - accuracy: 0.88 - ETA: 0s - loss: 0.4084 - accuracy: 0.86 - ETA: 0s - loss: 0.4121 - accuracy: 0.86 - ETA: 0s - loss: 0.4060 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4117 - accuracy: 0.8634 - val_loss: 0.7392 - val_accuracy: 0.7209\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.87 - ETA: 0s - loss: 0.4030 - accuracy: 0.86 - ETA: 0s - loss: 0.4975 - accuracy: 0.85 - ETA: 0s - loss: 0.4847 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4724 - accuracy: 0.8559 - val_loss: 0.9132 - val_accuracy: 0.6791\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4925 - accuracy: 0.87 - ETA: 0s - loss: 0.4834 - accuracy: 0.87 - ETA: 0s - loss: 0.4585 - accuracy: 0.86 - ETA: 0s - loss: 0.4559 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4509 - accuracy: 0.8555 - val_loss: 0.6839 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.87 - ETA: 0s - loss: 0.3866 - accuracy: 0.87 - ETA: 0s - loss: 0.4059 - accuracy: 0.87 - ETA: 0s - loss: 0.4195 - accuracy: 0.86 - ETA: 0s - loss: 0.4275 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8649 - val_loss: 0.6893 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.96 - ETA: 0s - loss: 0.3678 - accuracy: 0.88 - ETA: 0s - loss: 0.3791 - accuracy: 0.88 - ETA: 0s - loss: 0.4035 - accuracy: 0.87 - ETA: 0s - loss: 0.3983 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4145 - accuracy: 0.8776 - val_loss: 0.7570 - val_accuracy: 0.7104\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.84 - ETA: 0s - loss: 0.3841 - accuracy: 0.88 - ETA: 0s - loss: 0.3928 - accuracy: 0.88 - ETA: 0s - loss: 0.3818 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8828 - val_loss: 1.0119 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.81 - ETA: 0s - loss: 0.3919 - accuracy: 0.88 - ETA: 0s - loss: 0.3923 - accuracy: 0.88 - ETA: 0s - loss: 0.4151 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8798 - val_loss: 0.8610 - val_accuracy: 0.7015\n",
      "Epoch 15/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.1943 - accuracy: 1.00 - ETA: 0s - loss: 0.3190 - accuracy: 0.90 - ETA: 0s - loss: 0.3247 - accuracy: 0.90 - ETA: 0s - loss: 0.3299 - accuracy: 0.9033Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8962 - val_loss: 1.2486 - val_accuracy: 0.6925\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: e80ac770479902aa50c895895eb800c1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7457711497942606</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6911491344270662</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 290</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2399 - accuracy: 0.56 - ETA: 0s - loss: 13.1398 - accuracy: 0.593 - ETA: 0s - loss: 9.4117 - accuracy: 0.593 - ETA: 0s - loss: 7.1748 - accuracy: 0.60 - ETA: 0s - loss: 5.6709 - accuracy: 0.61 - ETA: 0s - loss: 4.7937 - accuracy: 0.59 - ETA: 0s - loss: 4.1777 - accuracy: 0.55 - ETA: 0s - loss: 3.7327 - accuracy: 0.53 - 1s 7ms/step - loss: 3.7327 - accuracy: 0.5327 - val_loss: 0.6889 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.28 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.60 - ETA: 0s - loss: 0.6956 - accuracy: 0.62 - ETA: 0s - loss: 0.6920 - accuracy: 0.63 - ETA: 0s - loss: 0.6945 - accuracy: 0.59 - ETA: 0s - loss: 0.7014 - accuracy: 0.55 - 0s 4ms/step - loss: 0.6996 - accuracy: 0.5338 - val_loss: 0.6935 - val_accuracy: 0.3030\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7558 - accuracy: 0.46 - ETA: 0s - loss: 0.6844 - accuracy: 0.47 - ETA: 0s - loss: 0.6942 - accuracy: 0.55 - ETA: 0s - loss: 0.6991 - accuracy: 0.50 - ETA: 0s - loss: 0.6955 - accuracy: 0.46 - ETA: 0s - loss: 0.7020 - accuracy: 0.44 - ETA: 0s - loss: 0.6994 - accuracy: 0.42 - ETA: 0s - loss: 0.6996 - accuracy: 0.43 - 0s 5ms/step - loss: 0.6996 - accuracy: 0.4345 - val_loss: 0.6740 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.71 - ETA: 0s - loss: 0.6737 - accuracy: 0.72 - ETA: 0s - loss: 0.7014 - accuracy: 0.67 - ETA: 0s - loss: 0.6973 - accuracy: 0.58 - ETA: 0s - loss: 0.6954 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6959 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6960 - accuracy: 0.5454 - val_loss: 0.6868 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7622 - accuracy: 0.37 - ETA: 0s - loss: 0.6919 - accuracy: 0.34 - ETA: 0s - loss: 0.6855 - accuracy: 0.38 - ETA: 0s - loss: 0.6905 - accuracy: 0.47 - ETA: 0s - loss: 0.6913 - accuracy: 0.45 - ETA: 0s - loss: 0.7000 - accuracy: 0.43 - ETA: 0s - loss: 0.6998 - accuracy: 0.41 - 0s 5ms/step - loss: 0.6976 - accuracy: 0.4099 - val_loss: 0.6942 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.34 - ETA: 0s - loss: 0.6994 - accuracy: 0.60 - ETA: 0s - loss: 0.6924 - accuracy: 0.64 - ETA: 0s - loss: 0.6909 - accuracy: 0.66 - ETA: 0s - loss: 0.6909 - accuracy: 0.67 - ETA: 0s - loss: 0.6892 - accuracy: 0.67 - ETA: 0s - loss: 0.6899 - accuracy: 0.67 - 0s 5ms/step - loss: 0.6945 - accuracy: 0.6637 - val_loss: 0.6960 - val_accuracy: 0.3045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.21 - ETA: 0s - loss: 0.6894 - accuracy: 0.31 - ETA: 0s - loss: 0.6980 - accuracy: 0.32 - ETA: 0s - loss: 0.7002 - accuracy: 0.32 - ETA: 0s - loss: 0.6946 - accuracy: 0.31 - ETA: 0s - loss: 0.6932 - accuracy: 0.38 - ETA: 0s - loss: 0.6934 - accuracy: 0.43 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.4509 - val_loss: 0.6836 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.71 - ETA: 0s - loss: 0.6890 - accuracy: 0.44 - ETA: 0s - loss: 0.6893 - accuracy: 0.37 - ETA: 0s - loss: 0.6922 - accuracy: 0.36 - ETA: 0s - loss: 0.6897 - accuracy: 0.39 - ETA: 0s - loss: 0.6940 - accuracy: 0.45 - ETA: 0s - loss: 0.6952 - accuracy: 0.45 - 0s 4ms/step - loss: 0.7052 - accuracy: 0.4457 - val_loss: 0.7066 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.31 - ETA: 0s - loss: 0.7005 - accuracy: 0.31 - ETA: 0s - loss: 0.6869 - accuracy: 0.39 - ETA: 0s - loss: 0.6923 - accuracy: 0.49 - ETA: 0s - loss: 0.6960 - accuracy: 0.53 - ETA: 0s - loss: 0.6946 - accuracy: 0.48 - ETA: 0s - loss: 0.6950 - accuracy: 0.45 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.4483 - val_loss: 0.6932 - val_accuracy: 0.3045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.31 - ETA: 0s - loss: 0.6714 - accuracy: 0.70 - ETA: 0s - loss: 0.6603 - accuracy: 0.72 - ETA: 0s - loss: 0.6669 - accuracy: 0.72 - ETA: 0s - loss: 0.6851 - accuracy: 0.70 - ETA: 0s - loss: 0.6883 - accuracy: 0.65 - ETA: 0s - loss: 0.6919 - accuracy: 0.60 - ETA: 0s - loss: 0.6944 - accuracy: 0.56 - 0s 5ms/step - loss: 0.6943 - accuracy: 0.5435 - val_loss: 0.7190 - val_accuracy: 0.3045\n",
      "Epoch 11/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7166 - accuracy: 0.34 - ETA: 0s - loss: 0.6996 - accuracy: 0.31 - ETA: 0s - loss: 0.7047 - accuracy: 0.35 - ETA: 0s - loss: 0.6909 - accuracy: 0.37 - ETA: 0s - loss: 0.6912 - accuracy: 0.46 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.5139Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.5050 - val_loss: 0.6992 - val_accuracy: 0.3045\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9908 - accuracy: 0.75 - ETA: 0s - loss: 13.4662 - accuracy: 0.593 - ETA: 0s - loss: 10.5342 - accuracy: 0.602 - ETA: 0s - loss: 7.4579 - accuracy: 0.624 - ETA: 0s - loss: 5.7472 - accuracy: 0.61 - ETA: 0s - loss: 4.7228 - accuracy: 0.57 - ETA: 0s - loss: 4.0851 - accuracy: 0.53 - 1s 6ms/step - loss: 3.7718 - accuracy: 0.5327 - val_loss: 0.6712 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.65 - ETA: 0s - loss: 0.7050 - accuracy: 0.64 - ETA: 0s - loss: 0.6972 - accuracy: 0.65 - ETA: 0s - loss: 0.7030 - accuracy: 0.61 - ETA: 0s - loss: 0.7056 - accuracy: 0.54 - ETA: 0s - loss: 0.7018 - accuracy: 0.51 - 0s 4ms/step - loss: 0.6992 - accuracy: 0.5405 - val_loss: 0.6654 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.65 - ETA: 0s - loss: 0.7133 - accuracy: 0.63 - ETA: 0s - loss: 0.7026 - accuracy: 0.56 - ETA: 0s - loss: 0.7004 - accuracy: 0.49 - ETA: 0s - loss: 0.6961 - accuracy: 0.46 - ETA: 0s - loss: 0.6965 - accuracy: 0.44 - ETA: 0s - loss: 0.6905 - accuracy: 0.46 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - 0s 5ms/step - loss: 0.6939 - accuracy: 0.5024 - val_loss: 0.6955 - val_accuracy: 0.3045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.31 - ETA: 0s - loss: 0.6977 - accuracy: 0.34 - ETA: 0s - loss: 0.6920 - accuracy: 0.35 - ETA: 0s - loss: 0.6771 - accuracy: 0.49 - ETA: 0s - loss: 0.6946 - accuracy: 0.54 - ETA: 0s - loss: 0.6955 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.47 - 0s 5ms/step - loss: 0.6963 - accuracy: 0.4606 - val_loss: 0.6902 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.34 - ETA: 0s - loss: 0.7008 - accuracy: 0.34 - ETA: 0s - loss: 0.6971 - accuracy: 0.33 - ETA: 0s - loss: 0.6936 - accuracy: 0.42 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.48 - 0s 4ms/step - loss: 0.6941 - accuracy: 0.4864 - val_loss: 0.6843 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.78 - ETA: 0s - loss: 0.6884 - accuracy: 0.69 - ETA: 0s - loss: 0.6902 - accuracy: 0.64 - ETA: 0s - loss: 0.6872 - accuracy: 0.61 - ETA: 0s - loss: 0.6951 - accuracy: 0.61 - ETA: 0s - loss: 0.6946 - accuracy: 0.57 - ETA: 0s - loss: 0.6967 - accuracy: 0.54 - ETA: 0s - loss: 0.6977 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.48 - 0s 6ms/step - loss: 0.6943 - accuracy: 0.4879 - val_loss: 0.6804 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.62 - ETA: 0s - loss: 0.6843 - accuracy: 0.70 - ETA: 0s - loss: 0.6722 - accuracy: 0.71 - ETA: 0s - loss: 0.7017 - accuracy: 0.67 - ETA: 0s - loss: 0.7054 - accuracy: 0.58 - ETA: 0s - loss: 0.6992 - accuracy: 0.51 - ETA: 0s - loss: 0.6999 - accuracy: 0.52 - 0s 5ms/step - loss: 0.6963 - accuracy: 0.5524 - val_loss: 0.6762 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.65 - ETA: 0s - loss: 0.7107 - accuracy: 0.67 - ETA: 0s - loss: 0.6821 - accuracy: 0.70 - ETA: 0s - loss: 0.6832 - accuracy: 0.70 - ETA: 0s - loss: 0.6921 - accuracy: 0.69 - ETA: 0s - loss: 0.6949 - accuracy: 0.63 - ETA: 0s - loss: 0.6945 - accuracy: 0.57 - ETA: 0s - loss: 0.6947 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6943 - accuracy: 0.5278 - val_loss: 0.6863 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7437 - accuracy: 0.62 - ETA: 0s - loss: 0.6820 - accuracy: 0.70 - ETA: 0s - loss: 0.6945 - accuracy: 0.68 - ETA: 0s - loss: 0.6854 - accuracy: 0.63 - ETA: 0s - loss: 0.6879 - accuracy: 0.65 - ETA: 0s - loss: 0.6950 - accuracy: 0.61 - ETA: 0s - loss: 0.6941 - accuracy: 0.56 - 0s 5ms/step - loss: 0.6942 - accuracy: 0.5338 - val_loss: 0.6895 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7609 - accuracy: 0.34 - ETA: 0s - loss: 0.6907 - accuracy: 0.56 - ETA: 0s - loss: 0.6941 - accuracy: 0.60 - ETA: 0s - loss: 0.7020 - accuracy: 0.57 - ETA: 0s - loss: 0.7052 - accuracy: 0.51 - ETA: 0s - loss: 0.7029 - accuracy: 0.46 - ETA: 0s - loss: 0.7013 - accuracy: 0.44 - ETA: 0s - loss: 0.6980 - accuracy: 0.46 - 0s 5ms/step - loss: 0.6937 - accuracy: 0.4845 - val_loss: 0.6630 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.6584 - accuracy: 0.75 - ETA: 0s - loss: 0.6816 - accuracy: 0.71 - ETA: 0s - loss: 0.6925 - accuracy: 0.70 - ETA: 0s - loss: 0.6907 - accuracy: 0.64 - ETA: 0s - loss: 0.6901 - accuracy: 0.61 - ETA: 0s - loss: 0.6897 - accuracy: 0.63 - ETA: 0s - loss: 0.6922 - accuracy: 0.6067Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6945 - accuracy: 0.5786 - val_loss: 0.7006 - val_accuracy: 0.3045\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5129 - accuracy: 0.62 - ETA: 0s - loss: 15.4958 - accuracy: 0.603 - ETA: 0s - loss: 12.5710 - accuracy: 0.580 - ETA: 0s - loss: 9.3229 - accuracy: 0.577 - ETA: 0s - loss: 7.1878 - accuracy: 0.59 - ETA: 0s - loss: 5.8043 - accuracy: 0.61 - ETA: 0s - loss: 4.8511 - accuracy: 0.63 - 0s 6ms/step - loss: 4.4165 - accuracy: 0.6398 - val_loss: 0.6657 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7743 - accuracy: 0.62 - ETA: 0s - loss: 0.7161 - accuracy: 0.63 - ETA: 0s - loss: 0.7899 - accuracy: 0.53 - ETA: 0s - loss: 0.7693 - accuracy: 0.48 - ETA: 0s - loss: 0.7531 - accuracy: 0.47 - ETA: 0s - loss: 0.7478 - accuracy: 0.51 - 0s 4ms/step - loss: 0.7389 - accuracy: 0.5405 - val_loss: 0.6592 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.71 - ETA: 0s - loss: 0.6930 - accuracy: 0.70 - ETA: 0s - loss: 0.7020 - accuracy: 0.62 - ETA: 0s - loss: 0.7017 - accuracy: 0.53 - ETA: 0s - loss: 0.7019 - accuracy: 0.51 - ETA: 0s - loss: 0.6987 - accuracy: 0.48 - ETA: 0s - loss: 0.6983 - accuracy: 0.49 - 0s 4ms/step - loss: 0.7004 - accuracy: 0.5028 - val_loss: 0.6697 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7156 - accuracy: 0.62 - ETA: 0s - loss: 0.7135 - accuracy: 0.50 - ETA: 0s - loss: 0.6999 - accuracy: 0.42 - ETA: 0s - loss: 0.6948 - accuracy: 0.41 - ETA: 0s - loss: 0.6914 - accuracy: 0.48 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5207 - val_loss: 0.7022 - val_accuracy: 0.3045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7662 - accuracy: 0.40 - ETA: 0s - loss: 0.7091 - accuracy: 0.33 - ETA: 0s - loss: 0.6963 - accuracy: 0.32 - ETA: 0s - loss: 0.6943 - accuracy: 0.44 - ETA: 0s - loss: 0.7023 - accuracy: 0.50 - ETA: 0s - loss: 0.6991 - accuracy: 0.53 - ETA: 0s - loss: 0.7017 - accuracy: 0.53 - 0s 5ms/step - loss: 0.7017 - accuracy: 0.5177 - val_loss: 0.6963 - val_accuracy: 0.3045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.25 - ETA: 0s - loss: 0.6991 - accuracy: 0.33 - ETA: 0s - loss: 0.6935 - accuracy: 0.39 - ETA: 0s - loss: 0.6935 - accuracy: 0.48 - ETA: 0s - loss: 0.6946 - accuracy: 0.53 - ETA: 0s - loss: 0.6956 - accuracy: 0.54 - ETA: 0s - loss: 0.6950 - accuracy: 0.52 - 0s 4ms/step - loss: 0.6953 - accuracy: 0.5308 - val_loss: 0.6778 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.65 - ETA: 0s - loss: 0.6647 - accuracy: 0.71 - ETA: 0s - loss: 0.6828 - accuracy: 0.69 - ETA: 0s - loss: 0.6835 - accuracy: 0.70 - ETA: 0s - loss: 0.6857 - accuracy: 0.69 - ETA: 0s - loss: 0.6860 - accuracy: 0.67 - ETA: 0s - loss: 0.6910 - accuracy: 0.65 - 0s 5ms/step - loss: 0.6921 - accuracy: 0.6219 - val_loss: 0.7011 - val_accuracy: 0.3045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6338 - accuracy: 0.21 - ETA: 0s - loss: 0.6849 - accuracy: 0.31 - ETA: 0s - loss: 0.6921 - accuracy: 0.43 - ETA: 0s - loss: 0.6981 - accuracy: 0.48 - ETA: 0s - loss: 0.6962 - accuracy: 0.44 - ETA: 0s - loss: 0.6955 - accuracy: 0.44 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - 0s 5ms/step - loss: 0.6951 - accuracy: 0.5039 - val_loss: 0.6813 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.68 - ETA: 0s - loss: 0.7090 - accuracy: 0.51 - ETA: 0s - loss: 0.7018 - accuracy: 0.42 - ETA: 0s - loss: 0.6965 - accuracy: 0.40 - ETA: 0s - loss: 0.7003 - accuracy: 0.39 - ETA: 0s - loss: 0.7017 - accuracy: 0.38 - 0s 4ms/step - loss: 0.6949 - accuracy: 0.3890 - val_loss: 0.6650 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7555 - accuracy: 0.59 - ETA: 0s - loss: 0.6890 - accuracy: 0.70 - ETA: 0s - loss: 0.7021 - accuracy: 0.69 - ETA: 0s - loss: 0.7007 - accuracy: 0.66 - ETA: 0s - loss: 0.6985 - accuracy: 0.58 - ETA: 0s - loss: 0.6969 - accuracy: 0.58 - ETA: 0s - loss: 0.6954 - accuracy: 0.60 - 0s 5ms/step - loss: 0.6963 - accuracy: 0.5767 - val_loss: 0.6973 - val_accuracy: 0.3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.7208 - accuracy: 0.37 - ETA: 0s - loss: 0.6982 - accuracy: 0.34 - ETA: 0s - loss: 0.6931 - accuracy: 0.33 - ETA: 0s - loss: 0.7022 - accuracy: 0.37 - ETA: 0s - loss: 0.6954 - accuracy: 0.39 - ETA: 0s - loss: 0.6968 - accuracy: 0.44 - ETA: 0s - loss: 0.6944 - accuracy: 0.4838Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6944 - accuracy: 0.4856 - val_loss: 0.6832 - val_accuracy: 0.6955\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: fe5de96d056427f760bfecc7b977bcd2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6955223679542542</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9795734207006955</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 310</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9404 - accuracy: 0.62 - ETA: 0s - loss: 3.3182 - accuracy: 0.51 - ETA: 0s - loss: 2.1153 - accuracy: 0.60 - ETA: 0s - loss: 1.6378 - accuracy: 0.63 - ETA: 0s - loss: 1.4216 - accuracy: 0.64 - ETA: 0s - loss: 1.2538 - accuracy: 0.66 - ETA: 0s - loss: 1.1499 - accuracy: 0.67 - ETA: 0s - loss: 1.0903 - accuracy: 0.68 - ETA: 0s - loss: 1.0401 - accuracy: 0.67 - 1s 7ms/step - loss: 1.0397 - accuracy: 0.6782 - val_loss: 0.5726 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.84 - ETA: 0s - loss: 0.5955 - accuracy: 0.69 - ETA: 0s - loss: 0.5896 - accuracy: 0.72 - ETA: 0s - loss: 0.5787 - accuracy: 0.73 - ETA: 0s - loss: 0.5788 - accuracy: 0.74 - ETA: 0s - loss: 0.5664 - accuracy: 0.75 - ETA: 0s - loss: 0.5741 - accuracy: 0.74 - ETA: 0s - loss: 0.5711 - accuracy: 0.75 - 1s 6ms/step - loss: 0.5752 - accuracy: 0.7510 - val_loss: 0.5839 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.78 - ETA: 0s - loss: 0.5534 - accuracy: 0.79 - ETA: 0s - loss: 0.5421 - accuracy: 0.76 - ETA: 0s - loss: 0.5254 - accuracy: 0.76 - ETA: 0s - loss: 0.5133 - accuracy: 0.76 - ETA: 0s - loss: 0.5068 - accuracy: 0.77 - ETA: 0s - loss: 0.5134 - accuracy: 0.76 - ETA: 0s - loss: 0.5169 - accuracy: 0.76 - 1s 6ms/step - loss: 0.5208 - accuracy: 0.7667 - val_loss: 0.5549 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.81 - ETA: 0s - loss: 0.5113 - accuracy: 0.80 - ETA: 0s - loss: 0.4941 - accuracy: 0.81 - ETA: 0s - loss: 0.4973 - accuracy: 0.81 - ETA: 0s - loss: 0.4961 - accuracy: 0.80 - ETA: 0s - loss: 0.4985 - accuracy: 0.79 - ETA: 0s - loss: 0.5095 - accuracy: 0.79 - ETA: 0s - loss: 0.4986 - accuracy: 0.80 - ETA: 0s - loss: 0.5113 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5153 - accuracy: 0.7977 - val_loss: 0.6922 - val_accuracy: 0.6075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.71 - ETA: 0s - loss: 0.4849 - accuracy: 0.71 - ETA: 0s - loss: 0.4996 - accuracy: 0.76 - ETA: 0s - loss: 0.4793 - accuracy: 0.78 - ETA: 0s - loss: 0.4629 - accuracy: 0.79 - ETA: 0s - loss: 0.4617 - accuracy: 0.79 - ETA: 0s - loss: 0.4781 - accuracy: 0.78 - ETA: 0s - loss: 0.4828 - accuracy: 0.79 - ETA: 0s - loss: 0.4865 - accuracy: 0.79 - 1s 6ms/step - loss: 0.4911 - accuracy: 0.7917 - val_loss: 0.7150 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.90 - ETA: 0s - loss: 0.4453 - accuracy: 0.83 - ETA: 0s - loss: 0.4312 - accuracy: 0.83 - ETA: 0s - loss: 0.5257 - accuracy: 0.81 - ETA: 0s - loss: 0.5263 - accuracy: 0.80 - ETA: 0s - loss: 0.5241 - accuracy: 0.80 - ETA: 0s - loss: 0.5109 - accuracy: 0.80 - ETA: 0s - loss: 0.4995 - accuracy: 0.80 - ETA: 0s - loss: 0.5043 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5043 - accuracy: 0.8108 - val_loss: 0.5645 - val_accuracy: 0.6582\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.81 - ETA: 0s - loss: 0.4291 - accuracy: 0.78 - ETA: 0s - loss: 0.4519 - accuracy: 0.79 - ETA: 0s - loss: 0.4399 - accuracy: 0.78 - ETA: 0s - loss: 0.4300 - accuracy: 0.77 - ETA: 0s - loss: 0.4412 - accuracy: 0.76 - ETA: 0s - loss: 0.4350 - accuracy: 0.77 - ETA: 0s - loss: 0.4331 - accuracy: 0.77 - ETA: 0s - loss: 0.4335 - accuracy: 0.78 - 0s 6ms/step - loss: 0.4335 - accuracy: 0.7846 - val_loss: 0.6686 - val_accuracy: 0.7090\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.84 - ETA: 0s - loss: 0.3667 - accuracy: 0.82 - ETA: 0s - loss: 0.3710 - accuracy: 0.84 - ETA: 0s - loss: 0.3995 - accuracy: 0.83 - ETA: 0s - loss: 0.3867 - accuracy: 0.83 - ETA: 0s - loss: 0.3833 - accuracy: 0.83 - ETA: 0s - loss: 0.3833 - accuracy: 0.82 - ETA: 0s - loss: 0.3816 - accuracy: 0.83 - ETA: 0s - loss: 0.4058 - accuracy: 0.82 - 0s 6ms/step - loss: 0.4066 - accuracy: 0.8216 - val_loss: 0.6119 - val_accuracy: 0.6478\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4411 - accuracy: 0.93 - ETA: 0s - loss: 0.3752 - accuracy: 0.84 - ETA: 0s - loss: 0.3709 - accuracy: 0.84 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3827 - accuracy: 0.81 - ETA: 0s - loss: 0.3822 - accuracy: 0.80 - ETA: 0s - loss: 0.3848 - accuracy: 0.80 - ETA: 0s - loss: 0.3835 - accuracy: 0.81 - ETA: 0s - loss: 0.3818 - accuracy: 0.82 - 0s 6ms/step - loss: 0.3815 - accuracy: 0.8205 - val_loss: 0.6356 - val_accuracy: 0.6582\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.75 - ETA: 0s - loss: 0.3862 - accuracy: 0.83 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3739 - accuracy: 0.82 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.81 - ETA: 0s - loss: 0.3829 - accuracy: 0.81 - ETA: 0s - loss: 0.3787 - accuracy: 0.81 - ETA: 0s - loss: 0.3736 - accuracy: 0.81 - 1s 6ms/step - loss: 0.3662 - accuracy: 0.8152 - val_loss: 1.2049 - val_accuracy: 0.6776\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7134 - accuracy: 0.87 - ETA: 0s - loss: 0.3317 - accuracy: 0.85 - ETA: 0s - loss: 0.3204 - accuracy: 0.83 - ETA: 0s - loss: 0.3184 - accuracy: 0.83 - ETA: 0s - loss: 0.3392 - accuracy: 0.82 - ETA: 0s - loss: 0.3375 - accuracy: 0.81 - ETA: 0s - loss: 0.3403 - accuracy: 0.81 - ETA: 0s - loss: 0.3351 - accuracy: 0.81 - ETA: 0s - loss: 0.3368 - accuracy: 0.80 - 1s 6ms/step - loss: 0.3358 - accuracy: 0.8089 - val_loss: 0.6820 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - ETA: 0s - loss: 0.2592 - accuracy: 0.86 - ETA: 0s - loss: 0.2432 - accuracy: 0.88 - ETA: 0s - loss: 0.2667 - accuracy: 0.87 - ETA: 0s - loss: 0.2828 - accuracy: 0.85 - ETA: 0s - loss: 0.2945 - accuracy: 0.84 - ETA: 0s - loss: 0.3356 - accuracy: 0.82 - ETA: 0s - loss: 0.3429 - accuracy: 0.82 - ETA: 0s - loss: 0.3391 - accuracy: 0.82 - 0s 6ms/step - loss: 0.3407 - accuracy: 0.8242 - val_loss: 0.9489 - val_accuracy: 0.6657\n",
      "Epoch 13/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.96 - ETA: 0s - loss: 0.2773 - accuracy: 0.85 - ETA: 0s - loss: 0.2708 - accuracy: 0.85 - ETA: 0s - loss: 0.3305 - accuracy: 0.85 - ETA: 0s - loss: 0.3664 - accuracy: 0.83 - ETA: 0s - loss: 0.3746 - accuracy: 0.82 - ETA: 0s - loss: 0.3716 - accuracy: 0.82 - ETA: 0s - loss: 0.3734 - accuracy: 0.81 - ETA: 0s - loss: 0.3683 - accuracy: 0.80 - ETA: 0s - loss: 0.3690 - accuracy: 0.7986Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.3694 - accuracy: 0.7988 - val_loss: 0.9209 - val_accuracy: 0.6209\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.9738 - accuracy: 0.59 - ETA: 0s - loss: 2.3774 - accuracy: 0.50 - ETA: 0s - loss: 1.5386 - accuracy: 0.59 - ETA: 0s - loss: 1.2310 - accuracy: 0.64 - ETA: 0s - loss: 1.0817 - accuracy: 0.66 - ETA: 0s - loss: 0.9922 - accuracy: 0.66 - ETA: 0s - loss: 0.9366 - accuracy: 0.67 - ETA: 0s - loss: 0.8895 - accuracy: 0.67 - ETA: 0s - loss: 0.8474 - accuracy: 0.68 - 1s 7ms/step - loss: 0.8441 - accuracy: 0.6883 - val_loss: 0.6313 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.78 - ETA: 0s - loss: 0.5520 - accuracy: 0.79 - ETA: 0s - loss: 0.5486 - accuracy: 0.78 - ETA: 0s - loss: 0.5590 - accuracy: 0.77 - ETA: 0s - loss: 0.5740 - accuracy: 0.76 - ETA: 0s - loss: 0.5769 - accuracy: 0.74 - ETA: 0s - loss: 0.5844 - accuracy: 0.74 - ETA: 0s - loss: 0.5786 - accuracy: 0.74 - ETA: 0s - loss: 0.5768 - accuracy: 0.75 - 0s 6ms/step - loss: 0.5805 - accuracy: 0.7518 - val_loss: 0.5625 - val_accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.71 - ETA: 0s - loss: 0.5338 - accuracy: 0.78 - ETA: 0s - loss: 0.5444 - accuracy: 0.78 - ETA: 0s - loss: 0.5367 - accuracy: 0.78 - ETA: 0s - loss: 0.5335 - accuracy: 0.78 - ETA: 0s - loss: 0.5381 - accuracy: 0.78 - ETA: 0s - loss: 0.5306 - accuracy: 0.78 - ETA: 0s - loss: 0.5273 - accuracy: 0.78 - ETA: 0s - loss: 0.5245 - accuracy: 0.78 - ETA: 0s - loss: 0.5307 - accuracy: 0.78 - ETA: 0s - loss: 0.5363 - accuracy: 0.78 - 1s 7ms/step - loss: 0.5385 - accuracy: 0.7865 - val_loss: 0.5724 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.84 - ETA: 0s - loss: 0.6364 - accuracy: 0.77 - ETA: 0s - loss: 0.5608 - accuracy: 0.78 - ETA: 0s - loss: 0.5567 - accuracy: 0.79 - ETA: 0s - loss: 0.5459 - accuracy: 0.79 - ETA: 0s - loss: 0.5316 - accuracy: 0.80 - ETA: 0s - loss: 0.5344 - accuracy: 0.80 - ETA: 0s - loss: 0.5286 - accuracy: 0.79 - ETA: 0s - loss: 0.5389 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5404 - accuracy: 0.7917 - val_loss: 0.6324 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.75 - ETA: 0s - loss: 0.4522 - accuracy: 0.78 - ETA: 0s - loss: 0.8606 - accuracy: 0.78 - ETA: 0s - loss: 0.8146 - accuracy: 0.75 - ETA: 0s - loss: 0.7693 - accuracy: 0.75 - ETA: 0s - loss: 0.7581 - accuracy: 0.74 - ETA: 0s - loss: 0.7262 - accuracy: 0.74 - ETA: 0s - loss: 0.7061 - accuracy: 0.74 - ETA: 0s - loss: 0.6900 - accuracy: 0.74 - ETA: 0s - loss: 0.6776 - accuracy: 0.75 - 1s 6ms/step - loss: 0.6743 - accuracy: 0.7540 - val_loss: 0.6048 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.78 - ETA: 0s - loss: 0.5121 - accuracy: 0.83 - ETA: 0s - loss: 0.5328 - accuracy: 0.81 - ETA: 0s - loss: 0.5470 - accuracy: 0.80 - ETA: 0s - loss: 0.5346 - accuracy: 0.81 - ETA: 0s - loss: 0.5496 - accuracy: 0.80 - ETA: 0s - loss: 0.5508 - accuracy: 0.79 - ETA: 0s - loss: 0.5815 - accuracy: 0.78 - ETA: 0s - loss: 0.5819 - accuracy: 0.78 - ETA: 0s - loss: 0.5779 - accuracy: 0.78 - 1s 7ms/step - loss: 0.5826 - accuracy: 0.7865 - val_loss: 0.9404 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.81 - ETA: 0s - loss: 0.5498 - accuracy: 0.80 - ETA: 0s - loss: 0.5542 - accuracy: 0.81 - ETA: 0s - loss: 0.5534 - accuracy: 0.81 - ETA: 0s - loss: 0.5451 - accuracy: 0.81 - ETA: 0s - loss: 0.5467 - accuracy: 0.81 - ETA: 0s - loss: 0.5503 - accuracy: 0.80 - ETA: 0s - loss: 0.5428 - accuracy: 0.81 - ETA: 0s - loss: 0.5455 - accuracy: 0.80 - ETA: 0s - loss: 0.5463 - accuracy: 0.81 - 1s 7ms/step - loss: 0.5458 - accuracy: 0.8111 - val_loss: 0.6018 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.75 - ETA: 0s - loss: 0.5584 - accuracy: 0.79 - ETA: 0s - loss: 0.5266 - accuracy: 0.81 - ETA: 0s - loss: 0.5122 - accuracy: 0.82 - ETA: 0s - loss: 0.5134 - accuracy: 0.82 - ETA: 0s - loss: 0.5341 - accuracy: 0.81 - ETA: 0s - loss: 0.5255 - accuracy: 0.82 - ETA: 0s - loss: 0.5234 - accuracy: 0.82 - ETA: 0s - loss: 0.5256 - accuracy: 0.81 - ETA: 0s - loss: 0.5178 - accuracy: 0.82 - 1s 7ms/step - loss: 0.5197 - accuracy: 0.8193 - val_loss: 0.5449 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.81 - ETA: 0s - loss: 0.5248 - accuracy: 0.82 - ETA: 0s - loss: 0.5010 - accuracy: 0.82 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - ETA: 0s - loss: 0.5082 - accuracy: 0.81 - ETA: 0s - loss: 0.5152 - accuracy: 0.80 - ETA: 0s - loss: 0.5195 - accuracy: 0.79 - ETA: 0s - loss: 0.5455 - accuracy: 0.79 - ETA: 0s - loss: 0.5429 - accuracy: 0.79 - 1s 6ms/step - loss: 0.5477 - accuracy: 0.7954 - val_loss: 0.6596 - val_accuracy: 0.7358\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - ETA: 0s - loss: 0.5406 - accuracy: 0.82 - ETA: 0s - loss: 0.5092 - accuracy: 0.83 - ETA: 0s - loss: 0.4899 - accuracy: 0.83 - ETA: 0s - loss: 0.4784 - accuracy: 0.84 - ETA: 0s - loss: 0.4628 - accuracy: 0.85 - ETA: 0s - loss: 0.4627 - accuracy: 0.85 - ETA: 0s - loss: 0.4687 - accuracy: 0.84 - ETA: 0s - loss: 0.4764 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4820 - accuracy: 0.8406 - val_loss: 0.6330 - val_accuracy: 0.7164\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.75 - ETA: 0s - loss: 0.4531 - accuracy: 0.84 - ETA: 0s - loss: 0.4753 - accuracy: 0.83 - ETA: 0s - loss: 0.4561 - accuracy: 0.85 - ETA: 0s - loss: 0.4337 - accuracy: 0.86 - ETA: 0s - loss: 0.4464 - accuracy: 0.85 - ETA: 0s - loss: 0.4641 - accuracy: 0.85 - ETA: 0s - loss: 0.4660 - accuracy: 0.84 - ETA: 0s - loss: 0.4668 - accuracy: 0.84 - 1s 6ms/step - loss: 0.4685 - accuracy: 0.8414 - val_loss: 0.5906 - val_accuracy: 0.7149\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.84 - ETA: 0s - loss: 0.3885 - accuracy: 0.86 - ETA: 0s - loss: 0.4270 - accuracy: 0.85 - ETA: 0s - loss: 0.4540 - accuracy: 0.84 - ETA: 0s - loss: 0.4487 - accuracy: 0.85 - ETA: 0s - loss: 0.4412 - accuracy: 0.85 - ETA: 0s - loss: 0.4308 - accuracy: 0.86 - ETA: 0s - loss: 0.4413 - accuracy: 0.85 - ETA: 0s - loss: 0.4496 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4488 - accuracy: 0.8462 - val_loss: 0.8139 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.78 - ETA: 0s - loss: 0.4652 - accuracy: 0.85 - ETA: 0s - loss: 0.4039 - accuracy: 0.88 - ETA: 0s - loss: 0.4258 - accuracy: 0.88 - ETA: 0s - loss: 0.5112 - accuracy: 0.86 - ETA: 0s - loss: 0.5236 - accuracy: 0.85 - ETA: 0s - loss: 0.5164 - accuracy: 0.84 - ETA: 0s - loss: 0.5207 - accuracy: 0.84 - ETA: 0s - loss: 0.5275 - accuracy: 0.83 - 1s 6ms/step - loss: 0.5256 - accuracy: 0.8365 - val_loss: 0.6172 - val_accuracy: 0.7194\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.78 - ETA: 0s - loss: 0.4689 - accuracy: 0.81 - ETA: 0s - loss: 0.4530 - accuracy: 0.83 - ETA: 0s - loss: 0.4376 - accuracy: 0.84 - ETA: 0s - loss: 0.4410 - accuracy: 0.84 - ETA: 0s - loss: 0.4429 - accuracy: 0.85 - ETA: 0s - loss: 0.4422 - accuracy: 0.84 - ETA: 0s - loss: 0.4293 - accuracy: 0.85 - ETA: 0s - loss: 0.4288 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4685 - accuracy: 0.8518 - val_loss: 0.5896 - val_accuracy: 0.7194\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0232 - accuracy: 0.78 - ETA: 0s - loss: 0.6293 - accuracy: 0.79 - ETA: 0s - loss: 0.5450 - accuracy: 0.83 - ETA: 0s - loss: 0.5142 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.84 - ETA: 0s - loss: 0.5119 - accuracy: 0.83 - ETA: 0s - loss: 0.5243 - accuracy: 0.83 - ETA: 0s - loss: 0.5265 - accuracy: 0.83 - ETA: 0s - loss: 0.5493 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5494 - accuracy: 0.8261 - val_loss: 0.6178 - val_accuracy: 0.7269\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.84 - ETA: 0s - loss: 0.3668 - accuracy: 0.88 - ETA: 0s - loss: 0.4113 - accuracy: 0.88 - ETA: 0s - loss: 0.4333 - accuracy: 0.87 - ETA: 0s - loss: 0.5020 - accuracy: 0.85 - ETA: 0s - loss: 0.5106 - accuracy: 0.84 - ETA: 0s - loss: 0.5047 - accuracy: 0.84 - ETA: 0s - loss: 0.4887 - accuracy: 0.84 - ETA: 0s - loss: 0.4877 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4845 - accuracy: 0.8473 - val_loss: 0.6807 - val_accuracy: 0.7134\n",
      "Epoch 17/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.3180 - accuracy: 0.90 - ETA: 0s - loss: 0.3988 - accuracy: 0.88 - ETA: 0s - loss: 0.4010 - accuracy: 0.88 - ETA: 0s - loss: 0.4030 - accuracy: 0.87 - ETA: 0s - loss: 0.4100 - accuracy: 0.87 - ETA: 0s - loss: 0.4010 - accuracy: 0.87 - ETA: 0s - loss: 0.4044 - accuracy: 0.87 - ETA: 0s - loss: 0.4126 - accuracy: 0.8742Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.4150 - accuracy: 0.8712 - val_loss: 0.8563 - val_accuracy: 0.7104\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.78 - ETA: 0s - loss: 2.3915 - accuracy: 0.66 - ETA: 0s - loss: 1.8771 - accuracy: 0.60 - ETA: 0s - loss: 1.4667 - accuracy: 0.62 - ETA: 0s - loss: 1.2800 - accuracy: 0.63 - ETA: 0s - loss: 1.1407 - accuracy: 0.65 - ETA: 0s - loss: 1.0639 - accuracy: 0.66 - ETA: 0s - loss: 0.9998 - accuracy: 0.67 - ETA: 0s - loss: 0.9559 - accuracy: 0.67 - 1s 7ms/step - loss: 0.9490 - accuracy: 0.6779 - val_loss: 0.6238 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.68 - ETA: 0s - loss: 0.5425 - accuracy: 0.75 - ETA: 0s - loss: 0.5267 - accuracy: 0.76 - ETA: 0s - loss: 0.5720 - accuracy: 0.73 - ETA: 0s - loss: 0.5713 - accuracy: 0.74 - ETA: 0s - loss: 0.5724 - accuracy: 0.74 - ETA: 0s - loss: 0.5693 - accuracy: 0.74 - ETA: 0s - loss: 0.5833 - accuracy: 0.73 - ETA: 0s - loss: 0.5856 - accuracy: 0.73 - ETA: 0s - loss: 0.5973 - accuracy: 0.73 - 1s 7ms/step - loss: 0.6040 - accuracy: 0.7219 - val_loss: 0.5825 - val_accuracy: 0.7373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.78 - ETA: 0s - loss: 0.4996 - accuracy: 0.80 - ETA: 0s - loss: 0.5142 - accuracy: 0.81 - ETA: 0s - loss: 0.5112 - accuracy: 0.80 - ETA: 0s - loss: 0.5052 - accuracy: 0.80 - ETA: 0s - loss: 0.5172 - accuracy: 0.79 - ETA: 0s - loss: 0.5131 - accuracy: 0.78 - ETA: 0s - loss: 0.5185 - accuracy: 0.78 - ETA: 0s - loss: 0.5442 - accuracy: 0.78 - 1s 6ms/step - loss: 0.5502 - accuracy: 0.7824 - val_loss: 0.5721 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.93 - ETA: 0s - loss: 0.4964 - accuracy: 0.79 - ETA: 0s - loss: 0.5136 - accuracy: 0.81 - ETA: 0s - loss: 0.5182 - accuracy: 0.80 - ETA: 0s - loss: 0.5300 - accuracy: 0.80 - ETA: 0s - loss: 0.5393 - accuracy: 0.80 - ETA: 0s - loss: 0.5436 - accuracy: 0.79 - ETA: 0s - loss: 0.5400 - accuracy: 0.79 - ETA: 0s - loss: 0.5378 - accuracy: 0.79 - 1s 6ms/step - loss: 0.5365 - accuracy: 0.7951 - val_loss: 0.6474 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.87 - ETA: 0s - loss: 0.4103 - accuracy: 0.83 - ETA: 0s - loss: 0.4865 - accuracy: 0.79 - ETA: 0s - loss: 0.4765 - accuracy: 0.79 - ETA: 0s - loss: 0.4786 - accuracy: 0.79 - ETA: 0s - loss: 0.4708 - accuracy: 0.79 - ETA: 0s - loss: 0.4667 - accuracy: 0.80 - ETA: 0s - loss: 0.4605 - accuracy: 0.80 - ETA: 0s - loss: 0.4724 - accuracy: 0.81 - 0s 6ms/step - loss: 0.4759 - accuracy: 0.8093 - val_loss: 0.6091 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.81 - ETA: 0s - loss: 0.5003 - accuracy: 0.77 - ETA: 0s - loss: 0.5152 - accuracy: 0.77 - ETA: 0s - loss: 0.4944 - accuracy: 0.79 - ETA: 0s - loss: 0.4818 - accuracy: 0.81 - ETA: 0s - loss: 0.4699 - accuracy: 0.82 - ETA: 0s - loss: 0.4708 - accuracy: 0.82 - ETA: 0s - loss: 0.4719 - accuracy: 0.82 - ETA: 0s - loss: 0.4796 - accuracy: 0.80 - 0s 6ms/step - loss: 0.4943 - accuracy: 0.8040 - val_loss: 0.6733 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.78 - ETA: 0s - loss: 0.6151 - accuracy: 0.74 - ETA: 0s - loss: 0.6350 - accuracy: 0.74 - ETA: 0s - loss: 0.6016 - accuracy: 0.77 - ETA: 0s - loss: 0.5639 - accuracy: 0.78 - ETA: 0s - loss: 0.5445 - accuracy: 0.79 - ETA: 0s - loss: 0.5353 - accuracy: 0.79 - ETA: 0s - loss: 0.5268 - accuracy: 0.80 - ETA: 0s - loss: 0.5243 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5206 - accuracy: 0.8063 - val_loss: 0.7862 - val_accuracy: 0.6776\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.75 - ETA: 0s - loss: 0.5229 - accuracy: 0.79 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - ETA: 0s - loss: 0.5053 - accuracy: 0.82 - ETA: 0s - loss: 0.5087 - accuracy: 0.81 - ETA: 0s - loss: 0.5090 - accuracy: 0.82 - ETA: 0s - loss: 0.5074 - accuracy: 0.82 - ETA: 0s - loss: 0.5058 - accuracy: 0.81 - ETA: 0s - loss: 0.5049 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5051 - accuracy: 0.8182 - val_loss: 0.7289 - val_accuracy: 0.7269\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.75 - ETA: 0s - loss: 0.4477 - accuracy: 0.82 - ETA: 0s - loss: 0.4466 - accuracy: 0.82 - ETA: 0s - loss: 0.5326 - accuracy: 0.82 - ETA: 0s - loss: 0.5262 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.82 - ETA: 0s - loss: 0.5265 - accuracy: 0.81 - ETA: 0s - loss: 0.5310 - accuracy: 0.82 - ETA: 0s - loss: 0.5261 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5295 - accuracy: 0.8242 - val_loss: 0.7310 - val_accuracy: 0.6343\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.87 - ETA: 0s - loss: 0.4646 - accuracy: 0.82 - ETA: 0s - loss: 0.4434 - accuracy: 0.81 - ETA: 0s - loss: 0.4503 - accuracy: 0.82 - ETA: 0s - loss: 0.4935 - accuracy: 0.81 - ETA: 0s - loss: 0.4865 - accuracy: 0.82 - ETA: 0s - loss: 0.4904 - accuracy: 0.82 - ETA: 0s - loss: 0.4982 - accuracy: 0.81 - ETA: 0s - loss: 0.5012 - accuracy: 0.81 - ETA: 0s - loss: 0.5082 - accuracy: 0.81 - 1s 6ms/step - loss: 0.5072 - accuracy: 0.8163 - val_loss: 0.7257 - val_accuracy: 0.7269\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.84 - ETA: 0s - loss: 0.3740 - accuracy: 0.88 - ETA: 0s - loss: 0.4912 - accuracy: 0.83 - ETA: 0s - loss: 0.5102 - accuracy: 0.81 - ETA: 0s - loss: 0.5122 - accuracy: 0.81 - ETA: 0s - loss: 0.5020 - accuracy: 0.81 - ETA: 0s - loss: 0.5038 - accuracy: 0.81 - ETA: 0s - loss: 0.5032 - accuracy: 0.82 - ETA: 0s - loss: 0.5075 - accuracy: 0.82 - ETA: 0s - loss: 0.5031 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5030 - accuracy: 0.8242 - val_loss: 0.7482 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.93 - ETA: 0s - loss: 0.3782 - accuracy: 0.84 - ETA: 0s - loss: 0.4108 - accuracy: 0.82 - ETA: 0s - loss: 0.4197 - accuracy: 0.82 - ETA: 0s - loss: 0.4177 - accuracy: 0.82 - ETA: 0s - loss: 0.4415 - accuracy: 0.82 - ETA: 0s - loss: 0.4455 - accuracy: 0.82 - ETA: 0s - loss: 0.4552 - accuracy: 0.82 - ETA: 0s - loss: 0.4540 - accuracy: 0.82 - ETA: 0s - loss: 0.4525 - accuracy: 0.82 - 1s 7ms/step - loss: 0.4571 - accuracy: 0.8287 - val_loss: 0.7238 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4213 - accuracy: 0.84 - ETA: 0s - loss: 0.3944 - accuracy: 0.86 - ETA: 0s - loss: 0.4632 - accuracy: 0.84 - ETA: 0s - loss: 0.4595 - accuracy: 0.84 - ETA: 0s - loss: 0.4363 - accuracy: 0.85 - ETA: 0s - loss: 0.4317 - accuracy: 0.85 - ETA: 0s - loss: 0.4421 - accuracy: 0.85 - ETA: 0s - loss: 0.4403 - accuracy: 0.84 - ETA: 0s - loss: 0.4352 - accuracy: 0.84 - ETA: 0s - loss: 0.4322 - accuracy: 0.8484Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.4303 - accuracy: 0.8485 - val_loss: 0.7985 - val_accuracy: 0.7119\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ef8464f9878ddd202cec3b034c280b00</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7442785898844401</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.514096053590477</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 500</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1754 - accuracy: 0.31 - ETA: 0s - loss: 3.1531 - accuracy: 0.55 - ETA: 0s - loss: 2.0321 - accuracy: 0.61 - ETA: 0s - loss: 1.6042 - accuracy: 0.63 - ETA: 0s - loss: 1.3540 - accuracy: 0.66 - 0s 5ms/step - loss: 1.2751 - accuracy: 0.6670 - val_loss: 0.5952 - val_accuracy: 0.6985\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7244 - accuracy: 0.62 - ETA: 0s - loss: 0.6745 - accuracy: 0.73 - ETA: 0s - loss: 0.6717 - accuracy: 0.72 - ETA: 0s - loss: 0.6585 - accuracy: 0.73 - ETA: 0s - loss: 0.6450 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6432 - accuracy: 0.7409 - val_loss: 0.6129 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.87 - ETA: 0s - loss: 0.5920 - accuracy: 0.76 - ETA: 0s - loss: 0.6146 - accuracy: 0.75 - ETA: 0s - loss: 0.6143 - accuracy: 0.76 - ETA: 0s - loss: 0.6088 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6054 - accuracy: 0.7678 - val_loss: 0.5910 - val_accuracy: 0.7030\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4114 - accuracy: 0.90 - ETA: 0s - loss: 0.5702 - accuracy: 0.80 - ETA: 0s - loss: 0.6264 - accuracy: 0.78 - ETA: 0s - loss: 0.6258 - accuracy: 0.76 - ETA: 0s - loss: 0.6414 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6464 - accuracy: 0.7645 - val_loss: 0.5963 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.84 - ETA: 0s - loss: 0.7509 - accuracy: 0.73 - ETA: 0s - loss: 0.7121 - accuracy: 0.73 - ETA: 0s - loss: 0.6978 - accuracy: 0.73 - ETA: 0s - loss: 0.6852 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6810 - accuracy: 0.7372 - val_loss: 0.6098 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6099 - accuracy: 0.84 - ETA: 0s - loss: 0.6099 - accuracy: 0.78 - ETA: 0s - loss: 0.6595 - accuracy: 0.75 - ETA: 0s - loss: 0.6631 - accuracy: 0.75 - ETA: 0s - loss: 0.6602 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6610 - accuracy: 0.7469 - val_loss: 0.6127 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.84 - ETA: 0s - loss: 0.6821 - accuracy: 0.73 - ETA: 0s - loss: 0.6748 - accuracy: 0.73 - ETA: 0s - loss: 0.6852 - accuracy: 0.73 - ETA: 0s - loss: 0.6932 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6981 - accuracy: 0.7357 - val_loss: 0.6310 - val_accuracy: 0.7134\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.68 - ETA: 0s - loss: 0.6937 - accuracy: 0.73 - ETA: 0s - loss: 0.6972 - accuracy: 0.72 - ETA: 0s - loss: 0.7139 - accuracy: 0.70 - ETA: 0s - loss: 0.6984 - accuracy: 0.70 - 0s 3ms/step - loss: 0.7202 - accuracy: 0.7103 - val_loss: 0.6482 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.68 - ETA: 0s - loss: 0.6630 - accuracy: 0.68 - ETA: 0s - loss: 0.6971 - accuracy: 0.67 - ETA: 0s - loss: 0.6986 - accuracy: 0.64 - ETA: 0s - loss: 0.6960 - accuracy: 0.62 - ETA: 0s - loss: 0.6990 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6997 - accuracy: 0.5991 - val_loss: 0.6601 - val_accuracy: 0.6985\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.50 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6979 - accuracy: 0.55 - ETA: 0s - loss: 0.6967 - accuracy: 0.53 - ETA: 0s - loss: 0.6939 - accuracy: 0.55 - 0s 3ms/step - loss: 0.6950 - accuracy: 0.5640 - val_loss: 0.6609 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.75 - ETA: 0s - loss: 0.6804 - accuracy: 0.67 - ETA: 0s - loss: 0.6839 - accuracy: 0.61 - ETA: 0s - loss: 0.6919 - accuracy: 0.60 - ETA: 0s - loss: 0.6910 - accuracy: 0.59 - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5838 - val_loss: 0.6790 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.40 - ETA: 0s - loss: 0.6838 - accuracy: 0.46 - ETA: 0s - loss: 0.6767 - accuracy: 0.55 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6888 - accuracy: 0.59 - ETA: 0s - loss: 0.6898 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5913 - val_loss: 0.6862 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7236 - accuracy: 0.43 - ETA: 0s - loss: 0.6902 - accuracy: 0.36 - ETA: 0s - loss: 0.6849 - accuracy: 0.43 - ETA: 0s - loss: 0.6950 - accuracy: 0.48 - ETA: 0s - loss: 0.6976 - accuracy: 0.47 - ETA: 0s - loss: 0.6925 - accuracy: 0.46 - ETA: 0s - loss: 0.6906 - accuracy: 0.47 - ETA: 0s - loss: 0.6917 - accuracy: 0.48 - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5021 - val_loss: 0.6735 - val_accuracy: 0.6985\n",
      "Epoch 14/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6698 - accuracy: 0.75 - ETA: 0s - loss: 0.6870 - accuracy: 0.63 - ETA: 0s - loss: 0.6784 - accuracy: 0.63 - ETA: 0s - loss: 0.6860 - accuracy: 0.62 - ETA: 0s - loss: 0.6856 - accuracy: 0.62 - ETA: 0s - loss: 0.6894 - accuracy: 0.62 - ETA: 0s - loss: 0.6911 - accuracy: 0.62 - ETA: 0s - loss: 0.6873 - accuracy: 0.63 - ETA: 0s - loss: 0.6891 - accuracy: 0.6402Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.6379 - val_loss: 0.6820 - val_accuracy: 0.6970\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9514 - accuracy: 0.40 - ETA: 0s - loss: 4.1777 - accuracy: 0.56 - ETA: 0s - loss: 2.4693 - accuracy: 0.58 - ETA: 0s - loss: 1.8392 - accuracy: 0.62 - ETA: 0s - loss: 1.5398 - accuracy: 0.64 - 0s 5ms/step - loss: 1.3754 - accuracy: 0.6581 - val_loss: 0.6056 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.75 - ETA: 0s - loss: 0.6183 - accuracy: 0.74 - ETA: 0s - loss: 0.6015 - accuracy: 0.78 - ETA: 0s - loss: 0.6051 - accuracy: 0.76 - ETA: 0s - loss: 0.6147 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6208 - accuracy: 0.7529 - val_loss: 0.6127 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.68 - ETA: 0s - loss: 0.6169 - accuracy: 0.72 - ETA: 0s - loss: 0.6354 - accuracy: 0.72 - ETA: 0s - loss: 0.6165 - accuracy: 0.75 - ETA: 0s - loss: 0.6224 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6209 - accuracy: 0.7536 - val_loss: 0.5781 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.81 - ETA: 0s - loss: 0.6199 - accuracy: 0.74 - ETA: 0s - loss: 0.6369 - accuracy: 0.74 - ETA: 0s - loss: 0.6315 - accuracy: 0.76 - ETA: 0s - loss: 0.6341 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6415 - accuracy: 0.7551 - val_loss: 0.5935 - val_accuracy: 0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8611 - accuracy: 0.68 - ETA: 0s - loss: 0.6031 - accuracy: 0.77 - ETA: 0s - loss: 0.6143 - accuracy: 0.75 - ETA: 0s - loss: 0.6352 - accuracy: 0.74 - ETA: 0s - loss: 0.6445 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6499 - accuracy: 0.7484 - val_loss: 0.6077 - val_accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.71 - ETA: 0s - loss: 0.6839 - accuracy: 0.74 - ETA: 0s - loss: 0.7286 - accuracy: 0.75 - ETA: 0s - loss: 0.7286 - accuracy: 0.75 - ETA: 0s - loss: 0.7370 - accuracy: 0.74 - 0s 3ms/step - loss: 0.7407 - accuracy: 0.7447 - val_loss: 0.6525 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.75 - ETA: 0s - loss: 0.8498 - accuracy: 0.73 - ETA: 0s - loss: 0.8009 - accuracy: 0.69 - ETA: 0s - loss: 0.8310 - accuracy: 0.68 - ETA: 0s - loss: 0.7845 - accuracy: 0.66 - 0s 3ms/step - loss: 0.7722 - accuracy: 0.6652 - val_loss: 0.6263 - val_accuracy: 0.7104\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.71 - ETA: 0s - loss: 0.6993 - accuracy: 0.73 - ETA: 0s - loss: 0.7198 - accuracy: 0.73 - ETA: 0s - loss: 0.7102 - accuracy: 0.70 - ETA: 0s - loss: 0.6995 - accuracy: 0.69 - 0s 3ms/step - loss: 0.6967 - accuracy: 0.6932 - val_loss: 0.6214 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7441 - accuracy: 0.50 - ETA: 0s - loss: 0.6418 - accuracy: 0.68 - ETA: 0s - loss: 0.6499 - accuracy: 0.67 - ETA: 0s - loss: 0.6726 - accuracy: 0.68 - ETA: 0s - loss: 0.6725 - accuracy: 0.69 - 0s 3ms/step - loss: 0.6744 - accuracy: 0.6920 - val_loss: 0.6357 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9320 - accuracy: 0.62 - ETA: 0s - loss: 0.7861 - accuracy: 0.68 - ETA: 0s - loss: 0.7369 - accuracy: 0.69 - ETA: 0s - loss: 0.7180 - accuracy: 0.68 - ETA: 0s - loss: 0.7045 - accuracy: 0.68 - 0s 3ms/step - loss: 0.7026 - accuracy: 0.6805 - val_loss: 0.6259 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.71 - ETA: 0s - loss: 0.8069 - accuracy: 0.69 - ETA: 0s - loss: 0.9694 - accuracy: 0.63 - ETA: 0s - loss: 0.9658 - accuracy: 0.62 - ETA: 0s - loss: 0.9005 - accuracy: 0.61 - 0s 3ms/step - loss: 0.8886 - accuracy: 0.6081 - val_loss: 0.6513 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.68 - ETA: 0s - loss: 0.7090 - accuracy: 0.55 - ETA: 0s - loss: 0.7100 - accuracy: 0.55 - ETA: 0s - loss: 0.7056 - accuracy: 0.54 - ETA: 0s - loss: 0.7028 - accuracy: 0.54 - 0s 3ms/step - loss: 0.7020 - accuracy: 0.5476 - val_loss: 0.6667 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.31 - ETA: 0s - loss: 0.6980 - accuracy: 0.52 - ETA: 0s - loss: 0.7024 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.52 - ETA: 0s - loss: 0.6977 - accuracy: 0.53 - 0s 3ms/step - loss: 0.6972 - accuracy: 0.5334 - val_loss: 0.6725 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.59 - ETA: 0s - loss: 0.7024 - accuracy: 0.54 - ETA: 0s - loss: 0.6947 - accuracy: 0.55 - ETA: 0s - loss: 0.6937 - accuracy: 0.55 - ETA: 0s - loss: 0.6943 - accuracy: 0.54 - 0s 3ms/step - loss: 0.6960 - accuracy: 0.5420 - val_loss: 0.6814 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.6972 - accuracy: 0.56 - ETA: 0s - loss: 0.6978 - accuracy: 0.47 - ETA: 0s - loss: 0.6976 - accuracy: 0.52 - ETA: 0s - loss: 0.6964 - accuracy: 0.55 - ETA: 0s - loss: 0.6955 - accuracy: 0.5347Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6955 - accuracy: 0.5416 - val_loss: 0.6773 - val_accuracy: 0.6955\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.59 - ETA: 0s - loss: 5.8059 - accuracy: 0.62 - ETA: 0s - loss: 3.1617 - accuracy: 0.62 - ETA: 0s - loss: 2.3152 - accuracy: 0.64 - ETA: 0s - loss: 1.8617 - accuracy: 0.66 - 0s 4ms/step - loss: 1.6875 - accuracy: 0.6809 - val_loss: 0.5977 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.65 - ETA: 0s - loss: 0.6936 - accuracy: 0.70 - ETA: 0s - loss: 0.6696 - accuracy: 0.69 - ETA: 0s - loss: 0.6575 - accuracy: 0.71 - ETA: 0s - loss: 0.6430 - accuracy: 0.72 - 0s 3ms/step - loss: 0.6389 - accuracy: 0.7301 - val_loss: 0.5979 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.84 - ETA: 0s - loss: 0.5369 - accuracy: 0.77 - ETA: 0s - loss: 0.5761 - accuracy: 0.77 - ETA: 0s - loss: 0.5955 - accuracy: 0.76 - ETA: 0s - loss: 0.6024 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5994 - accuracy: 0.7581 - val_loss: 0.5851 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.78 - ETA: 0s - loss: 0.6018 - accuracy: 0.74 - ETA: 0s - loss: 0.6164 - accuracy: 0.75 - ETA: 0s - loss: 0.5998 - accuracy: 0.76 - ETA: 0s - loss: 0.5970 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5942 - accuracy: 0.7716 - val_loss: 0.5691 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.84 - ETA: 0s - loss: 0.5914 - accuracy: 0.78 - ETA: 0s - loss: 0.5737 - accuracy: 0.78 - ETA: 0s - loss: 0.5789 - accuracy: 0.78 - ETA: 0s - loss: 0.5960 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6015 - accuracy: 0.7749 - val_loss: 0.5573 - val_accuracy: 0.7448\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.78 - ETA: 0s - loss: 0.5688 - accuracy: 0.76 - ETA: 0s - loss: 0.5893 - accuracy: 0.76 - ETA: 0s - loss: 0.5981 - accuracy: 0.76 - ETA: 0s - loss: 0.6089 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6173 - accuracy: 0.7633 - val_loss: 0.5520 - val_accuracy: 0.7597\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.71 - ETA: 0s - loss: 0.6729 - accuracy: 0.75 - ETA: 0s - loss: 0.6500 - accuracy: 0.75 - ETA: 0s - loss: 0.6756 - accuracy: 0.75 - ETA: 0s - loss: 0.6869 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6799 - accuracy: 0.7566 - val_loss: 0.5716 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7819 - accuracy: 0.71 - ETA: 0s - loss: 0.6216 - accuracy: 0.76 - ETA: 0s - loss: 0.6696 - accuracy: 0.76 - ETA: 0s - loss: 0.7230 - accuracy: 0.75 - ETA: 0s - loss: 0.7357 - accuracy: 0.75 - 0s 3ms/step - loss: 0.7382 - accuracy: 0.7525 - val_loss: 0.5884 - val_accuracy: 0.7179\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3708 - accuracy: 0.90 - ETA: 0s - loss: 0.7570 - accuracy: 0.74 - ETA: 0s - loss: 0.7406 - accuracy: 0.72 - ETA: 0s - loss: 0.7313 - accuracy: 0.72 - ETA: 0s - loss: 0.7119 - accuracy: 0.72 - 0s 3ms/step - loss: 0.7073 - accuracy: 0.7294 - val_loss: 0.6199 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.75 - ETA: 0s - loss: 0.7962 - accuracy: 0.75 - ETA: 0s - loss: 0.7445 - accuracy: 0.72 - ETA: 0s - loss: 0.7462 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7513 - accuracy: 0.7283 - val_loss: 0.6210 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7771 - accuracy: 0.78 - ETA: 0s - loss: 0.6591 - accuracy: 0.75 - ETA: 0s - loss: 0.7134 - accuracy: 0.73 - ETA: 0s - loss: 0.7410 - accuracy: 0.74 - ETA: 0s - loss: 0.7251 - accuracy: 0.72 - 0s 3ms/step - loss: 0.7276 - accuracy: 0.7182 - val_loss: 0.6375 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.65 - ETA: 0s - loss: 0.6959 - accuracy: 0.65 - ETA: 0s - loss: 0.6970 - accuracy: 0.61 - ETA: 0s - loss: 0.6891 - accuracy: 0.61 - ETA: 0s - loss: 0.6891 - accuracy: 0.63 - 0s 3ms/step - loss: 0.6873 - accuracy: 0.6368 - val_loss: 0.6422 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7275 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.65 - ETA: 0s - loss: 0.6792 - accuracy: 0.64 - ETA: 0s - loss: 0.6757 - accuracy: 0.63 - ETA: 0s - loss: 0.6807 - accuracy: 0.63 - 0s 3ms/step - loss: 0.6817 - accuracy: 0.6290 - val_loss: 0.6518 - val_accuracy: 0.7090\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5487 - accuracy: 0.78 - ETA: 0s - loss: 0.6952 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - 0s 3ms/step - loss: 0.6762 - accuracy: 0.5901 - val_loss: 0.6424 - val_accuracy: 0.7119\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8255 - accuracy: 0.50 - ETA: 0s - loss: 0.6441 - accuracy: 0.73 - ETA: 0s - loss: 0.6738 - accuracy: 0.71 - ETA: 0s - loss: 0.6838 - accuracy: 0.67 - ETA: 0s - loss: 0.7151 - accuracy: 0.63 - 0s 3ms/step - loss: 0.7123 - accuracy: 0.6372 - val_loss: 0.6592 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6283 - accuracy: 0.65 - ETA: 0s - loss: 0.6997 - accuracy: 0.68 - ETA: 0s - loss: 0.7329 - accuracy: 0.61 - ETA: 0s - loss: 0.7273 - accuracy: 0.58 - ETA: 0s - loss: 0.7142 - accuracy: 0.5990Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.7168 - accuracy: 0.5984 - val_loss: 0.6625 - val_accuracy: 0.6970\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b7915638ac499f3527679542671a665e</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7467661698659261</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8995450693680139</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 185</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.68 - ETA: 0s - loss: 2.4629 - accuracy: 0.59 - ETA: 0s - loss: 1.9234 - accuracy: 0.61 - ETA: 0s - loss: 1.5523 - accuracy: 0.63 - ETA: 0s - loss: 1.3564 - accuracy: 0.64 - 0s 5ms/step - loss: 1.2712 - accuracy: 0.6398 - val_loss: 0.6136 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.65 - ETA: 0s - loss: 0.6665 - accuracy: 0.71 - ETA: 0s - loss: 0.6579 - accuracy: 0.71 - ETA: 0s - loss: 0.6294 - accuracy: 0.71 - ETA: 0s - loss: 0.6113 - accuracy: 0.72 - 0s 3ms/step - loss: 0.6197 - accuracy: 0.7152 - val_loss: 0.6208 - val_accuracy: 0.6851\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.68 - ETA: 0s - loss: 0.5708 - accuracy: 0.72 - ETA: 0s - loss: 0.5497 - accuracy: 0.75 - ETA: 0s - loss: 0.5728 - accuracy: 0.74 - ETA: 0s - loss: 0.5696 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5692 - accuracy: 0.7454 - val_loss: 0.6093 - val_accuracy: 0.7060\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.81 - ETA: 0s - loss: 0.5727 - accuracy: 0.76 - ETA: 0s - loss: 0.5511 - accuracy: 0.77 - ETA: 0s - loss: 0.5520 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5512 - accuracy: 0.7678 - val_loss: 0.5483 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4487 - accuracy: 0.84 - ETA: 0s - loss: 0.4296 - accuracy: 0.81 - ETA: 0s - loss: 0.4528 - accuracy: 0.80 - ETA: 0s - loss: 0.4985 - accuracy: 0.79 - ETA: 0s - loss: 0.4962 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7943 - val_loss: 0.6091 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.81 - ETA: 0s - loss: 0.5013 - accuracy: 0.79 - ETA: 0s - loss: 0.5351 - accuracy: 0.79 - ETA: 0s - loss: 0.5025 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4913 - accuracy: 0.8134 - val_loss: 0.6215 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.81 - ETA: 0s - loss: 0.4604 - accuracy: 0.81 - ETA: 0s - loss: 0.4601 - accuracy: 0.81 - ETA: 0s - loss: 0.4760 - accuracy: 0.81 - ETA: 0s - loss: 0.5245 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5214 - accuracy: 0.8145 - val_loss: 0.7993 - val_accuracy: 0.6910\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.78 - ETA: 0s - loss: 0.5945 - accuracy: 0.81 - ETA: 0s - loss: 0.7115 - accuracy: 0.80 - ETA: 0s - loss: 0.7337 - accuracy: 0.79 - ETA: 0s - loss: 0.7037 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6876 - accuracy: 0.7910 - val_loss: 0.7169 - val_accuracy: 0.7119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.84 - ETA: 0s - loss: 0.6191 - accuracy: 0.80 - ETA: 0s - loss: 0.5838 - accuracy: 0.80 - ETA: 0s - loss: 0.5732 - accuracy: 0.80 - ETA: 0s - loss: 0.5716 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5695 - accuracy: 0.8089 - val_loss: 1.0053 - val_accuracy: 0.7000\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0593 - accuracy: 0.71 - ETA: 0s - loss: 0.6737 - accuracy: 0.81 - ETA: 0s - loss: 0.6147 - accuracy: 0.81 - ETA: 0s - loss: 0.5899 - accuracy: 0.81 - ETA: 0s - loss: 0.5995 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5897 - accuracy: 0.8052 - val_loss: 0.6576 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.75 - ETA: 0s - loss: 0.4523 - accuracy: 0.81 - ETA: 0s - loss: 0.5021 - accuracy: 0.82 - ETA: 0s - loss: 0.5691 - accuracy: 0.81 - ETA: 0s - loss: 0.5988 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5919 - accuracy: 0.8141 - val_loss: 0.8508 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.78 - ETA: 0s - loss: 0.5966 - accuracy: 0.82 - ETA: 0s - loss: 0.5902 - accuracy: 0.81 - ETA: 0s - loss: 0.6563 - accuracy: 0.80 - ETA: 0s - loss: 0.7445 - accuracy: 0.78 - 0s 3ms/step - loss: 0.7586 - accuracy: 0.7831 - val_loss: 0.6992 - val_accuracy: 0.7209\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5555 - accuracy: 0.75 - ETA: 0s - loss: 0.7622 - accuracy: 0.78 - ETA: 0s - loss: 0.7647 - accuracy: 0.74 - ETA: 0s - loss: 0.7458 - accuracy: 0.74 - ETA: 0s - loss: 0.6975 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6860 - accuracy: 0.7704 - val_loss: 0.6759 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.81 - ETA: 0s - loss: 0.6441 - accuracy: 0.80 - ETA: 0s - loss: 0.5784 - accuracy: 0.82 - ETA: 0s - loss: 0.5690 - accuracy: 0.81 - ETA: 0s - loss: 0.5775 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5700 - accuracy: 0.8137 - val_loss: 0.7127 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5184 - accuracy: 0.81 - ETA: 0s - loss: 0.4998 - accuracy: 0.85 - ETA: 0s - loss: 0.4941 - accuracy: 0.84 - ETA: 0s - loss: 0.5563 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5669 - accuracy: 0.8302 - val_loss: 0.7432 - val_accuracy: 0.7493\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.87 - ETA: 0s - loss: 0.5204 - accuracy: 0.84 - ETA: 0s - loss: 0.6066 - accuracy: 0.82 - ETA: 0s - loss: 0.6495 - accuracy: 0.82 - 0s 3ms/step - loss: 0.6443 - accuracy: 0.8216 - val_loss: 0.9014 - val_accuracy: 0.7284\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.84 - ETA: 0s - loss: 0.6922 - accuracy: 0.83 - ETA: 0s - loss: 0.7098 - accuracy: 0.82 - ETA: 0s - loss: 0.7042 - accuracy: 0.80 - ETA: 0s - loss: 0.6740 - accuracy: 0.81 - 0s 3ms/step - loss: 0.6665 - accuracy: 0.8066 - val_loss: 0.7055 - val_accuracy: 0.7403\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5972 - accuracy: 0.75 - ETA: 0s - loss: 0.5936 - accuracy: 0.82 - ETA: 0s - loss: 0.5970 - accuracy: 0.82 - ETA: 0s - loss: 0.6148 - accuracy: 0.82 - ETA: 0s - loss: 0.5963 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5956 - accuracy: 0.8249 - val_loss: 0.6263 - val_accuracy: 0.7433\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.78 - ETA: 0s - loss: 0.6586 - accuracy: 0.82 - ETA: 0s - loss: 0.6071 - accuracy: 0.83 - ETA: 0s - loss: 0.6103 - accuracy: 0.83 - ETA: 0s - loss: 0.5772 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5612 - accuracy: 0.8365 - val_loss: 0.7088 - val_accuracy: 0.7507\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4418 - accuracy: 0.84 - ETA: 0s - loss: 0.5918 - accuracy: 0.84 - ETA: 0s - loss: 0.5941 - accuracy: 0.84 - ETA: 0s - loss: 0.5620 - accuracy: 0.84 - ETA: 0s - loss: 0.5322 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5258 - accuracy: 0.8518 - val_loss: 0.8602 - val_accuracy: 0.7194\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7359 - accuracy: 0.75 - ETA: 0s - loss: 0.5167 - accuracy: 0.85 - ETA: 0s - loss: 0.5474 - accuracy: 0.84 - ETA: 0s - loss: 0.6556 - accuracy: 0.84 - ETA: 0s - loss: 0.6484 - accuracy: 0.83 - 0s 4ms/step - loss: 0.6227 - accuracy: 0.8406 - val_loss: 0.5767 - val_accuracy: 0.7597\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.81 - ETA: 0s - loss: 0.5277 - accuracy: 0.85 - ETA: 0s - loss: 0.5726 - accuracy: 0.84 - ETA: 0s - loss: 0.5740 - accuracy: 0.83 - ETA: 0s - loss: 0.5704 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5585 - accuracy: 0.8369 - val_loss: 1.0277 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8313 - accuracy: 0.62 - ETA: 0s - loss: 0.5095 - accuracy: 0.83 - ETA: 0s - loss: 0.7676 - accuracy: 0.83 - ETA: 0s - loss: 0.7129 - accuracy: 0.83 - ETA: 0s - loss: 0.6867 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6739 - accuracy: 0.8313 - val_loss: 0.6962 - val_accuracy: 0.7478\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.90 - ETA: 0s - loss: 0.4803 - accuracy: 0.83 - ETA: 0s - loss: 0.4902 - accuracy: 0.84 - ETA: 0s - loss: 0.4960 - accuracy: 0.84 - ETA: 0s - loss: 0.5008 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4986 - accuracy: 0.8417 - val_loss: 0.6864 - val_accuracy: 0.7269\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.84 - ETA: 0s - loss: 0.4602 - accuracy: 0.85 - ETA: 0s - loss: 0.4781 - accuracy: 0.84 - ETA: 0s - loss: 0.4729 - accuracy: 0.84 - ETA: 0s - loss: 0.4632 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4566 - accuracy: 0.8555 - val_loss: 0.9969 - val_accuracy: 0.7388\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.81 - ETA: 0s - loss: 0.4263 - accuracy: 0.87 - ETA: 0s - loss: 0.4380 - accuracy: 0.87 - ETA: 0s - loss: 0.4342 - accuracy: 0.87 - ETA: 0s - loss: 0.4912 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4949 - accuracy: 0.8675 - val_loss: 0.9249 - val_accuracy: 0.7313\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.90 - ETA: 0s - loss: 0.4735 - accuracy: 0.87 - ETA: 0s - loss: 0.4523 - accuracy: 0.87 - ETA: 0s - loss: 0.4758 - accuracy: 0.87 - ETA: 0s - loss: 0.4604 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4596 - accuracy: 0.8727 - val_loss: 1.1708 - val_accuracy: 0.7328\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4087 - accuracy: 0.87 - ETA: 0s - loss: 0.4044 - accuracy: 0.87 - ETA: 0s - loss: 0.4303 - accuracy: 0.88 - ETA: 0s - loss: 0.4449 - accuracy: 0.88 - ETA: 0s - loss: 0.4431 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4825 - accuracy: 0.8794 - val_loss: 0.9614 - val_accuracy: 0.7328\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.84 - ETA: 0s - loss: 0.4702 - accuracy: 0.86 - ETA: 0s - loss: 0.4588 - accuracy: 0.86 - ETA: 0s - loss: 0.4489 - accuracy: 0.86 - ETA: 0s - loss: 0.4294 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8757 - val_loss: 1.1970 - val_accuracy: 0.7403\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4239 - accuracy: 0.87 - ETA: 0s - loss: 0.4330 - accuracy: 0.88 - ETA: 0s - loss: 0.4082 - accuracy: 0.88 - ETA: 0s - loss: 0.4007 - accuracy: 0.89 - ETA: 0s - loss: 0.4045 - accuracy: 0.89 - 0s 3ms/step - loss: 0.4041 - accuracy: 0.8910 - val_loss: 1.8596 - val_accuracy: 0.7119\n",
      "Epoch 31/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.2921 - accuracy: 0.93 - ETA: 0s - loss: 0.4779 - accuracy: 0.86 - ETA: 0s - loss: 0.4328 - accuracy: 0.87 - ETA: 0s - loss: 0.4409 - accuracy: 0.88 - ETA: 0s - loss: 0.4285 - accuracy: 0.8832Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.8839 - val_loss: 1.1259 - val_accuracy: 0.7299\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8851 - accuracy: 0.31 - ETA: 0s - loss: 2.6497 - accuracy: 0.58 - ETA: 0s - loss: 1.8516 - accuracy: 0.58 - ETA: 0s - loss: 1.5601 - accuracy: 0.59 - ETA: 0s - loss: 1.3273 - accuracy: 0.61 - 0s 4ms/step - loss: 1.2284 - accuracy: 0.6204 - val_loss: 0.5892 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7679 - accuracy: 0.53 - ETA: 0s - loss: 0.6011 - accuracy: 0.70 - ETA: 0s - loss: 0.6044 - accuracy: 0.70 - ETA: 0s - loss: 0.6030 - accuracy: 0.71 - ETA: 0s - loss: 0.5930 - accuracy: 0.72 - 0s 3ms/step - loss: 0.5925 - accuracy: 0.7156 - val_loss: 0.5722 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.75 - ETA: 0s - loss: 0.6502 - accuracy: 0.71 - ETA: 0s - loss: 0.6257 - accuracy: 0.71 - ETA: 0s - loss: 0.6076 - accuracy: 0.72 - ETA: 0s - loss: 0.6032 - accuracy: 0.72 - 0s 3ms/step - loss: 0.5991 - accuracy: 0.7309 - val_loss: 0.6142 - val_accuracy: 0.7224\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.78 - ETA: 0s - loss: 0.5403 - accuracy: 0.77 - ETA: 0s - loss: 0.5463 - accuracy: 0.76 - ETA: 0s - loss: 0.5767 - accuracy: 0.76 - ETA: 0s - loss: 0.5775 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5881 - accuracy: 0.7663 - val_loss: 0.5646 - val_accuracy: 0.7045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.87 - ETA: 0s - loss: 0.6215 - accuracy: 0.78 - ETA: 0s - loss: 0.5665 - accuracy: 0.76 - ETA: 0s - loss: 0.5579 - accuracy: 0.76 - ETA: 0s - loss: 0.5572 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5608 - accuracy: 0.7723 - val_loss: 0.6633 - val_accuracy: 0.6866\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.84 - ETA: 0s - loss: 0.4536 - accuracy: 0.79 - ETA: 0s - loss: 0.4807 - accuracy: 0.80 - ETA: 0s - loss: 0.4757 - accuracy: 0.79 - ETA: 0s - loss: 0.4894 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7884 - val_loss: 0.6337 - val_accuracy: 0.6896\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.81 - ETA: 0s - loss: 0.4576 - accuracy: 0.80 - ETA: 0s - loss: 0.4493 - accuracy: 0.81 - ETA: 0s - loss: 0.4514 - accuracy: 0.80 - ETA: 0s - loss: 0.4565 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4605 - accuracy: 0.8122 - val_loss: 0.5971 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4561 - accuracy: 0.78 - ETA: 0s - loss: 0.4936 - accuracy: 0.81 - ETA: 0s - loss: 0.4986 - accuracy: 0.80 - ETA: 0s - loss: 0.5129 - accuracy: 0.81 - ETA: 0s - loss: 0.4980 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4965 - accuracy: 0.8093 - val_loss: 0.5930 - val_accuracy: 0.7075\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.84 - ETA: 0s - loss: 0.4987 - accuracy: 0.79 - ETA: 0s - loss: 0.4916 - accuracy: 0.80 - ETA: 0s - loss: 0.4958 - accuracy: 0.81 - ETA: 0s - loss: 0.4868 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4896 - accuracy: 0.8100 - val_loss: 0.7203 - val_accuracy: 0.6940\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.4364 - accuracy: 0.80 - ETA: 0s - loss: 0.4097 - accuracy: 0.81 - ETA: 0s - loss: 0.4140 - accuracy: 0.82 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8145 - val_loss: 0.6772 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.78 - ETA: 0s - loss: 0.3758 - accuracy: 0.81 - ETA: 0s - loss: 0.4672 - accuracy: 0.82 - ETA: 0s - loss: 0.4620 - accuracy: 0.82 - ETA: 0s - loss: 0.4628 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8219 - val_loss: 0.6685 - val_accuracy: 0.6881\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.71 - ETA: 0s - loss: 0.4043 - accuracy: 0.81 - ETA: 0s - loss: 0.4138 - accuracy: 0.82 - ETA: 0s - loss: 0.4588 - accuracy: 0.82 - ETA: 0s - loss: 0.4828 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5002 - accuracy: 0.8167 - val_loss: 0.6200 - val_accuracy: 0.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.75 - ETA: 0s - loss: 0.4675 - accuracy: 0.80 - ETA: 0s - loss: 0.5150 - accuracy: 0.82 - ETA: 0s - loss: 0.5011 - accuracy: 0.82 - ETA: 0s - loss: 0.4949 - accuracy: 0.8205Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4949 - accuracy: 0.8205 - val_loss: 0.7481 - val_accuracy: 0.7075\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7486 - accuracy: 0.59 - ETA: 0s - loss: 2.8223 - accuracy: 0.61 - ETA: 0s - loss: 2.1212 - accuracy: 0.61 - ETA: 0s - loss: 1.6471 - accuracy: 0.63 - ETA: 0s - loss: 1.4370 - accuracy: 0.64 - 0s 5ms/step - loss: 1.3171 - accuracy: 0.6529 - val_loss: 0.5996 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.65 - ETA: 0s - loss: 0.6489 - accuracy: 0.70 - ETA: 0s - loss: 0.6440 - accuracy: 0.71 - ETA: 0s - loss: 0.6490 - accuracy: 0.70 - ETA: 0s - loss: 0.6352 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6397 - accuracy: 0.7103 - val_loss: 0.5918 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.81 - ETA: 0s - loss: 0.5703 - accuracy: 0.74 - ETA: 0s - loss: 0.5863 - accuracy: 0.75 - ETA: 0s - loss: 0.5876 - accuracy: 0.75 - ETA: 0s - loss: 0.5826 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5875 - accuracy: 0.7499 - val_loss: 0.6092 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5124 - accuracy: 0.78 - ETA: 0s - loss: 0.4960 - accuracy: 0.79 - ETA: 0s - loss: 0.5218 - accuracy: 0.79 - ETA: 0s - loss: 0.5519 - accuracy: 0.78 - ETA: 0s - loss: 0.5610 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5768 - accuracy: 0.7734 - val_loss: 0.5847 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.81 - ETA: 0s - loss: 0.5224 - accuracy: 0.80 - ETA: 0s - loss: 0.5669 - accuracy: 0.78 - ETA: 0s - loss: 0.5714 - accuracy: 0.77 - ETA: 0s - loss: 0.5796 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5749 - accuracy: 0.7775 - val_loss: 0.6159 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.81 - ETA: 0s - loss: 0.5333 - accuracy: 0.80 - ETA: 0s - loss: 0.6247 - accuracy: 0.78 - ETA: 0s - loss: 0.6188 - accuracy: 0.77 - ETA: 0s - loss: 0.6347 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6164 - accuracy: 0.7798 - val_loss: 0.6080 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.78 - ETA: 0s - loss: 0.5436 - accuracy: 0.80 - ETA: 0s - loss: 0.5465 - accuracy: 0.79 - ETA: 0s - loss: 0.5361 - accuracy: 0.79 - ETA: 0s - loss: 0.5371 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5339 - accuracy: 0.7936 - val_loss: 0.5683 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.84 - ETA: 0s - loss: 0.4301 - accuracy: 0.84 - ETA: 0s - loss: 0.4361 - accuracy: 0.83 - ETA: 0s - loss: 0.4667 - accuracy: 0.82 - ETA: 0s - loss: 0.4905 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4979 - accuracy: 0.8119 - val_loss: 0.6064 - val_accuracy: 0.7313\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.68 - ETA: 0s - loss: 0.5844 - accuracy: 0.79 - ETA: 0s - loss: 0.5586 - accuracy: 0.80 - ETA: 0s - loss: 0.5383 - accuracy: 0.80 - ETA: 0s - loss: 0.5515 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5538 - accuracy: 0.8010 - val_loss: 0.6808 - val_accuracy: 0.7075\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5027 - accuracy: 0.81 - ETA: 0s - loss: 0.5503 - accuracy: 0.84 - ETA: 0s - loss: 0.5447 - accuracy: 0.82 - ETA: 0s - loss: 0.5312 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5164 - accuracy: 0.8268 - val_loss: 0.6996 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7398 - accuracy: 0.78 - ETA: 0s - loss: 0.5379 - accuracy: 0.81 - ETA: 0s - loss: 0.5606 - accuracy: 0.81 - ETA: 0s - loss: 0.5277 - accuracy: 0.82 - ETA: 0s - loss: 0.5070 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5031 - accuracy: 0.8331 - val_loss: 0.6090 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4188 - accuracy: 0.87 - ETA: 0s - loss: 0.5122 - accuracy: 0.82 - ETA: 0s - loss: 0.5241 - accuracy: 0.83 - ETA: 0s - loss: 0.5161 - accuracy: 0.83 - ETA: 0s - loss: 0.5354 - accuracy: 0.8240Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.8216 - val_loss: 0.6439 - val_accuracy: 0.7104\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 9c7433a77a1464715baa576dda4411cf</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7402985095977783</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6847305875606441</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 105</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.75 - ETA: 0s - loss: 4.9942 - accuracy: 0.63 - ETA: 0s - loss: 2.6886 - accuracy: 0.67 - ETA: 0s - loss: 1.9366 - accuracy: 0.69 - ETA: 0s - loss: 1.5664 - accuracy: 0.70 - ETA: 0s - loss: 1.3662 - accuracy: 0.70 - 0s 6ms/step - loss: 1.2591 - accuracy: 0.7088 - val_loss: 0.5880 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.87 - ETA: 0s - loss: 0.5487 - accuracy: 0.73 - ETA: 0s - loss: 0.5801 - accuracy: 0.71 - ETA: 0s - loss: 0.5625 - accuracy: 0.73 - ETA: 0s - loss: 0.5301 - accuracy: 0.76 - ETA: 0s - loss: 0.5289 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5352 - accuracy: 0.7689 - val_loss: 0.6085 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.87 - ETA: 0s - loss: 0.6539 - accuracy: 0.82 - ETA: 0s - loss: 0.5934 - accuracy: 0.79 - ETA: 0s - loss: 0.5644 - accuracy: 0.79 - ETA: 0s - loss: 0.5456 - accuracy: 0.78 - ETA: 0s - loss: 0.5258 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5321 - accuracy: 0.7865 - val_loss: 0.5509 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4553 - accuracy: 0.78 - ETA: 0s - loss: 0.4996 - accuracy: 0.75 - ETA: 0s - loss: 0.6161 - accuracy: 0.76 - ETA: 0s - loss: 0.5698 - accuracy: 0.77 - ETA: 0s - loss: 0.5376 - accuracy: 0.79 - ETA: 0s - loss: 0.5359 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5308 - accuracy: 0.7940 - val_loss: 0.6058 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.90 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.4349 - accuracy: 0.83 - ETA: 0s - loss: 0.4255 - accuracy: 0.83 - ETA: 0s - loss: 0.4432 - accuracy: 0.81 - ETA: 0s - loss: 0.4518 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4583 - accuracy: 0.8163 - val_loss: 0.7505 - val_accuracy: 0.6687\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.81 - ETA: 0s - loss: 0.5370 - accuracy: 0.81 - ETA: 0s - loss: 0.5844 - accuracy: 0.79 - ETA: 0s - loss: 0.5401 - accuracy: 0.78 - ETA: 0s - loss: 0.5590 - accuracy: 0.77 - ETA: 0s - loss: 0.5577 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5465 - accuracy: 0.7805 - val_loss: 0.7782 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.87 - ETA: 0s - loss: 0.4569 - accuracy: 0.80 - ETA: 0s - loss: 0.4526 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4412 - accuracy: 0.80 - ETA: 0s - loss: 0.4563 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7981 - val_loss: 0.7250 - val_accuracy: 0.6910\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.90 - ETA: 0s - loss: 0.3704 - accuracy: 0.86 - ETA: 0s - loss: 0.3738 - accuracy: 0.84 - ETA: 0s - loss: 0.3749 - accuracy: 0.84 - ETA: 0s - loss: 0.3904 - accuracy: 0.84 - ETA: 0s - loss: 0.4100 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4115 - accuracy: 0.8309 - val_loss: 0.5957 - val_accuracy: 0.7015\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.93 - ETA: 0s - loss: 0.3709 - accuracy: 0.84 - ETA: 0s - loss: 0.3870 - accuracy: 0.84 - ETA: 0s - loss: 0.3714 - accuracy: 0.85 - ETA: 0s - loss: 0.3803 - accuracy: 0.84 - ETA: 0s - loss: 0.3792 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8533 - val_loss: 0.8768 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.90 - ETA: 0s - loss: 0.3521 - accuracy: 0.87 - ETA: 0s - loss: 0.3300 - accuracy: 0.87 - ETA: 0s - loss: 0.3294 - accuracy: 0.87 - ETA: 0s - loss: 0.5073 - accuracy: 0.86 - ETA: 0s - loss: 0.4849 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4827 - accuracy: 0.8630 - val_loss: 0.5872 - val_accuracy: 0.7418\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7608 - accuracy: 0.75 - ETA: 0s - loss: 0.3916 - accuracy: 0.88 - ETA: 0s - loss: 0.4040 - accuracy: 0.87 - ETA: 0s - loss: 0.4525 - accuracy: 0.84 - ETA: 0s - loss: 0.4633 - accuracy: 0.83 - ETA: 0s - loss: 0.4528 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4502 - accuracy: 0.8376 - val_loss: 0.9947 - val_accuracy: 0.6627\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.84 - ETA: 0s - loss: 0.3389 - accuracy: 0.85 - ETA: 0s - loss: 0.3710 - accuracy: 0.84 - ETA: 0s - loss: 0.3485 - accuracy: 0.85 - ETA: 0s - loss: 0.3472 - accuracy: 0.85 - ETA: 0s - loss: 0.3521 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3537 - accuracy: 0.8604 - val_loss: 1.2861 - val_accuracy: 0.6507\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.90 - ETA: 0s - loss: 0.3259 - accuracy: 0.86 - ETA: 0s - loss: 0.3311 - accuracy: 0.87 - ETA: 0s - loss: 0.3251 - accuracy: 0.87 - ETA: 0s - loss: 0.3236 - accuracy: 0.87 - ETA: 0s - loss: 0.3083 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3103 - accuracy: 0.8820 - val_loss: 1.4722 - val_accuracy: 0.6866\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.93 - ETA: 0s - loss: 0.3415 - accuracy: 0.89 - ETA: 0s - loss: 0.3325 - accuracy: 0.88 - ETA: 0s - loss: 0.3200 - accuracy: 0.89 - ETA: 0s - loss: 0.3335 - accuracy: 0.88 - ETA: 0s - loss: 0.3403 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3364 - accuracy: 0.8832 - val_loss: 0.8796 - val_accuracy: 0.6746\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.84 - ETA: 0s - loss: 0.2518 - accuracy: 0.90 - ETA: 0s - loss: 0.4208 - accuracy: 0.88 - ETA: 0s - loss: 0.3871 - accuracy: 0.88 - ETA: 0s - loss: 0.4013 - accuracy: 0.87 - ETA: 0s - loss: 0.4051 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4104 - accuracy: 0.8701 - val_loss: 0.8366 - val_accuracy: 0.7164\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.90 - ETA: 0s - loss: 0.3642 - accuracy: 0.88 - ETA: 0s - loss: 0.3734 - accuracy: 0.87 - ETA: 0s - loss: 0.3502 - accuracy: 0.87 - ETA: 0s - loss: 0.4786 - accuracy: 0.87 - ETA: 0s - loss: 0.5876 - accuracy: 0.85 - 0s 4ms/step - loss: 0.6232 - accuracy: 0.8309 - val_loss: 0.6271 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.87 - ETA: 0s - loss: 0.9448 - accuracy: 0.70 - ETA: 0s - loss: 0.8388 - accuracy: 0.72 - ETA: 0s - loss: 0.7867 - accuracy: 0.73 - ETA: 0s - loss: 0.7607 - accuracy: 0.72 - ETA: 0s - loss: 0.7906 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7729 - accuracy: 0.7163 - val_loss: 0.6558 - val_accuracy: 0.6955\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.81 - ETA: 0s - loss: 0.6054 - accuracy: 0.49 - ETA: 0s - loss: 0.6187 - accuracy: 0.49 - ETA: 0s - loss: 0.6138 - accuracy: 0.50 - ETA: 0s - loss: 0.6207 - accuracy: 0.50 - ETA: 0s - loss: 0.6197 - accuracy: 0.49 - 0s 4ms/step - loss: 0.6163 - accuracy: 0.4894 - val_loss: 0.7059 - val_accuracy: 0.4269\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6103 - accuracy: 0.37 - ETA: 0s - loss: 0.5797 - accuracy: 0.47 - ETA: 0s - loss: 0.5755 - accuracy: 0.48 - ETA: 0s - loss: 0.5954 - accuracy: 0.48 - ETA: 0s - loss: 0.6016 - accuracy: 0.48 - ETA: 0s - loss: 0.6079 - accuracy: 0.48 - 0s 4ms/step - loss: 0.6101 - accuracy: 0.4871 - val_loss: 0.6953 - val_accuracy: 0.4269\n",
      "Epoch 20/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.6471 - accuracy: 0.37 - ETA: 0s - loss: 0.6233 - accuracy: 0.46 - ETA: 0s - loss: 0.6030 - accuracy: 0.48 - ETA: 0s - loss: 0.6074 - accuracy: 0.49 - ETA: 0s - loss: 0.6096 - accuracy: 0.49 - ETA: 0s - loss: 0.6082 - accuracy: 0.4854Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6073 - accuracy: 0.4871 - val_loss: 0.7129 - val_accuracy: 0.4269\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0970 - accuracy: 0.59 - ETA: 0s - loss: 2.1680 - accuracy: 0.61 - ETA: 0s - loss: 1.3548 - accuracy: 0.67 - ETA: 0s - loss: 1.1419 - accuracy: 0.69 - ETA: 0s - loss: 0.9925 - accuracy: 0.70 - ETA: 0s - loss: 0.9115 - accuracy: 0.69 - 0s 5ms/step - loss: 0.8661 - accuracy: 0.7040 - val_loss: 0.5801 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4625 - accuracy: 0.78 - ETA: 0s - loss: 0.5199 - accuracy: 0.76 - ETA: 0s - loss: 0.4978 - accuracy: 0.75 - ETA: 0s - loss: 0.5044 - accuracy: 0.75 - ETA: 0s - loss: 0.5203 - accuracy: 0.75 - ETA: 0s - loss: 0.5272 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5315 - accuracy: 0.7540 - val_loss: 0.7437 - val_accuracy: 0.6731\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.78 - ETA: 0s - loss: 0.5183 - accuracy: 0.81 - ETA: 0s - loss: 0.4832 - accuracy: 0.81 - ETA: 0s - loss: 0.4738 - accuracy: 0.80 - ETA: 0s - loss: 0.4851 - accuracy: 0.80 - ETA: 0s - loss: 0.4856 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4881 - accuracy: 0.8003 - val_loss: 0.6813 - val_accuracy: 0.6537\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.81 - ETA: 0s - loss: 0.4741 - accuracy: 0.82 - ETA: 0s - loss: 0.4794 - accuracy: 0.81 - ETA: 0s - loss: 0.5211 - accuracy: 0.80 - ETA: 0s - loss: 0.5232 - accuracy: 0.80 - ETA: 0s - loss: 0.5343 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5285 - accuracy: 0.7984 - val_loss: 1.0621 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.87 - ETA: 0s - loss: 0.5941 - accuracy: 0.79 - ETA: 0s - loss: 0.5827 - accuracy: 0.78 - ETA: 0s - loss: 0.5887 - accuracy: 0.76 - ETA: 0s - loss: 0.5718 - accuracy: 0.77 - ETA: 0s - loss: 0.5854 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5852 - accuracy: 0.7697 - val_loss: 0.6085 - val_accuracy: 0.7388\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4502 - accuracy: 0.84 - ETA: 0s - loss: 0.8576 - accuracy: 0.79 - ETA: 0s - loss: 0.7388 - accuracy: 0.78 - ETA: 0s - loss: 0.6974 - accuracy: 0.77 - ETA: 0s - loss: 0.6677 - accuracy: 0.76 - ETA: 0s - loss: 0.6446 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6422 - accuracy: 0.7730 - val_loss: 0.6365 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.87 - ETA: 0s - loss: 0.4662 - accuracy: 0.83 - ETA: 0s - loss: 0.4771 - accuracy: 0.82 - ETA: 0s - loss: 0.4740 - accuracy: 0.83 - ETA: 0s - loss: 0.4779 - accuracy: 0.83 - ETA: 0s - loss: 0.4762 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4784 - accuracy: 0.8268 - val_loss: 1.0521 - val_accuracy: 0.6881\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.87 - ETA: 0s - loss: 0.3840 - accuracy: 0.87 - ETA: 0s - loss: 0.4219 - accuracy: 0.86 - ETA: 0s - loss: 0.4259 - accuracy: 0.86 - ETA: 0s - loss: 0.4329 - accuracy: 0.85 - ETA: 0s - loss: 0.4403 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4407 - accuracy: 0.8511 - val_loss: 0.6657 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.84 - ETA: 0s - loss: 0.4128 - accuracy: 0.83 - ETA: 0s - loss: 0.4218 - accuracy: 0.83 - ETA: 0s - loss: 0.4171 - accuracy: 0.84 - ETA: 0s - loss: 0.4142 - accuracy: 0.84 - ETA: 0s - loss: 0.4134 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8514 - val_loss: 0.6960 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.93 - ETA: 0s - loss: 0.3616 - accuracy: 0.88 - ETA: 0s - loss: 0.3789 - accuracy: 0.87 - ETA: 0s - loss: 0.3941 - accuracy: 0.86 - ETA: 0s - loss: 0.3945 - accuracy: 0.85 - ETA: 0s - loss: 0.4041 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8518 - val_loss: 1.1314 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.84 - ETA: 0s - loss: 0.8018 - accuracy: 0.81 - ETA: 0s - loss: 0.6712 - accuracy: 0.79 - ETA: 0s - loss: 0.6089 - accuracy: 0.80 - ETA: 0s - loss: 0.5719 - accuracy: 0.80 - ETA: 0s - loss: 0.5542 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5507 - accuracy: 0.8025 - val_loss: 0.6564 - val_accuracy: 0.7119\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.96 - ETA: 0s - loss: 0.3536 - accuracy: 0.85 - ETA: 0s - loss: 0.3482 - accuracy: 0.87 - ETA: 0s - loss: 0.3717 - accuracy: 0.86 - ETA: 0s - loss: 0.3733 - accuracy: 0.86 - ETA: 0s - loss: 0.3750 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3730 - accuracy: 0.8611 - val_loss: 1.1201 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.84 - ETA: 0s - loss: 0.4139 - accuracy: 0.88 - ETA: 0s - loss: 0.3748 - accuracy: 0.88 - ETA: 0s - loss: 0.3536 - accuracy: 0.88 - ETA: 0s - loss: 0.3505 - accuracy: 0.88 - ETA: 0s - loss: 0.3554 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3576 - accuracy: 0.8791 - val_loss: 0.7876 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.75 - ETA: 0s - loss: 0.2831 - accuracy: 0.88 - ETA: 0s - loss: 0.2787 - accuracy: 0.90 - ETA: 0s - loss: 0.2859 - accuracy: 0.89 - ETA: 0s - loss: 0.3010 - accuracy: 0.88 - ETA: 0s - loss: 0.3138 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3109 - accuracy: 0.8847 - val_loss: 0.8579 - val_accuracy: 0.7104\n",
      "Epoch 15/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.2790 - accuracy: 0.87 - ETA: 0s - loss: 0.2750 - accuracy: 0.89 - ETA: 0s - loss: 0.2654 - accuracy: 0.90 - ETA: 0s - loss: 0.2811 - accuracy: 0.89 - ETA: 0s - loss: 0.2937 - accuracy: 0.89 - ETA: 0s - loss: 0.3059 - accuracy: 0.8863Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.8884 - val_loss: 0.9781 - val_accuracy: 0.6851\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7222 - accuracy: 0.75 - ETA: 0s - loss: 2.2148 - accuracy: 0.60 - ETA: 0s - loss: 1.4021 - accuracy: 0.66 - ETA: 0s - loss: 1.1207 - accuracy: 0.66 - ETA: 0s - loss: 0.9712 - accuracy: 0.69 - ETA: 0s - loss: 0.8838 - accuracy: 0.70 - 0s 5ms/step - loss: 0.8576 - accuracy: 0.7092 - val_loss: 0.5718 - val_accuracy: 0.6910\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.93 - ETA: 0s - loss: 0.5374 - accuracy: 0.77 - ETA: 0s - loss: 0.5713 - accuracy: 0.75 - ETA: 0s - loss: 0.5649 - accuracy: 0.75 - ETA: 0s - loss: 0.5514 - accuracy: 0.75 - ETA: 0s - loss: 0.5471 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5512 - accuracy: 0.7626 - val_loss: 0.6114 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.68 - ETA: 0s - loss: 0.4237 - accuracy: 0.83 - ETA: 0s - loss: 0.4753 - accuracy: 0.82 - ETA: 0s - loss: 0.5042 - accuracy: 0.81 - ETA: 0s - loss: 0.5128 - accuracy: 0.80 - ETA: 0s - loss: 0.5002 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5008 - accuracy: 0.8014 - val_loss: 0.5888 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.78 - ETA: 0s - loss: 0.4212 - accuracy: 0.81 - ETA: 0s - loss: 0.4327 - accuracy: 0.82 - ETA: 0s - loss: 0.4252 - accuracy: 0.82 - ETA: 0s - loss: 0.4439 - accuracy: 0.82 - ETA: 0s - loss: 0.4720 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4747 - accuracy: 0.8119 - val_loss: 0.7058 - val_accuracy: 0.6761\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.84 - ETA: 0s - loss: 0.4095 - accuracy: 0.82 - ETA: 0s - loss: 0.4230 - accuracy: 0.82 - ETA: 0s - loss: 0.4406 - accuracy: 0.82 - ETA: 0s - loss: 0.4544 - accuracy: 0.81 - ETA: 0s - loss: 0.4484 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4499 - accuracy: 0.8163 - val_loss: 0.6223 - val_accuracy: 0.7015\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.78 - ETA: 0s - loss: 0.4166 - accuracy: 0.84 - ETA: 0s - loss: 0.4171 - accuracy: 0.83 - ETA: 0s - loss: 0.4906 - accuracy: 0.82 - ETA: 0s - loss: 0.5048 - accuracy: 0.81 - ETA: 0s - loss: 0.5312 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5363 - accuracy: 0.8081 - val_loss: 0.6516 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.68 - ETA: 0s - loss: 0.5857 - accuracy: 0.79 - ETA: 0s - loss: 0.5769 - accuracy: 0.80 - ETA: 0s - loss: 0.5609 - accuracy: 0.80 - ETA: 0s - loss: 0.5451 - accuracy: 0.80 - ETA: 0s - loss: 0.5311 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5272 - accuracy: 0.8111 - val_loss: 1.0160 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3143 - accuracy: 0.75 - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - ETA: 0s - loss: 0.4684 - accuracy: 0.85 - ETA: 0s - loss: 0.4775 - accuracy: 0.84 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - ETA: 0s - loss: 0.4846 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4836 - accuracy: 0.8399 - val_loss: 0.6030 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.84 - ETA: 0s - loss: 0.4189 - accuracy: 0.85 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - ETA: 0s - loss: 0.5004 - accuracy: 0.83 - ETA: 0s - loss: 0.4985 - accuracy: 0.83 - ETA: 0s - loss: 0.4912 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4897 - accuracy: 0.8369 - val_loss: 1.2971 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.84 - ETA: 0s - loss: 0.4963 - accuracy: 0.83 - ETA: 0s - loss: 0.4189 - accuracy: 0.87 - ETA: 0s - loss: 0.4340 - accuracy: 0.86 - ETA: 0s - loss: 0.4392 - accuracy: 0.86 - ETA: 0s - loss: 0.4383 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4376 - accuracy: 0.8567 - val_loss: 0.7990 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4076 - accuracy: 0.90 - ETA: 0s - loss: 0.3775 - accuracy: 0.87 - ETA: 0s - loss: 0.3534 - accuracy: 0.89 - ETA: 0s - loss: 0.3673 - accuracy: 0.88 - ETA: 0s - loss: 0.3790 - accuracy: 0.87 - ETA: 0s - loss: 0.3862 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3884 - accuracy: 0.8750 - val_loss: 1.0412 - val_accuracy: 0.7194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.87 - ETA: 0s - loss: 0.4030 - accuracy: 0.84 - ETA: 0s - loss: 0.3971 - accuracy: 0.85 - ETA: 0s - loss: 0.4144 - accuracy: 0.86 - ETA: 0s - loss: 0.4075 - accuracy: 0.86 - ETA: 0s - loss: 0.4111 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4249 - accuracy: 0.8638 - val_loss: 1.4395 - val_accuracy: 0.6582\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.87 - ETA: 0s - loss: 0.6343 - accuracy: 0.86 - ETA: 0s - loss: 0.6036 - accuracy: 0.81 - ETA: 0s - loss: 0.5528 - accuracy: 0.82 - ETA: 0s - loss: 0.5277 - accuracy: 0.82 - ETA: 0s - loss: 0.5193 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5203 - accuracy: 0.8313 - val_loss: 0.9997 - val_accuracy: 0.6940\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.84 - ETA: 0s - loss: 0.4266 - accuracy: 0.85 - ETA: 0s - loss: 0.4284 - accuracy: 0.85 - ETA: 0s - loss: 0.4274 - accuracy: 0.84 - ETA: 0s - loss: 0.4277 - accuracy: 0.84 - ETA: 0s - loss: 0.4187 - accuracy: 0.85 - ETA: 0s - loss: 0.4066 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4070 - accuracy: 0.8548 - val_loss: 0.7475 - val_accuracy: 0.7000\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.84 - ETA: 0s - loss: 0.3325 - accuracy: 0.89 - ETA: 0s - loss: 0.3155 - accuracy: 0.90 - ETA: 0s - loss: 0.3163 - accuracy: 0.90 - ETA: 0s - loss: 0.3082 - accuracy: 0.90 - ETA: 0s - loss: 0.3214 - accuracy: 0.89 - ETA: 0s - loss: 0.3216 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3268 - accuracy: 0.8962 - val_loss: 0.7789 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.3079 - accuracy: 0.87 - ETA: 0s - loss: 0.2491 - accuracy: 0.92 - ETA: 0s - loss: 0.2578 - accuracy: 0.91 - ETA: 0s - loss: 0.2474 - accuracy: 0.92 - ETA: 0s - loss: 0.2690 - accuracy: 0.91 - ETA: 0s - loss: 0.2938 - accuracy: 0.9049Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2987 - accuracy: 0.9003 - val_loss: 0.9033 - val_accuracy: 0.7104\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: e9d06c7167e6ebcd7d077d4301704482</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7402985095977783</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.1896110311585153</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 145</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3519 - accuracy: 0.43 - ETA: 0s - loss: 1.8748 - accuracy: 0.59 - ETA: 0s - loss: 1.4111 - accuracy: 0.61 - 0s 4ms/step - loss: 1.2348 - accuracy: 0.6256 - val_loss: 0.5533 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.68 - ETA: 0s - loss: 0.7570 - accuracy: 0.66 - ETA: 0s - loss: 0.7371 - accuracy: 0.69 - 0s 2ms/step - loss: 0.7180 - accuracy: 0.6988 - val_loss: 0.6000 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6582 - accuracy: 0.75 - ETA: 0s - loss: 0.7070 - accuracy: 0.68 - ETA: 0s - loss: 0.6836 - accuracy: 0.69 - ETA: 0s - loss: 0.6601 - accuracy: 0.71 - ETA: 0s - loss: 0.6715 - accuracy: 0.71 - 0s 3ms/step - loss: 0.6705 - accuracy: 0.7130 - val_loss: 0.6339 - val_accuracy: 0.7075\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5635 - accuracy: 0.68 - ETA: 0s - loss: 0.6340 - accuracy: 0.75 - ETA: 0s - loss: 0.6495 - accuracy: 0.73 - ETA: 0s - loss: 0.6500 - accuracy: 0.72 - ETA: 0s - loss: 0.6569 - accuracy: 0.72 - 0s 3ms/step - loss: 0.6571 - accuracy: 0.7286 - val_loss: 0.6077 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6633 - accuracy: 0.75 - ETA: 0s - loss: 0.6496 - accuracy: 0.74 - ETA: 0s - loss: 0.6409 - accuracy: 0.75 - ETA: 0s - loss: 0.6468 - accuracy: 0.75 - ETA: 0s - loss: 0.6453 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6428 - accuracy: 0.7563 - val_loss: 0.6095 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.75 - ETA: 0s - loss: 0.6377 - accuracy: 0.75 - ETA: 0s - loss: 0.6050 - accuracy: 0.77 - ETA: 0s - loss: 0.6035 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6098 - accuracy: 0.7704 - val_loss: 0.5917 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.93 - ETA: 0s - loss: 0.6508 - accuracy: 0.78 - ETA: 0s - loss: 0.7836 - accuracy: 0.75 - ETA: 0s - loss: 0.9088 - accuracy: 0.72 - 0s 3ms/step - loss: 0.8790 - accuracy: 0.6812 - val_loss: 0.6631 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.81 - ETA: 0s - loss: 0.6842 - accuracy: 0.73 - ETA: 0s - loss: 0.6957 - accuracy: 0.72 - ETA: 0s - loss: 0.7092 - accuracy: 0.69 - 0s 2ms/step - loss: 0.7062 - accuracy: 0.6708 - val_loss: 0.6833 - val_accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.12 - ETA: 0s - loss: 0.6565 - accuracy: 0.68 - ETA: 0s - loss: 0.6734 - accuracy: 0.71 - ETA: 0s - loss: 0.6731 - accuracy: 0.71 - 0s 3ms/step - loss: 0.7094 - accuracy: 0.7074 - val_loss: 0.6510 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7119 - accuracy: 0.68 - ETA: 0s - loss: 0.7007 - accuracy: 0.72 - ETA: 0s - loss: 0.7003 - accuracy: 0.70 - ETA: 0s - loss: 0.7037 - accuracy: 0.70 - ETA: 0s - loss: 0.7006 - accuracy: 0.64 - ETA: 0s - loss: 0.6994 - accuracy: 0.56 - 0s 4ms/step - loss: 0.6996 - accuracy: 0.5622 - val_loss: 0.6927 - val_accuracy: 0.6985\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7421 - accuracy: 0.62 - ETA: 0s - loss: 0.6995 - accuracy: 0.65 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.7004 - accuracy: 0.59 - 0s 3ms/step - loss: 0.6999 - accuracy: 0.5312 - val_loss: 0.7031 - val_accuracy: 0.3045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7565 - accuracy: 0.40 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6953 - accuracy: 0.60 - ETA: 0s - loss: 0.6967 - accuracy: 0.54 - ETA: 0s - loss: 0.6951 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.47 - 0s 4ms/step - loss: 0.6936 - accuracy: 0.4797 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.65 - ETA: 0s - loss: 0.6993 - accuracy: 0.63 - ETA: 0s - loss: 0.7031 - accuracy: 0.47 - ETA: 0s - loss: 0.6989 - accuracy: 0.42 - 0s 3ms/step - loss: 0.6968 - accuracy: 0.4356 - val_loss: 0.6922 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7423 - accuracy: 0.62 - ETA: 1s - loss: 0.7114 - accuracy: 0.67 - ETA: 1s - loss: 0.7158 - accuracy: 0.66 - ETA: 1s - loss: 0.7159 - accuracy: 0.55 - ETA: 1s - loss: 0.7113 - accuracy: 0.53 - ETA: 0s - loss: 0.7064 - accuracy: 0.44 - ETA: 0s - loss: 0.6945 - accuracy: 0.37 - ETA: 0s - loss: 0.6983 - accuracy: 0.41 - ETA: 0s - loss: 0.7002 - accuracy: 0.39 - ETA: 0s - loss: 0.6962 - accuracy: 0.36 - 1s 7ms/step - loss: 0.6937 - accuracy: 0.3908 - val_loss: 0.6829 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.78 - ETA: 0s - loss: 0.6944 - accuracy: 0.69 - ETA: 0s - loss: 0.6999 - accuracy: 0.57 - ETA: 0s - loss: 0.6916 - accuracy: 0.56 - ETA: 0s - loss: 0.6929 - accuracy: 0.58 - 0s 3ms/step - loss: 0.6941 - accuracy: 0.5969 - val_loss: 0.6917 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.50 - ETA: 0s - loss: 0.7042 - accuracy: 0.35 - ETA: 0s - loss: 0.6982 - accuracy: 0.32 - ETA: 0s - loss: 0.6943 - accuracy: 0.32 - ETA: 0s - loss: 0.6938 - accuracy: 0.4155Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.4155 - val_loss: 0.6881 - val_accuracy: 0.6955\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9800 - accuracy: 0.56 - ETA: 0s - loss: 2.1424 - accuracy: 0.55 - ETA: 0s - loss: 1.5716 - accuracy: 0.59 - ETA: 0s - loss: 1.3200 - accuracy: 0.60 - 0s 4ms/step - loss: 1.2883 - accuracy: 0.6058 - val_loss: 0.5972 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.81 - ETA: 0s - loss: 0.7343 - accuracy: 0.68 - ETA: 0s - loss: 0.7171 - accuracy: 0.68 - ETA: 0s - loss: 0.7057 - accuracy: 0.69 - 0s 2ms/step - loss: 0.7047 - accuracy: 0.6909 - val_loss: 0.6338 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.75 - ETA: 0s - loss: 0.6573 - accuracy: 0.70 - ETA: 0s - loss: 0.6482 - accuracy: 0.71 - ETA: 0s - loss: 0.6569 - accuracy: 0.71 - 0s 3ms/step - loss: 0.6590 - accuracy: 0.7167 - val_loss: 0.6371 - val_accuracy: 0.7284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6504 - accuracy: 0.78 - ETA: 0s - loss: 0.6489 - accuracy: 0.71 - ETA: 0s - loss: 0.6476 - accuracy: 0.73 - ETA: 0s - loss: 0.6405 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6384 - accuracy: 0.7383 - val_loss: 0.6242 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.68 - ETA: 0s - loss: 0.6053 - accuracy: 0.76 - ETA: 0s - loss: 0.6179 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6234 - accuracy: 0.7544 - val_loss: 0.6077 - val_accuracy: 0.7194\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.68 - ETA: 0s - loss: 0.6105 - accuracy: 0.76 - ETA: 0s - loss: 0.5995 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5985 - accuracy: 0.7749 - val_loss: 0.6141 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.71 - ETA: 0s - loss: 0.5882 - accuracy: 0.78 - ETA: 0s - loss: 0.5743 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5785 - accuracy: 0.7869 - val_loss: 0.6054 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6130 - accuracy: 0.81 - ETA: 0s - loss: 0.5735 - accuracy: 0.78 - ETA: 0s - loss: 0.5595 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7992 - val_loss: 0.5919 - val_accuracy: 0.7239\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.81 - ETA: 0s - loss: 0.5585 - accuracy: 0.81 - ETA: 0s - loss: 0.5645 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5678 - accuracy: 0.8044 - val_loss: 0.6082 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.78 - ETA: 0s - loss: 0.5648 - accuracy: 0.79 - ETA: 0s - loss: 0.5586 - accuracy: 0.81 - ETA: 0s - loss: 0.5629 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5664 - accuracy: 0.8052 - val_loss: 0.6116 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.84 - ETA: 0s - loss: 0.6129 - accuracy: 0.78 - ETA: 0s - loss: 0.5928 - accuracy: 0.79 - ETA: 0s - loss: 0.5918 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5909 - accuracy: 0.7943 - val_loss: 0.5762 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.90 - ETA: 0s - loss: 0.5597 - accuracy: 0.80 - ETA: 0s - loss: 0.5606 - accuracy: 0.80 - ETA: 0s - loss: 0.5672 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5665 - accuracy: 0.8022 - val_loss: 0.5986 - val_accuracy: 0.7030\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.75 - ETA: 0s - loss: 0.5299 - accuracy: 0.83 - ETA: 0s - loss: 0.5242 - accuracy: 0.83 - ETA: 0s - loss: 0.5352 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5330 - accuracy: 0.8268 - val_loss: 0.6142 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - ETA: 0s - loss: 0.5314 - accuracy: 0.81 - ETA: 0s - loss: 0.5175 - accuracy: 0.82 - ETA: 0s - loss: 0.5358 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5390 - accuracy: 0.8175 - val_loss: 0.6150 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.81 - ETA: 0s - loss: 0.5740 - accuracy: 0.81 - ETA: 0s - loss: 0.5802 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5857 - accuracy: 0.7962 - val_loss: 0.6169 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.75 - ETA: 0s - loss: 0.5848 - accuracy: 0.80 - ETA: 0s - loss: 0.5866 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6041 - accuracy: 0.7925 - val_loss: 0.6231 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6990 - accuracy: 0.68 - ETA: 0s - loss: 0.6102 - accuracy: 0.77 - ETA: 0s - loss: 0.6111 - accuracy: 0.78 - ETA: 0s - loss: 0.6417 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6452 - accuracy: 0.7768 - val_loss: 0.6160 - val_accuracy: 0.7045\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.81 - ETA: 0s - loss: 0.7088 - accuracy: 0.76 - ETA: 0s - loss: 0.6658 - accuracy: 0.75 - ETA: 0s - loss: 0.7058 - accuracy: 0.75 - 0s 2ms/step - loss: 0.7120 - accuracy: 0.7514 - val_loss: 0.6150 - val_accuracy: 0.7015\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0296 - accuracy: 0.75 - ETA: 0s - loss: 0.7686 - accuracy: 0.74 - ETA: 0s - loss: 0.7332 - accuracy: 0.74 - ETA: 0s - loss: 0.7178 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7405 - accuracy: 0.7256 - val_loss: 0.6530 - val_accuracy: 0.7000\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.71 - ETA: 0s - loss: 0.6590 - accuracy: 0.69 - ETA: 0s - loss: 0.6605 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6928 - accuracy: 0.7126 - val_loss: 0.6373 - val_accuracy: 0.7119\n",
      "Epoch 21/50\n",
      "64/84 [=====================>........] - ETA: 0s - loss: 0.6321 - accuracy: 0.68 - ETA: 0s - loss: 0.7075 - accuracy: 0.71 - ETA: 0s - loss: 0.7485 - accuracy: 0.6382Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7310 - accuracy: 0.6577 - val_loss: 0.6370 - val_accuracy: 0.6955\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9919 - accuracy: 0.31 - ETA: 0s - loss: 2.0493 - accuracy: 0.50 - ETA: 0s - loss: 1.5272 - accuracy: 0.56 - ETA: 0s - loss: 1.3039 - accuracy: 0.58 - 0s 4ms/step - loss: 1.2609 - accuracy: 0.5808 - val_loss: 0.6329 - val_accuracy: 0.6821\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7574 - accuracy: 0.53 - ETA: 0s - loss: 0.7334 - accuracy: 0.43 - ETA: 0s - loss: 0.7149 - accuracy: 0.45 - ETA: 0s - loss: 0.7057 - accuracy: 0.44 - 0s 3ms/step - loss: 0.7019 - accuracy: 0.4490 - val_loss: 0.6399 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7473 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.68 - ETA: 0s - loss: 0.6838 - accuracy: 0.67 - 0s 2ms/step - loss: 0.6816 - accuracy: 0.6865 - val_loss: 0.6327 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.62 - ETA: 0s - loss: 0.6989 - accuracy: 0.67 - ETA: 0s - loss: 0.6821 - accuracy: 0.69 - 0s 2ms/step - loss: 0.6835 - accuracy: 0.6876 - val_loss: 0.6320 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9168 - accuracy: 0.59 - ETA: 0s - loss: 0.6491 - accuracy: 0.65 - ETA: 0s - loss: 0.6614 - accuracy: 0.67 - 0s 2ms/step - loss: 0.6597 - accuracy: 0.6898 - val_loss: 0.6344 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5302 - accuracy: 0.84 - ETA: 0s - loss: 0.6253 - accuracy: 0.75 - ETA: 0s - loss: 0.6314 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6394 - accuracy: 0.7454 - val_loss: 0.6468 - val_accuracy: 0.6985\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7937 - accuracy: 0.59 - ETA: 0s - loss: 0.6222 - accuracy: 0.75 - ETA: 0s - loss: 0.6183 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6218 - accuracy: 0.7641 - val_loss: 0.6156 - val_accuracy: 0.7134\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6335 - accuracy: 0.75 - ETA: 0s - loss: 0.6133 - accuracy: 0.75 - ETA: 0s - loss: 0.6070 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6163 - accuracy: 0.7592 - val_loss: 0.6096 - val_accuracy: 0.7164\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.81 - ETA: 0s - loss: 0.5767 - accuracy: 0.78 - ETA: 0s - loss: 0.6003 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6061 - accuracy: 0.7689 - val_loss: 0.5907 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.78 - ETA: 0s - loss: 0.6184 - accuracy: 0.75 - ETA: 0s - loss: 0.6255 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6256 - accuracy: 0.7544 - val_loss: 0.5938 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6602 - accuracy: 0.75 - ETA: 0s - loss: 0.6323 - accuracy: 0.76 - ETA: 0s - loss: 0.6411 - accuracy: 0.77 - 0s 2ms/step - loss: 0.6425 - accuracy: 0.7749 - val_loss: 0.6136 - val_accuracy: 0.7045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.84 - ETA: 0s - loss: 0.6376 - accuracy: 0.75 - ETA: 0s - loss: 0.6652 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6694 - accuracy: 0.7163 - val_loss: 0.6243 - val_accuracy: 0.7403\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8659 - accuracy: 0.75 - ETA: 0s - loss: 0.7363 - accuracy: 0.68 - ETA: 0s - loss: 0.7159 - accuracy: 0.59 - 0s 2ms/step - loss: 0.7021 - accuracy: 0.6107 - val_loss: 0.6312 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.5980 - accuracy: 0.84 - ETA: 0s - loss: 0.6998 - accuracy: 0.73 - ETA: 0s - loss: 0.6840 - accuracy: 0.7337Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.7264 - val_loss: 0.6493 - val_accuracy: 0.6955\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: fdc554f7a074237192ba71e7dfa47d9f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7417910496393839</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9390963406174774</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 60</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9814 - accuracy: 0.50 - ETA: 0s - loss: 1.8214 - accuracy: 0.60 - ETA: 0s - loss: 1.1969 - accuracy: 0.63 - ETA: 0s - loss: 0.9860 - accuracy: 0.67 - ETA: 0s - loss: 0.8879 - accuracy: 0.69 - ETA: 0s - loss: 0.8397 - accuracy: 0.69 - 0s 6ms/step - loss: 0.8111 - accuracy: 0.6991 - val_loss: 0.5695 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5969 - accuracy: 0.78 - ETA: 0s - loss: 0.5334 - accuracy: 0.79 - ETA: 0s - loss: 0.5089 - accuracy: 0.79 - ETA: 0s - loss: 0.5416 - accuracy: 0.76 - ETA: 0s - loss: 0.5650 - accuracy: 0.76 - ETA: 0s - loss: 0.5740 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5713 - accuracy: 0.7589 - val_loss: 0.6129 - val_accuracy: 0.7090\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.78 - ETA: 0s - loss: 0.4842 - accuracy: 0.78 - ETA: 0s - loss: 0.4675 - accuracy: 0.80 - ETA: 0s - loss: 0.4617 - accuracy: 0.79 - ETA: 0s - loss: 0.4626 - accuracy: 0.78 - ETA: 0s - loss: 0.4697 - accuracy: 0.78 - 0s 4ms/step - loss: 0.4693 - accuracy: 0.7861 - val_loss: 0.5832 - val_accuracy: 0.6866\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.84 - ETA: 0s - loss: 0.3997 - accuracy: 0.81 - ETA: 0s - loss: 0.3838 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.4341 - accuracy: 0.81 - ETA: 0s - loss: 0.4506 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4549 - accuracy: 0.8022 - val_loss: 0.8401 - val_accuracy: 0.6493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.90 - ETA: 0s - loss: 0.4221 - accuracy: 0.80 - ETA: 0s - loss: 0.4720 - accuracy: 0.79 - ETA: 0s - loss: 0.4705 - accuracy: 0.79 - ETA: 0s - loss: 0.4483 - accuracy: 0.79 - ETA: 0s - loss: 0.4366 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4366 - accuracy: 0.8018 - val_loss: 1.3282 - val_accuracy: 0.7030\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.84 - ETA: 0s - loss: 0.3599 - accuracy: 0.82 - ETA: 0s - loss: 0.3749 - accuracy: 0.82 - ETA: 0s - loss: 0.3811 - accuracy: 0.81 - ETA: 0s - loss: 0.3608 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.81 - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8137 - val_loss: 0.7299 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4181 - accuracy: 0.83 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.3998 - accuracy: 0.81 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8190 - val_loss: 0.7931 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.90 - ETA: 0s - loss: 0.3321 - accuracy: 0.83 - ETA: 0s - loss: 0.3072 - accuracy: 0.85 - ETA: 0s - loss: 0.3426 - accuracy: 0.84 - ETA: 0s - loss: 0.3616 - accuracy: 0.83 - ETA: 0s - loss: 0.3722 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8320 - val_loss: 1.0426 - val_accuracy: 0.6119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.81 - ETA: 0s - loss: 0.3040 - accuracy: 0.84 - ETA: 0s - loss: 0.3116 - accuracy: 0.84 - ETA: 0s - loss: 0.3086 - accuracy: 0.85 - ETA: 0s - loss: 0.3472 - accuracy: 0.85 - ETA: 0s - loss: 0.3554 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8436 - val_loss: 1.8826 - val_accuracy: 0.6910\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - ETA: 0s - loss: 0.5826 - accuracy: 0.82 - ETA: 0s - loss: 0.5163 - accuracy: 0.84 - ETA: 0s - loss: 0.4977 - accuracy: 0.84 - ETA: 0s - loss: 0.5010 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5054 - accuracy: 0.8257 - val_loss: 0.7093 - val_accuracy: 0.7269\n",
      "Epoch 11/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.81 - ETA: 0s - loss: 0.4250 - accuracy: 0.81 - ETA: 0s - loss: 0.4391 - accuracy: 0.82 - ETA: 0s - loss: 0.4478 - accuracy: 0.81 - ETA: 0s - loss: 0.4509 - accuracy: 0.8220Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.8223 - val_loss: 0.9156 - val_accuracy: 0.7224\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9909 - accuracy: 0.50 - ETA: 0s - loss: 1.9491 - accuracy: 0.58 - ETA: 0s - loss: 1.3122 - accuracy: 0.60 - ETA: 0s - loss: 1.0699 - accuracy: 0.63 - ETA: 0s - loss: 0.9320 - accuracy: 0.66 - ETA: 0s - loss: 0.8549 - accuracy: 0.67 - 0s 5ms/step - loss: 0.8368 - accuracy: 0.6738 - val_loss: 0.6700 - val_accuracy: 0.6776\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.78 - ETA: 0s - loss: 0.4855 - accuracy: 0.78 - ETA: 0s - loss: 0.4915 - accuracy: 0.77 - ETA: 0s - loss: 0.5047 - accuracy: 0.75 - ETA: 0s - loss: 0.5068 - accuracy: 0.76 - ETA: 0s - loss: 0.5102 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5096 - accuracy: 0.7592 - val_loss: 0.6210 - val_accuracy: 0.6881\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.87 - ETA: 0s - loss: 0.4201 - accuracy: 0.79 - ETA: 0s - loss: 0.4591 - accuracy: 0.77 - ETA: 0s - loss: 0.4596 - accuracy: 0.78 - ETA: 0s - loss: 0.4917 - accuracy: 0.77 - ETA: 0s - loss: 0.5095 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5168 - accuracy: 0.7652 - val_loss: 0.6015 - val_accuracy: 0.7119\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.75 - ETA: 0s - loss: 0.4400 - accuracy: 0.79 - ETA: 0s - loss: 0.4451 - accuracy: 0.79 - ETA: 0s - loss: 0.4457 - accuracy: 0.78 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - ETA: 0s - loss: 0.4461 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4461 - accuracy: 0.7947 - val_loss: 0.6123 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.71 - ETA: 0s - loss: 0.4387 - accuracy: 0.80 - ETA: 0s - loss: 0.4323 - accuracy: 0.79 - ETA: 0s - loss: 0.4241 - accuracy: 0.80 - ETA: 0s - loss: 0.4102 - accuracy: 0.80 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4025 - accuracy: 0.8134 - val_loss: 0.6330 - val_accuracy: 0.6716\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.93 - ETA: 0s - loss: 0.3292 - accuracy: 0.83 - ETA: 0s - loss: 0.3315 - accuracy: 0.84 - ETA: 0s - loss: 0.3518 - accuracy: 0.83 - ETA: 0s - loss: 0.3739 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8343 - val_loss: 0.8275 - val_accuracy: 0.6746\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.87 - ETA: 0s - loss: 0.3601 - accuracy: 0.83 - ETA: 0s - loss: 0.3517 - accuracy: 0.82 - ETA: 0s - loss: 0.3416 - accuracy: 0.83 - ETA: 0s - loss: 0.3423 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8410 - val_loss: 1.3670 - val_accuracy: 0.6343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.78 - ETA: 0s - loss: 0.3126 - accuracy: 0.86 - ETA: 0s - loss: 0.3202 - accuracy: 0.86 - ETA: 0s - loss: 0.3368 - accuracy: 0.85 - ETA: 0s - loss: 0.3429 - accuracy: 0.85 - ETA: 0s - loss: 0.3418 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8522 - val_loss: 0.7825 - val_accuracy: 0.6836\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.96 - ETA: 0s - loss: 0.3232 - accuracy: 0.86 - ETA: 0s - loss: 0.3497 - accuracy: 0.84 - ETA: 0s - loss: 0.3312 - accuracy: 0.85 - ETA: 0s - loss: 0.3247 - accuracy: 0.85 - ETA: 0s - loss: 0.3315 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3305 - accuracy: 0.8578 - val_loss: 1.1460 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 0.93 - ETA: 0s - loss: 0.2580 - accuracy: 0.89 - ETA: 0s - loss: 0.2812 - accuracy: 0.87 - ETA: 0s - loss: 0.3346 - accuracy: 0.86 - ETA: 0s - loss: 0.3491 - accuracy: 0.86 - ETA: 0s - loss: 0.3445 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3445 - accuracy: 0.8619 - val_loss: 2.8276 - val_accuracy: 0.6642\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.87 - ETA: 0s - loss: 0.3972 - accuracy: 0.84 - ETA: 0s - loss: 0.3655 - accuracy: 0.84 - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - ETA: 0s - loss: 0.3293 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8567 - val_loss: 3.1883 - val_accuracy: 0.6194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.84 - ETA: 0s - loss: 0.3510 - accuracy: 0.85 - ETA: 0s - loss: 0.3054 - accuracy: 0.86 - ETA: 0s - loss: 0.3246 - accuracy: 0.87 - ETA: 0s - loss: 0.3322 - accuracy: 0.86 - ETA: 0s - loss: 0.3300 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8634 - val_loss: 0.7719 - val_accuracy: 0.6806\n",
      "Epoch 13/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4308 - accuracy: 0.78 - ETA: 0s - loss: 0.3069 - accuracy: 0.87 - ETA: 0s - loss: 0.3018 - accuracy: 0.87 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2924 - accuracy: 0.88 - ETA: 0s - loss: 0.2906 - accuracy: 0.8879Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2941 - accuracy: 0.8865 - val_loss: 0.9035 - val_accuracy: 0.6836\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9530 - accuracy: 0.43 - ETA: 0s - loss: 2.6956 - accuracy: 0.58 - ETA: 0s - loss: 1.5991 - accuracy: 0.66 - ETA: 0s - loss: 1.2566 - accuracy: 0.68 - ETA: 0s - loss: 1.0838 - accuracy: 0.69 - ETA: 0s - loss: 0.9821 - accuracy: 0.70 - 0s 5ms/step - loss: 0.9476 - accuracy: 0.7115 - val_loss: 0.5847 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.90 - ETA: 0s - loss: 0.5487 - accuracy: 0.76 - ETA: 0s - loss: 0.5618 - accuracy: 0.75 - ETA: 0s - loss: 0.5536 - accuracy: 0.76 - ETA: 0s - loss: 0.5510 - accuracy: 0.75 - ETA: 0s - loss: 0.5475 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7637 - val_loss: 0.6420 - val_accuracy: 0.7209\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - ETA: 0s - loss: 0.5008 - accuracy: 0.81 - ETA: 0s - loss: 0.5131 - accuracy: 0.79 - ETA: 0s - loss: 0.5085 - accuracy: 0.80 - ETA: 0s - loss: 0.5125 - accuracy: 0.79 - ETA: 0s - loss: 0.5059 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5096 - accuracy: 0.8037 - val_loss: 0.6298 - val_accuracy: 0.7254\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.75 - ETA: 0s - loss: 0.4778 - accuracy: 0.83 - ETA: 0s - loss: 0.4657 - accuracy: 0.83 - ETA: 0s - loss: 0.4801 - accuracy: 0.82 - ETA: 0s - loss: 0.4902 - accuracy: 0.81 - ETA: 0s - loss: 0.4921 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5006 - accuracy: 0.8190 - val_loss: 0.7670 - val_accuracy: 0.6821\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.90 - ETA: 0s - loss: 0.6175 - accuracy: 0.84 - ETA: 0s - loss: 0.5551 - accuracy: 0.83 - ETA: 0s - loss: 0.5460 - accuracy: 0.82 - ETA: 0s - loss: 0.5362 - accuracy: 0.81 - ETA: 0s - loss: 0.5548 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5544 - accuracy: 0.8119 - val_loss: 0.6656 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.84 - ETA: 0s - loss: 0.4109 - accuracy: 0.82 - ETA: 0s - loss: 0.4256 - accuracy: 0.82 - ETA: 0s - loss: 0.4449 - accuracy: 0.82 - ETA: 0s - loss: 0.4461 - accuracy: 0.82 - ETA: 0s - loss: 0.4608 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4608 - accuracy: 0.8186 - val_loss: 0.7339 - val_accuracy: 0.7000\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.81 - ETA: 0s - loss: 0.5070 - accuracy: 0.78 - ETA: 0s - loss: 0.4709 - accuracy: 0.81 - ETA: 0s - loss: 0.4572 - accuracy: 0.82 - ETA: 0s - loss: 0.4411 - accuracy: 0.83 - ETA: 0s - loss: 0.4331 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8406 - val_loss: 0.7280 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.87 - ETA: 0s - loss: 0.3805 - accuracy: 0.87 - ETA: 0s - loss: 0.3825 - accuracy: 0.87 - ETA: 0s - loss: 0.3860 - accuracy: 0.86 - ETA: 0s - loss: 0.4031 - accuracy: 0.85 - ETA: 0s - loss: 0.3998 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4084 - accuracy: 0.8552 - val_loss: 0.6893 - val_accuracy: 0.7119\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.71 - ETA: 0s - loss: 0.3754 - accuracy: 0.88 - ETA: 0s - loss: 0.3711 - accuracy: 0.87 - ETA: 0s - loss: 0.3911 - accuracy: 0.86 - ETA: 0s - loss: 0.4376 - accuracy: 0.87 - ETA: 0s - loss: 0.4322 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4755 - accuracy: 0.8690 - val_loss: 0.7076 - val_accuracy: 0.7030\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.84 - ETA: 0s - loss: 0.3864 - accuracy: 0.86 - ETA: 0s - loss: 0.4182 - accuracy: 0.86 - ETA: 0s - loss: 0.4242 - accuracy: 0.85 - ETA: 0s - loss: 0.4232 - accuracy: 0.85 - ETA: 0s - loss: 0.4114 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4086 - accuracy: 0.8645 - val_loss: 0.7711 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.90 - ETA: 0s - loss: 0.3547 - accuracy: 0.87 - ETA: 0s - loss: 0.3540 - accuracy: 0.87 - ETA: 0s - loss: 0.3633 - accuracy: 0.87 - ETA: 0s - loss: 0.3998 - accuracy: 0.86 - ETA: 0s - loss: 0.3856 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8761 - val_loss: 0.8743 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.87 - ETA: 0s - loss: 0.3570 - accuracy: 0.89 - ETA: 0s - loss: 0.3822 - accuracy: 0.88 - ETA: 0s - loss: 0.3651 - accuracy: 0.88 - ETA: 0s - loss: 0.3535 - accuracy: 0.88 - ETA: 0s - loss: 0.3516 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3484 - accuracy: 0.8869 - val_loss: 1.0814 - val_accuracy: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.84 - ETA: 0s - loss: 0.3577 - accuracy: 0.88 - ETA: 0s - loss: 0.3841 - accuracy: 0.87 - ETA: 0s - loss: 0.4152 - accuracy: 0.86 - ETA: 0s - loss: 0.4046 - accuracy: 0.87 - ETA: 0s - loss: 0.4149 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8750 - val_loss: 0.5976 - val_accuracy: 0.7478\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.81 - ETA: 0s - loss: 0.2909 - accuracy: 0.90 - ETA: 0s - loss: 0.3515 - accuracy: 0.89 - ETA: 0s - loss: 0.3673 - accuracy: 0.88 - ETA: 0s - loss: 0.3689 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8832 - val_loss: 1.1302 - val_accuracy: 0.6881\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.96 - ETA: 0s - loss: 0.3936 - accuracy: 0.87 - ETA: 0s - loss: 0.3990 - accuracy: 0.86 - ETA: 0s - loss: 0.3790 - accuracy: 0.88 - ETA: 0s - loss: 0.3678 - accuracy: 0.88 - ETA: 0s - loss: 0.3528 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3501 - accuracy: 0.8895 - val_loss: 1.1447 - val_accuracy: 0.7015\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.93 - ETA: 0s - loss: 0.3009 - accuracy: 0.90 - ETA: 0s - loss: 0.3245 - accuracy: 0.89 - ETA: 0s - loss: 0.3203 - accuracy: 0.90 - ETA: 0s - loss: 0.3150 - accuracy: 0.90 - ETA: 0s - loss: 0.3151 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3147 - accuracy: 0.9037 - val_loss: 0.7300 - val_accuracy: 0.7358\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.93 - ETA: 0s - loss: 0.2490 - accuracy: 0.93 - ETA: 0s - loss: 0.2356 - accuracy: 0.94 - ETA: 0s - loss: 0.2485 - accuracy: 0.93 - ETA: 0s - loss: 0.2645 - accuracy: 0.93 - ETA: 0s - loss: 0.2797 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2814 - accuracy: 0.9231 - val_loss: 0.9043 - val_accuracy: 0.7403\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.93 - ETA: 0s - loss: 0.3137 - accuracy: 0.91 - ETA: 0s - loss: 0.2864 - accuracy: 0.92 - ETA: 0s - loss: 0.2906 - accuracy: 0.92 - ETA: 0s - loss: 0.2685 - accuracy: 0.92 - ETA: 0s - loss: 0.2779 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2838 - accuracy: 0.9239 - val_loss: 1.2035 - val_accuracy: 0.7104\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.90 - ETA: 0s - loss: 0.2767 - accuracy: 0.93 - ETA: 0s - loss: 0.2952 - accuracy: 0.92 - ETA: 0s - loss: 0.2934 - accuracy: 0.92 - ETA: 0s - loss: 0.2786 - accuracy: 0.92 - ETA: 0s - loss: 0.2876 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2850 - accuracy: 0.9246 - val_loss: 0.8396 - val_accuracy: 0.7493\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.96 - ETA: 0s - loss: 0.2515 - accuracy: 0.94 - ETA: 0s - loss: 0.2431 - accuracy: 0.94 - ETA: 0s - loss: 0.2704 - accuracy: 0.93 - ETA: 0s - loss: 0.2852 - accuracy: 0.93 - ETA: 0s - loss: 0.3018 - accuracy: 0.92 - 0s 4ms/step - loss: 0.3024 - accuracy: 0.9220 - val_loss: 1.1051 - val_accuracy: 0.7149\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.93 - ETA: 0s - loss: 0.2768 - accuracy: 0.93 - ETA: 0s - loss: 0.2869 - accuracy: 0.92 - ETA: 0s - loss: 0.2926 - accuracy: 0.92 - ETA: 0s - loss: 0.2874 - accuracy: 0.92 - ETA: 0s - loss: 0.2846 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2837 - accuracy: 0.9231 - val_loss: 1.0539 - val_accuracy: 0.7493\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 1.00 - ETA: 0s - loss: 0.2673 - accuracy: 0.93 - ETA: 0s - loss: 0.2667 - accuracy: 0.93 - ETA: 0s - loss: 0.2641 - accuracy: 0.93 - ETA: 0s - loss: 0.2719 - accuracy: 0.92 - ETA: 0s - loss: 0.2717 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2640 - accuracy: 0.9302 - val_loss: 3.6098 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8686 - accuracy: 0.90 - ETA: 0s - loss: 0.3457 - accuracy: 0.91 - ETA: 0s - loss: 0.2916 - accuracy: 0.93 - ETA: 0s - loss: 0.2841 - accuracy: 0.93 - ETA: 0s - loss: 0.2925 - accuracy: 0.92 - ETA: 0s - loss: 0.2965 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2992 - accuracy: 0.9239 - val_loss: 0.9996 - val_accuracy: 0.7119\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.96 - ETA: 0s - loss: 0.2428 - accuracy: 0.94 - ETA: 0s - loss: 0.2581 - accuracy: 0.93 - ETA: 0s - loss: 0.2403 - accuracy: 0.93 - ETA: 0s - loss: 0.2537 - accuracy: 0.93 - ETA: 0s - loss: 0.2624 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2682 - accuracy: 0.9302 - val_loss: 0.7898 - val_accuracy: 0.7284\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 1.00 - ETA: 0s - loss: 0.2943 - accuracy: 0.92 - ETA: 0s - loss: 0.2694 - accuracy: 0.92 - ETA: 0s - loss: 0.2897 - accuracy: 0.92 - ETA: 0s - loss: 0.2980 - accuracy: 0.92 - ETA: 0s - loss: 0.2936 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2850 - accuracy: 0.9283 - val_loss: 1.0107 - val_accuracy: 0.7269\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 1.00 - ETA: 0s - loss: 0.2158 - accuracy: 0.95 - ETA: 0s - loss: 0.2102 - accuracy: 0.95 - ETA: 0s - loss: 0.2197 - accuracy: 0.95 - ETA: 0s - loss: 0.2211 - accuracy: 0.95 - ETA: 0s - loss: 0.2346 - accuracy: 0.94 - 0s 4ms/step - loss: 0.2394 - accuracy: 0.9448 - val_loss: 0.6862 - val_accuracy: 0.7299\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.96 - ETA: 0s - loss: 0.2464 - accuracy: 0.94 - ETA: 0s - loss: 0.2335 - accuracy: 0.94 - ETA: 0s - loss: 0.2343 - accuracy: 0.94 - ETA: 0s - loss: 0.2297 - accuracy: 0.94 - ETA: 0s - loss: 0.2561 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2431 - accuracy: 0.9425 - val_loss: 2.1468 - val_accuracy: 0.7119\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.96 - ETA: 0s - loss: 0.2133 - accuracy: 0.95 - ETA: 0s - loss: 0.2231 - accuracy: 0.94 - ETA: 0s - loss: 0.2246 - accuracy: 0.94 - ETA: 0s - loss: 0.2397 - accuracy: 0.94 - ETA: 0s - loss: 0.2352 - accuracy: 0.94 - 0s 4ms/step - loss: 0.2376 - accuracy: 0.9440 - val_loss: 1.0180 - val_accuracy: 0.7284\n",
      "Epoch 29/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.96 - ETA: 0s - loss: 0.2784 - accuracy: 0.93 - ETA: 0s - loss: 0.2772 - accuracy: 0.94 - ETA: 0s - loss: 0.2807 - accuracy: 0.93 - ETA: 0s - loss: 0.2699 - accuracy: 0.93 - ETA: 0s - loss: 0.2569 - accuracy: 0.9422Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2768 - accuracy: 0.9406 - val_loss: 4.6305 - val_accuracy: 0.6821\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 824cb331cb650843897f52c5cdf1ab19</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7313432892163595</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.05010701398723161</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7557 - accuracy: 0.75 - ETA: 0s - loss: 1.7714 - accuracy: 0.57 - ETA: 0s - loss: 1.3748 - accuracy: 0.59 - ETA: 0s - loss: 1.1693 - accuracy: 0.62 - ETA: 0s - loss: 1.0503 - accuracy: 0.63 - ETA: 0s - loss: 0.9608 - accuracy: 0.65 - 0s 5ms/step - loss: 0.9478 - accuracy: 0.6573 - val_loss: 0.5547 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.68 - ETA: 0s - loss: 0.5917 - accuracy: 0.70 - ETA: 0s - loss: 0.5987 - accuracy: 0.69 - ETA: 0s - loss: 0.5965 - accuracy: 0.69 - ETA: 0s - loss: 0.5905 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.70 - 0s 4ms/step - loss: 0.5715 - accuracy: 0.7144 - val_loss: 0.5915 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.75 - ETA: 0s - loss: 0.5531 - accuracy: 0.72 - ETA: 0s - loss: 0.5337 - accuracy: 0.74 - ETA: 0s - loss: 0.5190 - accuracy: 0.74 - ETA: 0s - loss: 0.5238 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7581 - val_loss: 0.6612 - val_accuracy: 0.6627\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.75 - ETA: 0s - loss: 0.3776 - accuracy: 0.82 - ETA: 0s - loss: 0.4163 - accuracy: 0.80 - ETA: 0s - loss: 0.4438 - accuracy: 0.78 - ETA: 0s - loss: 0.4588 - accuracy: 0.78 - 0s 3ms/step - loss: 0.4612 - accuracy: 0.7805 - val_loss: 0.5899 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.81 - ETA: 0s - loss: 0.3824 - accuracy: 0.81 - ETA: 0s - loss: 0.4066 - accuracy: 0.81 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4132 - accuracy: 0.8104 - val_loss: 0.7260 - val_accuracy: 0.7209\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.90 - ETA: 0s - loss: 0.4277 - accuracy: 0.79 - ETA: 0s - loss: 0.4202 - accuracy: 0.80 - ETA: 0s - loss: 0.4056 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8152 - val_loss: 0.7460 - val_accuracy: 0.6791\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.87 - ETA: 0s - loss: 0.4216 - accuracy: 0.82 - ETA: 0s - loss: 0.4066 - accuracy: 0.81 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.81 - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8257 - val_loss: 0.9959 - val_accuracy: 0.6776\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.90 - ETA: 0s - loss: 0.3268 - accuracy: 0.84 - ETA: 0s - loss: 0.3391 - accuracy: 0.84 - ETA: 0s - loss: 0.3241 - accuracy: 0.84 - ETA: 0s - loss: 0.3231 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8387 - val_loss: 0.7800 - val_accuracy: 0.7075\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.78 - ETA: 0s - loss: 0.3102 - accuracy: 0.84 - ETA: 0s - loss: 0.3555 - accuracy: 0.82 - ETA: 0s - loss: 0.3732 - accuracy: 0.81 - ETA: 0s - loss: 0.3863 - accuracy: 0.81 - ETA: 0s - loss: 0.3829 - accuracy: 0.80 - 0s 4ms/step - loss: 0.3829 - accuracy: 0.8089 - val_loss: 1.4612 - val_accuracy: 0.6836\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.75 - ETA: 0s - loss: 0.3789 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.81 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3628 - accuracy: 0.83 - ETA: 0s - loss: 0.3590 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3590 - accuracy: 0.8380 - val_loss: 2.1229 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.2687 - accuracy: 0.87 - ETA: 0s - loss: 0.5293 - accuracy: 0.82 - ETA: 0s - loss: 0.4704 - accuracy: 0.82 - ETA: 0s - loss: 0.4226 - accuracy: 0.82 - ETA: 0s - loss: 0.4258 - accuracy: 0.8249Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8234 - val_loss: 0.9291 - val_accuracy: 0.6791\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.68 - ETA: 0s - loss: 2.3590 - accuracy: 0.62 - ETA: 0s - loss: 1.7732 - accuracy: 0.61 - ETA: 0s - loss: 1.4339 - accuracy: 0.62 - ETA: 0s - loss: 1.2579 - accuracy: 0.61 - ETA: 0s - loss: 1.1326 - accuracy: 0.63 - 0s 5ms/step - loss: 1.0980 - accuracy: 0.6405 - val_loss: 0.5549 - val_accuracy: 0.7358\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.78 - ETA: 0s - loss: 0.6081 - accuracy: 0.71 - ETA: 0s - loss: 0.5751 - accuracy: 0.71 - ETA: 0s - loss: 0.5700 - accuracy: 0.72 - ETA: 0s - loss: 0.5768 - accuracy: 0.72 - 0s 3ms/step - loss: 0.5761 - accuracy: 0.7230 - val_loss: 0.6928 - val_accuracy: 0.6851\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.68 - ETA: 0s - loss: 0.5320 - accuracy: 0.76 - ETA: 0s - loss: 0.5164 - accuracy: 0.77 - ETA: 0s - loss: 0.4947 - accuracy: 0.77 - ETA: 0s - loss: 0.5023 - accuracy: 0.77 - ETA: 0s - loss: 0.5038 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5038 - accuracy: 0.7764 - val_loss: 0.5882 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.84 - ETA: 0s - loss: 0.4262 - accuracy: 0.83 - ETA: 0s - loss: 0.4261 - accuracy: 0.83 - ETA: 0s - loss: 0.4314 - accuracy: 0.82 - ETA: 0s - loss: 0.4528 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4538 - accuracy: 0.8093 - val_loss: 0.5641 - val_accuracy: 0.7179\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.71 - ETA: 0s - loss: 0.3699 - accuracy: 0.83 - ETA: 0s - loss: 0.4068 - accuracy: 0.83 - ETA: 0s - loss: 0.4072 - accuracy: 0.83 - ETA: 0s - loss: 0.4182 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8294 - val_loss: 0.7495 - val_accuracy: 0.6776\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.84 - ETA: 0s - loss: 0.3989 - accuracy: 0.84 - ETA: 0s - loss: 0.3734 - accuracy: 0.85 - ETA: 0s - loss: 0.3975 - accuracy: 0.84 - ETA: 0s - loss: 0.3950 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3950 - accuracy: 0.8406 - val_loss: 0.7554 - val_accuracy: 0.7164\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.90 - ETA: 0s - loss: 0.3281 - accuracy: 0.87 - ETA: 0s - loss: 0.3355 - accuracy: 0.86 - ETA: 0s - loss: 0.3728 - accuracy: 0.85 - ETA: 0s - loss: 0.3938 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8369 - val_loss: 0.7322 - val_accuracy: 0.6672\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.84 - ETA: 0s - loss: 0.3658 - accuracy: 0.84 - ETA: 0s - loss: 0.3584 - accuracy: 0.85 - ETA: 0s - loss: 0.3398 - accuracy: 0.85 - ETA: 0s - loss: 0.3380 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8596 - val_loss: 0.7993 - val_accuracy: 0.7104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.90 - ETA: 0s - loss: 0.3270 - accuracy: 0.87 - ETA: 0s - loss: 0.3130 - accuracy: 0.87 - ETA: 0s - loss: 0.3208 - accuracy: 0.86 - ETA: 0s - loss: 0.3321 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8652 - val_loss: 0.9697 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7907 - accuracy: 0.78 - ETA: 0s - loss: 0.3420 - accuracy: 0.87 - ETA: 0s - loss: 0.3303 - accuracy: 0.86 - ETA: 0s - loss: 0.3221 - accuracy: 0.86 - ETA: 0s - loss: 0.3181 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8694 - val_loss: 0.9293 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.2780 - accuracy: 0.90 - ETA: 0s - loss: 0.2390 - accuracy: 0.88 - ETA: 0s - loss: 0.2745 - accuracy: 0.88 - ETA: 0s - loss: 0.3048 - accuracy: 0.87 - ETA: 0s - loss: 0.3061 - accuracy: 0.8673Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3031 - accuracy: 0.8708 - val_loss: 0.8893 - val_accuracy: 0.7224\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6746 - accuracy: 0.75 - ETA: 0s - loss: 1.7929 - accuracy: 0.61 - ETA: 0s - loss: 1.3762 - accuracy: 0.62 - ETA: 0s - loss: 1.2679 - accuracy: 0.62 - ETA: 0s - loss: 1.1479 - accuracy: 0.64 - ETA: 0s - loss: 1.0457 - accuracy: 0.64 - ETA: 0s - loss: 0.9655 - accuracy: 0.65 - 0s 6ms/step - loss: 0.9374 - accuracy: 0.6670 - val_loss: 0.5469 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5818 - accuracy: 0.65 - ETA: 0s - loss: 0.5679 - accuracy: 0.74 - ETA: 0s - loss: 0.5551 - accuracy: 0.74 - ETA: 0s - loss: 0.5830 - accuracy: 0.71 - ETA: 0s - loss: 0.5824 - accuracy: 0.72 - ETA: 0s - loss: 0.5731 - accuracy: 0.72 - 0s 4ms/step - loss: 0.5646 - accuracy: 0.7312 - val_loss: 0.7951 - val_accuracy: 0.6239\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8500 - accuracy: 0.62 - ETA: 0s - loss: 0.5116 - accuracy: 0.78 - ETA: 0s - loss: 0.4901 - accuracy: 0.79 - ETA: 0s - loss: 0.5018 - accuracy: 0.79 - ETA: 0s - loss: 0.5064 - accuracy: 0.78 - ETA: 0s - loss: 0.5081 - accuracy: 0.78 - ETA: 0s - loss: 0.5038 - accuracy: 0.78 - ETA: 0s - loss: 0.5062 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5062 - accuracy: 0.7831 - val_loss: 0.5486 - val_accuracy: 0.7269\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6568 - accuracy: 0.78 - ETA: 0s - loss: 0.4271 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.82 - ETA: 0s - loss: 0.4714 - accuracy: 0.80 - ETA: 0s - loss: 0.4597 - accuracy: 0.80 - ETA: 0s - loss: 0.4630 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4594 - accuracy: 0.8096 - val_loss: 0.6365 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.93 - ETA: 0s - loss: 0.5225 - accuracy: 0.79 - ETA: 0s - loss: 0.4801 - accuracy: 0.79 - ETA: 0s - loss: 0.4511 - accuracy: 0.81 - ETA: 0s - loss: 0.4478 - accuracy: 0.82 - ETA: 0s - loss: 0.4580 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4574 - accuracy: 0.8197 - val_loss: 0.8415 - val_accuracy: 0.6522\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.78 - ETA: 0s - loss: 0.4249 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.84 - ETA: 0s - loss: 0.3914 - accuracy: 0.84 - ETA: 0s - loss: 0.4115 - accuracy: 0.83 - ETA: 0s - loss: 0.4151 - accuracy: 0.83 - ETA: 0s - loss: 0.4228 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4228 - accuracy: 0.8275 - val_loss: 0.5456 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4240 - accuracy: 0.84 - ETA: 0s - loss: 0.3912 - accuracy: 0.83 - ETA: 0s - loss: 0.3917 - accuracy: 0.84 - ETA: 0s - loss: 0.3982 - accuracy: 0.84 - ETA: 0s - loss: 0.4014 - accuracy: 0.84 - ETA: 0s - loss: 0.4100 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4093 - accuracy: 0.8361 - val_loss: 0.5648 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.93 - ETA: 0s - loss: 0.4048 - accuracy: 0.84 - ETA: 0s - loss: 0.3918 - accuracy: 0.86 - ETA: 0s - loss: 0.3678 - accuracy: 0.86 - ETA: 0s - loss: 0.3715 - accuracy: 0.86 - ETA: 0s - loss: 0.3841 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3932 - accuracy: 0.8537 - val_loss: 0.7294 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.81 - ETA: 0s - loss: 0.3314 - accuracy: 0.87 - ETA: 0s - loss: 0.3345 - accuracy: 0.87 - ETA: 0s - loss: 0.3668 - accuracy: 0.87 - ETA: 0s - loss: 0.3905 - accuracy: 0.85 - ETA: 0s - loss: 0.3935 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3947 - accuracy: 0.8533 - val_loss: 0.6126 - val_accuracy: 0.6910\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.75 - ETA: 0s - loss: 0.4259 - accuracy: 0.82 - ETA: 0s - loss: 0.4132 - accuracy: 0.83 - ETA: 0s - loss: 0.4010 - accuracy: 0.84 - ETA: 0s - loss: 0.4095 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8473 - val_loss: 0.6233 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.87 - ETA: 0s - loss: 0.3200 - accuracy: 0.86 - ETA: 0s - loss: 0.3769 - accuracy: 0.87 - ETA: 0s - loss: 0.3912 - accuracy: 0.86 - ETA: 0s - loss: 0.4496 - accuracy: 0.85 - ETA: 0s - loss: 0.4972 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4994 - accuracy: 0.8358 - val_loss: 0.8168 - val_accuracy: 0.6164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.81 - ETA: 0s - loss: 0.4383 - accuracy: 0.82 - ETA: 0s - loss: 0.4053 - accuracy: 0.84 - ETA: 0s - loss: 0.4174 - accuracy: 0.84 - ETA: 0s - loss: 0.4355 - accuracy: 0.83 - ETA: 0s - loss: 0.4445 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4445 - accuracy: 0.8313 - val_loss: 0.8023 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.93 - ETA: 0s - loss: 0.3958 - accuracy: 0.84 - ETA: 0s - loss: 0.4336 - accuracy: 0.84 - ETA: 0s - loss: 0.4310 - accuracy: 0.84 - ETA: 0s - loss: 0.4183 - accuracy: 0.84 - ETA: 0s - loss: 0.4110 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4086 - accuracy: 0.8466 - val_loss: 0.6862 - val_accuracy: 0.7119\n",
      "Epoch 14/50\n",
      "70/84 [========================>.....] - ETA: 0s - loss: 0.2118 - accuracy: 0.96 - ETA: 0s - loss: 0.2693 - accuracy: 0.90 - ETA: 0s - loss: 0.3575 - accuracy: 0.87 - ETA: 0s - loss: 0.3547 - accuracy: 0.86 - ETA: 0s - loss: 0.3572 - accuracy: 0.8679Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8652 - val_loss: 0.7598 - val_accuracy: 0.7090\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 5435f46dfc233f174155cfe3f125890d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7353233893712362</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.39094450425788796</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7931 - accuracy: 0.68 - ETA: 0s - loss: 0.8237 - accuracy: 0.67 - ETA: 0s - loss: 0.7127 - accuracy: 0.69 - ETA: 0s - loss: 0.6773 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6773 - accuracy: 0.7021 - val_loss: 0.5868 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.75 - ETA: 0s - loss: 0.5060 - accuracy: 0.75 - ETA: 0s - loss: 0.5136 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7630 - val_loss: 0.6441 - val_accuracy: 0.6866\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.84 - ETA: 0s - loss: 0.4539 - accuracy: 0.81 - ETA: 0s - loss: 0.4468 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8089 - val_loss: 0.6507 - val_accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.84 - ETA: 0s - loss: 0.4276 - accuracy: 0.80 - ETA: 0s - loss: 0.4654 - accuracy: 0.80 - ETA: 0s - loss: 0.4773 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7962 - val_loss: 0.7596 - val_accuracy: 0.6567\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5935 - accuracy: 0.59 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8234 - val_loss: 0.6493 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.93 - ETA: 0s - loss: 0.3159 - accuracy: 0.85 - ETA: 0s - loss: 0.3713 - accuracy: 0.84 - ETA: 0s - loss: 0.3613 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3607 - accuracy: 0.8440 - val_loss: 0.7408 - val_accuracy: 0.6985\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.93 - ETA: 0s - loss: 0.3024 - accuracy: 0.88 - ETA: 0s - loss: 0.3752 - accuracy: 0.85 - ETA: 0s - loss: 0.3940 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3955 - accuracy: 0.8462 - val_loss: 0.7450 - val_accuracy: 0.6821\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.87 - ETA: 0s - loss: 0.4437 - accuracy: 0.84 - ETA: 0s - loss: 0.4271 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8369 - val_loss: 1.4304 - val_accuracy: 0.6866\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.90 - ETA: 0s - loss: 0.3251 - accuracy: 0.88 - ETA: 0s - loss: 0.3505 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8615 - val_loss: 0.7258 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.87 - ETA: 0s - loss: 0.3052 - accuracy: 0.87 - ETA: 0s - loss: 0.2947 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8641 - val_loss: 0.7887 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.93 - ETA: 0s - loss: 0.2834 - accuracy: 0.88 - ETA: 0s - loss: 0.3295 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8727 - val_loss: 0.9712 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6378 - accuracy: 0.78 - ETA: 0s - loss: 0.2859 - accuracy: 0.89 - ETA: 0s - loss: 0.3221 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8824 - val_loss: 1.4501 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.90 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9082 - val_loss: 1.6841 - val_accuracy: 0.6418\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.87 - ETA: 0s - loss: 0.2448 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.88 - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8813 - val_loss: 1.5544 - val_accuracy: 0.6910\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3347 - accuracy: 0.84 - ETA: 0s - loss: 0.3141 - accuracy: 0.89 - ETA: 0s - loss: 0.3254 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8813 - val_loss: 0.7629 - val_accuracy: 0.7209\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.93 - ETA: 0s - loss: 0.2308 - accuracy: 0.91 - ETA: 0s - loss: 0.2274 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9197 - val_loss: 1.3855 - val_accuracy: 0.6776\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 1.00 - ETA: 0s - loss: 0.1857 - accuracy: 0.93 - ETA: 0s - loss: 0.2065 - accuracy: 0.92 - ETA: 0s - loss: 0.2194 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2194 - accuracy: 0.9246 - val_loss: 1.1002 - val_accuracy: 0.7075\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.93 - ETA: 0s - loss: 0.1740 - accuracy: 0.94 - ETA: 0s - loss: 0.1857 - accuracy: 0.93 - ETA: 0s - loss: 0.1884 - accuracy: 0.93 - 0s 2ms/step - loss: 0.1895 - accuracy: 0.9347 - val_loss: 1.7572 - val_accuracy: 0.6537\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.93 - ETA: 0s - loss: 0.1657 - accuracy: 0.95 - ETA: 0s - loss: 0.1513 - accuracy: 0.95 - ETA: 0s - loss: 0.1610 - accuracy: 0.94 - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9477 - val_loss: 4.8329 - val_accuracy: 0.7030\n",
      "Epoch 20/50\n",
      "61/84 [====================>.........] - ETA: 0s - loss: 0.0588 - accuracy: 0.96 - ETA: 0s - loss: 0.6700 - accuracy: 0.87 - ETA: 0s - loss: 0.9908 - accuracy: 0.7848Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.9785 - accuracy: 0.7596 - val_loss: 2.6652 - val_accuracy: 0.6955\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7431 - accuracy: 0.71 - ETA: 0s - loss: 0.9115 - accuracy: 0.66 - ETA: 0s - loss: 0.7632 - accuracy: 0.68 - ETA: 0s - loss: 0.7031 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6988 - accuracy: 0.6947 - val_loss: 0.5706 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.75 - ETA: 0s - loss: 0.4819 - accuracy: 0.77 - ETA: 0s - loss: 0.4870 - accuracy: 0.77 - ETA: 0s - loss: 0.4970 - accuracy: 0.76 - 0s 2ms/step - loss: 0.4962 - accuracy: 0.7671 - val_loss: 0.6823 - val_accuracy: 0.6567\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.75 - ETA: 0s - loss: 0.4238 - accuracy: 0.79 - ETA: 0s - loss: 0.4501 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7869 - val_loss: 0.6502 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.81 - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.4083 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8078 - val_loss: 0.6329 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.75 - ETA: 0s - loss: 0.4466 - accuracy: 0.82 - ETA: 0s - loss: 0.4077 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8167 - val_loss: 1.2370 - val_accuracy: 0.7075\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.87 - ETA: 0s - loss: 0.3749 - accuracy: 0.84 - ETA: 0s - loss: 0.3602 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8376 - val_loss: 0.9934 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.68 - ETA: 0s - loss: 0.3108 - accuracy: 0.85 - ETA: 0s - loss: 0.3092 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8541 - val_loss: 1.6368 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.84 - ETA: 0s - loss: 0.2788 - accuracy: 0.86 - ETA: 0s - loss: 0.3144 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8630 - val_loss: 3.1362 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.96 - ETA: 0s - loss: 0.2828 - accuracy: 0.86 - ETA: 0s - loss: 0.3372 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8391 - val_loss: 0.9964 - val_accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.90 - ETA: 0s - loss: 0.4217 - accuracy: 0.84 - ETA: 0s - loss: 0.3865 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8574 - val_loss: 0.7911 - val_accuracy: 0.7134\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.93 - ETA: 0s - loss: 0.2993 - accuracy: 0.89 - ETA: 0s - loss: 0.3704 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8682 - val_loss: 0.6869 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.87 - ETA: 0s - loss: 0.3550 - accuracy: 0.87 - ETA: 0s - loss: 0.3349 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8645 - val_loss: 1.8444 - val_accuracy: 0.6836\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.81 - ETA: 0s - loss: 0.2732 - accuracy: 0.89 - ETA: 0s - loss: 0.2957 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8809 - val_loss: 2.5508 - val_accuracy: 0.6866\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.96 - ETA: 0s - loss: 0.2989 - accuracy: 0.90 - ETA: 0s - loss: 0.4568 - accuracy: 0.89 - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8854 - val_loss: 1.3025 - val_accuracy: 0.6940\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.90 - ETA: 0s - loss: 0.2624 - accuracy: 0.89 - ETA: 0s - loss: 0.2665 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8925 - val_loss: 1.1312 - val_accuracy: 0.6940\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.93 - ETA: 0s - loss: 0.2785 - accuracy: 0.89 - ETA: 0s - loss: 0.2603 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2575 - accuracy: 0.9082 - val_loss: 1.5314 - val_accuracy: 0.6433\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.93 - ETA: 0s - loss: 0.2665 - accuracy: 0.90 - ETA: 0s - loss: 0.2310 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9201 - val_loss: 2.1513 - val_accuracy: 0.6761\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.90 - ETA: 0s - loss: 0.1833 - accuracy: 0.93 - ETA: 0s - loss: 0.1875 - accuracy: 0.93 - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9369 - val_loss: 10.7530 - val_accuracy: 0.7209\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.90 - ETA: 0s - loss: 0.1731 - accuracy: 0.93 - ETA: 0s - loss: 0.2287 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9130 - val_loss: 5.5451 - val_accuracy: 0.6940\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.96 - ETA: 0s - loss: 0.2159 - accuracy: 0.92 - ETA: 0s - loss: 0.3777 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8951 - val_loss: 2.1580 - val_accuracy: 0.6970\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.93 - ETA: 0s - loss: 0.1866 - accuracy: 0.93 - ETA: 0s - loss: 0.1943 - accuracy: 0.93 - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9280 - val_loss: 4.5557 - val_accuracy: 0.7179\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.96 - ETA: 0s - loss: 0.1538 - accuracy: 0.94 - ETA: 0s - loss: 0.1583 - accuracy: 0.94 - 0s 2ms/step - loss: 0.1524 - accuracy: 0.9433 - val_loss: 8.8713 - val_accuracy: 0.7134\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 1.00 - ETA: 0s - loss: 0.1239 - accuracy: 0.96 - ETA: 0s - loss: 0.2410 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2923 - accuracy: 0.9235 - val_loss: 1.3421 - val_accuracy: 0.7478\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 1.00 - ETA: 0s - loss: 0.2438 - accuracy: 0.89 - ETA: 0s - loss: 0.2330 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9160 - val_loss: 2.6321 - val_accuracy: 0.7134\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 1.00 - ETA: 0s - loss: 0.2342 - accuracy: 0.93 - ETA: 0s - loss: 0.2123 - accuracy: 0.92 - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9343 - val_loss: 3.6989 - val_accuracy: 0.7493\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.90 - ETA: 0s - loss: 0.1502 - accuracy: 0.94 - ETA: 0s - loss: 0.2547 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2369 - accuracy: 0.9384 - val_loss: 1.1639 - val_accuracy: 0.7090\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 1.00 - ETA: 0s - loss: 0.1437 - accuracy: 0.94 - ETA: 0s - loss: 0.1607 - accuracy: 0.94 - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9421 - val_loss: 2.5290 - val_accuracy: 0.6881\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.90 - ETA: 0s - loss: 0.1386 - accuracy: 0.95 - ETA: 0s - loss: 0.1268 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9395 - val_loss: 0.6447 - val_accuracy: 0.7090\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.87 - ETA: 0s - loss: 0.3324 - accuracy: 0.88 - ETA: 0s - loss: 0.2525 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9250 - val_loss: 1.3681 - val_accuracy: 0.7134\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 1.00 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1153 - accuracy: 0.96 - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9638 - val_loss: 1.6342 - val_accuracy: 0.7000\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 1.00 - ETA: 0s - loss: 0.0908 - accuracy: 0.97 - ETA: 0s - loss: 0.1199 - accuracy: 0.96 - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9675 - val_loss: 1.5148 - val_accuracy: 0.7284\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.00 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0955 - accuracy: 0.97 - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9742 - val_loss: 1.6780 - val_accuracy: 0.7075\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 1.00 - ETA: 0s - loss: 0.0569 - accuracy: 0.98 - ETA: 0s - loss: 0.0698 - accuracy: 0.98 - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9780 - val_loss: 2.7224 - val_accuracy: 0.6836\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9787 - val_loss: 1.9197 - val_accuracy: 0.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "63/84 [=====================>........] - ETA: 0s - loss: 0.2443 - accuracy: 0.90 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0808 - accuracy: 0.9772Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9735 - val_loss: 2.2519 - val_accuracy: 0.6746\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.78 - ETA: 0s - loss: 0.8152 - accuracy: 0.67 - ETA: 0s - loss: 0.7227 - accuracy: 0.67 - ETA: 0s - loss: 0.6788 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6755 - accuracy: 0.6876 - val_loss: 0.5859 - val_accuracy: 0.6925\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.84 - ETA: 0s - loss: 0.5456 - accuracy: 0.74 - ETA: 0s - loss: 0.5313 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7495 - val_loss: 0.6696 - val_accuracy: 0.6776\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.75 - ETA: 0s - loss: 0.4514 - accuracy: 0.77 - ETA: 0s - loss: 0.4643 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4482 - accuracy: 0.7880 - val_loss: 0.8221 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.87 - ETA: 0s - loss: 0.4041 - accuracy: 0.82 - ETA: 0s - loss: 0.4129 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4179 - accuracy: 0.8104 - val_loss: 0.6583 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.81 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - ETA: 0s - loss: 0.3582 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8343 - val_loss: 0.7704 - val_accuracy: 0.6940\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4274 - accuracy: 0.81 - ETA: 0s - loss: 0.3459 - accuracy: 0.84 - ETA: 0s - loss: 0.3736 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8302 - val_loss: 0.7876 - val_accuracy: 0.6716\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.87 - ETA: 0s - loss: 0.3638 - accuracy: 0.81 - ETA: 0s - loss: 0.3613 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8302 - val_loss: 1.2305 - val_accuracy: 0.6836\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.2815 - accuracy: 0.85 - ETA: 0s - loss: 0.2952 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8593 - val_loss: 1.2416 - val_accuracy: 0.6104\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.78 - ETA: 0s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3538 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8331 - val_loss: 1.0718 - val_accuracy: 0.6299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.90 - ETA: 0s - loss: 0.3153 - accuracy: 0.86 - ETA: 0s - loss: 0.3260 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8626 - val_loss: 2.1469 - val_accuracy: 0.6522\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.81 - ETA: 0s - loss: 0.3400 - accuracy: 0.85 - ETA: 0s - loss: 0.3198 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8697 - val_loss: 1.4695 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 1.00 - ETA: 0s - loss: 0.2374 - accuracy: 0.90 - ETA: 0s - loss: 0.2912 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8764 - val_loss: 1.7174 - val_accuracy: 0.6134\n",
      "Epoch 13/50\n",
      "63/84 [=====================>........] - ETA: 0s - loss: 0.4427 - accuracy: 0.78 - ETA: 0s - loss: 0.3664 - accuracy: 0.86 - ETA: 0s - loss: 0.4049 - accuracy: 0.8646Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8690 - val_loss: 2.2531 - val_accuracy: 0.7104\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cb687cd51d5712548c66f3b337666983</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7313432892163595</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.02948526779185212</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 70</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6885 - accuracy: 0.71 - ETA: 0s - loss: 3.0102 - accuracy: 0.59 - ETA: 0s - loss: 1.9601 - accuracy: 0.62 - ETA: 0s - loss: 1.4956 - accuracy: 0.62 - ETA: 0s - loss: 1.3204 - accuracy: 0.63 - ETA: 0s - loss: 1.1797 - accuracy: 0.66 - ETA: 0s - loss: 1.0916 - accuracy: 0.66 - ETA: 0s - loss: 1.0377 - accuracy: 0.66 - 1s 7ms/step - loss: 0.9997 - accuracy: 0.6704 - val_loss: 0.5974 - val_accuracy: 0.7493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.65 - ETA: 0s - loss: 0.5915 - accuracy: 0.71 - ETA: 0s - loss: 0.5734 - accuracy: 0.73 - ETA: 0s - loss: 0.5799 - accuracy: 0.75 - ETA: 0s - loss: 0.5789 - accuracy: 0.75 - ETA: 0s - loss: 0.5725 - accuracy: 0.76 - ETA: 0s - loss: 0.5719 - accuracy: 0.77 - ETA: 0s - loss: 0.5790 - accuracy: 0.76 - 0s 6ms/step - loss: 0.5822 - accuracy: 0.7622 - val_loss: 0.5702 - val_accuracy: 0.7612\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.81 - ETA: 0s - loss: 0.5269 - accuracy: 0.77 - ETA: 0s - loss: 0.5290 - accuracy: 0.79 - ETA: 0s - loss: 0.5379 - accuracy: 0.78 - ETA: 0s - loss: 0.5347 - accuracy: 0.79 - ETA: 0s - loss: 0.5435 - accuracy: 0.78 - ETA: 0s - loss: 0.5508 - accuracy: 0.78 - ETA: 0s - loss: 0.5511 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5451 - accuracy: 0.7857 - val_loss: 0.5336 - val_accuracy: 0.7433\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.81 - ETA: 0s - loss: 0.5076 - accuracy: 0.76 - ETA: 0s - loss: 0.5176 - accuracy: 0.79 - ETA: 0s - loss: 0.5132 - accuracy: 0.79 - ETA: 0s - loss: 0.4985 - accuracy: 0.80 - ETA: 0s - loss: 0.5051 - accuracy: 0.80 - ETA: 0s - loss: 0.5128 - accuracy: 0.79 - ETA: 0s - loss: 0.5102 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5110 - accuracy: 0.7988 - val_loss: 0.5477 - val_accuracy: 0.7209\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.85 - ETA: 0s - loss: 0.4439 - accuracy: 0.85 - ETA: 0s - loss: 0.6020 - accuracy: 0.82 - ETA: 0s - loss: 0.5771 - accuracy: 0.82 - ETA: 0s - loss: 0.5765 - accuracy: 0.81 - ETA: 0s - loss: 0.5684 - accuracy: 0.81 - ETA: 0s - loss: 0.5656 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5642 - accuracy: 0.8137 - val_loss: 0.5588 - val_accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.90 - ETA: 0s - loss: 0.5091 - accuracy: 0.82 - ETA: 0s - loss: 0.4963 - accuracy: 0.82 - ETA: 0s - loss: 0.5119 - accuracy: 0.81 - ETA: 0s - loss: 0.5132 - accuracy: 0.81 - ETA: 0s - loss: 0.5159 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.81 - ETA: 0s - loss: 0.5245 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5274 - accuracy: 0.8096 - val_loss: 0.5579 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.75 - ETA: 0s - loss: 0.4935 - accuracy: 0.78 - ETA: 0s - loss: 0.5379 - accuracy: 0.78 - ETA: 0s - loss: 0.5255 - accuracy: 0.79 - ETA: 0s - loss: 0.5153 - accuracy: 0.79 - ETA: 0s - loss: 0.5047 - accuracy: 0.80 - ETA: 0s - loss: 0.5274 - accuracy: 0.80 - ETA: 0s - loss: 0.5413 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5371 - accuracy: 0.8126 - val_loss: 0.6412 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.90 - ETA: 0s - loss: 0.4676 - accuracy: 0.86 - ETA: 0s - loss: 0.4806 - accuracy: 0.85 - ETA: 0s - loss: 0.5621 - accuracy: 0.84 - ETA: 0s - loss: 0.5774 - accuracy: 0.82 - ETA: 0s - loss: 0.5685 - accuracy: 0.82 - ETA: 0s - loss: 0.5605 - accuracy: 0.82 - ETA: 0s - loss: 0.5438 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5433 - accuracy: 0.8287 - val_loss: 0.5442 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6094 - accuracy: 0.81 - ETA: 0s - loss: 0.4808 - accuracy: 0.84 - ETA: 0s - loss: 0.4805 - accuracy: 0.84 - ETA: 0s - loss: 0.5510 - accuracy: 0.81 - ETA: 0s - loss: 0.5678 - accuracy: 0.81 - ETA: 0s - loss: 0.5492 - accuracy: 0.82 - ETA: 0s - loss: 0.5480 - accuracy: 0.82 - ETA: 0s - loss: 0.5449 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5406 - accuracy: 0.8257 - val_loss: 0.7839 - val_accuracy: 0.7522\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.96 - ETA: 0s - loss: 0.4540 - accuracy: 0.85 - ETA: 0s - loss: 0.4805 - accuracy: 0.85 - ETA: 0s - loss: 0.5434 - accuracy: 0.84 - ETA: 0s - loss: 0.5334 - accuracy: 0.84 - ETA: 0s - loss: 0.5283 - accuracy: 0.83 - ETA: 0s - loss: 0.5170 - accuracy: 0.84 - ETA: 0s - loss: 0.5158 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5123 - accuracy: 0.8421 - val_loss: 0.5990 - val_accuracy: 0.7463\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.90 - ETA: 0s - loss: 0.4583 - accuracy: 0.85 - ETA: 0s - loss: 0.4587 - accuracy: 0.85 - ETA: 0s - loss: 0.4473 - accuracy: 0.86 - ETA: 0s - loss: 0.4549 - accuracy: 0.85 - ETA: 0s - loss: 0.4590 - accuracy: 0.85 - ETA: 0s - loss: 0.4690 - accuracy: 0.85 - ETA: 0s - loss: 0.4610 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4569 - accuracy: 0.8537 - val_loss: 0.6433 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5626 - accuracy: 0.81 - ETA: 0s - loss: 0.4459 - accuracy: 0.86 - ETA: 0s - loss: 0.4712 - accuracy: 0.85 - ETA: 0s - loss: 0.4593 - accuracy: 0.85 - ETA: 0s - loss: 0.4555 - accuracy: 0.85 - ETA: 0s - loss: 0.4592 - accuracy: 0.85 - ETA: 0s - loss: 0.4591 - accuracy: 0.85 - ETA: 0s - loss: 0.4506 - accuracy: 0.8626Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.8634 - val_loss: 0.6289 - val_accuracy: 0.7299\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7538 - accuracy: 0.50 - ETA: 0s - loss: 3.7698 - accuracy: 0.55 - ETA: 0s - loss: 2.2011 - accuracy: 0.57 - ETA: 0s - loss: 1.6772 - accuracy: 0.62 - ETA: 0s - loss: 1.4216 - accuracy: 0.62 - ETA: 0s - loss: 1.2878 - accuracy: 0.64 - ETA: 0s - loss: 1.1742 - accuracy: 0.64 - ETA: 0s - loss: 1.1068 - accuracy: 0.64 - 1s 7ms/step - loss: 1.0541 - accuracy: 0.6562 - val_loss: 0.6552 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.71 - ETA: 0s - loss: 0.5844 - accuracy: 0.78 - ETA: 0s - loss: 0.6627 - accuracy: 0.73 - ETA: 0s - loss: 0.6420 - accuracy: 0.72 - ETA: 0s - loss: 0.6143 - accuracy: 0.73 - ETA: 0s - loss: 0.6150 - accuracy: 0.73 - ETA: 0s - loss: 0.6154 - accuracy: 0.73 - ETA: 0s - loss: 0.6200 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6161 - accuracy: 0.7365 - val_loss: 0.6143 - val_accuracy: 0.7299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.87 - ETA: 0s - loss: 0.6304 - accuracy: 0.78 - ETA: 0s - loss: 0.6061 - accuracy: 0.77 - ETA: 0s - loss: 0.5926 - accuracy: 0.78 - ETA: 0s - loss: 0.5738 - accuracy: 0.79 - ETA: 0s - loss: 0.5835 - accuracy: 0.78 - ETA: 0s - loss: 0.5826 - accuracy: 0.78 - ETA: 0s - loss: 0.5854 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5840 - accuracy: 0.7813 - val_loss: 0.6019 - val_accuracy: 0.7015\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.59 - ETA: 0s - loss: 0.5435 - accuracy: 0.82 - ETA: 0s - loss: 0.5352 - accuracy: 0.80 - ETA: 0s - loss: 0.5361 - accuracy: 0.80 - ETA: 0s - loss: 0.5409 - accuracy: 0.79 - ETA: 0s - loss: 0.5380 - accuracy: 0.79 - ETA: 0s - loss: 0.5496 - accuracy: 0.79 - ETA: 0s - loss: 0.5510 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5522 - accuracy: 0.7962 - val_loss: 0.5679 - val_accuracy: 0.7403\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.78 - ETA: 0s - loss: 0.5338 - accuracy: 0.81 - ETA: 0s - loss: 0.5089 - accuracy: 0.82 - ETA: 0s - loss: 0.5115 - accuracy: 0.82 - ETA: 0s - loss: 0.5259 - accuracy: 0.82 - ETA: 0s - loss: 0.5426 - accuracy: 0.81 - ETA: 0s - loss: 0.5336 - accuracy: 0.81 - ETA: 0s - loss: 0.5325 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5357 - accuracy: 0.8149 - val_loss: 0.5610 - val_accuracy: 0.7552\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.84 - ETA: 0s - loss: 0.5265 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.81 - ETA: 0s - loss: 0.5081 - accuracy: 0.82 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - ETA: 0s - loss: 0.5053 - accuracy: 0.82 - ETA: 0s - loss: 0.4970 - accuracy: 0.82 - ETA: 0s - loss: 0.4965 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4937 - accuracy: 0.8317 - val_loss: 0.5876 - val_accuracy: 0.7418\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.78 - ETA: 0s - loss: 0.4523 - accuracy: 0.84 - ETA: 0s - loss: 0.5037 - accuracy: 0.83 - ETA: 0s - loss: 0.5006 - accuracy: 0.83 - ETA: 0s - loss: 0.4976 - accuracy: 0.83 - ETA: 0s - loss: 0.5132 - accuracy: 0.82 - ETA: 0s - loss: 0.5024 - accuracy: 0.83 - ETA: 0s - loss: 0.4970 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4980 - accuracy: 0.8324 - val_loss: 0.5797 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.78 - ETA: 0s - loss: 0.4427 - accuracy: 0.86 - ETA: 0s - loss: 0.4677 - accuracy: 0.85 - ETA: 0s - loss: 0.4653 - accuracy: 0.85 - ETA: 0s - loss: 0.4793 - accuracy: 0.84 - ETA: 0s - loss: 0.4781 - accuracy: 0.84 - ETA: 0s - loss: 0.4849 - accuracy: 0.84 - ETA: 0s - loss: 0.4928 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4970 - accuracy: 0.8376 - val_loss: 0.5921 - val_accuracy: 0.7463\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.90 - ETA: 0s - loss: 0.5097 - accuracy: 0.85 - ETA: 0s - loss: 0.5086 - accuracy: 0.84 - ETA: 0s - loss: 0.4947 - accuracy: 0.84 - ETA: 0s - loss: 0.4974 - accuracy: 0.84 - ETA: 0s - loss: 0.5259 - accuracy: 0.83 - ETA: 0s - loss: 0.5308 - accuracy: 0.82 - ETA: 0s - loss: 0.5348 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5366 - accuracy: 0.8261 - val_loss: 0.5745 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7838 - accuracy: 0.75 - ETA: 0s - loss: 0.6325 - accuracy: 0.77 - ETA: 0s - loss: 0.5784 - accuracy: 0.80 - ETA: 0s - loss: 0.5751 - accuracy: 0.80 - ETA: 0s - loss: 0.5766 - accuracy: 0.81 - ETA: 0s - loss: 0.5779 - accuracy: 0.81 - ETA: 0s - loss: 0.5910 - accuracy: 0.80 - ETA: 0s - loss: 0.5833 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5808 - accuracy: 0.8081 - val_loss: 0.6295 - val_accuracy: 0.7537\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.71 - ETA: 0s - loss: 0.5316 - accuracy: 0.81 - ETA: 0s - loss: 0.5785 - accuracy: 0.83 - ETA: 0s - loss: 0.5832 - accuracy: 0.82 - ETA: 0s - loss: 0.5816 - accuracy: 0.82 - ETA: 0s - loss: 0.5592 - accuracy: 0.82 - ETA: 0s - loss: 0.5558 - accuracy: 0.83 - ETA: 0s - loss: 0.5490 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5439 - accuracy: 0.8350 - val_loss: 0.7353 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.81 - ETA: 0s - loss: 0.4821 - accuracy: 0.82 - ETA: 0s - loss: 0.5095 - accuracy: 0.81 - ETA: 0s - loss: 0.4858 - accuracy: 0.83 - ETA: 0s - loss: 0.4909 - accuracy: 0.83 - ETA: 0s - loss: 0.4903 - accuracy: 0.84 - ETA: 0s - loss: 0.5035 - accuracy: 0.83 - ETA: 0s - loss: 0.5107 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5049 - accuracy: 0.8294 - val_loss: 0.6254 - val_accuracy: 0.7478\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.81 - ETA: 0s - loss: 0.5111 - accuracy: 0.83 - ETA: 0s - loss: 0.5096 - accuracy: 0.83 - ETA: 0s - loss: 0.5258 - accuracy: 0.83 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.5143 - accuracy: 0.83 - ETA: 0s - loss: 0.5074 - accuracy: 0.84 - ETA: 0s - loss: 0.4990 - accuracy: 0.84 - ETA: 0s - loss: 0.5049 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5054 - accuracy: 0.8391 - val_loss: 0.5801 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.81 - ETA: 0s - loss: 0.4795 - accuracy: 0.84 - ETA: 0s - loss: 0.4539 - accuracy: 0.86 - ETA: 0s - loss: 0.4590 - accuracy: 0.85 - ETA: 0s - loss: 0.4695 - accuracy: 0.85 - ETA: 0s - loss: 0.4717 - accuracy: 0.85 - ETA: 0s - loss: 0.4667 - accuracy: 0.85 - ETA: 0s - loss: 0.4669 - accuracy: 0.85 - ETA: 0s - loss: 0.4690 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4690 - accuracy: 0.8559 - val_loss: 0.5893 - val_accuracy: 0.7448\n",
      "Epoch 15/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.90 - ETA: 0s - loss: 0.4241 - accuracy: 0.87 - ETA: 0s - loss: 0.4121 - accuracy: 0.87 - ETA: 0s - loss: 0.4374 - accuracy: 0.86 - ETA: 0s - loss: 0.4459 - accuracy: 0.85 - ETA: 0s - loss: 0.4472 - accuracy: 0.85 - ETA: 0s - loss: 0.4468 - accuracy: 0.85 - ETA: 0s - loss: 0.4451 - accuracy: 0.85 - ETA: 0s - loss: 0.4551 - accuracy: 0.8607Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.4539 - accuracy: 0.8615 - val_loss: 0.6379 - val_accuracy: 0.7493\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9281 - accuracy: 0.53 - ETA: 0s - loss: 2.9280 - accuracy: 0.55 - ETA: 0s - loss: 1.8967 - accuracy: 0.58 - ETA: 0s - loss: 1.4921 - accuracy: 0.62 - ETA: 0s - loss: 1.2707 - accuracy: 0.64 - ETA: 0s - loss: 1.1646 - accuracy: 0.65 - ETA: 0s - loss: 1.0807 - accuracy: 0.66 - ETA: 0s - loss: 1.0159 - accuracy: 0.67 - 1s 7ms/step - loss: 0.9800 - accuracy: 0.6812 - val_loss: 0.5772 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6582 - accuracy: 0.59 - ETA: 0s - loss: 0.6175 - accuracy: 0.67 - ETA: 0s - loss: 0.6416 - accuracy: 0.71 - ETA: 0s - loss: 0.6420 - accuracy: 0.69 - ETA: 0s - loss: 0.6493 - accuracy: 0.69 - ETA: 0s - loss: 0.6496 - accuracy: 0.70 - ETA: 0s - loss: 0.6477 - accuracy: 0.70 - ETA: 0s - loss: 0.6402 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6339 - accuracy: 0.7111 - val_loss: 0.5588 - val_accuracy: 0.6821\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.75 - ETA: 0s - loss: 0.6360 - accuracy: 0.74 - ETA: 0s - loss: 0.6009 - accuracy: 0.75 - ETA: 0s - loss: 0.5966 - accuracy: 0.76 - ETA: 0s - loss: 0.5758 - accuracy: 0.76 - ETA: 0s - loss: 0.5722 - accuracy: 0.76 - ETA: 0s - loss: 0.5740 - accuracy: 0.76 - ETA: 0s - loss: 0.5862 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5813 - accuracy: 0.7671 - val_loss: 0.5842 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.87 - ETA: 0s - loss: 0.5347 - accuracy: 0.77 - ETA: 0s - loss: 0.5416 - accuracy: 0.77 - ETA: 0s - loss: 0.5629 - accuracy: 0.78 - ETA: 0s - loss: 0.5728 - accuracy: 0.78 - ETA: 0s - loss: 0.5694 - accuracy: 0.78 - ETA: 0s - loss: 0.5670 - accuracy: 0.78 - ETA: 0s - loss: 0.5667 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5813 - accuracy: 0.7857 - val_loss: 0.5814 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.62 - ETA: 0s - loss: 0.5093 - accuracy: 0.81 - ETA: 0s - loss: 0.5249 - accuracy: 0.80 - ETA: 0s - loss: 0.5614 - accuracy: 0.78 - ETA: 0s - loss: 0.5813 - accuracy: 0.78 - ETA: 0s - loss: 0.5814 - accuracy: 0.78 - ETA: 0s - loss: 0.5767 - accuracy: 0.79 - ETA: 0s - loss: 0.5782 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5730 - accuracy: 0.7910 - val_loss: 0.5947 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.75 - ETA: 0s - loss: 0.4837 - accuracy: 0.79 - ETA: 0s - loss: 0.5164 - accuracy: 0.80 - ETA: 0s - loss: 0.5053 - accuracy: 0.80 - ETA: 0s - loss: 0.4999 - accuracy: 0.81 - ETA: 0s - loss: 0.5160 - accuracy: 0.80 - ETA: 0s - loss: 0.5304 - accuracy: 0.80 - ETA: 0s - loss: 0.5424 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5450 - accuracy: 0.8003 - val_loss: 0.5694 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.81 - ETA: 0s - loss: 0.5852 - accuracy: 0.79 - ETA: 0s - loss: 0.5874 - accuracy: 0.77 - ETA: 0s - loss: 0.5733 - accuracy: 0.78 - ETA: 0s - loss: 0.5674 - accuracy: 0.79 - ETA: 0s - loss: 0.5522 - accuracy: 0.80 - ETA: 0s - loss: 0.5650 - accuracy: 0.79 - ETA: 0s - loss: 0.5670 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5712 - accuracy: 0.7984 - val_loss: 0.5738 - val_accuracy: 0.7448\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.87 - ETA: 0s - loss: 0.5229 - accuracy: 0.82 - ETA: 0s - loss: 0.5320 - accuracy: 0.81 - ETA: 0s - loss: 0.5402 - accuracy: 0.81 - ETA: 0s - loss: 0.5285 - accuracy: 0.82 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - ETA: 0s - loss: 0.5156 - accuracy: 0.82 - ETA: 0s - loss: 0.5228 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5243 - accuracy: 0.8216 - val_loss: 0.6666 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.78 - ETA: 0s - loss: 0.4717 - accuracy: 0.85 - ETA: 0s - loss: 0.4919 - accuracy: 0.84 - ETA: 0s - loss: 0.5078 - accuracy: 0.84 - ETA: 0s - loss: 0.5195 - accuracy: 0.83 - ETA: 0s - loss: 0.5196 - accuracy: 0.83 - ETA: 0s - loss: 0.5164 - accuracy: 0.83 - ETA: 0s - loss: 0.5188 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5167 - accuracy: 0.8365 - val_loss: 0.5496 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.87 - ETA: 0s - loss: 0.4808 - accuracy: 0.83 - ETA: 0s - loss: 0.4761 - accuracy: 0.84 - ETA: 0s - loss: 0.4763 - accuracy: 0.84 - ETA: 0s - loss: 0.5000 - accuracy: 0.84 - ETA: 0s - loss: 0.5173 - accuracy: 0.83 - ETA: 0s - loss: 0.5208 - accuracy: 0.83 - ETA: 0s - loss: 0.5190 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5189 - accuracy: 0.8294 - val_loss: 0.5550 - val_accuracy: 0.7463\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.81 - ETA: 0s - loss: 0.5024 - accuracy: 0.82 - ETA: 0s - loss: 0.6207 - accuracy: 0.82 - ETA: 0s - loss: 0.5931 - accuracy: 0.83 - ETA: 0s - loss: 0.5798 - accuracy: 0.82 - ETA: 0s - loss: 0.5719 - accuracy: 0.82 - ETA: 0s - loss: 0.5530 - accuracy: 0.82 - ETA: 0s - loss: 0.5433 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5362 - accuracy: 0.8343 - val_loss: 0.6030 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.90 - ETA: 0s - loss: 0.4339 - accuracy: 0.87 - ETA: 0s - loss: 0.4649 - accuracy: 0.86 - ETA: 0s - loss: 0.4818 - accuracy: 0.85 - ETA: 0s - loss: 0.4850 - accuracy: 0.85 - ETA: 0s - loss: 0.4906 - accuracy: 0.84 - ETA: 0s - loss: 0.4810 - accuracy: 0.85 - ETA: 0s - loss: 0.4868 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4898 - accuracy: 0.8492 - val_loss: 0.5550 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.78 - ETA: 0s - loss: 0.4907 - accuracy: 0.84 - ETA: 0s - loss: 0.4952 - accuracy: 0.83 - ETA: 0s - loss: 0.4940 - accuracy: 0.83 - ETA: 0s - loss: 0.4909 - accuracy: 0.84 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - ETA: 0s - loss: 0.4927 - accuracy: 0.84 - ETA: 0s - loss: 0.4846 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4764 - accuracy: 0.8499 - val_loss: 0.5962 - val_accuracy: 0.7403\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4238 - accuracy: 0.87 - ETA: 0s - loss: 0.4253 - accuracy: 0.86 - ETA: 0s - loss: 0.4019 - accuracy: 0.88 - ETA: 0s - loss: 0.4252 - accuracy: 0.87 - ETA: 0s - loss: 0.4300 - accuracy: 0.87 - ETA: 0s - loss: 0.4451 - accuracy: 0.86 - ETA: 0s - loss: 0.4514 - accuracy: 0.85 - ETA: 0s - loss: 0.4490 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4536 - accuracy: 0.8608 - val_loss: 0.6164 - val_accuracy: 0.7373\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4737 - accuracy: 0.84 - ETA: 0s - loss: 0.4432 - accuracy: 0.85 - ETA: 0s - loss: 0.4279 - accuracy: 0.87 - ETA: 0s - loss: 0.4329 - accuracy: 0.87 - ETA: 0s - loss: 0.4430 - accuracy: 0.86 - ETA: 0s - loss: 0.4409 - accuracy: 0.86 - ETA: 0s - loss: 0.4501 - accuracy: 0.86 - ETA: 0s - loss: 0.4568 - accuracy: 0.85 - ETA: 0s - loss: 0.4527 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4527 - accuracy: 0.8626 - val_loss: 0.6683 - val_accuracy: 0.7463\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.96 - ETA: 0s - loss: 0.5123 - accuracy: 0.83 - ETA: 0s - loss: 0.4836 - accuracy: 0.84 - ETA: 0s - loss: 0.4958 - accuracy: 0.83 - ETA: 0s - loss: 0.4961 - accuracy: 0.83 - ETA: 0s - loss: 0.4846 - accuracy: 0.84 - ETA: 0s - loss: 0.4818 - accuracy: 0.84 - ETA: 0s - loss: 0.4815 - accuracy: 0.84 - ETA: 0s - loss: 0.4818 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4819 - accuracy: 0.8514 - val_loss: 0.5805 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.93 - ETA: 0s - loss: 0.4953 - accuracy: 0.84 - ETA: 0s - loss: 0.4890 - accuracy: 0.84 - ETA: 0s - loss: 0.5188 - accuracy: 0.84 - ETA: 0s - loss: 0.5339 - accuracy: 0.85 - ETA: 0s - loss: 0.5310 - accuracy: 0.84 - ETA: 0s - loss: 0.5112 - accuracy: 0.85 - ETA: 0s - loss: 0.5087 - accuracy: 0.85 - ETA: 0s - loss: 0.5067 - accuracy: 0.85 - 0s 6ms/step - loss: 0.5062 - accuracy: 0.8567 - val_loss: 0.6483 - val_accuracy: 0.7582\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.78 - ETA: 0s - loss: 0.5712 - accuracy: 0.82 - ETA: 0s - loss: 0.5595 - accuracy: 0.82 - ETA: 0s - loss: 0.5677 - accuracy: 0.83 - ETA: 0s - loss: 0.5479 - accuracy: 0.83 - ETA: 0s - loss: 0.5403 - accuracy: 0.84 - ETA: 0s - loss: 0.5381 - accuracy: 0.84 - ETA: 0s - loss: 0.5420 - accuracy: 0.83 - ETA: 0s - loss: 0.5424 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5442 - accuracy: 0.8339 - val_loss: 0.7208 - val_accuracy: 0.7493\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.71 - ETA: 0s - loss: 0.5313 - accuracy: 0.81 - ETA: 0s - loss: 0.4885 - accuracy: 0.84 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5150 - accuracy: 0.83 - ETA: 0s - loss: 0.5458 - accuracy: 0.82 - ETA: 0s - loss: 0.5404 - accuracy: 0.82 - ETA: 0s - loss: 0.5388 - accuracy: 0.82 - ETA: 0s - loss: 0.5759 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5833 - accuracy: 0.8167 - val_loss: 0.6337 - val_accuracy: 0.7149\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5535 - accuracy: 0.81 - ETA: 0s - loss: 0.5649 - accuracy: 0.79 - ETA: 0s - loss: 1.4129 - accuracy: 0.79 - ETA: 0s - loss: 1.1660 - accuracy: 0.79 - ETA: 0s - loss: 1.0716 - accuracy: 0.78 - ETA: 0s - loss: 0.9775 - accuracy: 0.78 - ETA: 0s - loss: 0.9162 - accuracy: 0.78 - ETA: 0s - loss: 0.8684 - accuracy: 0.78 - ETA: 0s - loss: 0.8374 - accuracy: 0.78 - 0s 6ms/step - loss: 0.8330 - accuracy: 0.7794 - val_loss: 0.9319 - val_accuracy: 0.7343\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.71 - ETA: 0s - loss: 0.5811 - accuracy: 0.72 - ETA: 0s - loss: 0.5848 - accuracy: 0.74 - ETA: 0s - loss: 0.5997 - accuracy: 0.76 - ETA: 0s - loss: 0.5874 - accuracy: 0.78 - ETA: 0s - loss: 0.5847 - accuracy: 0.77 - ETA: 0s - loss: 0.5829 - accuracy: 0.77 - ETA: 0s - loss: 0.5902 - accuracy: 0.77 - ETA: 0s - loss: 0.5912 - accuracy: 0.77 - 0s 6ms/step - loss: 0.5944 - accuracy: 0.7742 - val_loss: 1.1205 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6182 - accuracy: 0.71 - ETA: 0s - loss: 0.5859 - accuracy: 0.76 - ETA: 0s - loss: 0.5867 - accuracy: 0.78 - ETA: 0s - loss: 0.5960 - accuracy: 0.77 - ETA: 0s - loss: 0.6009 - accuracy: 0.77 - ETA: 0s - loss: 0.5999 - accuracy: 0.77 - ETA: 0s - loss: 0.6064 - accuracy: 0.77 - ETA: 0s - loss: 0.5979 - accuracy: 0.78 - ETA: 0s - loss: 0.5996 - accuracy: 0.77 - 0s 6ms/step - loss: 0.6010 - accuracy: 0.7772 - val_loss: 0.9559 - val_accuracy: 0.7478\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.81 - ETA: 0s - loss: 0.5500 - accuracy: 0.76 - ETA: 0s - loss: 0.5620 - accuracy: 0.78 - ETA: 0s - loss: 0.5697 - accuracy: 0.78 - ETA: 0s - loss: 0.5665 - accuracy: 0.79 - ETA: 0s - loss: 0.5658 - accuracy: 0.79 - ETA: 0s - loss: 0.5746 - accuracy: 0.79 - ETA: 0s - loss: 0.5839 - accuracy: 0.78 - ETA: 0s - loss: 0.5882 - accuracy: 0.78 - 0s 6ms/step - loss: 0.5849 - accuracy: 0.7872 - val_loss: 0.7270 - val_accuracy: 0.7433\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5846 - accuracy: 0.81 - ETA: 0s - loss: 0.5491 - accuracy: 0.81 - ETA: 0s - loss: 0.5654 - accuracy: 0.79 - ETA: 0s - loss: 0.5707 - accuracy: 0.80 - ETA: 0s - loss: 0.5827 - accuracy: 0.79 - ETA: 0s - loss: 0.6690 - accuracy: 0.79 - ETA: 0s - loss: 0.6517 - accuracy: 0.79 - ETA: 0s - loss: 0.6401 - accuracy: 0.79 - ETA: 0s - loss: 0.6297 - accuracy: 0.79 - 0s 6ms/step - loss: 0.6297 - accuracy: 0.7940 - val_loss: 0.7388 - val_accuracy: 0.7358\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.93 - ETA: 0s - loss: 0.5120 - accuracy: 0.84 - ETA: 0s - loss: 0.5243 - accuracy: 0.83 - ETA: 0s - loss: 0.5182 - accuracy: 0.83 - ETA: 0s - loss: 0.5288 - accuracy: 0.83 - ETA: 0s - loss: 0.5198 - accuracy: 0.83 - ETA: 0s - loss: 0.5179 - accuracy: 0.83 - ETA: 0s - loss: 0.5182 - accuracy: 0.83 - ETA: 0s - loss: 0.5179 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5179 - accuracy: 0.8361 - val_loss: 0.6779 - val_accuracy: 0.7403\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.81 - ETA: 0s - loss: 0.4607 - accuracy: 0.86 - ETA: 0s - loss: 0.4921 - accuracy: 0.84 - ETA: 0s - loss: 0.5012 - accuracy: 0.84 - ETA: 0s - loss: 0.5073 - accuracy: 0.83 - ETA: 0s - loss: 0.5110 - accuracy: 0.83 - ETA: 0s - loss: 0.5103 - accuracy: 0.83 - ETA: 0s - loss: 0.5101 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5183 - accuracy: 0.8335 - val_loss: 0.7629 - val_accuracy: 0.7418\n",
      "Epoch 27/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.5001 - accuracy: 0.84 - ETA: 0s - loss: 0.5245 - accuracy: 0.83 - ETA: 0s - loss: 0.5334 - accuracy: 0.82 - ETA: 0s - loss: 0.5233 - accuracy: 0.82 - ETA: 0s - loss: 0.5175 - accuracy: 0.82 - ETA: 0s - loss: 0.5139 - accuracy: 0.82 - ETA: 0s - loss: 0.5178 - accuracy: 0.82 - ETA: 0s - loss: 0.5140 - accuracy: 0.8300Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5099 - accuracy: 0.8335 - val_loss: 0.8355 - val_accuracy: 0.7164\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 18609f029804625858c2594148b0e6cd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7582089503606161</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7739571405227555</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 185</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8516 - accuracy: 0.65 - ETA: 0s - loss: 2.1218 - accuracy: 0.60 - ETA: 0s - loss: 1.4463 - accuracy: 0.64 - ETA: 0s - loss: 1.1792 - accuracy: 0.63 - ETA: 0s - loss: 1.0521 - accuracy: 0.63 - ETA: 0s - loss: 0.9770 - accuracy: 0.64 - ETA: 0s - loss: 0.9199 - accuracy: 0.65 - ETA: 0s - loss: 0.8808 - accuracy: 0.65 - 1s 7ms/step - loss: 0.8442 - accuracy: 0.6700 - val_loss: 0.6110 - val_accuracy: 0.6866\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.78 - ETA: 0s - loss: 0.6264 - accuracy: 0.69 - ETA: 0s - loss: 0.6212 - accuracy: 0.70 - ETA: 0s - loss: 0.6165 - accuracy: 0.72 - ETA: 0s - loss: 0.6189 - accuracy: 0.69 - ETA: 0s - loss: 0.6329 - accuracy: 0.67 - ETA: 0s - loss: 0.6267 - accuracy: 0.67 - ETA: 0s - loss: 0.6224 - accuracy: 0.68 - 0s 6ms/step - loss: 0.6180 - accuracy: 0.6861 - val_loss: 0.6323 - val_accuracy: 0.6925\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.68 - ETA: 0s - loss: 0.5551 - accuracy: 0.73 - ETA: 0s - loss: 0.5612 - accuracy: 0.72 - ETA: 0s - loss: 0.5636 - accuracy: 0.71 - ETA: 0s - loss: 0.5790 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.70 - ETA: 0s - loss: 0.5788 - accuracy: 0.71 - 0s 5ms/step - loss: 0.5839 - accuracy: 0.7133 - val_loss: 0.6439 - val_accuracy: 0.6716\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.68 - ETA: 0s - loss: 0.5624 - accuracy: 0.71 - ETA: 0s - loss: 0.5315 - accuracy: 0.73 - ETA: 0s - loss: 0.5237 - accuracy: 0.73 - ETA: 0s - loss: 0.5489 - accuracy: 0.71 - ETA: 0s - loss: 0.5624 - accuracy: 0.69 - ETA: 0s - loss: 0.5711 - accuracy: 0.68 - ETA: 0s - loss: 0.5693 - accuracy: 0.68 - 0s 5ms/step - loss: 0.5704 - accuracy: 0.6868 - val_loss: 0.6254 - val_accuracy: 0.6343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.65 - ETA: 0s - loss: 0.5173 - accuracy: 0.71 - ETA: 0s - loss: 0.4892 - accuracy: 0.74 - ETA: 0s - loss: 0.5088 - accuracy: 0.72 - ETA: 0s - loss: 0.5224 - accuracy: 0.69 - ETA: 0s - loss: 0.5395 - accuracy: 0.68 - ETA: 0s - loss: 0.5435 - accuracy: 0.68 - ETA: 0s - loss: 0.5404 - accuracy: 0.69 - 0s 5ms/step - loss: 0.5400 - accuracy: 0.6928 - val_loss: 0.6575 - val_accuracy: 0.6507\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.71 - ETA: 0s - loss: 0.5047 - accuracy: 0.70 - ETA: 0s - loss: 0.5189 - accuracy: 0.69 - ETA: 0s - loss: 0.5125 - accuracy: 0.70 - ETA: 0s - loss: 0.5159 - accuracy: 0.69 - ETA: 0s - loss: 0.5081 - accuracy: 0.69 - ETA: 0s - loss: 0.5050 - accuracy: 0.69 - ETA: 0s - loss: 0.5115 - accuracy: 0.68 - 0s 5ms/step - loss: 0.5128 - accuracy: 0.6794 - val_loss: 0.7641 - val_accuracy: 0.5313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4098 - accuracy: 0.78 - ETA: 0s - loss: 0.5046 - accuracy: 0.67 - ETA: 0s - loss: 0.5052 - accuracy: 0.68 - ETA: 0s - loss: 0.5122 - accuracy: 0.68 - ETA: 0s - loss: 0.5102 - accuracy: 0.69 - ETA: 0s - loss: 0.5137 - accuracy: 0.69 - ETA: 0s - loss: 0.5122 - accuracy: 0.69 - ETA: 0s - loss: 0.5115 - accuracy: 0.69 - 0s 5ms/step - loss: 0.5124 - accuracy: 0.6909 - val_loss: 0.7747 - val_accuracy: 0.5642\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4192 - accuracy: 0.68 - ETA: 0s - loss: 0.5405 - accuracy: 0.67 - ETA: 0s - loss: 0.5686 - accuracy: 0.61 - ETA: 0s - loss: 0.5784 - accuracy: 0.58 - ETA: 0s - loss: 0.5839 - accuracy: 0.58 - ETA: 0s - loss: 0.5730 - accuracy: 0.59 - ETA: 0s - loss: 0.5750 - accuracy: 0.59 - ETA: 0s - loss: 0.5743 - accuracy: 0.59 - 0s 5ms/step - loss: 0.5742 - accuracy: 0.5950 - val_loss: 0.7898 - val_accuracy: 0.5537\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.81 - ETA: 0s - loss: 0.4976 - accuracy: 0.69 - ETA: 0s - loss: 0.5219 - accuracy: 0.65 - ETA: 0s - loss: 0.5153 - accuracy: 0.65 - ETA: 0s - loss: 0.5099 - accuracy: 0.67 - ETA: 0s - loss: 0.5294 - accuracy: 0.66 - ETA: 0s - loss: 0.5237 - accuracy: 0.67 - ETA: 0s - loss: 0.5194 - accuracy: 0.67 - 0s 5ms/step - loss: 0.5151 - accuracy: 0.6753 - val_loss: 0.8836 - val_accuracy: 0.6149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.75 - ETA: 0s - loss: 0.4457 - accuracy: 0.70 - ETA: 0s - loss: 0.4332 - accuracy: 0.72 - ETA: 0s - loss: 0.4487 - accuracy: 0.72 - ETA: 0s - loss: 0.4582 - accuracy: 0.72 - ETA: 0s - loss: 0.4630 - accuracy: 0.72 - ETA: 0s - loss: 0.4631 - accuracy: 0.72 - ETA: 0s - loss: 0.4570 - accuracy: 0.73 - 0s 5ms/step - loss: 0.4605 - accuracy: 0.7413 - val_loss: 0.7653 - val_accuracy: 0.6194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.75 - ETA: 0s - loss: 0.4794 - accuracy: 0.71 - ETA: 0s - loss: 0.4460 - accuracy: 0.75 - ETA: 0s - loss: 0.4407 - accuracy: 0.76 - ETA: 0s - loss: 0.4346 - accuracy: 0.76 - ETA: 0s - loss: 0.4362 - accuracy: 0.76 - ETA: 0s - loss: 0.4324 - accuracy: 0.77 - ETA: 0s - loss: 0.4382 - accuracy: 0.77 - 0s 6ms/step - loss: 0.4410 - accuracy: 0.7719 - val_loss: 0.5933 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.93 - ETA: 0s - loss: 0.4489 - accuracy: 0.79 - ETA: 0s - loss: 0.4464 - accuracy: 0.78 - ETA: 0s - loss: 0.4529 - accuracy: 0.79 - ETA: 0s - loss: 0.4670 - accuracy: 0.79 - ETA: 0s - loss: 0.4677 - accuracy: 0.79 - ETA: 0s - loss: 0.4601 - accuracy: 0.79 - ETA: 0s - loss: 0.4509 - accuracy: 0.80 - 0s 6ms/step - loss: 0.4498 - accuracy: 0.7981 - val_loss: 0.6810 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.84 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.3993 - accuracy: 0.80 - ETA: 0s - loss: 0.4002 - accuracy: 0.80 - ETA: 0s - loss: 0.3918 - accuracy: 0.80 - ETA: 0s - loss: 0.3995 - accuracy: 0.79 - ETA: 0s - loss: 0.3933 - accuracy: 0.80 - ETA: 0s - loss: 0.3896 - accuracy: 0.80 - 0s 5ms/step - loss: 0.3929 - accuracy: 0.8025 - val_loss: 0.7336 - val_accuracy: 0.6642\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.87 - ETA: 0s - loss: 0.3361 - accuracy: 0.82 - ETA: 0s - loss: 0.3292 - accuracy: 0.83 - ETA: 0s - loss: 0.3509 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - ETA: 0s - loss: 0.4205 - accuracy: 0.81 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4155 - accuracy: 0.8163 - val_loss: 0.7224 - val_accuracy: 0.7030\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.90 - ETA: 0s - loss: 0.3633 - accuracy: 0.84 - ETA: 0s - loss: 0.3830 - accuracy: 0.83 - ETA: 0s - loss: 0.3886 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4015 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8059 - val_loss: 0.7091 - val_accuracy: 0.6090\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.75 - ETA: 0s - loss: 0.4671 - accuracy: 0.77 - ETA: 0s - loss: 0.4540 - accuracy: 0.77 - ETA: 0s - loss: 0.4413 - accuracy: 0.78 - ETA: 0s - loss: 0.4307 - accuracy: 0.79 - ETA: 0s - loss: 0.4284 - accuracy: 0.79 - ETA: 0s - loss: 0.4261 - accuracy: 0.79 - ETA: 0s - loss: 0.4219 - accuracy: 0.78 - 0s 5ms/step - loss: 0.4218 - accuracy: 0.7898 - val_loss: 0.7448 - val_accuracy: 0.6448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.81 - ETA: 0s - loss: 0.3789 - accuracy: 0.84 - ETA: 0s - loss: 0.4172 - accuracy: 0.82 - ETA: 0s - loss: 0.4448 - accuracy: 0.81 - ETA: 0s - loss: 0.4576 - accuracy: 0.81 - ETA: 0s - loss: 0.4718 - accuracy: 0.80 - ETA: 0s - loss: 0.4728 - accuracy: 0.80 - ETA: 0s - loss: 0.4603 - accuracy: 0.81 - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - 0s 6ms/step - loss: 0.4643 - accuracy: 0.8160 - val_loss: 0.6856 - val_accuracy: 0.6791\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.90 - ETA: 0s - loss: 0.4660 - accuracy: 0.81 - ETA: 0s - loss: 0.4382 - accuracy: 0.82 - ETA: 0s - loss: 0.4513 - accuracy: 0.81 - ETA: 0s - loss: 0.4442 - accuracy: 0.81 - ETA: 0s - loss: 0.4395 - accuracy: 0.81 - ETA: 0s - loss: 0.4259 - accuracy: 0.82 - ETA: 0s - loss: 0.4214 - accuracy: 0.82 - ETA: 0s - loss: 0.4131 - accuracy: 0.82 - 0s 6ms/step - loss: 0.4131 - accuracy: 0.8246 - val_loss: 0.7358 - val_accuracy: 0.7045\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.93 - ETA: 0s - loss: 0.3003 - accuracy: 0.88 - ETA: 0s - loss: 0.3258 - accuracy: 0.85 - ETA: 0s - loss: 0.3656 - accuracy: 0.82 - ETA: 0s - loss: 0.3767 - accuracy: 0.82 - ETA: 0s - loss: 0.3742 - accuracy: 0.82 - ETA: 0s - loss: 0.3850 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3840 - accuracy: 0.82 - 0s 6ms/step - loss: 0.3840 - accuracy: 0.8264 - val_loss: 0.7169 - val_accuracy: 0.6836\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3849 - accuracy: 0.75 - ETA: 0s - loss: 0.3590 - accuracy: 0.82 - ETA: 0s - loss: 0.3572 - accuracy: 0.81 - ETA: 0s - loss: 0.3560 - accuracy: 0.82 - ETA: 0s - loss: 0.3637 - accuracy: 0.81 - ETA: 0s - loss: 0.3628 - accuracy: 0.81 - ETA: 0s - loss: 0.3724 - accuracy: 0.80 - ETA: 0s - loss: 0.3753 - accuracy: 0.80 - ETA: 0s - loss: 0.3709 - accuracy: 0.80 - 0s 6ms/step - loss: 0.3722 - accuracy: 0.8066 - val_loss: 1.1984 - val_accuracy: 0.6687\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3163 - accuracy: 0.81 - ETA: 0s - loss: 0.3525 - accuracy: 0.82 - ETA: 0s - loss: 0.3621 - accuracy: 0.81 - ETA: 0s - loss: 0.3589 - accuracy: 0.82 - ETA: 0s - loss: 0.3459 - accuracy: 0.83 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3476 - accuracy: 0.84 - ETA: 0s - loss: 0.3598 - accuracy: 0.83 - ETA: 0s - loss: 0.3645 - accuracy: 0.83 - 0s 6ms/step - loss: 0.3644 - accuracy: 0.8361 - val_loss: 0.7518 - val_accuracy: 0.6776\n",
      "Epoch 22/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.3911 - accuracy: 0.81 - ETA: 0s - loss: 0.3396 - accuracy: 0.85 - ETA: 0s - loss: 0.3563 - accuracy: 0.84 - ETA: 0s - loss: 0.3683 - accuracy: 0.83 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.3656 - accuracy: 0.83 - ETA: 0s - loss: 0.3677 - accuracy: 0.83 - ETA: 0s - loss: 0.3590 - accuracy: 0.8364Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3578 - accuracy: 0.8373 - val_loss: 0.9556 - val_accuracy: 0.6910\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7688 - accuracy: 0.71 - ETA: 0s - loss: 3.1830 - accuracy: 0.60 - ETA: 0s - loss: 1.9707 - accuracy: 0.61 - ETA: 0s - loss: 1.5144 - accuracy: 0.65 - ETA: 0s - loss: 1.2906 - accuracy: 0.68 - ETA: 0s - loss: 1.1777 - accuracy: 0.68 - ETA: 0s - loss: 1.0768 - accuracy: 0.70 - ETA: 0s - loss: 1.0086 - accuracy: 0.70 - ETA: 0s - loss: 0.9599 - accuracy: 0.71 - 1s 7ms/step - loss: 0.9537 - accuracy: 0.7088 - val_loss: 0.5863 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7554 - accuracy: 0.56 - ETA: 0s - loss: 0.6219 - accuracy: 0.75 - ETA: 0s - loss: 0.6103 - accuracy: 0.74 - ETA: 0s - loss: 0.6113 - accuracy: 0.73 - ETA: 0s - loss: 0.6133 - accuracy: 0.73 - ETA: 0s - loss: 0.6082 - accuracy: 0.74 - ETA: 0s - loss: 0.6028 - accuracy: 0.75 - ETA: 0s - loss: 0.5957 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5867 - accuracy: 0.7626 - val_loss: 0.6314 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3794 - accuracy: 0.81 - ETA: 0s - loss: 0.4770 - accuracy: 0.77 - ETA: 0s - loss: 0.4826 - accuracy: 0.79 - ETA: 0s - loss: 0.5131 - accuracy: 0.78 - ETA: 0s - loss: 0.5295 - accuracy: 0.77 - ETA: 0s - loss: 0.5283 - accuracy: 0.78 - ETA: 0s - loss: 0.5278 - accuracy: 0.78 - ETA: 0s - loss: 0.5355 - accuracy: 0.78 - 0s 6ms/step - loss: 0.5336 - accuracy: 0.7861 - val_loss: 0.5706 - val_accuracy: 0.7478\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7293 - accuracy: 0.68 - ETA: 0s - loss: 0.5700 - accuracy: 0.76 - ETA: 0s - loss: 0.5098 - accuracy: 0.78 - ETA: 0s - loss: 0.5028 - accuracy: 0.79 - ETA: 0s - loss: 0.4898 - accuracy: 0.80 - ETA: 0s - loss: 0.5192 - accuracy: 0.80 - ETA: 0s - loss: 0.5391 - accuracy: 0.79 - ETA: 0s - loss: 0.5394 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5406 - accuracy: 0.7966 - val_loss: 0.6162 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.87 - ETA: 0s - loss: 0.4585 - accuracy: 0.82 - ETA: 0s - loss: 0.4874 - accuracy: 0.82 - ETA: 0s - loss: 0.5223 - accuracy: 0.80 - ETA: 0s - loss: 0.5331 - accuracy: 0.79 - ETA: 0s - loss: 0.5366 - accuracy: 0.80 - ETA: 0s - loss: 0.5344 - accuracy: 0.80 - ETA: 0s - loss: 0.5332 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5332 - accuracy: 0.8029 - val_loss: 0.6283 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.78 - ETA: 0s - loss: 0.4942 - accuracy: 0.79 - ETA: 0s - loss: 0.4991 - accuracy: 0.79 - ETA: 0s - loss: 0.4757 - accuracy: 0.81 - ETA: 0s - loss: 0.9750 - accuracy: 0.82 - ETA: 0s - loss: 0.9166 - accuracy: 0.81 - ETA: 0s - loss: 0.8594 - accuracy: 0.81 - ETA: 0s - loss: 0.8179 - accuracy: 0.80 - ETA: 0s - loss: 0.7766 - accuracy: 0.80 - ETA: 0s - loss: 0.7534 - accuracy: 0.80 - 1s 6ms/step - loss: 0.7534 - accuracy: 0.8029 - val_loss: 0.7274 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5596 - accuracy: 0.78 - ETA: 0s - loss: 0.5082 - accuracy: 0.82 - ETA: 0s - loss: 0.5419 - accuracy: 0.80 - ETA: 0s - loss: 0.5284 - accuracy: 0.81 - ETA: 0s - loss: 0.5193 - accuracy: 0.82 - ETA: 0s - loss: 0.5089 - accuracy: 0.82 - ETA: 0s - loss: 0.5182 - accuracy: 0.82 - ETA: 0s - loss: 0.5224 - accuracy: 0.81 - ETA: 0s - loss: 0.5157 - accuracy: 0.81 - 0s 6ms/step - loss: 0.5168 - accuracy: 0.8175 - val_loss: 0.7047 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4498 - accuracy: 0.84 - ETA: 0s - loss: 0.5094 - accuracy: 0.82 - ETA: 0s - loss: 0.5223 - accuracy: 0.81 - ETA: 0s - loss: 0.5314 - accuracy: 0.80 - ETA: 0s - loss: 0.5186 - accuracy: 0.81 - ETA: 0s - loss: 0.5085 - accuracy: 0.82 - ETA: 0s - loss: 0.4976 - accuracy: 0.82 - ETA: 0s - loss: 0.5008 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5017 - accuracy: 0.8257 - val_loss: 0.6509 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.87 - ETA: 0s - loss: 0.4333 - accuracy: 0.84 - ETA: 0s - loss: 0.4485 - accuracy: 0.84 - ETA: 0s - loss: 0.4761 - accuracy: 0.83 - ETA: 0s - loss: 0.4699 - accuracy: 0.83 - ETA: 0s - loss: 0.4682 - accuracy: 0.83 - ETA: 0s - loss: 0.4731 - accuracy: 0.83 - ETA: 0s - loss: 0.4720 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4785 - accuracy: 0.8317 - val_loss: 0.8064 - val_accuracy: 0.7269\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.84 - ETA: 0s - loss: 0.4660 - accuracy: 0.86 - ETA: 0s - loss: 0.4504 - accuracy: 0.86 - ETA: 0s - loss: 0.4704 - accuracy: 0.85 - ETA: 0s - loss: 0.4725 - accuracy: 0.85 - ETA: 0s - loss: 0.4660 - accuracy: 0.85 - ETA: 0s - loss: 0.4624 - accuracy: 0.85 - ETA: 0s - loss: 0.4625 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4593 - accuracy: 0.8544 - val_loss: 0.8894 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.78 - ETA: 0s - loss: 0.4556 - accuracy: 0.87 - ETA: 0s - loss: 0.5049 - accuracy: 0.88 - ETA: 0s - loss: 0.5049 - accuracy: 0.86 - ETA: 0s - loss: 0.4919 - accuracy: 0.86 - ETA: 0s - loss: 0.4958 - accuracy: 0.85 - ETA: 0s - loss: 0.4887 - accuracy: 0.85 - ETA: 0s - loss: 0.4911 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4934 - accuracy: 0.8485 - val_loss: 0.6480 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.87 - ETA: 0s - loss: 0.4368 - accuracy: 0.83 - ETA: 0s - loss: 0.4401 - accuracy: 0.83 - ETA: 0s - loss: 0.4395 - accuracy: 0.83 - ETA: 0s - loss: 0.4406 - accuracy: 0.84 - ETA: 0s - loss: 0.4282 - accuracy: 0.85 - ETA: 0s - loss: 0.4316 - accuracy: 0.85 - ETA: 0s - loss: 0.4340 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4349 - accuracy: 0.8567 - val_loss: 0.7219 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5991 - accuracy: 0.78 - ETA: 0s - loss: 0.3633 - accuracy: 0.88 - ETA: 0s - loss: 0.3667 - accuracy: 0.87 - ETA: 0s - loss: 0.3897 - accuracy: 0.87 - ETA: 0s - loss: 0.4110 - accuracy: 0.87 - ETA: 0s - loss: 0.3940 - accuracy: 0.87 - ETA: 0s - loss: 0.4073 - accuracy: 0.87 - ETA: 0s - loss: 0.4052 - accuracy: 0.87 - ETA: 0s - loss: 0.4167 - accuracy: 0.8720Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.4174 - accuracy: 0.8712 - val_loss: 0.5869 - val_accuracy: 0.7433\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.68 - ETA: 0s - loss: 1.8245 - accuracy: 0.59 - ETA: 0s - loss: 1.2263 - accuracy: 0.64 - ETA: 0s - loss: 1.0204 - accuracy: 0.66 - ETA: 0s - loss: 0.9159 - accuracy: 0.68 - ETA: 0s - loss: 0.8587 - accuracy: 0.69 - ETA: 0s - loss: 0.8231 - accuracy: 0.69 - ETA: 0s - loss: 0.7896 - accuracy: 0.70 - 1s 7ms/step - loss: 0.7693 - accuracy: 0.7047 - val_loss: 0.5838 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4544 - accuracy: 0.81 - ETA: 0s - loss: 0.5777 - accuracy: 0.77 - ETA: 0s - loss: 0.5554 - accuracy: 0.79 - ETA: 0s - loss: 0.5557 - accuracy: 0.78 - ETA: 0s - loss: 0.5558 - accuracy: 0.78 - ETA: 0s - loss: 0.5561 - accuracy: 0.78 - ETA: 0s - loss: 0.5577 - accuracy: 0.77 - ETA: 0s - loss: 0.5611 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5602 - accuracy: 0.7742 - val_loss: 0.6485 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.78 - ETA: 0s - loss: 0.5821 - accuracy: 0.77 - ETA: 0s - loss: 0.5916 - accuracy: 0.76 - ETA: 0s - loss: 0.5746 - accuracy: 0.77 - ETA: 0s - loss: 0.5687 - accuracy: 0.77 - ETA: 0s - loss: 0.5606 - accuracy: 0.78 - ETA: 0s - loss: 0.5571 - accuracy: 0.78 - ETA: 0s - loss: 0.5502 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5516 - accuracy: 0.7828 - val_loss: 0.5818 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.90 - ETA: 0s - loss: 0.4274 - accuracy: 0.83 - ETA: 0s - loss: 0.4655 - accuracy: 0.82 - ETA: 0s - loss: 0.4927 - accuracy: 0.81 - ETA: 0s - loss: 0.4894 - accuracy: 0.82 - ETA: 0s - loss: 0.4910 - accuracy: 0.82 - ETA: 0s - loss: 0.4916 - accuracy: 0.81 - ETA: 0s - loss: 0.5042 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5090 - accuracy: 0.8115 - val_loss: 0.6281 - val_accuracy: 0.6881\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.90 - ETA: 0s - loss: 0.4728 - accuracy: 0.83 - ETA: 0s - loss: 0.4998 - accuracy: 0.82 - ETA: 0s - loss: 0.5745 - accuracy: 0.80 - ETA: 0s - loss: 0.5557 - accuracy: 0.80 - ETA: 0s - loss: 0.5354 - accuracy: 0.80 - ETA: 0s - loss: 0.5433 - accuracy: 0.80 - ETA: 0s - loss: 0.5324 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5302 - accuracy: 0.8093 - val_loss: 0.5943 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.84 - ETA: 0s - loss: 0.4702 - accuracy: 0.84 - ETA: 0s - loss: 0.4983 - accuracy: 0.82 - ETA: 0s - loss: 0.4831 - accuracy: 0.83 - ETA: 0s - loss: 0.4688 - accuracy: 0.83 - ETA: 0s - loss: 0.4838 - accuracy: 0.83 - ETA: 0s - loss: 0.4966 - accuracy: 0.82 - ETA: 0s - loss: 0.4968 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5012 - accuracy: 0.8290 - val_loss: 0.9568 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.96 - ETA: 0s - loss: 0.4884 - accuracy: 0.85 - ETA: 0s - loss: 0.4990 - accuracy: 0.84 - ETA: 0s - loss: 0.4971 - accuracy: 0.84 - ETA: 0s - loss: 0.4801 - accuracy: 0.84 - ETA: 0s - loss: 0.4829 - accuracy: 0.84 - ETA: 0s - loss: 0.4795 - accuracy: 0.84 - ETA: 0s - loss: 0.4792 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4798 - accuracy: 0.8395 - val_loss: 0.6593 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.93 - ETA: 0s - loss: 0.4558 - accuracy: 0.83 - ETA: 0s - loss: 0.4401 - accuracy: 0.85 - ETA: 0s - loss: 0.4673 - accuracy: 0.83 - ETA: 0s - loss: 0.4627 - accuracy: 0.84 - ETA: 0s - loss: 0.4616 - accuracy: 0.84 - ETA: 0s - loss: 0.4612 - accuracy: 0.84 - ETA: 0s - loss: 0.5591 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5566 - accuracy: 0.8391 - val_loss: 0.7408 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.78 - ETA: 0s - loss: 0.4537 - accuracy: 0.84 - ETA: 0s - loss: 0.4794 - accuracy: 0.83 - ETA: 0s - loss: 0.4843 - accuracy: 0.83 - ETA: 0s - loss: 0.4832 - accuracy: 0.83 - ETA: 0s - loss: 0.4806 - accuracy: 0.83 - ETA: 0s - loss: 0.4744 - accuracy: 0.84 - ETA: 0s - loss: 0.4872 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4909 - accuracy: 0.8395 - val_loss: 0.5445 - val_accuracy: 0.7433\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.84 - ETA: 0s - loss: 0.5485 - accuracy: 0.82 - ETA: 0s - loss: 0.4691 - accuracy: 0.84 - ETA: 0s - loss: 0.4727 - accuracy: 0.84 - ETA: 0s - loss: 0.4831 - accuracy: 0.84 - ETA: 0s - loss: 0.4885 - accuracy: 0.84 - ETA: 0s - loss: 0.4838 - accuracy: 0.84 - ETA: 0s - loss: 0.4784 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4774 - accuracy: 0.8466 - val_loss: 0.9111 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.84 - ETA: 0s - loss: 0.4572 - accuracy: 0.84 - ETA: 0s - loss: 0.4685 - accuracy: 0.84 - ETA: 0s - loss: 0.4743 - accuracy: 0.84 - ETA: 0s - loss: 0.4707 - accuracy: 0.84 - ETA: 0s - loss: 0.4719 - accuracy: 0.84 - ETA: 0s - loss: 0.4596 - accuracy: 0.85 - ETA: 0s - loss: 0.4499 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4453 - accuracy: 0.8578 - val_loss: 1.2885 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.81 - ETA: 0s - loss: 0.3641 - accuracy: 0.91 - ETA: 0s - loss: 0.4156 - accuracy: 0.88 - ETA: 0s - loss: 0.4375 - accuracy: 0.87 - ETA: 0s - loss: 0.4269 - accuracy: 0.87 - ETA: 0s - loss: 0.4317 - accuracy: 0.86 - ETA: 0s - loss: 0.4275 - accuracy: 0.86 - ETA: 0s - loss: 0.4226 - accuracy: 0.87 - ETA: 0s - loss: 0.4270 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8694 - val_loss: 0.7572 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.87 - ETA: 0s - loss: 0.3973 - accuracy: 0.88 - ETA: 0s - loss: 0.3840 - accuracy: 0.89 - ETA: 0s - loss: 0.4008 - accuracy: 0.88 - ETA: 0s - loss: 0.4080 - accuracy: 0.88 - ETA: 0s - loss: 0.4015 - accuracy: 0.87 - ETA: 0s - loss: 0.4008 - accuracy: 0.88 - ETA: 0s - loss: 0.3997 - accuracy: 0.88 - ETA: 0s - loss: 0.4063 - accuracy: 0.87 - 0s 6ms/step - loss: 0.4062 - accuracy: 0.8794 - val_loss: 0.9134 - val_accuracy: 0.7209\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.90 - ETA: 0s - loss: 0.4182 - accuracy: 0.86 - ETA: 0s - loss: 0.3871 - accuracy: 0.88 - ETA: 0s - loss: 0.3712 - accuracy: 0.89 - ETA: 0s - loss: 0.3593 - accuracy: 0.89 - ETA: 0s - loss: 0.3759 - accuracy: 0.89 - ETA: 0s - loss: 0.3772 - accuracy: 0.89 - ETA: 0s - loss: 0.3778 - accuracy: 0.89 - ETA: 0s - loss: 0.3798 - accuracy: 0.89 - 0s 6ms/step - loss: 0.3777 - accuracy: 0.8918 - val_loss: 0.9561 - val_accuracy: 0.7418\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.87 - ETA: 0s - loss: 0.3274 - accuracy: 0.91 - ETA: 0s - loss: 0.3739 - accuracy: 0.89 - ETA: 0s - loss: 0.3835 - accuracy: 0.88 - ETA: 0s - loss: 0.3887 - accuracy: 0.88 - ETA: 0s - loss: 0.3933 - accuracy: 0.88 - ETA: 0s - loss: 0.3907 - accuracy: 0.88 - ETA: 0s - loss: 0.3937 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3917 - accuracy: 0.8850 - val_loss: 0.7172 - val_accuracy: 0.7433\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.84 - ETA: 0s - loss: 0.4097 - accuracy: 0.86 - ETA: 0s - loss: 0.4049 - accuracy: 0.87 - ETA: 0s - loss: 0.3894 - accuracy: 0.88 - ETA: 0s - loss: 0.3844 - accuracy: 0.88 - ETA: 0s - loss: 0.3778 - accuracy: 0.88 - ETA: 0s - loss: 0.3843 - accuracy: 0.88 - ETA: 0s - loss: 0.3854 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3861 - accuracy: 0.8839 - val_loss: 0.8954 - val_accuracy: 0.7343\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.93 - ETA: 0s - loss: 0.3220 - accuracy: 0.91 - ETA: 0s - loss: 0.3505 - accuracy: 0.90 - ETA: 0s - loss: 0.3608 - accuracy: 0.89 - ETA: 0s - loss: 0.3563 - accuracy: 0.89 - ETA: 0s - loss: 0.3703 - accuracy: 0.89 - ETA: 0s - loss: 0.3886 - accuracy: 0.88 - ETA: 0s - loss: 0.3912 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8839 - val_loss: 0.7837 - val_accuracy: 0.7284\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.78 - ETA: 0s - loss: 0.3471 - accuracy: 0.89 - ETA: 0s - loss: 0.3506 - accuracy: 0.89 - ETA: 0s - loss: 0.3497 - accuracy: 0.89 - ETA: 0s - loss: 0.3505 - accuracy: 0.90 - ETA: 0s - loss: 0.3566 - accuracy: 0.89 - ETA: 0s - loss: 0.3681 - accuracy: 0.89 - ETA: 0s - loss: 0.3649 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8951 - val_loss: 1.1765 - val_accuracy: 0.7343\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.93 - ETA: 0s - loss: 0.4075 - accuracy: 0.87 - ETA: 0s - loss: 0.3719 - accuracy: 0.88 - ETA: 0s - loss: 0.3751 - accuracy: 0.88 - ETA: 0s - loss: 0.3758 - accuracy: 0.88 - ETA: 0s - loss: 0.3678 - accuracy: 0.89 - ETA: 0s - loss: 0.3639 - accuracy: 0.89 - ETA: 0s - loss: 0.3628 - accuracy: 0.89 - 1s 6ms/step - loss: 0.3584 - accuracy: 0.8992 - val_loss: 1.0441 - val_accuracy: 0.7552\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.96 - ETA: 0s - loss: 0.3317 - accuracy: 0.90 - ETA: 0s - loss: 0.3149 - accuracy: 0.91 - ETA: 0s - loss: 0.3322 - accuracy: 0.91 - ETA: 0s - loss: 0.3285 - accuracy: 0.91 - ETA: 0s - loss: 0.3401 - accuracy: 0.90 - ETA: 0s - loss: 0.3502 - accuracy: 0.90 - ETA: 0s - loss: 0.3506 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3500 - accuracy: 0.9048 - val_loss: 1.1088 - val_accuracy: 0.7358\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.90 - ETA: 0s - loss: 0.2894 - accuracy: 0.92 - ETA: 0s - loss: 0.3276 - accuracy: 0.91 - ETA: 0s - loss: 0.3288 - accuracy: 0.91 - ETA: 0s - loss: 0.3733 - accuracy: 0.89 - ETA: 0s - loss: 0.3809 - accuracy: 0.89 - ETA: 0s - loss: 0.3741 - accuracy: 0.89 - ETA: 0s - loss: 0.3780 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8973 - val_loss: 1.1872 - val_accuracy: 0.7254\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.87 - ETA: 0s - loss: 0.3458 - accuracy: 0.90 - ETA: 0s - loss: 0.3401 - accuracy: 0.91 - ETA: 0s - loss: 0.3588 - accuracy: 0.90 - ETA: 0s - loss: 0.3567 - accuracy: 0.90 - ETA: 0s - loss: 0.3510 - accuracy: 0.90 - ETA: 0s - loss: 0.3609 - accuracy: 0.90 - ETA: 0s - loss: 0.3569 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3547 - accuracy: 0.9052 - val_loss: 1.3065 - val_accuracy: 0.7284\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.84 - ETA: 0s - loss: 0.3304 - accuracy: 0.92 - ETA: 0s - loss: 0.3136 - accuracy: 0.92 - ETA: 0s - loss: 0.3248 - accuracy: 0.92 - ETA: 0s - loss: 0.3512 - accuracy: 0.91 - ETA: 0s - loss: 0.3807 - accuracy: 0.90 - ETA: 0s - loss: 0.4512 - accuracy: 0.89 - ETA: 0s - loss: 0.4579 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4604 - accuracy: 0.8794 - val_loss: 0.7830 - val_accuracy: 0.7433\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.84 - ETA: 0s - loss: 0.4884 - accuracy: 0.85 - ETA: 0s - loss: 0.4555 - accuracy: 0.86 - ETA: 0s - loss: 0.4496 - accuracy: 0.86 - ETA: 0s - loss: 0.4408 - accuracy: 0.86 - ETA: 0s - loss: 0.4333 - accuracy: 0.86 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - ETA: 0s - loss: 0.4408 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4378 - accuracy: 0.8649 - val_loss: 1.0256 - val_accuracy: 0.7313\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.93 - ETA: 0s - loss: 0.3160 - accuracy: 0.90 - ETA: 0s - loss: 0.3585 - accuracy: 0.89 - ETA: 0s - loss: 0.4006 - accuracy: 0.88 - ETA: 0s - loss: 0.4148 - accuracy: 0.88 - ETA: 0s - loss: 0.4408 - accuracy: 0.87 - ETA: 0s - loss: 0.4497 - accuracy: 0.87 - ETA: 0s - loss: 0.4497 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4519 - accuracy: 0.8764 - val_loss: 0.7790 - val_accuracy: 0.7373\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4426 - accuracy: 0.87 - ETA: 0s - loss: 0.5006 - accuracy: 0.86 - ETA: 0s - loss: 0.4584 - accuracy: 0.86 - ETA: 0s - loss: 0.4696 - accuracy: 0.85 - ETA: 0s - loss: 0.4492 - accuracy: 0.86 - ETA: 0s - loss: 0.4358 - accuracy: 0.87 - ETA: 0s - loss: 0.4213 - accuracy: 0.88 - ETA: 0s - loss: 0.4153 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4151 - accuracy: 0.8828 - val_loss: 1.2088 - val_accuracy: 0.7388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.87 - ETA: 0s - loss: 0.2851 - accuracy: 0.93 - ETA: 0s - loss: 0.3206 - accuracy: 0.91 - ETA: 0s - loss: 0.3171 - accuracy: 0.92 - ETA: 0s - loss: 0.3602 - accuracy: 0.90 - ETA: 0s - loss: 0.3737 - accuracy: 0.90 - ETA: 0s - loss: 0.3855 - accuracy: 0.89 - ETA: 0s - loss: 0.3838 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3817 - accuracy: 0.8973 - val_loss: 1.3117 - val_accuracy: 0.7388\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.93 - ETA: 0s - loss: 0.4217 - accuracy: 0.89 - ETA: 0s - loss: 0.4066 - accuracy: 0.88 - ETA: 0s - loss: 0.3894 - accuracy: 0.89 - ETA: 0s - loss: 0.3803 - accuracy: 0.89 - ETA: 0s - loss: 0.3694 - accuracy: 0.89 - ETA: 0s - loss: 0.3772 - accuracy: 0.90 - ETA: 0s - loss: 0.3858 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3861 - accuracy: 0.8947 - val_loss: 0.8007 - val_accuracy: 0.7284\n",
      "Epoch 29/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.2809 - accuracy: 0.87 - ETA: 0s - loss: 0.3154 - accuracy: 0.91 - ETA: 0s - loss: 0.3290 - accuracy: 0.91 - ETA: 0s - loss: 0.3260 - accuracy: 0.91 - ETA: 0s - loss: 0.3534 - accuracy: 0.90 - ETA: 0s - loss: 0.3542 - accuracy: 0.90 - ETA: 0s - loss: 0.3496 - accuracy: 0.90 - ETA: 0s - loss: 0.3663 - accuracy: 0.9018Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3754 - accuracy: 0.8992 - val_loss: 1.2652 - val_accuracy: 0.7104\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a2f9b5271cb692e27a8922610c36d46b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.737313429514567</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5405778817182668</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 90</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8317 - accuracy: 0.59 - ETA: 0s - loss: 1.7562 - accuracy: 0.54 - ETA: 0s - loss: 1.2009 - accuracy: 0.62 - ETA: 0s - loss: 1.0251 - accuracy: 0.64 - ETA: 0s - loss: 0.9128 - accuracy: 0.65 - ETA: 0s - loss: 0.8519 - accuracy: 0.66 - 0s 5ms/step - loss: 0.8426 - accuracy: 0.6700 - val_loss: 0.5650 - val_accuracy: 0.7433\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.87 - ETA: 0s - loss: 0.5689 - accuracy: 0.77 - ETA: 0s - loss: 0.5539 - accuracy: 0.77 - ETA: 0s - loss: 0.5346 - accuracy: 0.77 - ETA: 0s - loss: 0.5328 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7764 - val_loss: 0.6303 - val_accuracy: 0.6791\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6007 - accuracy: 0.71 - ETA: 0s - loss: 0.5103 - accuracy: 0.76 - ETA: 0s - loss: 0.4805 - accuracy: 0.78 - ETA: 0s - loss: 0.4706 - accuracy: 0.80 - ETA: 0s - loss: 0.4770 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4920 - accuracy: 0.8052 - val_loss: 0.6109 - val_accuracy: 0.6881\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.84 - ETA: 0s - loss: 0.4741 - accuracy: 0.78 - ETA: 0s - loss: 0.4686 - accuracy: 0.79 - ETA: 0s - loss: 0.4901 - accuracy: 0.79 - ETA: 0s - loss: 0.4804 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4802 - accuracy: 0.8085 - val_loss: 0.6181 - val_accuracy: 0.7104\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.78 - ETA: 0s - loss: 0.4537 - accuracy: 0.82 - ETA: 0s - loss: 0.4584 - accuracy: 0.83 - ETA: 0s - loss: 0.4594 - accuracy: 0.83 - ETA: 0s - loss: 0.4601 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4627 - accuracy: 0.8272 - val_loss: 0.5812 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.87 - ETA: 0s - loss: 0.4282 - accuracy: 0.85 - ETA: 0s - loss: 0.4099 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.84 - ETA: 0s - loss: 0.4491 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4386 - accuracy: 0.8432 - val_loss: 5.7431 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.8726 - accuracy: 0.84 - ETA: 0s - loss: 0.5455 - accuracy: 0.82 - ETA: 0s - loss: 0.4808 - accuracy: 0.82 - ETA: 0s - loss: 0.4596 - accuracy: 0.82 - ETA: 0s - loss: 0.4547 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4554 - accuracy: 0.8264 - val_loss: 0.7496 - val_accuracy: 0.6836\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.75 - ETA: 0s - loss: 0.3783 - accuracy: 0.85 - ETA: 0s - loss: 0.3600 - accuracy: 0.85 - ETA: 0s - loss: 0.3565 - accuracy: 0.86 - ETA: 0s - loss: 0.3498 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8641 - val_loss: 1.2121 - val_accuracy: 0.6657\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.84 - ETA: 0s - loss: 0.4529 - accuracy: 0.85 - ETA: 0s - loss: 0.3835 - accuracy: 0.85 - ETA: 0s - loss: 0.4040 - accuracy: 0.85 - ETA: 0s - loss: 0.4008 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8559 - val_loss: 0.9708 - val_accuracy: 0.6851\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.84 - ETA: 0s - loss: 0.4580 - accuracy: 0.84 - ETA: 0s - loss: 0.4283 - accuracy: 0.85 - ETA: 0s - loss: 0.4402 - accuracy: 0.84 - ETA: 0s - loss: 0.4151 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4051 - accuracy: 0.8522 - val_loss: 0.7618 - val_accuracy: 0.6866\n",
      "Epoch 11/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.84 - ETA: 0s - loss: 0.3932 - accuracy: 0.86 - ETA: 0s - loss: 0.3969 - accuracy: 0.86 - ETA: 0s - loss: 0.4033 - accuracy: 0.86 - ETA: 0s - loss: 0.3841 - accuracy: 0.86 - ETA: 0s - loss: 0.3661 - accuracy: 0.8720Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3632 - accuracy: 0.8723 - val_loss: 1.7908 - val_accuracy: 0.7104\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8378 - accuracy: 0.50 - ETA: 0s - loss: 1.8893 - accuracy: 0.62 - ETA: 0s - loss: 1.2906 - accuracy: 0.62 - ETA: 0s - loss: 1.0657 - accuracy: 0.64 - ETA: 0s - loss: 0.9336 - accuracy: 0.66 - ETA: 0s - loss: 0.8586 - accuracy: 0.67 - 0s 5ms/step - loss: 0.8477 - accuracy: 0.6756 - val_loss: 0.5721 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.62 - ETA: 0s - loss: 0.5100 - accuracy: 0.74 - ETA: 0s - loss: 0.5289 - accuracy: 0.73 - ETA: 0s - loss: 0.4965 - accuracy: 0.75 - ETA: 0s - loss: 0.5194 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5216 - accuracy: 0.7548 - val_loss: 0.5926 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.65 - ETA: 0s - loss: 0.3966 - accuracy: 0.81 - ETA: 0s - loss: 0.4348 - accuracy: 0.80 - ETA: 0s - loss: 0.4506 - accuracy: 0.79 - ETA: 0s - loss: 0.4753 - accuracy: 0.79 - ETA: 0s - loss: 0.4789 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4814 - accuracy: 0.7917 - val_loss: 0.6578 - val_accuracy: 0.6896\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.78 - ETA: 0s - loss: 0.4368 - accuracy: 0.83 - ETA: 0s - loss: 0.4109 - accuracy: 0.82 - ETA: 0s - loss: 0.4251 - accuracy: 0.81 - ETA: 0s - loss: 0.4377 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8149 - val_loss: 0.7152 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.93 - ETA: 0s - loss: 0.3948 - accuracy: 0.85 - ETA: 0s - loss: 0.3890 - accuracy: 0.84 - ETA: 0s - loss: 0.4485 - accuracy: 0.81 - ETA: 0s - loss: 0.4466 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4448 - accuracy: 0.8137 - val_loss: 0.6241 - val_accuracy: 0.7060\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.93 - ETA: 0s - loss: 0.3641 - accuracy: 0.82 - ETA: 0s - loss: 0.3605 - accuracy: 0.82 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - ETA: 0s - loss: 0.3644 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8272 - val_loss: 0.8898 - val_accuracy: 0.6493\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.75 - ETA: 0s - loss: 0.3203 - accuracy: 0.84 - ETA: 0s - loss: 0.3635 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.85 - ETA: 0s - loss: 0.3440 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8533 - val_loss: 0.6927 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.84 - ETA: 0s - loss: 0.2357 - accuracy: 0.89 - ETA: 0s - loss: 0.2905 - accuracy: 0.87 - ETA: 0s - loss: 0.2945 - accuracy: 0.87 - ETA: 0s - loss: 0.2941 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8675 - val_loss: 1.0003 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.90 - ETA: 0s - loss: 0.2517 - accuracy: 0.90 - ETA: 0s - loss: 0.2878 - accuracy: 0.88 - ETA: 0s - loss: 0.2876 - accuracy: 0.87 - ETA: 0s - loss: 0.2882 - accuracy: 0.87 - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8723 - val_loss: 0.7939 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.87 - ETA: 0s - loss: 0.2587 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.88 - ETA: 0s - loss: 0.2566 - accuracy: 0.88 - 0s 3ms/step - loss: 0.2638 - accuracy: 0.8854 - val_loss: 0.8301 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.96 - ETA: 0s - loss: 0.2572 - accuracy: 0.90 - ETA: 0s - loss: 0.2696 - accuracy: 0.89 - ETA: 0s - loss: 0.2926 - accuracy: 0.88 - ETA: 0s - loss: 0.2966 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8750 - val_loss: 1.5088 - val_accuracy: 0.6597\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.87 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.3579 - accuracy: 0.84 - ETA: 0s - loss: 0.3590 - accuracy: 0.84 - ETA: 0s - loss: 0.3551 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3428 - accuracy: 0.8511 - val_loss: 1.1926 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.93 - ETA: 0s - loss: 0.2472 - accuracy: 0.89 - ETA: 0s - loss: 0.2701 - accuracy: 0.89 - ETA: 0s - loss: 0.2848 - accuracy: 0.88 - ETA: 0s - loss: 0.2823 - accuracy: 0.88 - 0s 3ms/step - loss: 0.2834 - accuracy: 0.8850 - val_loss: 1.8903 - val_accuracy: 0.6851\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.87 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2892 - accuracy: 0.88 - ETA: 0s - loss: 0.2791 - accuracy: 0.89 - ETA: 0s - loss: 0.2646 - accuracy: 0.89 - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8936 - val_loss: 2.1286 - val_accuracy: 0.7299\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.96 - ETA: 0s - loss: 0.2739 - accuracy: 0.94 - ETA: 0s - loss: 0.2427 - accuracy: 0.93 - ETA: 0s - loss: 0.2570 - accuracy: 0.92 - ETA: 0s - loss: 0.2422 - accuracy: 0.91 - 0s 3ms/step - loss: 0.2429 - accuracy: 0.9141 - val_loss: 1.7671 - val_accuracy: 0.7090\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.93 - ETA: 0s - loss: 0.1858 - accuracy: 0.92 - ETA: 0s - loss: 0.1716 - accuracy: 0.93 - ETA: 0s - loss: 0.5010 - accuracy: 0.87 - ETA: 0s - loss: 0.5346 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5180 - accuracy: 0.8522 - val_loss: 1.1642 - val_accuracy: 0.7030\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.84 - ETA: 0s - loss: 0.3367 - accuracy: 0.87 - ETA: 0s - loss: 0.3474 - accuracy: 0.87 - ETA: 0s - loss: 0.3451 - accuracy: 0.87 - ETA: 0s - loss: 0.3509 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8768 - val_loss: 0.9694 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.96 - ETA: 0s - loss: 0.2554 - accuracy: 0.91 - ETA: 0s - loss: 0.2877 - accuracy: 0.90 - ETA: 0s - loss: 0.3016 - accuracy: 0.90 - ETA: 0s - loss: 0.3071 - accuracy: 0.89 - ETA: 0s - loss: 0.3155 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8959 - val_loss: 0.8194 - val_accuracy: 0.7463\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.96 - ETA: 0s - loss: 0.3100 - accuracy: 0.89 - ETA: 0s - loss: 0.2708 - accuracy: 0.89 - ETA: 0s - loss: 0.2764 - accuracy: 0.90 - ETA: 0s - loss: 0.2771 - accuracy: 0.89 - ETA: 0s - loss: 0.2923 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2925 - accuracy: 0.8914 - val_loss: 0.8127 - val_accuracy: 0.7373\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.84 - ETA: 0s - loss: 0.2924 - accuracy: 0.87 - ETA: 0s - loss: 0.2891 - accuracy: 0.87 - ETA: 0s - loss: 0.2886 - accuracy: 0.88 - ETA: 0s - loss: 0.2885 - accuracy: 0.88 - ETA: 0s - loss: 0.2907 - accuracy: 0.88 - 0s 4ms/step - loss: 0.2909 - accuracy: 0.8806 - val_loss: 1.4074 - val_accuracy: 0.7045\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.96 - ETA: 0s - loss: 0.1771 - accuracy: 0.94 - ETA: 0s - loss: 0.2394 - accuracy: 0.93 - ETA: 0s - loss: 0.2537 - accuracy: 0.91 - ETA: 0s - loss: 0.2412 - accuracy: 0.92 - ETA: 0s - loss: 0.2360 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2387 - accuracy: 0.9209 - val_loss: 2.7361 - val_accuracy: 0.6851\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.96 - ETA: 0s - loss: 0.3398 - accuracy: 0.90 - ETA: 0s - loss: 0.3658 - accuracy: 0.87 - ETA: 0s - loss: 0.3447 - accuracy: 0.89 - ETA: 0s - loss: 0.3156 - accuracy: 0.89 - ETA: 0s - loss: 0.3003 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2929 - accuracy: 0.9022 - val_loss: 1.4589 - val_accuracy: 0.7328\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.96 - ETA: 0s - loss: 0.2315 - accuracy: 0.94 - ETA: 0s - loss: 0.2078 - accuracy: 0.93 - ETA: 0s - loss: 0.2228 - accuracy: 0.93 - ETA: 0s - loss: 0.2397 - accuracy: 0.92 - ETA: 0s - loss: 0.2429 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2429 - accuracy: 0.9201 - val_loss: 1.4907 - val_accuracy: 0.6731\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - ETA: 0s - loss: 0.2294 - accuracy: 0.90 - ETA: 0s - loss: 0.2462 - accuracy: 0.91 - ETA: 0s - loss: 0.2276 - accuracy: 0.92 - ETA: 0s - loss: 0.2354 - accuracy: 0.92 - ETA: 0s - loss: 0.2290 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2287 - accuracy: 0.9239 - val_loss: 1.6867 - val_accuracy: 0.7090\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 1.00 - ETA: 0s - loss: 0.1757 - accuracy: 0.94 - ETA: 0s - loss: 0.1713 - accuracy: 0.94 - ETA: 0s - loss: 0.1782 - accuracy: 0.94 - ETA: 0s - loss: 0.1640 - accuracy: 0.95 - ETA: 0s - loss: 0.1904 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1897 - accuracy: 0.9481 - val_loss: 1.9551 - val_accuracy: 0.7030\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.93 - ETA: 0s - loss: 0.3136 - accuracy: 0.95 - ETA: 0s - loss: 0.2661 - accuracy: 0.94 - ETA: 0s - loss: 0.2368 - accuracy: 0.94 - ETA: 0s - loss: 0.2227 - accuracy: 0.94 - ETA: 0s - loss: 0.2108 - accuracy: 0.94 - 0s 4ms/step - loss: 0.2141 - accuracy: 0.9474 - val_loss: 1.4036 - val_accuracy: 0.7134\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 1.00 - ETA: 0s - loss: 0.1380 - accuracy: 0.96 - ETA: 0s - loss: 0.1388 - accuracy: 0.96 - ETA: 0s - loss: 0.1458 - accuracy: 0.96 - ETA: 0s - loss: 0.1473 - accuracy: 0.96 - ETA: 0s - loss: 0.1617 - accuracy: 0.95 - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9533 - val_loss: 1.4418 - val_accuracy: 0.7075\n",
      "Epoch 28/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.0917 - accuracy: 0.96 - ETA: 0s - loss: 0.2186 - accuracy: 0.94 - ETA: 0s - loss: 0.3594 - accuracy: 0.91 - ETA: 0s - loss: 0.3621 - accuracy: 0.90 - ETA: 0s - loss: 0.3475 - accuracy: 0.90 - ETA: 0s - loss: 0.3364 - accuracy: 0.9090Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.9097 - val_loss: 2.4332 - val_accuracy: 0.7075\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.62 - ETA: 0s - loss: 1.6914 - accuracy: 0.59 - ETA: 0s - loss: 1.1804 - accuracy: 0.66 - ETA: 0s - loss: 0.9477 - accuracy: 0.70 - ETA: 0s - loss: 0.8621 - accuracy: 0.70 - ETA: 0s - loss: 0.8138 - accuracy: 0.70 - 0s 5ms/step - loss: 0.8027 - accuracy: 0.7130 - val_loss: 0.6469 - val_accuracy: 0.6731\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.75 - ETA: 0s - loss: 0.5246 - accuracy: 0.76 - ETA: 0s - loss: 0.5511 - accuracy: 0.75 - ETA: 0s - loss: 0.5395 - accuracy: 0.75 - ETA: 0s - loss: 0.5509 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5548 - accuracy: 0.7563 - val_loss: 0.6852 - val_accuracy: 0.6373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.65 - ETA: 0s - loss: 0.6734 - accuracy: 0.75 - ETA: 0s - loss: 0.5870 - accuracy: 0.77 - ETA: 0s - loss: 0.5509 - accuracy: 0.77 - ETA: 0s - loss: 0.5393 - accuracy: 0.77 - ETA: 0s - loss: 0.5303 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5278 - accuracy: 0.7801 - val_loss: 0.5796 - val_accuracy: 0.7194\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.96 - ETA: 0s - loss: 0.4188 - accuracy: 0.83 - ETA: 0s - loss: 0.4377 - accuracy: 0.82 - ETA: 0s - loss: 0.4466 - accuracy: 0.81 - ETA: 0s - loss: 0.4459 - accuracy: 0.82 - ETA: 0s - loss: 0.4529 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4528 - accuracy: 0.8152 - val_loss: 0.6078 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.84 - ETA: 0s - loss: 0.3384 - accuracy: 0.87 - ETA: 0s - loss: 0.3699 - accuracy: 0.86 - ETA: 0s - loss: 0.3894 - accuracy: 0.84 - ETA: 0s - loss: 0.3894 - accuracy: 0.84 - ETA: 0s - loss: 0.3915 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3915 - accuracy: 0.8373 - val_loss: 0.7782 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.71 - ETA: 0s - loss: 0.3316 - accuracy: 0.86 - ETA: 0s - loss: 0.3467 - accuracy: 0.84 - ETA: 0s - loss: 0.3438 - accuracy: 0.84 - ETA: 0s - loss: 0.3432 - accuracy: 0.84 - ETA: 0s - loss: 0.3395 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3398 - accuracy: 0.8447 - val_loss: 0.8216 - val_accuracy: 0.7000\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.90 - ETA: 0s - loss: 0.2848 - accuracy: 0.87 - ETA: 0s - loss: 0.2922 - accuracy: 0.87 - ETA: 0s - loss: 0.3108 - accuracy: 0.86 - ETA: 0s - loss: 0.3108 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8634 - val_loss: 0.7649 - val_accuracy: 0.7134\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.87 - ETA: 0s - loss: 0.2844 - accuracy: 0.87 - ETA: 0s - loss: 0.3218 - accuracy: 0.87 - ETA: 0s - loss: 0.3475 - accuracy: 0.85 - ETA: 0s - loss: 0.3468 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3586 - accuracy: 0.8514 - val_loss: 0.6972 - val_accuracy: 0.6746\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.84 - ETA: 0s - loss: 0.3989 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.85 - ETA: 0s - loss: 0.3763 - accuracy: 0.85 - ETA: 0s - loss: 0.3773 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8585 - val_loss: 0.8658 - val_accuracy: 0.6776\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.87 - ETA: 0s - loss: 0.2641 - accuracy: 0.87 - ETA: 0s - loss: 0.2809 - accuracy: 0.87 - ETA: 0s - loss: 0.3013 - accuracy: 0.87 - ETA: 0s - loss: 0.3086 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8660 - val_loss: 0.8379 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.84 - ETA: 0s - loss: 0.2584 - accuracy: 0.89 - ETA: 0s - loss: 0.2521 - accuracy: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.89 - ETA: 0s - loss: 0.2563 - accuracy: 0.89 - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8955 - val_loss: 1.0323 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.90 - ETA: 0s - loss: 0.2356 - accuracy: 0.90 - ETA: 0s - loss: 0.2369 - accuracy: 0.89 - ETA: 0s - loss: 0.2295 - accuracy: 0.89 - ETA: 0s - loss: 0.2282 - accuracy: 0.89 - 0s 3ms/step - loss: 0.2353 - accuracy: 0.8918 - val_loss: 0.8879 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.81 - ETA: 0s - loss: 0.3018 - accuracy: 0.88 - ETA: 0s - loss: 0.2820 - accuracy: 0.90 - ETA: 0s - loss: 0.2992 - accuracy: 0.89 - ETA: 0s - loss: 0.3430 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8723 - val_loss: 5.3331 - val_accuracy: 0.6821\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.90 - ETA: 0s - loss: 0.4390 - accuracy: 0.81 - ETA: 0s - loss: 0.4728 - accuracy: 0.82 - ETA: 0s - loss: 0.4821 - accuracy: 0.82 - ETA: 0s - loss: 0.4929 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4778 - accuracy: 0.8305 - val_loss: 0.7160 - val_accuracy: 0.6910\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.87 - ETA: 0s - loss: 0.3550 - accuracy: 0.86 - ETA: 0s - loss: 0.3613 - accuracy: 0.87 - ETA: 0s - loss: 0.3746 - accuracy: 0.87 - ETA: 0s - loss: 0.3766 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8641 - val_loss: 0.8266 - val_accuracy: 0.6821\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.84 - ETA: 0s - loss: 0.3640 - accuracy: 0.85 - ETA: 0s - loss: 0.3606 - accuracy: 0.85 - ETA: 0s - loss: 0.3432 - accuracy: 0.86 - ETA: 0s - loss: 0.3328 - accuracy: 0.87 - ETA: 0s - loss: 0.3263 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3282 - accuracy: 0.8824 - val_loss: 0.8063 - val_accuracy: 0.7149\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.96 - ETA: 0s - loss: 0.2686 - accuracy: 0.91 - ETA: 0s - loss: 0.2777 - accuracy: 0.92 - ETA: 0s - loss: 0.2612 - accuracy: 0.93 - ETA: 0s - loss: 0.2542 - accuracy: 0.93 - ETA: 0s - loss: 0.2840 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2859 - accuracy: 0.9179 - val_loss: 0.8854 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.90 - ETA: 0s - loss: 0.2444 - accuracy: 0.91 - ETA: 0s - loss: 0.2356 - accuracy: 0.91 - ETA: 0s - loss: 0.2558 - accuracy: 0.91 - ETA: 0s - loss: 0.3029 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3062 - accuracy: 0.9018 - val_loss: 1.5186 - val_accuracy: 0.6791\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.96 - ETA: 0s - loss: 0.3396 - accuracy: 0.90 - ETA: 0s - loss: 0.4375 - accuracy: 0.89 - ETA: 0s - loss: 0.4355 - accuracy: 0.88 - ETA: 0s - loss: 0.4362 - accuracy: 0.87 - ETA: 0s - loss: 0.4754 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4754 - accuracy: 0.8742 - val_loss: 5.1101 - val_accuracy: 0.6657\n",
      "Epoch 20/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6410 - accuracy: 0.87 - ETA: 0s - loss: 0.8131 - accuracy: 0.78 - ETA: 0s - loss: 0.6414 - accuracy: 0.82 - ETA: 0s - loss: 0.5948 - accuracy: 0.84 - ETA: 0s - loss: 0.5596 - accuracy: 0.84 - ETA: 0s - loss: 0.5254 - accuracy: 0.8535Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5195 - accuracy: 0.8552 - val_loss: 0.8595 - val_accuracy: 0.7149\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 8ba9f009b7b3e5e9a1b40bc27278fa8f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7427860498428345</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.16529025842380474</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 65</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.87 - ETA: 0s - loss: 1.3268 - accuracy: 0.64 - ETA: 0s - loss: 1.0660 - accuracy: 0.65 - ETA: 0s - loss: 0.9241 - accuracy: 0.67 - 0s 4ms/step - loss: 0.9092 - accuracy: 0.6723 - val_loss: 0.6250 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.68 - ETA: 0s - loss: 0.6033 - accuracy: 0.76 - ETA: 0s - loss: 0.6305 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6258 - accuracy: 0.7462 - val_loss: 0.5968 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.78 - ETA: 0s - loss: 0.5878 - accuracy: 0.77 - ETA: 0s - loss: 0.5891 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5810 - accuracy: 0.7805 - val_loss: 0.5709 - val_accuracy: 0.7463\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.90 - ETA: 0s - loss: 0.5524 - accuracy: 0.78 - ETA: 0s - loss: 0.5670 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7813 - val_loss: 0.5998 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.87 - ETA: 0s - loss: 0.5037 - accuracy: 0.82 - ETA: 0s - loss: 0.5280 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5371 - accuracy: 0.8070 - val_loss: 0.5821 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.81 - ETA: 0s - loss: 0.5043 - accuracy: 0.82 - ETA: 0s - loss: 0.5137 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5191 - accuracy: 0.8156 - val_loss: 0.5977 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.75 - ETA: 0s - loss: 0.5054 - accuracy: 0.81 - ETA: 0s - loss: 0.4945 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5483 - accuracy: 0.8227 - val_loss: 0.5757 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.81 - ETA: 0s - loss: 0.5164 - accuracy: 0.81 - ETA: 0s - loss: 0.5115 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5236 - accuracy: 0.8182 - val_loss: 0.5925 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.75 - ETA: 0s - loss: 0.4926 - accuracy: 0.83 - ETA: 0s - loss: 0.5149 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5186 - accuracy: 0.8246 - val_loss: 0.6270 - val_accuracy: 0.7239\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8179 - accuracy: 0.56 - ETA: 0s - loss: 0.5563 - accuracy: 0.81 - ETA: 0s - loss: 0.5366 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5268 - accuracy: 0.8234 - val_loss: 0.5360 - val_accuracy: 0.7582\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.93 - ETA: 0s - loss: 0.5229 - accuracy: 0.81 - ETA: 0s - loss: 0.5050 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5021 - accuracy: 0.8257 - val_loss: 0.5927 - val_accuracy: 0.7239\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.78 - ETA: 0s - loss: 0.7010 - accuracy: 0.83 - ETA: 0s - loss: 0.6213 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5995 - accuracy: 0.8275 - val_loss: 0.6129 - val_accuracy: 0.7433\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.84 - ETA: 0s - loss: 0.5312 - accuracy: 0.83 - ETA: 0s - loss: 0.5145 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8391 - val_loss: 0.6510 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4048 - accuracy: 0.90 - ETA: 0s - loss: 0.4821 - accuracy: 0.85 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5026 - accuracy: 0.8365 - val_loss: 0.6138 - val_accuracy: 0.7254\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.75 - ETA: 0s - loss: 0.5068 - accuracy: 0.82 - ETA: 0s - loss: 0.4867 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8436 - val_loss: 0.6210 - val_accuracy: 0.7418\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.81 - ETA: 0s - loss: 0.4512 - accuracy: 0.85 - ETA: 0s - loss: 0.4906 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8578 - val_loss: 0.5959 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.93 - ETA: 0s - loss: 0.4637 - accuracy: 0.85 - ETA: 0s - loss: 0.5670 - accuracy: 0.85 - 0s 2ms/step - loss: 0.6719 - accuracy: 0.8354 - val_loss: 0.6926 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.78 - ETA: 0s - loss: 0.6583 - accuracy: 0.77 - ETA: 0s - loss: 0.6055 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5809 - accuracy: 0.7962 - val_loss: 0.5767 - val_accuracy: 0.7388\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.81 - ETA: 0s - loss: 0.5560 - accuracy: 0.79 - ETA: 0s - loss: 0.5462 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5373 - accuracy: 0.8093 - val_loss: 0.5847 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.6257 - accuracy: 0.75 - ETA: 0s - loss: 0.5321 - accuracy: 0.80 - ETA: 0s - loss: 0.5157 - accuracy: 0.8246Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8253 - val_loss: 0.6233 - val_accuracy: 0.7179\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9992 - accuracy: 0.40 - ETA: 0s - loss: 1.4234 - accuracy: 0.59 - ETA: 0s - loss: 1.0729 - accuracy: 0.65 - ETA: 0s - loss: 0.9159 - accuracy: 0.65 - 0s 3ms/step - loss: 0.9108 - accuracy: 0.6581 - val_loss: 0.5993 - val_accuracy: 0.6985\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.78 - ETA: 0s - loss: 0.5849 - accuracy: 0.72 - ETA: 0s - loss: 0.5702 - accuracy: 0.73 - ETA: 0s - loss: 0.5660 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7421 - val_loss: 0.5462 - val_accuracy: 0.7493\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.84 - ETA: 0s - loss: 0.5489 - accuracy: 0.76 - ETA: 0s - loss: 0.5278 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7790 - val_loss: 0.5656 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.78 - ETA: 0s - loss: 0.4656 - accuracy: 0.82 - ETA: 0s - loss: 0.4844 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4842 - accuracy: 0.8152 - val_loss: 0.5373 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.81 - ETA: 0s - loss: 0.4462 - accuracy: 0.84 - ETA: 0s - loss: 0.4675 - accuracy: 0.83 - ETA: 0s - loss: 0.4642 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4608 - accuracy: 0.8272 - val_loss: 0.5521 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.75 - ETA: 0s - loss: 0.4565 - accuracy: 0.81 - ETA: 0s - loss: 0.4263 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8391 - val_loss: 0.6091 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.96 - ETA: 0s - loss: 0.3400 - accuracy: 0.88 - ETA: 0s - loss: 0.3945 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8414 - val_loss: 0.5549 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.87 - ETA: 0s - loss: 0.4981 - accuracy: 0.82 - ETA: 0s - loss: 0.4718 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8328 - val_loss: 0.6819 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.90 - ETA: 0s - loss: 0.4026 - accuracy: 0.86 - ETA: 0s - loss: 0.3921 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8559 - val_loss: 0.6139 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.93 - ETA: 0s - loss: 0.4341 - accuracy: 0.85 - ETA: 0s - loss: 0.4231 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8582 - val_loss: 0.6852 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.93 - ETA: 0s - loss: 0.3458 - accuracy: 0.88 - ETA: 0s - loss: 0.3576 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3664 - accuracy: 0.8798 - val_loss: 0.8332 - val_accuracy: 0.7149\n",
      "Epoch 12/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.5165 - accuracy: 0.81 - ETA: 0s - loss: 0.3687 - accuracy: 0.87 - ETA: 0s - loss: 0.3587 - accuracy: 0.8800Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8750 - val_loss: 0.7433 - val_accuracy: 0.7269\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.50 - ETA: 0s - loss: 1.7677 - accuracy: 0.63 - ETA: 0s - loss: 1.2871 - accuracy: 0.63 - ETA: 0s - loss: 1.0917 - accuracy: 0.64 - 0s 4ms/step - loss: 1.0626 - accuracy: 0.6487 - val_loss: 0.6125 - val_accuracy: 0.6627\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5912 - accuracy: 0.78 - ETA: 0s - loss: 0.5880 - accuracy: 0.73 - ETA: 0s - loss: 0.5985 - accuracy: 0.72 - 0s 2ms/step - loss: 0.5938 - accuracy: 0.7335 - val_loss: 0.5852 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.68 - ETA: 0s - loss: 0.5423 - accuracy: 0.78 - ETA: 0s - loss: 0.5576 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5637 - accuracy: 0.7716 - val_loss: 0.5801 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.90 - ETA: 0s - loss: 0.5205 - accuracy: 0.80 - ETA: 0s - loss: 0.5191 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5234 - accuracy: 0.8025 - val_loss: 0.5745 - val_accuracy: 0.7358\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.87 - ETA: 0s - loss: 0.4893 - accuracy: 0.81 - ETA: 0s - loss: 0.5179 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5242 - accuracy: 0.8044 - val_loss: 0.5909 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.84 - ETA: 0s - loss: 0.4526 - accuracy: 0.84 - ETA: 0s - loss: 0.4777 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8201 - val_loss: 0.5497 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.75 - ETA: 0s - loss: 0.4695 - accuracy: 0.82 - ETA: 0s - loss: 0.4669 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4669 - accuracy: 0.8335 - val_loss: 0.5720 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.90 - ETA: 0s - loss: 0.4894 - accuracy: 0.83 - ETA: 0s - loss: 0.4524 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4554 - accuracy: 0.8414 - val_loss: 0.6546 - val_accuracy: 0.7254\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.96 - ETA: 0s - loss: 0.3950 - accuracy: 0.87 - ETA: 0s - loss: 0.4408 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4384 - accuracy: 0.8458 - val_loss: 0.7947 - val_accuracy: 0.6403\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.84 - ETA: 0s - loss: 0.4049 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4319 - accuracy: 0.8511 - val_loss: 0.6675 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.87 - ETA: 0s - loss: 0.5059 - accuracy: 0.82 - ETA: 0s - loss: 0.5011 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8350 - val_loss: 1.1477 - val_accuracy: 0.6358\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.87 - ETA: 0s - loss: 0.4107 - accuracy: 0.87 - ETA: 0s - loss: 0.4277 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4390 - accuracy: 0.8582 - val_loss: 0.9713 - val_accuracy: 0.7015\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - ETA: 0s - loss: 0.4303 - accuracy: 0.84 - ETA: 0s - loss: 0.4214 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8585 - val_loss: 0.9318 - val_accuracy: 0.7119\n",
      "Epoch 14/50\n",
      "62/84 [=====================>........] - ETA: 0s - loss: 0.5089 - accuracy: 0.81 - ETA: 0s - loss: 0.3906 - accuracy: 0.87 - ETA: 0s - loss: 0.4103 - accuracy: 0.8654Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8585 - val_loss: 0.7188 - val_accuracy: 0.7343\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: e9e3e4b0467252e6a913876b586a1b5f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7477611899375916</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5201918568626546</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0969 - accuracy: 0.43 - ETA: 0s - loss: 5.4199 - accuracy: 0.54 - ETA: 0s - loss: 3.6047 - accuracy: 0.57 - ETA: 0s - loss: 2.7419 - accuracy: 0.59 - ETA: 0s - loss: 2.2243 - accuracy: 0.61 - ETA: 0s - loss: 1.9009 - accuracy: 0.62 - 0s 6ms/step - loss: 1.6967 - accuracy: 0.6424 - val_loss: 0.6056 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8824 - accuracy: 0.78 - ETA: 0s - loss: 0.6723 - accuracy: 0.70 - ETA: 0s - loss: 0.6595 - accuracy: 0.72 - ETA: 0s - loss: 0.6462 - accuracy: 0.73 - ETA: 0s - loss: 0.6538 - accuracy: 0.72 - ETA: 0s - loss: 0.6461 - accuracy: 0.72 - ETA: 0s - loss: 0.6423 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6423 - accuracy: 0.7279 - val_loss: 0.6008 - val_accuracy: 0.7269\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.73 - ETA: 0s - loss: 0.6602 - accuracy: 0.72 - ETA: 0s - loss: 0.6630 - accuracy: 0.73 - ETA: 0s - loss: 0.6739 - accuracy: 0.72 - ETA: 0s - loss: 0.6730 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6694 - accuracy: 0.7297 - val_loss: 0.5977 - val_accuracy: 0.7448\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.65 - ETA: 0s - loss: 0.6782 - accuracy: 0.74 - ETA: 0s - loss: 0.6599 - accuracy: 0.73 - ETA: 0s - loss: 0.6511 - accuracy: 0.73 - ETA: 0s - loss: 0.6497 - accuracy: 0.75 - ETA: 0s - loss: 0.6512 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6480 - accuracy: 0.7469 - val_loss: 0.5964 - val_accuracy: 0.7433\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.71 - ETA: 0s - loss: 0.6543 - accuracy: 0.73 - ETA: 0s - loss: 0.6547 - accuracy: 0.74 - ETA: 0s - loss: 0.6220 - accuracy: 0.75 - ETA: 0s - loss: 0.6164 - accuracy: 0.75 - ETA: 0s - loss: 0.6207 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6196 - accuracy: 0.7439 - val_loss: 0.6260 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.59 - ETA: 0s - loss: 0.6114 - accuracy: 0.76 - ETA: 0s - loss: 0.6187 - accuracy: 0.76 - ETA: 0s - loss: 0.6204 - accuracy: 0.75 - ETA: 0s - loss: 0.6273 - accuracy: 0.75 - ETA: 0s - loss: 0.6362 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6256 - accuracy: 0.7619 - val_loss: 0.6081 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.81 - ETA: 0s - loss: 0.6099 - accuracy: 0.75 - ETA: 0s - loss: 0.5991 - accuracy: 0.76 - ETA: 0s - loss: 0.6200 - accuracy: 0.75 - ETA: 0s - loss: 0.6146 - accuracy: 0.75 - ETA: 0s - loss: 0.6252 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6221 - accuracy: 0.7619 - val_loss: 0.5900 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.65 - ETA: 0s - loss: 0.6416 - accuracy: 0.77 - ETA: 0s - loss: 0.6588 - accuracy: 0.76 - ETA: 0s - loss: 0.6361 - accuracy: 0.77 - ETA: 0s - loss: 0.6459 - accuracy: 0.77 - ETA: 0s - loss: 0.6553 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6608 - accuracy: 0.7611 - val_loss: 0.6093 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8875 - accuracy: 0.68 - ETA: 0s - loss: 0.7876 - accuracy: 0.73 - ETA: 0s - loss: 0.7079 - accuracy: 0.76 - ETA: 0s - loss: 0.7284 - accuracy: 0.76 - ETA: 0s - loss: 0.7221 - accuracy: 0.75 - ETA: 0s - loss: 0.7322 - accuracy: 0.76 - 0s 4ms/step - loss: 0.7162 - accuracy: 0.7626 - val_loss: 0.5911 - val_accuracy: 0.7269\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.75 - ETA: 0s - loss: 0.7022 - accuracy: 0.76 - ETA: 0s - loss: 0.6966 - accuracy: 0.76 - ETA: 0s - loss: 0.6841 - accuracy: 0.76 - ETA: 0s - loss: 0.6878 - accuracy: 0.76 - ETA: 0s - loss: 0.6857 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6854 - accuracy: 0.7574 - val_loss: 0.6261 - val_accuracy: 0.7134\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.68 - ETA: 0s - loss: 0.6920 - accuracy: 0.74 - ETA: 0s - loss: 0.6533 - accuracy: 0.76 - ETA: 0s - loss: 0.6606 - accuracy: 0.77 - ETA: 0s - loss: 0.6764 - accuracy: 0.76 - ETA: 0s - loss: 0.6900 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6871 - accuracy: 0.7555 - val_loss: 0.6177 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.71 - ETA: 0s - loss: 0.6658 - accuracy: 0.72 - ETA: 0s - loss: 0.6844 - accuracy: 0.71 - ETA: 0s - loss: 0.6679 - accuracy: 0.72 - ETA: 0s - loss: 0.6697 - accuracy: 0.73 - ETA: 0s - loss: 0.6657 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6603 - accuracy: 0.7462 - val_loss: 0.6427 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.6332 - accuracy: 0.81 - ETA: 0s - loss: 0.6676 - accuracy: 0.75 - ETA: 0s - loss: 0.6508 - accuracy: 0.75 - ETA: 0s - loss: 0.6465 - accuracy: 0.75 - ETA: 0s - loss: 0.6524 - accuracy: 0.75 - ETA: 0s - loss: 0.6746 - accuracy: 0.7509Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6781 - accuracy: 0.7488 - val_loss: 0.6479 - val_accuracy: 0.6955\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.6431 - accuracy: 0.53 - ETA: 0s - loss: 7.3131 - accuracy: 0.63 - ETA: 0s - loss: 5.0089 - accuracy: 0.63 - ETA: 0s - loss: 3.9182 - accuracy: 0.64 - ETA: 0s - loss: 3.1103 - accuracy: 0.63 - ETA: 0s - loss: 2.6412 - accuracy: 0.62 - ETA: 0s - loss: 2.2905 - accuracy: 0.62 - 0s 6ms/step - loss: 2.2905 - accuracy: 0.6297 - val_loss: 0.6155 - val_accuracy: 0.7313\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7776 - accuracy: 0.46 - ETA: 0s - loss: 0.6962 - accuracy: 0.65 - ETA: 0s - loss: 0.7055 - accuracy: 0.67 - ETA: 0s - loss: 0.6846 - accuracy: 0.70 - ETA: 0s - loss: 0.6876 - accuracy: 0.70 - ETA: 0s - loss: 0.6916 - accuracy: 0.69 - ETA: 0s - loss: 0.6982 - accuracy: 0.69 - 0s 4ms/step - loss: 0.6992 - accuracy: 0.6928 - val_loss: 0.6158 - val_accuracy: 0.7328\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7845 - accuracy: 0.56 - ETA: 0s - loss: 0.6441 - accuracy: 0.72 - ETA: 0s - loss: 0.6512 - accuracy: 0.73 - ETA: 0s - loss: 0.6719 - accuracy: 0.71 - ETA: 0s - loss: 0.6993 - accuracy: 0.71 - ETA: 0s - loss: 0.6941 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6975 - accuracy: 0.7107 - val_loss: 0.6224 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7457 - accuracy: 0.71 - ETA: 0s - loss: 0.7300 - accuracy: 0.70 - ETA: 0s - loss: 0.7226 - accuracy: 0.71 - ETA: 0s - loss: 0.7063 - accuracy: 0.72 - ETA: 0s - loss: 0.6830 - accuracy: 0.73 - ETA: 0s - loss: 0.7046 - accuracy: 0.72 - 0s 4ms/step - loss: 0.7047 - accuracy: 0.7245 - val_loss: 0.6327 - val_accuracy: 0.6985\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8800 - accuracy: 0.71 - ETA: 0s - loss: 0.7397 - accuracy: 0.70 - ETA: 0s - loss: 0.7017 - accuracy: 0.72 - ETA: 0s - loss: 0.7067 - accuracy: 0.73 - ETA: 0s - loss: 0.7256 - accuracy: 0.73 - ETA: 0s - loss: 0.7196 - accuracy: 0.73 - 0s 4ms/step - loss: 0.7231 - accuracy: 0.7346 - val_loss: 0.6164 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.59 - ETA: 0s - loss: 0.6628 - accuracy: 0.73 - ETA: 0s - loss: 0.7086 - accuracy: 0.74 - ETA: 0s - loss: 0.7028 - accuracy: 0.73 - ETA: 0s - loss: 0.6977 - accuracy: 0.73 - ETA: 0s - loss: 0.6966 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6954 - accuracy: 0.7380 - val_loss: 0.6311 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.87 - ETA: 0s - loss: 0.6641 - accuracy: 0.76 - ETA: 0s - loss: 0.6940 - accuracy: 0.72 - ETA: 0s - loss: 0.6696 - accuracy: 0.73 - ETA: 0s - loss: 0.6590 - accuracy: 0.74 - ETA: 0s - loss: 0.6546 - accuracy: 0.74 - ETA: 0s - loss: 0.6533 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6526 - accuracy: 0.7458 - val_loss: 0.6270 - val_accuracy: 0.7134\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.87 - ETA: 0s - loss: 0.5785 - accuracy: 0.79 - ETA: 0s - loss: 0.6112 - accuracy: 0.77 - ETA: 0s - loss: 0.6029 - accuracy: 0.77 - ETA: 0s - loss: 0.6054 - accuracy: 0.76 - ETA: 0s - loss: 0.6394 - accuracy: 0.75 - ETA: 0s - loss: 0.6563 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6591 - accuracy: 0.7492 - val_loss: 0.6099 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1592 - accuracy: 0.71 - ETA: 0s - loss: 0.6973 - accuracy: 0.75 - ETA: 0s - loss: 0.6650 - accuracy: 0.76 - ETA: 0s - loss: 0.7003 - accuracy: 0.73 - ETA: 0s - loss: 0.6817 - accuracy: 0.74 - ETA: 0s - loss: 0.6901 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6901 - accuracy: 0.7536 - val_loss: 0.6185 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.81 - ETA: 0s - loss: 0.7518 - accuracy: 0.74 - ETA: 0s - loss: 0.7177 - accuracy: 0.74 - ETA: 0s - loss: 0.6721 - accuracy: 0.76 - ETA: 0s - loss: 0.6865 - accuracy: 0.75 - ETA: 0s - loss: 0.6770 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6715 - accuracy: 0.7697 - val_loss: 0.6247 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.75 - ETA: 0s - loss: 0.7716 - accuracy: 0.76 - ETA: 0s - loss: 0.7380 - accuracy: 0.75 - ETA: 0s - loss: 0.7183 - accuracy: 0.75 - ETA: 0s - loss: 0.7250 - accuracy: 0.74 - ETA: 0s - loss: 0.7233 - accuracy: 0.74 - 0s 4ms/step - loss: 0.7309 - accuracy: 0.7409 - val_loss: 0.6211 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.7262 - accuracy: 0.68 - ETA: 0s - loss: 0.7241 - accuracy: 0.75 - ETA: 0s - loss: 0.7076 - accuracy: 0.76 - ETA: 0s - loss: 0.6929 - accuracy: 0.76 - ETA: 0s - loss: 0.6906 - accuracy: 0.76 - ETA: 0s - loss: 0.6927 - accuracy: 0.7610Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7386 - accuracy: 0.7563 - val_loss: 0.6343 - val_accuracy: 0.7075\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.8175 - accuracy: 0.59 - ETA: 0s - loss: 6.4856 - accuracy: 0.59 - ETA: 0s - loss: 4.5194 - accuracy: 0.60 - ETA: 0s - loss: 3.4021 - accuracy: 0.59 - ETA: 0s - loss: 2.9273 - accuracy: 0.59 - ETA: 0s - loss: 2.5393 - accuracy: 0.60 - ETA: 0s - loss: 2.2297 - accuracy: 0.61 - 0s 6ms/step - loss: 2.1250 - accuracy: 0.6189 - val_loss: 0.6224 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7193 - accuracy: 0.62 - ETA: 0s - loss: 0.7037 - accuracy: 0.70 - ETA: 0s - loss: 0.7050 - accuracy: 0.69 - ETA: 0s - loss: 0.7012 - accuracy: 0.70 - ETA: 0s - loss: 0.6972 - accuracy: 0.71 - ETA: 0s - loss: 0.6907 - accuracy: 0.71 - ETA: 0s - loss: 0.6830 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6776 - accuracy: 0.7178 - val_loss: 0.6223 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.84 - ETA: 0s - loss: 0.5998 - accuracy: 0.76 - ETA: 0s - loss: 0.6157 - accuracy: 0.76 - ETA: 0s - loss: 0.6276 - accuracy: 0.76 - ETA: 0s - loss: 0.6349 - accuracy: 0.75 - ETA: 0s - loss: 0.6447 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6462 - accuracy: 0.7432 - val_loss: 0.6039 - val_accuracy: 0.7448\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6748 - accuracy: 0.68 - ETA: 0s - loss: 0.6980 - accuracy: 0.75 - ETA: 0s - loss: 0.6843 - accuracy: 0.72 - ETA: 0s - loss: 0.6773 - accuracy: 0.72 - ETA: 0s - loss: 0.6775 - accuracy: 0.73 - ETA: 0s - loss: 0.6790 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6699 - accuracy: 0.7383 - val_loss: 0.6143 - val_accuracy: 0.7015\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.81 - ETA: 0s - loss: 0.6343 - accuracy: 0.75 - ETA: 0s - loss: 0.6250 - accuracy: 0.75 - ETA: 0s - loss: 0.6218 - accuracy: 0.75 - ETA: 0s - loss: 0.6202 - accuracy: 0.75 - ETA: 0s - loss: 0.6310 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6286 - accuracy: 0.7577 - val_loss: 0.6175 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.59 - ETA: 0s - loss: 0.6251 - accuracy: 0.75 - ETA: 0s - loss: 0.6029 - accuracy: 0.77 - ETA: 0s - loss: 0.6035 - accuracy: 0.76 - ETA: 0s - loss: 0.6330 - accuracy: 0.76 - ETA: 0s - loss: 0.6390 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6351 - accuracy: 0.7592 - val_loss: 0.6041 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.87 - ETA: 0s - loss: 0.5821 - accuracy: 0.79 - ETA: 0s - loss: 0.6072 - accuracy: 0.79 - ETA: 0s - loss: 0.6118 - accuracy: 0.78 - ETA: 0s - loss: 0.6116 - accuracy: 0.77 - ETA: 0s - loss: 0.6075 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6055 - accuracy: 0.7742 - val_loss: 0.6255 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1042 - accuracy: 0.68 - ETA: 0s - loss: 0.6546 - accuracy: 0.75 - ETA: 0s - loss: 0.6305 - accuracy: 0.76 - ETA: 0s - loss: 0.6126 - accuracy: 0.77 - ETA: 0s - loss: 0.6148 - accuracy: 0.77 - ETA: 0s - loss: 0.6188 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6135 - accuracy: 0.7734 - val_loss: 0.5959 - val_accuracy: 0.7388\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.75 - ETA: 0s - loss: 0.6075 - accuracy: 0.77 - ETA: 0s - loss: 0.6046 - accuracy: 0.79 - ETA: 0s - loss: 0.6425 - accuracy: 0.78 - ETA: 0s - loss: 0.6422 - accuracy: 0.77 - ETA: 0s - loss: 0.6350 - accuracy: 0.77 - ETA: 0s - loss: 0.6395 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6412 - accuracy: 0.7678 - val_loss: 0.6253 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.75 - ETA: 0s - loss: 0.6411 - accuracy: 0.76 - ETA: 0s - loss: 0.6681 - accuracy: 0.76 - ETA: 0s - loss: 0.6830 - accuracy: 0.75 - ETA: 0s - loss: 0.6946 - accuracy: 0.75 - ETA: 0s - loss: 0.6907 - accuracy: 0.75 - ETA: 0s - loss: 0.6890 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6826 - accuracy: 0.7589 - val_loss: 0.6273 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5926 - accuracy: 0.84 - ETA: 0s - loss: 0.8316 - accuracy: 0.77 - ETA: 0s - loss: 0.7760 - accuracy: 0.76 - ETA: 0s - loss: 0.7628 - accuracy: 0.75 - ETA: 0s - loss: 0.7733 - accuracy: 0.74 - ETA: 0s - loss: 0.7564 - accuracy: 0.74 - ETA: 0s - loss: 0.7602 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7566 - accuracy: 0.7499 - val_loss: 0.6282 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.81 - ETA: 0s - loss: 0.7404 - accuracy: 0.74 - ETA: 0s - loss: 0.7260 - accuracy: 0.73 - ETA: 0s - loss: 0.7439 - accuracy: 0.72 - ETA: 0s - loss: 0.7175 - accuracy: 0.73 - ETA: 0s - loss: 0.7143 - accuracy: 0.73 - ETA: 0s - loss: 0.7061 - accuracy: 0.73 - ETA: 0s - loss: 0.7009 - accuracy: 0.74 - 0s 6ms/step - loss: 0.7000 - accuracy: 0.7409 - val_loss: 0.6223 - val_accuracy: 0.7164\n",
      "Epoch 13/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.7605 - accuracy: 0.65 - ETA: 0s - loss: 0.7630 - accuracy: 0.71 - ETA: 0s - loss: 0.7398 - accuracy: 0.72 - ETA: 0s - loss: 0.6999 - accuracy: 0.74 - ETA: 0s - loss: 0.7213 - accuracy: 0.73 - ETA: 0s - loss: 0.7788 - accuracy: 0.73 - ETA: 0s - loss: 0.7532 - accuracy: 0.73 - ETA: 0s - loss: 0.7687 - accuracy: 0.73 - ETA: 0s - loss: 0.7846 - accuracy: 0.7355Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.7776 - accuracy: 0.7357 - val_loss: 0.6448 - val_accuracy: 0.7030\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 233bb5c5140dda2ccdbf4c5309fe220b</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7407960295677185</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9414138487449869</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7333 - accuracy: 0.65 - ETA: 0s - loss: 4.9618 - accuracy: 0.63 - ETA: 0s - loss: 3.0088 - accuracy: 0.61 - ETA: 0s - loss: 2.1869 - accuracy: 0.61 - ETA: 0s - loss: 1.7377 - accuracy: 0.61 - ETA: 0s - loss: 1.4898 - accuracy: 0.62 - ETA: 0s - loss: 1.3460 - accuracy: 0.62 - 1s 7ms/step - loss: 1.2839 - accuracy: 0.6282 - val_loss: 0.6156 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7881 - accuracy: 0.50 - ETA: 0s - loss: 0.7453 - accuracy: 0.61 - ETA: 0s - loss: 0.7243 - accuracy: 0.60 - ETA: 0s - loss: 0.6905 - accuracy: 0.63 - ETA: 0s - loss: 0.6843 - accuracy: 0.65 - ETA: 0s - loss: 0.6826 - accuracy: 0.66 - 0s 4ms/step - loss: 0.6862 - accuracy: 0.6745 - val_loss: 0.6222 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7595 - accuracy: 0.62 - ETA: 0s - loss: 0.6871 - accuracy: 0.70 - ETA: 0s - loss: 0.6587 - accuracy: 0.69 - ETA: 0s - loss: 0.6769 - accuracy: 0.69 - ETA: 0s - loss: 0.6640 - accuracy: 0.70 - ETA: 0s - loss: 0.6569 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6595 - accuracy: 0.7238 - val_loss: 0.5987 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.71 - ETA: 0s - loss: 0.6442 - accuracy: 0.73 - ETA: 0s - loss: 0.6706 - accuracy: 0.73 - ETA: 0s - loss: 0.6722 - accuracy: 0.73 - ETA: 0s - loss: 0.6790 - accuracy: 0.72 - ETA: 0s - loss: 0.6705 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6651 - accuracy: 0.7335 - val_loss: 0.6000 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.71 - ETA: 0s - loss: 0.6557 - accuracy: 0.73 - ETA: 0s - loss: 0.6401 - accuracy: 0.75 - ETA: 0s - loss: 0.6382 - accuracy: 0.75 - ETA: 0s - loss: 0.6384 - accuracy: 0.75 - ETA: 0s - loss: 0.6708 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6583 - accuracy: 0.7589 - val_loss: 0.5993 - val_accuracy: 0.7493\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.68 - ETA: 0s - loss: 0.6526 - accuracy: 0.74 - ETA: 0s - loss: 0.6742 - accuracy: 0.74 - ETA: 0s - loss: 0.6641 - accuracy: 0.75 - ETA: 0s - loss: 0.6619 - accuracy: 0.75 - ETA: 0s - loss: 0.6538 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6788 - accuracy: 0.7518 - val_loss: 0.5982 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.87 - ETA: 0s - loss: 0.6668 - accuracy: 0.75 - ETA: 0s - loss: 0.6577 - accuracy: 0.73 - ETA: 0s - loss: 0.6579 - accuracy: 0.71 - ETA: 0s - loss: 0.6566 - accuracy: 0.70 - ETA: 0s - loss: 0.6600 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6565 - accuracy: 0.6902 - val_loss: 0.6855 - val_accuracy: 0.7507\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.65 - ETA: 0s - loss: 0.6979 - accuracy: 0.68 - ETA: 0s - loss: 0.6752 - accuracy: 0.70 - ETA: 0s - loss: 0.6688 - accuracy: 0.69 - ETA: 0s - loss: 0.6679 - accuracy: 0.68 - ETA: 0s - loss: 0.6642 - accuracy: 0.67 - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6637 - val_loss: 0.6081 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.59 - ETA: 0s - loss: 0.7411 - accuracy: 0.64 - ETA: 0s - loss: 0.8601 - accuracy: 0.62 - ETA: 0s - loss: 0.8923 - accuracy: 0.65 - ETA: 0s - loss: 0.8351 - accuracy: 0.65 - ETA: 0s - loss: 0.8034 - accuracy: 0.65 - 0s 4ms/step - loss: 0.7914 - accuracy: 0.6573 - val_loss: 0.6316 - val_accuracy: 0.7030\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.71 - ETA: 0s - loss: 0.6984 - accuracy: 0.61 - ETA: 0s - loss: 0.7082 - accuracy: 0.56 - ETA: 0s - loss: 0.7067 - accuracy: 0.52 - ETA: 0s - loss: 0.6950 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5890 - val_loss: 0.6472 - val_accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7694 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.63 - ETA: 0s - loss: 0.6687 - accuracy: 0.63 - ETA: 0s - loss: 0.6841 - accuracy: 0.63 - ETA: 0s - loss: 0.6876 - accuracy: 0.63 - ETA: 0s - loss: 0.6890 - accuracy: 0.63 - 0s 4ms/step - loss: 0.6906 - accuracy: 0.6305 - val_loss: 0.6701 - val_accuracy: 0.6970\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.65 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6971 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.55 - ETA: 0s - loss: 0.6923 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5715 - val_loss: 0.6693 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7778 - accuracy: 0.62 - ETA: 0s - loss: 0.6980 - accuracy: 0.48 - ETA: 0s - loss: 0.6842 - accuracy: 0.47 - ETA: 0s - loss: 0.6850 - accuracy: 0.50 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6889 - accuracy: 0.58 - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5797 - val_loss: 0.6704 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.68 - ETA: 0s - loss: 0.6871 - accuracy: 0.49 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - ETA: 0s - loss: 0.6861 - accuracy: 0.59 - ETA: 0s - loss: 0.6875 - accuracy: 0.60 - ETA: 0s - loss: 0.6900 - accuracy: 0.61 - 0s 4ms/step - loss: 0.6912 - accuracy: 0.6103 - val_loss: 0.6743 - val_accuracy: 0.7000\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.46 - ETA: 0s - loss: 0.6892 - accuracy: 0.41 - ETA: 0s - loss: 0.6944 - accuracy: 0.47 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6942 - accuracy: 0.5330 - val_loss: 0.6789 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.31 - ETA: 0s - loss: 0.7052 - accuracy: 0.37 - ETA: 0s - loss: 0.6993 - accuracy: 0.36 - ETA: 0s - loss: 0.7070 - accuracy: 0.36 - ETA: 0s - loss: 0.6967 - accuracy: 0.39 - ETA: 0s - loss: 0.6984 - accuracy: 0.43 - ETA: 0s - loss: 0.6934 - accuracy: 0.46 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.4703 - val_loss: 0.6706 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7579 - accuracy: 0.59 - ETA: 0s - loss: 0.6934 - accuracy: 0.65 - ETA: 0s - loss: 0.6847 - accuracy: 0.67 - ETA: 0s - loss: 0.6869 - accuracy: 0.68 - ETA: 0s - loss: 0.6887 - accuracy: 0.68 - ETA: 0s - loss: 0.6911 - accuracy: 0.63 - ETA: 0s - loss: 0.6927 - accuracy: 0.5938Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5868 - val_loss: 0.7003 - val_accuracy: 0.3045\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.53 - ETA: 0s - loss: 2.6349 - accuracy: 0.59 - ETA: 0s - loss: 1.8710 - accuracy: 0.59 - ETA: 0s - loss: 1.5108 - accuracy: 0.62 - ETA: 0s - loss: 1.3749 - accuracy: 0.61 - ETA: 0s - loss: 1.2590 - accuracy: 0.62 - ETA: 0s - loss: 1.1725 - accuracy: 0.62 - 0s 6ms/step - loss: 1.1701 - accuracy: 0.6275 - val_loss: 0.5776 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.62 - ETA: 0s - loss: 0.6941 - accuracy: 0.68 - ETA: 0s - loss: 0.6736 - accuracy: 0.68 - ETA: 0s - loss: 0.6640 - accuracy: 0.68 - ETA: 0s - loss: 0.6755 - accuracy: 0.68 - ETA: 0s - loss: 0.6801 - accuracy: 0.67 - ETA: 0s - loss: 0.6808 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6808 - accuracy: 0.6820 - val_loss: 0.5754 - val_accuracy: 0.7567\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6248 - accuracy: 0.56 - ETA: 0s - loss: 0.6419 - accuracy: 0.68 - ETA: 0s - loss: 0.6352 - accuracy: 0.65 - ETA: 0s - loss: 0.6418 - accuracy: 0.67 - ETA: 0s - loss: 0.6569 - accuracy: 0.67 - ETA: 0s - loss: 0.6640 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6702 - accuracy: 0.6902 - val_loss: 0.6357 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.78 - ETA: 0s - loss: 0.6163 - accuracy: 0.77 - ETA: 0s - loss: 0.6300 - accuracy: 0.76 - ETA: 0s - loss: 0.6311 - accuracy: 0.76 - ETA: 0s - loss: 0.6481 - accuracy: 0.75 - ETA: 0s - loss: 0.6499 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6516 - accuracy: 0.7633 - val_loss: 0.6268 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.81 - ETA: 0s - loss: 0.6245 - accuracy: 0.76 - ETA: 0s - loss: 0.6545 - accuracy: 0.74 - ETA: 0s - loss: 0.6594 - accuracy: 0.74 - ETA: 0s - loss: 0.6736 - accuracy: 0.73 - ETA: 0s - loss: 0.6719 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6686 - accuracy: 0.7451 - val_loss: 0.6183 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6791 - accuracy: 0.68 - ETA: 0s - loss: 0.6834 - accuracy: 0.71 - ETA: 0s - loss: 0.6541 - accuracy: 0.74 - ETA: 0s - loss: 0.6576 - accuracy: 0.74 - ETA: 0s - loss: 0.6603 - accuracy: 0.74 - ETA: 0s - loss: 0.6596 - accuracy: 0.73 - ETA: 0s - loss: 0.6620 - accuracy: 0.72 - 0s 5ms/step - loss: 0.6646 - accuracy: 0.7219 - val_loss: 0.6287 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.75 - ETA: 0s - loss: 0.6595 - accuracy: 0.66 - ETA: 0s - loss: 0.6597 - accuracy: 0.65 - ETA: 0s - loss: 0.6490 - accuracy: 0.67 - ETA: 0s - loss: 0.6530 - accuracy: 0.67 - ETA: 0s - loss: 0.6540 - accuracy: 0.67 - ETA: 0s - loss: 0.6544 - accuracy: 0.67 - ETA: 0s - loss: 0.6637 - accuracy: 0.65 - ETA: 0s - loss: 0.6630 - accuracy: 0.64 - 0s 6ms/step - loss: 0.6628 - accuracy: 0.6428 - val_loss: 0.6102 - val_accuracy: 0.7463\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.69 - ETA: 0s - loss: 0.6380 - accuracy: 0.67 - ETA: 0s - loss: 0.6305 - accuracy: 0.68 - ETA: 0s - loss: 0.6312 - accuracy: 0.69 - ETA: 0s - loss: 0.6446 - accuracy: 0.69 - ETA: 0s - loss: 0.6475 - accuracy: 0.69 - ETA: 0s - loss: 0.6399 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6527 - accuracy: 0.7074 - val_loss: 0.6328 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7183 - accuracy: 0.56 - ETA: 0s - loss: 0.6427 - accuracy: 0.73 - ETA: 0s - loss: 0.6580 - accuracy: 0.72 - ETA: 0s - loss: 0.6720 - accuracy: 0.70 - ETA: 0s - loss: 0.6801 - accuracy: 0.70 - ETA: 0s - loss: 0.6958 - accuracy: 0.68 - ETA: 0s - loss: 0.6938 - accuracy: 0.66 - ETA: 0s - loss: 0.6880 - accuracy: 0.64 - ETA: 0s - loss: 0.6808 - accuracy: 0.65 - ETA: 0s - loss: 0.6828 - accuracy: 0.65 - 1s 6ms/step - loss: 0.6851 - accuracy: 0.6562 - val_loss: 0.6408 - val_accuracy: 0.7179\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.84 - ETA: 0s - loss: 0.6618 - accuracy: 0.69 - ETA: 0s - loss: 0.6855 - accuracy: 0.69 - ETA: 0s - loss: 0.6859 - accuracy: 0.69 - ETA: 0s - loss: 0.6809 - accuracy: 0.70 - ETA: 0s - loss: 0.6782 - accuracy: 0.69 - ETA: 0s - loss: 0.6827 - accuracy: 0.69 - ETA: 0s - loss: 0.6956 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6951 - accuracy: 0.7003 - val_loss: 0.7207 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.68 - ETA: 0s - loss: 0.7137 - accuracy: 0.72 - ETA: 0s - loss: 0.7065 - accuracy: 0.69 - ETA: 0s - loss: 0.7658 - accuracy: 0.69 - ETA: 0s - loss: 0.7593 - accuracy: 0.67 - ETA: 0s - loss: 0.8615 - accuracy: 0.63 - ETA: 0s - loss: 0.8404 - accuracy: 0.60 - 0s 5ms/step - loss: 0.8233 - accuracy: 0.5831 - val_loss: 0.6636 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.6411 - accuracy: 0.56 - ETA: 0s - loss: 0.7018 - accuracy: 0.57 - ETA: 0s - loss: 0.6977 - accuracy: 0.55 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.57 - ETA: 0s - loss: 0.7019 - accuracy: 0.57 - ETA: 0s - loss: 0.7340 - accuracy: 0.53 - ETA: 0s - loss: 0.7243 - accuracy: 0.5075Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7238 - accuracy: 0.5088 - val_loss: 0.6599 - val_accuracy: 0.6955\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8486 - accuracy: 0.53 - ETA: 0s - loss: 2.3690 - accuracy: 0.53 - ETA: 0s - loss: 1.5543 - accuracy: 0.61 - ETA: 0s - loss: 1.3215 - accuracy: 0.64 - ETA: 0s - loss: 1.1573 - accuracy: 0.65 - ETA: 0s - loss: 1.0533 - accuracy: 0.66 - 0s 5ms/step - loss: 1.0060 - accuracy: 0.6734 - val_loss: 0.6090 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.84 - ETA: 0s - loss: 0.6059 - accuracy: 0.78 - ETA: 0s - loss: 0.6153 - accuracy: 0.75 - ETA: 0s - loss: 0.6122 - accuracy: 0.75 - ETA: 0s - loss: 0.6325 - accuracy: 0.75 - ETA: 0s - loss: 0.6404 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6426 - accuracy: 0.7499 - val_loss: 0.6253 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.75 - ETA: 0s - loss: 0.6728 - accuracy: 0.73 - ETA: 0s - loss: 0.6707 - accuracy: 0.72 - ETA: 0s - loss: 0.6718 - accuracy: 0.72 - ETA: 0s - loss: 0.6643 - accuracy: 0.74 - ETA: 0s - loss: 0.6612 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6576 - accuracy: 0.7473 - val_loss: 0.6146 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.78 - ETA: 0s - loss: 0.6218 - accuracy: 0.77 - ETA: 0s - loss: 0.6428 - accuracy: 0.76 - ETA: 0s - loss: 0.6417 - accuracy: 0.75 - ETA: 0s - loss: 0.6416 - accuracy: 0.74 - ETA: 0s - loss: 0.6463 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6576 - accuracy: 0.7525 - val_loss: 0.6321 - val_accuracy: 0.7373\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.1804 - accuracy: 0.81 - ETA: 0s - loss: 0.8179 - accuracy: 0.71 - ETA: 0s - loss: 0.7199 - accuracy: 0.74 - ETA: 0s - loss: 0.7138 - accuracy: 0.74 - ETA: 0s - loss: 0.7013 - accuracy: 0.73 - ETA: 0s - loss: 0.6886 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6880 - accuracy: 0.7380 - val_loss: 0.6174 - val_accuracy: 0.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7673 - accuracy: 0.62 - ETA: 0s - loss: 0.6950 - accuracy: 0.69 - ETA: 0s - loss: 0.6954 - accuracy: 0.66 - ETA: 0s - loss: 0.6960 - accuracy: 0.64 - ETA: 0s - loss: 0.7010 - accuracy: 0.63 - ETA: 0s - loss: 0.7012 - accuracy: 0.63 - ETA: 0s - loss: 0.6974 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6961 - accuracy: 0.6278 - val_loss: 0.6577 - val_accuracy: 0.6985\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.62 - ETA: 0s - loss: 0.6738 - accuracy: 0.65 - ETA: 0s - loss: 0.6713 - accuracy: 0.66 - ETA: 0s - loss: 0.6705 - accuracy: 0.66 - ETA: 0s - loss: 0.6662 - accuracy: 0.66 - ETA: 0s - loss: 0.6735 - accuracy: 0.66 - 0s 4ms/step - loss: 0.6732 - accuracy: 0.6652 - val_loss: 0.6374 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.69 - ETA: 0s - loss: 0.6749 - accuracy: 0.65 - ETA: 0s - loss: 0.6878 - accuracy: 0.65 - ETA: 0s - loss: 0.6771 - accuracy: 0.66 - ETA: 0s - loss: 0.6665 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6607 - accuracy: 0.6943 - val_loss: 0.6259 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.71 - ETA: 0s - loss: 0.6692 - accuracy: 0.71 - ETA: 0s - loss: 0.6620 - accuracy: 0.72 - ETA: 0s - loss: 0.6582 - accuracy: 0.72 - ETA: 0s - loss: 0.6569 - accuracy: 0.72 - ETA: 0s - loss: 0.6760 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6708 - accuracy: 0.7342 - val_loss: 0.6236 - val_accuracy: 0.7313\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2973 - accuracy: 0.56 - ETA: 0s - loss: 0.7541 - accuracy: 0.69 - ETA: 0s - loss: 0.7128 - accuracy: 0.70 - ETA: 0s - loss: 0.6875 - accuracy: 0.70 - ETA: 0s - loss: 0.6771 - accuracy: 0.71 - ETA: 0s - loss: 0.6860 - accuracy: 0.72 - ETA: 0s - loss: 0.6806 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6752 - accuracy: 0.7234 - val_loss: 0.7175 - val_accuracy: 0.7254\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.75 - ETA: 0s - loss: 0.6488 - accuracy: 0.71 - ETA: 0s - loss: 0.7030 - accuracy: 0.70 - ETA: 0s - loss: 0.7023 - accuracy: 0.69 - ETA: 0s - loss: 0.6970 - accuracy: 0.69 - ETA: 0s - loss: 0.6962 - accuracy: 0.68 - ETA: 0s - loss: 0.6922 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6918 - accuracy: 0.6883 - val_loss: 0.6733 - val_accuracy: 0.6985\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.75 - ETA: 0s - loss: 0.6794 - accuracy: 0.68 - ETA: 0s - loss: 0.7005 - accuracy: 0.66 - ETA: 0s - loss: 0.6981 - accuracy: 0.55 - ETA: 0s - loss: 0.6912 - accuracy: 0.50 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5528 - val_loss: 0.6804 - val_accuracy: 0.6985\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.65 - ETA: 0s - loss: 0.6630 - accuracy: 0.70 - ETA: 0s - loss: 0.6726 - accuracy: 0.70 - ETA: 0s - loss: 0.6794 - accuracy: 0.69 - ETA: 0s - loss: 0.6797 - accuracy: 0.69 - ETA: 0s - loss: 0.6872 - accuracy: 0.66 - 0s 4ms/step - loss: 0.6900 - accuracy: 0.6305 - val_loss: 0.6991 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.6655 - accuracy: 0.25 - ETA: 0s - loss: 0.6879 - accuracy: 0.29 - ETA: 0s - loss: 0.6855 - accuracy: 0.32 - ETA: 0s - loss: 0.6838 - accuracy: 0.41 - ETA: 0s - loss: 0.6823 - accuracy: 0.47 - ETA: 0s - loss: 0.6849 - accuracy: 0.5152Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5271 - val_loss: 0.6890 - val_accuracy: 0.6985\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a269e51be7ace45cc5ba7953584c53a4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7482587099075317</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9326281367254985</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 310</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.71 - ETA: 0s - loss: 4.3077 - accuracy: 0.59 - ETA: 0s - loss: 2.3765 - accuracy: 0.62 - ETA: 0s - loss: 1.7576 - accuracy: 0.65 - ETA: 0s - loss: 1.4846 - accuracy: 0.67 - ETA: 0s - loss: 1.3259 - accuracy: 0.68 - ETA: 0s - loss: 1.2021 - accuracy: 0.69 - ETA: 0s - loss: 1.1182 - accuracy: 0.70 - 1s 7ms/step - loss: 1.1182 - accuracy: 0.7059 - val_loss: 0.5724 - val_accuracy: 0.7299\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.84 - ETA: 0s - loss: 0.6391 - accuracy: 0.75 - ETA: 0s - loss: 0.6115 - accuracy: 0.76 - ETA: 0s - loss: 0.5919 - accuracy: 0.76 - ETA: 0s - loss: 0.5837 - accuracy: 0.76 - ETA: 0s - loss: 0.5781 - accuracy: 0.76 - ETA: 0s - loss: 0.5728 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5703 - accuracy: 0.7727 - val_loss: 0.7969 - val_accuracy: 0.6776\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4820 - accuracy: 0.84 - ETA: 0s - loss: 0.5301 - accuracy: 0.75 - ETA: 0s - loss: 0.5594 - accuracy: 0.74 - ETA: 0s - loss: 0.5612 - accuracy: 0.76 - ETA: 0s - loss: 0.5620 - accuracy: 0.77 - ETA: 0s - loss: 0.5566 - accuracy: 0.78 - ETA: 0s - loss: 0.5583 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5566 - accuracy: 0.7783 - val_loss: 0.6687 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.71 - ETA: 0s - loss: 0.5415 - accuracy: 0.77 - ETA: 0s - loss: 0.5533 - accuracy: 0.77 - ETA: 0s - loss: 0.5181 - accuracy: 0.79 - ETA: 0s - loss: 0.5625 - accuracy: 0.79 - ETA: 0s - loss: 0.5603 - accuracy: 0.79 - ETA: 0s - loss: 0.5641 - accuracy: 0.78 - ETA: 0s - loss: 0.5679 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5679 - accuracy: 0.7880 - val_loss: 0.7002 - val_accuracy: 0.6925\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4604 - accuracy: 0.78 - ETA: 0s - loss: 0.6186 - accuracy: 0.73 - ETA: 0s - loss: 0.5863 - accuracy: 0.77 - ETA: 0s - loss: 0.6294 - accuracy: 0.77 - ETA: 0s - loss: 0.6034 - accuracy: 0.78 - ETA: 0s - loss: 0.5824 - accuracy: 0.79 - ETA: 0s - loss: 0.5732 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5778 - accuracy: 0.7954 - val_loss: 0.6410 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.81 - ETA: 0s - loss: 0.5879 - accuracy: 0.77 - ETA: 0s - loss: 0.5557 - accuracy: 0.79 - ETA: 0s - loss: 0.5788 - accuracy: 0.78 - ETA: 0s - loss: 0.5491 - accuracy: 0.80 - ETA: 0s - loss: 0.5451 - accuracy: 0.80 - ETA: 0s - loss: 0.5431 - accuracy: 0.80 - ETA: 0s - loss: 0.5428 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5446 - accuracy: 0.8066 - val_loss: 0.6041 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.75 - ETA: 0s - loss: 0.5245 - accuracy: 0.80 - ETA: 0s - loss: 0.5166 - accuracy: 0.81 - ETA: 0s - loss: 0.5018 - accuracy: 0.82 - ETA: 0s - loss: 0.4994 - accuracy: 0.82 - ETA: 0s - loss: 0.5110 - accuracy: 0.82 - ETA: 0s - loss: 0.5098 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5060 - accuracy: 0.8264 - val_loss: 0.7405 - val_accuracy: 0.7493\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.84 - ETA: 0s - loss: 0.4732 - accuracy: 0.84 - ETA: 0s - loss: 0.4888 - accuracy: 0.83 - ETA: 0s - loss: 0.4936 - accuracy: 0.83 - ETA: 0s - loss: 0.5135 - accuracy: 0.83 - ETA: 0s - loss: 0.5066 - accuracy: 0.83 - ETA: 0s - loss: 0.5070 - accuracy: 0.83 - ETA: 0s - loss: 0.5072 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5072 - accuracy: 0.8328 - val_loss: 0.7821 - val_accuracy: 0.7328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5061 - accuracy: 0.81 - ETA: 0s - loss: 0.5116 - accuracy: 0.82 - ETA: 0s - loss: 0.5301 - accuracy: 0.82 - ETA: 0s - loss: 0.5052 - accuracy: 0.83 - ETA: 0s - loss: 0.5058 - accuracy: 0.83 - ETA: 0s - loss: 0.4978 - accuracy: 0.83 - ETA: 0s - loss: 0.5033 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4981 - accuracy: 0.8384 - val_loss: 0.7395 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.96 - ETA: 0s - loss: 0.3814 - accuracy: 0.89 - ETA: 0s - loss: 0.4309 - accuracy: 0.87 - ETA: 0s - loss: 0.4520 - accuracy: 0.86 - ETA: 0s - loss: 0.4624 - accuracy: 0.85 - ETA: 0s - loss: 0.4725 - accuracy: 0.84 - ETA: 0s - loss: 0.4749 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4694 - accuracy: 0.8443 - val_loss: 0.6711 - val_accuracy: 0.7299\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.78 - ETA: 0s - loss: 0.4573 - accuracy: 0.84 - ETA: 0s - loss: 0.5083 - accuracy: 0.84 - ETA: 0s - loss: 0.4913 - accuracy: 0.83 - ETA: 0s - loss: 0.4883 - accuracy: 0.84 - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - ETA: 0s - loss: 0.4803 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4731 - accuracy: 0.8481 - val_loss: 0.6663 - val_accuracy: 0.7418\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.96 - ETA: 0s - loss: 0.3865 - accuracy: 0.89 - ETA: 0s - loss: 0.4350 - accuracy: 0.86 - ETA: 0s - loss: 0.4459 - accuracy: 0.85 - ETA: 0s - loss: 0.4482 - accuracy: 0.85 - ETA: 0s - loss: 0.4521 - accuracy: 0.85 - ETA: 0s - loss: 0.4609 - accuracy: 0.85 - ETA: 0s - loss: 0.4694 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4694 - accuracy: 0.8499 - val_loss: 0.7612 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.84 - ETA: 0s - loss: 0.5077 - accuracy: 0.83 - ETA: 0s - loss: 0.4980 - accuracy: 0.82 - ETA: 0s - loss: 0.4981 - accuracy: 0.82 - ETA: 0s - loss: 0.4909 - accuracy: 0.82 - ETA: 0s - loss: 0.4769 - accuracy: 0.83 - ETA: 0s - loss: 0.4751 - accuracy: 0.83 - ETA: 0s - loss: 0.4765 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4783 - accuracy: 0.8369 - val_loss: 1.2277 - val_accuracy: 0.7418\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.78 - ETA: 0s - loss: 0.4767 - accuracy: 0.86 - ETA: 0s - loss: 0.4990 - accuracy: 0.84 - ETA: 0s - loss: 0.5240 - accuracy: 0.83 - ETA: 0s - loss: 0.5156 - accuracy: 0.83 - ETA: 0s - loss: 0.5246 - accuracy: 0.83 - ETA: 0s - loss: 0.5386 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5286 - accuracy: 0.8324 - val_loss: 1.0348 - val_accuracy: 0.7537\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.81 - ETA: 0s - loss: 0.7728 - accuracy: 0.84 - ETA: 0s - loss: 0.6432 - accuracy: 0.84 - ETA: 0s - loss: 0.5935 - accuracy: 0.84 - ETA: 0s - loss: 0.5715 - accuracy: 0.84 - ETA: 0s - loss: 0.5599 - accuracy: 0.83 - ETA: 0s - loss: 0.5595 - accuracy: 0.83 - ETA: 0s - loss: 0.5515 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5538 - accuracy: 0.8346 - val_loss: 1.1083 - val_accuracy: 0.7522\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.81 - ETA: 0s - loss: 0.5513 - accuracy: 0.81 - ETA: 0s - loss: 0.5106 - accuracy: 0.83 - ETA: 0s - loss: 0.5036 - accuracy: 0.84 - ETA: 0s - loss: 0.5004 - accuracy: 0.84 - ETA: 0s - loss: 0.4837 - accuracy: 0.85 - ETA: 0s - loss: 0.4952 - accuracy: 0.84 - ETA: 0s - loss: 0.4871 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4876 - accuracy: 0.8485 - val_loss: 0.7902 - val_accuracy: 0.7522\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5328 - accuracy: 0.81 - ETA: 0s - loss: 0.4479 - accuracy: 0.86 - ETA: 0s - loss: 0.4474 - accuracy: 0.86 - ETA: 0s - loss: 0.4531 - accuracy: 0.86 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - ETA: 0s - loss: 0.4536 - accuracy: 0.85 - ETA: 0s - loss: 0.4527 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4532 - accuracy: 0.8559 - val_loss: 0.8633 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.81 - ETA: 0s - loss: 0.4117 - accuracy: 0.87 - ETA: 0s - loss: 0.4182 - accuracy: 0.87 - ETA: 0s - loss: 0.4200 - accuracy: 0.86 - ETA: 0s - loss: 0.4165 - accuracy: 0.87 - ETA: 0s - loss: 0.4194 - accuracy: 0.87 - ETA: 0s - loss: 0.4257 - accuracy: 0.87 - ETA: 0s - loss: 0.4292 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4292 - accuracy: 0.8727 - val_loss: 0.7375 - val_accuracy: 0.7373\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.81 - ETA: 0s - loss: 0.4409 - accuracy: 0.86 - ETA: 0s - loss: 0.4723 - accuracy: 0.84 - ETA: 0s - loss: 0.4496 - accuracy: 0.86 - ETA: 0s - loss: 0.4543 - accuracy: 0.85 - ETA: 0s - loss: 0.4549 - accuracy: 0.85 - ETA: 0s - loss: 0.4587 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4527 - accuracy: 0.8570 - val_loss: 1.0305 - val_accuracy: 0.7224\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.90 - ETA: 0s - loss: 0.4380 - accuracy: 0.87 - ETA: 0s - loss: 0.4409 - accuracy: 0.86 - ETA: 0s - loss: 0.4807 - accuracy: 0.85 - ETA: 0s - loss: 0.4798 - accuracy: 0.85 - ETA: 0s - loss: 0.5151 - accuracy: 0.83 - ETA: 0s - loss: 0.5358 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5632 - accuracy: 0.8264 - val_loss: 2.3179 - val_accuracy: 0.7537\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5464 - accuracy: 0.84 - ETA: 0s - loss: 0.7894 - accuracy: 0.82 - ETA: 0s - loss: 0.8697 - accuracy: 0.77 - ETA: 0s - loss: 0.8335 - accuracy: 0.74 - ETA: 0s - loss: 0.7948 - accuracy: 0.73 - ETA: 0s - loss: 0.7711 - accuracy: 0.73 - ETA: 0s - loss: 0.7981 - accuracy: 0.73 - ETA: 0s - loss: 0.7809 - accuracy: 0.72 - 0s 5ms/step - loss: 0.7809 - accuracy: 0.7294 - val_loss: 0.6986 - val_accuracy: 0.7030\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.75 - ETA: 0s - loss: 0.6676 - accuracy: 0.72 - ETA: 0s - loss: 0.6724 - accuracy: 0.72 - ETA: 0s - loss: 0.6759 - accuracy: 0.71 - ETA: 0s - loss: 0.6754 - accuracy: 0.71 - ETA: 0s - loss: 0.6772 - accuracy: 0.71 - ETA: 0s - loss: 0.6808 - accuracy: 0.70 - ETA: 0s - loss: 0.6855 - accuracy: 0.70 - ETA: 0s - loss: 0.6838 - accuracy: 0.71 - 0s 6ms/step - loss: 0.6833 - accuracy: 0.7115 - val_loss: 0.6846 - val_accuracy: 0.6970\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6137 - accuracy: 0.81 - ETA: 0s - loss: 0.6552 - accuracy: 0.75 - ETA: 0s - loss: 0.6812 - accuracy: 0.71 - ETA: 0s - loss: 0.6874 - accuracy: 0.70 - ETA: 0s - loss: 0.6851 - accuracy: 0.71 - ETA: 0s - loss: 0.6915 - accuracy: 0.70 - ETA: 0s - loss: 0.6905 - accuracy: 0.70 - 0s 5ms/step - loss: 0.6923 - accuracy: 0.6771 - val_loss: 0.6945 - val_accuracy: 0.3045\n",
      "Epoch 24/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.7006 - accuracy: 0.31 - ETA: 0s - loss: 0.6979 - accuracy: 0.31 - ETA: 0s - loss: 0.6955 - accuracy: 0.30 - ETA: 0s - loss: 0.6987 - accuracy: 0.31 - ETA: 0s - loss: 0.6973 - accuracy: 0.30 - ETA: 0s - loss: 0.6955 - accuracy: 0.30 - ETA: 0s - loss: 0.6929 - accuracy: 0.3150Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6921 - accuracy: 0.3561 - val_loss: 0.6901 - val_accuracy: 0.6970\n",
      "Epoch 00024: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6332 - accuracy: 0.75 - ETA: 0s - loss: 4.9770 - accuracy: 0.58 - ETA: 0s - loss: 2.7991 - accuracy: 0.61 - ETA: 0s - loss: 2.1148 - accuracy: 0.62 - ETA: 0s - loss: 1.7332 - accuracy: 0.66 - ETA: 0s - loss: 1.5143 - accuracy: 0.67 - ETA: 0s - loss: 1.3562 - accuracy: 0.68 - ETA: 0s - loss: 1.2480 - accuracy: 0.68 - 1s 6ms/step - loss: 1.2280 - accuracy: 0.6853 - val_loss: 0.5735 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.81 - ETA: 0s - loss: 0.5121 - accuracy: 0.78 - ETA: 0s - loss: 0.5306 - accuracy: 0.76 - ETA: 0s - loss: 0.5307 - accuracy: 0.76 - ETA: 0s - loss: 0.5246 - accuracy: 0.76 - ETA: 0s - loss: 0.5320 - accuracy: 0.76 - ETA: 0s - loss: 0.5352 - accuracy: 0.76 - ETA: 0s - loss: 0.5334 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5326 - accuracy: 0.7615 - val_loss: 0.6365 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.81 - ETA: 0s - loss: 0.5281 - accuracy: 0.77 - ETA: 0s - loss: 0.5179 - accuracy: 0.78 - ETA: 0s - loss: 0.5051 - accuracy: 0.79 - ETA: 0s - loss: 0.5115 - accuracy: 0.79 - ETA: 0s - loss: 0.5014 - accuracy: 0.80 - ETA: 0s - loss: 0.4952 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4900 - accuracy: 0.8033 - val_loss: 0.5532 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.93 - ETA: 0s - loss: 0.3819 - accuracy: 0.84 - ETA: 0s - loss: 0.4687 - accuracy: 0.82 - ETA: 0s - loss: 0.4814 - accuracy: 0.81 - ETA: 0s - loss: 0.4801 - accuracy: 0.80 - ETA: 0s - loss: 0.4788 - accuracy: 0.80 - ETA: 0s - loss: 0.4705 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4835 - accuracy: 0.8033 - val_loss: 0.5796 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.84 - ETA: 0s - loss: 0.4388 - accuracy: 0.82 - ETA: 0s - loss: 0.4141 - accuracy: 0.84 - ETA: 0s - loss: 0.4505 - accuracy: 0.81 - ETA: 0s - loss: 0.4692 - accuracy: 0.81 - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - ETA: 0s - loss: 0.4542 - accuracy: 0.81 - ETA: 0s - loss: 0.4487 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8246 - val_loss: 0.6474 - val_accuracy: 0.6985\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.93 - ETA: 0s - loss: 0.4372 - accuracy: 0.84 - ETA: 0s - loss: 0.4653 - accuracy: 0.81 - ETA: 0s - loss: 0.4434 - accuracy: 0.82 - ETA: 0s - loss: 0.4243 - accuracy: 0.82 - ETA: 0s - loss: 0.4495 - accuracy: 0.81 - ETA: 0s - loss: 0.4508 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4509 - accuracy: 0.8134 - val_loss: 0.6059 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.87 - ETA: 0s - loss: 0.3913 - accuracy: 0.84 - ETA: 0s - loss: 0.4246 - accuracy: 0.83 - ETA: 0s - loss: 0.4381 - accuracy: 0.82 - ETA: 0s - loss: 0.4334 - accuracy: 0.82 - ETA: 0s - loss: 0.4277 - accuracy: 0.82 - ETA: 0s - loss: 0.4281 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4305 - accuracy: 0.8283 - val_loss: 0.6610 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.75 - ETA: 0s - loss: 0.3822 - accuracy: 0.84 - ETA: 0s - loss: 0.4398 - accuracy: 0.82 - ETA: 0s - loss: 0.4399 - accuracy: 0.82 - ETA: 0s - loss: 0.4361 - accuracy: 0.82 - ETA: 0s - loss: 0.4269 - accuracy: 0.82 - ETA: 0s - loss: 0.4295 - accuracy: 0.82 - ETA: 0s - loss: 0.4233 - accuracy: 0.83 - ETA: 0s - loss: 0.4217 - accuracy: 0.83 - 0s 6ms/step - loss: 0.4229 - accuracy: 0.8309 - val_loss: 0.6777 - val_accuracy: 0.7000\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 1.00 - ETA: 0s - loss: 0.3658 - accuracy: 0.86 - ETA: 0s - loss: 0.4387 - accuracy: 0.86 - ETA: 0s - loss: 0.4321 - accuracy: 0.86 - ETA: 0s - loss: 0.4522 - accuracy: 0.85 - ETA: 0s - loss: 0.4600 - accuracy: 0.83 - ETA: 0s - loss: 0.4605 - accuracy: 0.83 - ETA: 0s - loss: 0.4504 - accuracy: 0.83 - ETA: 0s - loss: 0.4414 - accuracy: 0.84 - 0s 6ms/step - loss: 0.4426 - accuracy: 0.8432 - val_loss: 0.8391 - val_accuracy: 0.6597\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.84 - ETA: 0s - loss: 0.4482 - accuracy: 0.82 - ETA: 0s - loss: 0.4752 - accuracy: 0.82 - ETA: 0s - loss: 0.4685 - accuracy: 0.83 - ETA: 0s - loss: 0.4482 - accuracy: 0.83 - ETA: 0s - loss: 0.4445 - accuracy: 0.83 - ETA: 0s - loss: 0.4512 - accuracy: 0.83 - ETA: 0s - loss: 0.4458 - accuracy: 0.83 - ETA: 0s - loss: 0.4410 - accuracy: 0.84 - ETA: 0s - loss: 0.4372 - accuracy: 0.84 - 1s 6ms/step - loss: 0.4358 - accuracy: 0.8443 - val_loss: 0.7723 - val_accuracy: 0.7134\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.81 - ETA: 0s - loss: 0.3912 - accuracy: 0.88 - ETA: 0s - loss: 0.3917 - accuracy: 0.87 - ETA: 0s - loss: 0.3800 - accuracy: 0.87 - ETA: 0s - loss: 0.3753 - accuracy: 0.87 - ETA: 0s - loss: 0.3850 - accuracy: 0.87 - ETA: 0s - loss: 0.3915 - accuracy: 0.86 - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8716 - val_loss: 1.8990 - val_accuracy: 0.7030\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.93 - ETA: 0s - loss: 0.4399 - accuracy: 0.88 - ETA: 0s - loss: 0.4861 - accuracy: 0.86 - ETA: 0s - loss: 0.5089 - accuracy: 0.85 - ETA: 0s - loss: 0.4843 - accuracy: 0.85 - ETA: 0s - loss: 0.4650 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.4501 - accuracy: 0.85 - ETA: 0s - loss: 0.4419 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4419 - accuracy: 0.8552 - val_loss: 0.7243 - val_accuracy: 0.7209\n",
      "Epoch 13/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4663 - accuracy: 0.78 - ETA: 0s - loss: 0.4127 - accuracy: 0.83 - ETA: 0s - loss: 0.3701 - accuracy: 0.85 - ETA: 0s - loss: 0.3668 - accuracy: 0.87 - ETA: 0s - loss: 0.3569 - accuracy: 0.87 - ETA: 0s - loss: 0.3573 - accuracy: 0.88 - ETA: 0s - loss: 0.3449 - accuracy: 0.89 - ETA: 0s - loss: 0.3468 - accuracy: 0.8904Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.3404 - accuracy: 0.8932 - val_loss: 0.9987 - val_accuracy: 0.7179\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.65 - ETA: 0s - loss: 5.1026 - accuracy: 0.55 - ETA: 0s - loss: 2.6997 - accuracy: 0.58 - ETA: 0s - loss: 1.7846 - accuracy: 0.63 - ETA: 0s - loss: 1.4895 - accuracy: 0.64 - ETA: 0s - loss: 1.3484 - accuracy: 0.65 - ETA: 0s - loss: 1.2055 - accuracy: 0.67 - ETA: 0s - loss: 1.1044 - accuracy: 0.68 - ETA: 0s - loss: 1.0194 - accuracy: 0.69 - ETA: 0s - loss: 0.9600 - accuracy: 0.69 - 1s 8ms/step - loss: 0.9558 - accuracy: 0.6962 - val_loss: 0.6747 - val_accuracy: 0.6060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.62 - ETA: 0s - loss: 0.5221 - accuracy: 0.73 - ETA: 0s - loss: 0.4916 - accuracy: 0.76 - ETA: 0s - loss: 0.4966 - accuracy: 0.76 - ETA: 0s - loss: 0.5064 - accuracy: 0.76 - ETA: 0s - loss: 0.5082 - accuracy: 0.75 - ETA: 0s - loss: 0.5164 - accuracy: 0.75 - ETA: 0s - loss: 0.5240 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5259 - accuracy: 0.7604 - val_loss: 0.7272 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.90 - ETA: 0s - loss: 0.5156 - accuracy: 0.78 - ETA: 0s - loss: 0.4792 - accuracy: 0.80 - ETA: 0s - loss: 0.4914 - accuracy: 0.80 - ETA: 0s - loss: 0.4912 - accuracy: 0.80 - ETA: 0s - loss: 0.5001 - accuracy: 0.80 - ETA: 0s - loss: 0.5066 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5035 - accuracy: 0.8033 - val_loss: 0.6215 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.90 - ETA: 0s - loss: 0.4330 - accuracy: 0.84 - ETA: 0s - loss: 0.4312 - accuracy: 0.84 - ETA: 0s - loss: 0.4371 - accuracy: 0.83 - ETA: 0s - loss: 0.4405 - accuracy: 0.83 - ETA: 0s - loss: 0.4481 - accuracy: 0.82 - ETA: 0s - loss: 0.4488 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4451 - accuracy: 0.8302 - val_loss: 0.7653 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.68 - ETA: 0s - loss: 0.3452 - accuracy: 0.88 - ETA: 0s - loss: 0.3645 - accuracy: 0.86 - ETA: 0s - loss: 0.3950 - accuracy: 0.85 - ETA: 0s - loss: 0.4965 - accuracy: 0.84 - ETA: 0s - loss: 0.4887 - accuracy: 0.84 - ETA: 0s - loss: 0.4787 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4794 - accuracy: 0.8436 - val_loss: 0.8370 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.84 - ETA: 0s - loss: 0.3757 - accuracy: 0.87 - ETA: 0s - loss: 0.3982 - accuracy: 0.86 - ETA: 0s - loss: 0.3848 - accuracy: 0.86 - ETA: 0s - loss: 0.4118 - accuracy: 0.85 - ETA: 0s - loss: 0.4173 - accuracy: 0.85 - ETA: 0s - loss: 0.4220 - accuracy: 0.85 - ETA: 0s - loss: 0.4231 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8544 - val_loss: 0.5711 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.90 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4335 - accuracy: 0.85 - ETA: 0s - loss: 0.4319 - accuracy: 0.84 - ETA: 0s - loss: 0.4296 - accuracy: 0.84 - ETA: 0s - loss: 0.4330 - accuracy: 0.84 - ETA: 0s - loss: 0.4254 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4203 - accuracy: 0.8496 - val_loss: 0.8418 - val_accuracy: 0.6985\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.96 - ETA: 0s - loss: 0.3151 - accuracy: 0.89 - ETA: 0s - loss: 0.3318 - accuracy: 0.88 - ETA: 0s - loss: 0.3625 - accuracy: 0.88 - ETA: 0s - loss: 0.3654 - accuracy: 0.87 - ETA: 0s - loss: 0.3752 - accuracy: 0.87 - ETA: 0s - loss: 0.3806 - accuracy: 0.86 - ETA: 0s - loss: 0.3865 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3845 - accuracy: 0.8652 - val_loss: 0.7623 - val_accuracy: 0.7075\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.90 - ETA: 0s - loss: 0.3542 - accuracy: 0.87 - ETA: 0s - loss: 0.3693 - accuracy: 0.86 - ETA: 0s - loss: 0.3758 - accuracy: 0.86 - ETA: 0s - loss: 0.3944 - accuracy: 0.86 - ETA: 0s - loss: 0.3854 - accuracy: 0.86 - ETA: 0s - loss: 0.3858 - accuracy: 0.87 - ETA: 0s - loss: 0.3837 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3946 - accuracy: 0.8664 - val_loss: 0.8401 - val_accuracy: 0.6925\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.87 - ETA: 0s - loss: 0.3892 - accuracy: 0.86 - ETA: 0s - loss: 0.3584 - accuracy: 0.87 - ETA: 0s - loss: 0.3444 - accuracy: 0.88 - ETA: 0s - loss: 0.3211 - accuracy: 0.89 - ETA: 0s - loss: 0.4797 - accuracy: 0.88 - ETA: 0s - loss: 0.4768 - accuracy: 0.87 - ETA: 0s - loss: 0.4743 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4743 - accuracy: 0.8638 - val_loss: 0.9744 - val_accuracy: 0.7090\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.96 - ETA: 0s - loss: 0.4605 - accuracy: 0.85 - ETA: 0s - loss: 0.3957 - accuracy: 0.87 - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.4328 - accuracy: 0.87 - ETA: 0s - loss: 0.4302 - accuracy: 0.87 - ETA: 0s - loss: 0.4416 - accuracy: 0.86 - ETA: 0s - loss: 0.4337 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4340 - accuracy: 0.8630 - val_loss: 0.9286 - val_accuracy: 0.6896\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.81 - ETA: 0s - loss: 0.3256 - accuracy: 0.88 - ETA: 0s - loss: 0.3263 - accuracy: 0.88 - ETA: 0s - loss: 0.3410 - accuracy: 0.88 - ETA: 0s - loss: 0.4313 - accuracy: 0.88 - ETA: 0s - loss: 0.4558 - accuracy: 0.86 - ETA: 0s - loss: 0.4440 - accuracy: 0.87 - ETA: 0s - loss: 0.4454 - accuracy: 0.86 - ETA: 0s - loss: 0.4600 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4600 - accuracy: 0.8645 - val_loss: 1.9152 - val_accuracy: 0.7119\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 1.00 - ETA: 0s - loss: 0.5621 - accuracy: 0.89 - ETA: 0s - loss: 0.7686 - accuracy: 0.82 - ETA: 0s - loss: 0.7344 - accuracy: 0.80 - ETA: 0s - loss: 0.6949 - accuracy: 0.80 - ETA: 0s - loss: 0.6571 - accuracy: 0.80 - ETA: 0s - loss: 0.6167 - accuracy: 0.80 - ETA: 0s - loss: 0.6015 - accuracy: 0.80 - ETA: 0s - loss: 0.5835 - accuracy: 0.80 - 0s 6ms/step - loss: 0.5790 - accuracy: 0.8108 - val_loss: 1.1347 - val_accuracy: 0.6836\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.75 - ETA: 0s - loss: 0.3937 - accuracy: 0.87 - ETA: 0s - loss: 0.4401 - accuracy: 0.87 - ETA: 0s - loss: 0.4536 - accuracy: 0.86 - ETA: 0s - loss: 0.4428 - accuracy: 0.87 - ETA: 0s - loss: 0.4360 - accuracy: 0.87 - ETA: 0s - loss: 0.4374 - accuracy: 0.86 - ETA: 0s - loss: 0.4297 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4315 - accuracy: 0.8697 - val_loss: 0.7424 - val_accuracy: 0.7075\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.81 - ETA: 0s - loss: 0.3763 - accuracy: 0.87 - ETA: 0s - loss: 0.3729 - accuracy: 0.88 - ETA: 0s - loss: 0.3836 - accuracy: 0.87 - ETA: 0s - loss: 0.3644 - accuracy: 0.88 - ETA: 0s - loss: 0.3611 - accuracy: 0.88 - ETA: 0s - loss: 0.3833 - accuracy: 0.88 - ETA: 0s - loss: 0.3863 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3842 - accuracy: 0.8888 - val_loss: 0.9305 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2694 - accuracy: 0.90 - ETA: 0s - loss: 0.3637 - accuracy: 0.90 - ETA: 0s - loss: 0.3690 - accuracy: 0.89 - ETA: 0s - loss: 0.3893 - accuracy: 0.88 - ETA: 0s - loss: 0.3981 - accuracy: 0.88 - ETA: 0s - loss: 0.3901 - accuracy: 0.88 - ETA: 0s - loss: 0.3829 - accuracy: 0.88 - ETA: 0s - loss: 0.3773 - accuracy: 0.8878Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3832 - accuracy: 0.8858 - val_loss: 1.5097 - val_accuracy: 0.7194\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 642fb24c3b14176fabd755916ae88e9c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7437810897827148</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.36572239117363947</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.71 - ETA: 0s - loss: 2.0735 - accuracy: 0.62 - ETA: 0s - loss: 1.3547 - accuracy: 0.69 - ETA: 0s - loss: 1.0995 - accuracy: 0.69 - ETA: 0s - loss: 0.9685 - accuracy: 0.71 - ETA: 0s - loss: 0.8875 - accuracy: 0.71 - 0s 5ms/step - loss: 0.8825 - accuracy: 0.7200 - val_loss: 0.5709 - val_accuracy: 0.7299\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.75 - ETA: 0s - loss: 0.4929 - accuracy: 0.79 - ETA: 0s - loss: 0.5014 - accuracy: 0.78 - ETA: 0s - loss: 0.4976 - accuracy: 0.79 - ETA: 0s - loss: 0.5062 - accuracy: 0.78 - ETA: 0s - loss: 0.5101 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5104 - accuracy: 0.7824 - val_loss: 0.5586 - val_accuracy: 0.7463\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.90 - ETA: 0s - loss: 0.3867 - accuracy: 0.83 - ETA: 0s - loss: 0.4253 - accuracy: 0.80 - ETA: 0s - loss: 0.4345 - accuracy: 0.79 - ETA: 0s - loss: 0.4530 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7999 - val_loss: 0.6121 - val_accuracy: 0.7075\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.82 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4252 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4287 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4280 - accuracy: 0.8081 - val_loss: 0.7833 - val_accuracy: 0.6940\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.75 - ETA: 0s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.3644 - accuracy: 0.84 - ETA: 0s - loss: 0.3636 - accuracy: 0.83 - ETA: 0s - loss: 0.3661 - accuracy: 0.84 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8384 - val_loss: 0.7428 - val_accuracy: 0.7194\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.87 - ETA: 0s - loss: 0.3258 - accuracy: 0.86 - ETA: 0s - loss: 0.3251 - accuracy: 0.86 - ETA: 0s - loss: 0.3312 - accuracy: 0.85 - ETA: 0s - loss: 0.3380 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8451 - val_loss: 0.8017 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.87 - ETA: 0s - loss: 0.3177 - accuracy: 0.86 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - ETA: 0s - loss: 0.3343 - accuracy: 0.85 - ETA: 0s - loss: 0.3423 - accuracy: 0.85 - ETA: 0s - loss: 0.3394 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3444 - accuracy: 0.8499 - val_loss: 0.8008 - val_accuracy: 0.7149\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3315 - accuracy: 0.87 - ETA: 0s - loss: 0.2996 - accuracy: 0.88 - ETA: 0s - loss: 0.3108 - accuracy: 0.87 - ETA: 0s - loss: 0.3169 - accuracy: 0.87 - ETA: 0s - loss: 0.3189 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8708 - val_loss: 0.7629 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3702 - accuracy: 0.84 - ETA: 0s - loss: 0.2742 - accuracy: 0.89 - ETA: 0s - loss: 0.2971 - accuracy: 0.87 - ETA: 0s - loss: 0.3072 - accuracy: 0.86 - ETA: 0s - loss: 0.3131 - accuracy: 0.86 - ETA: 0s - loss: 0.3102 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3107 - accuracy: 0.8641 - val_loss: 0.9741 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.96 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2958 - accuracy: 0.88 - ETA: 0s - loss: 0.2851 - accuracy: 0.88 - ETA: 0s - loss: 0.2861 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3002 - accuracy: 0.8862 - val_loss: 0.7279 - val_accuracy: 0.7045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3320 - accuracy: 0.84 - ETA: 0s - loss: 0.3137 - accuracy: 0.86 - ETA: 0s - loss: 0.2818 - accuracy: 0.88 - ETA: 0s - loss: 0.2892 - accuracy: 0.88 - ETA: 0s - loss: 0.3014 - accuracy: 0.88 - ETA: 0s - loss: 0.3127 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8813 - val_loss: 1.5175 - val_accuracy: 0.7224\n",
      "Epoch 12/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.2949 - accuracy: 0.84 - ETA: 0s - loss: 0.3095 - accuracy: 0.89 - ETA: 0s - loss: 0.2907 - accuracy: 0.89 - ETA: 0s - loss: 0.3409 - accuracy: 0.89 - ETA: 0s - loss: 0.3386 - accuracy: 0.89 - ETA: 0s - loss: 0.3459 - accuracy: 0.8846Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.8802 - val_loss: 1.4381 - val_accuracy: 0.7373\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.71 - ETA: 0s - loss: 1.5332 - accuracy: 0.65 - ETA: 0s - loss: 1.0328 - accuracy: 0.69 - ETA: 0s - loss: 0.8731 - accuracy: 0.70 - ETA: 0s - loss: 0.8126 - accuracy: 0.70 - ETA: 0s - loss: 0.7747 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7669 - accuracy: 0.7032 - val_loss: 0.5889 - val_accuracy: 0.6940\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.71 - ETA: 0s - loss: 0.4837 - accuracy: 0.78 - ETA: 0s - loss: 0.5179 - accuracy: 0.77 - ETA: 0s - loss: 0.5334 - accuracy: 0.76 - ETA: 0s - loss: 0.5243 - accuracy: 0.77 - ETA: 0s - loss: 0.5324 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5303 - accuracy: 0.7596 - val_loss: 0.5528 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.90 - ETA: 0s - loss: 0.4654 - accuracy: 0.81 - ETA: 0s - loss: 0.4650 - accuracy: 0.81 - ETA: 0s - loss: 0.4776 - accuracy: 0.79 - ETA: 0s - loss: 0.4727 - accuracy: 0.79 - ETA: 0s - loss: 0.4819 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4822 - accuracy: 0.7951 - val_loss: 0.6812 - val_accuracy: 0.6672\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.78 - ETA: 0s - loss: 0.4340 - accuracy: 0.83 - ETA: 0s - loss: 0.4404 - accuracy: 0.82 - ETA: 0s - loss: 0.4229 - accuracy: 0.82 - ETA: 0s - loss: 0.4648 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5035 - accuracy: 0.8010 - val_loss: 0.9304 - val_accuracy: 0.6478\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.75 - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4264 - accuracy: 0.82 - ETA: 0s - loss: 0.4260 - accuracy: 0.83 - ETA: 0s - loss: 0.4192 - accuracy: 0.83 - ETA: 0s - loss: 0.4294 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4304 - accuracy: 0.8234 - val_loss: 0.6542 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.84 - ETA: 0s - loss: 0.3319 - accuracy: 0.87 - ETA: 0s - loss: 0.3647 - accuracy: 0.87 - ETA: 0s - loss: 0.3675 - accuracy: 0.86 - ETA: 0s - loss: 0.3736 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3769 - accuracy: 0.8593 - val_loss: 0.8594 - val_accuracy: 0.6567\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.87 - ETA: 0s - loss: 0.3215 - accuracy: 0.88 - ETA: 0s - loss: 0.3222 - accuracy: 0.86 - ETA: 0s - loss: 0.3221 - accuracy: 0.86 - ETA: 0s - loss: 0.3361 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3441 - accuracy: 0.8559 - val_loss: 0.8075 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.93 - ETA: 0s - loss: 0.3312 - accuracy: 0.88 - ETA: 0s - loss: 0.3331 - accuracy: 0.86 - ETA: 0s - loss: 0.3531 - accuracy: 0.85 - ETA: 0s - loss: 0.3439 - accuracy: 0.86 - ETA: 0s - loss: 0.3473 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3473 - accuracy: 0.8623 - val_loss: 1.1240 - val_accuracy: 0.6731\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.84 - ETA: 0s - loss: 0.3005 - accuracy: 0.88 - ETA: 0s - loss: 0.3307 - accuracy: 0.88 - ETA: 0s - loss: 0.3225 - accuracy: 0.87 - ETA: 0s - loss: 0.3191 - accuracy: 0.87 - ETA: 0s - loss: 0.3208 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8735 - val_loss: 2.3750 - val_accuracy: 0.7075\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.93 - ETA: 0s - loss: 0.3806 - accuracy: 0.86 - ETA: 0s - loss: 0.3542 - accuracy: 0.84 - ETA: 0s - loss: 0.4093 - accuracy: 0.84 - ETA: 0s - loss: 0.4070 - accuracy: 0.84 - ETA: 0s - loss: 0.4008 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3982 - accuracy: 0.8425 - val_loss: 1.3976 - val_accuracy: 0.6657\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5296 - accuracy: 0.75 - ETA: 0s - loss: 0.3328 - accuracy: 0.86 - ETA: 0s - loss: 0.2845 - accuracy: 0.87 - ETA: 0s - loss: 0.3147 - accuracy: 0.88 - ETA: 0s - loss: 0.3512 - accuracy: 0.86 - ETA: 0s - loss: 0.3939 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8436 - val_loss: 0.5770 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.78 - ETA: 0s - loss: 0.5305 - accuracy: 0.79 - ETA: 0s - loss: 0.7629 - accuracy: 0.79 - ETA: 0s - loss: 0.8068 - accuracy: 0.74 - ETA: 0s - loss: 0.7774 - accuracy: 0.74 - 0s 3ms/step - loss: 0.7709 - accuracy: 0.7316 - val_loss: 0.6146 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.87 - ETA: 0s - loss: 0.6471 - accuracy: 0.72 - ETA: 0s - loss: 0.6457 - accuracy: 0.71 - ETA: 0s - loss: 0.6552 - accuracy: 0.68 - ETA: 0s - loss: 0.6609 - accuracy: 0.64 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6185 - val_loss: 0.6818 - val_accuracy: 0.4582\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.52 - ETA: 0s - loss: 0.6951 - accuracy: 0.45 - ETA: 0s - loss: 0.6988 - accuracy: 0.43 - ETA: 0s - loss: 0.6897 - accuracy: 0.41 - ETA: 0s - loss: 0.6868 - accuracy: 0.40 - 0s 4ms/step - loss: 0.6881 - accuracy: 0.4024 - val_loss: 0.6894 - val_accuracy: 0.3716\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8341 - accuracy: 0.31 - ETA: 0s - loss: 0.6817 - accuracy: 0.36 - ETA: 0s - loss: 0.6837 - accuracy: 0.36 - ETA: 0s - loss: 0.6824 - accuracy: 0.37 - ETA: 0s - loss: 0.6802 - accuracy: 0.36 - ETA: 0s - loss: 0.6873 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6878 - accuracy: 0.3785 - val_loss: 0.6966 - val_accuracy: 0.3716\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.31 - ETA: 0s - loss: 0.6739 - accuracy: 0.37 - ETA: 0s - loss: 0.6795 - accuracy: 0.37 - ETA: 0s - loss: 0.6841 - accuracy: 0.37 - ETA: 0s - loss: 0.6858 - accuracy: 0.38 - ETA: 0s - loss: 0.6878 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6878 - accuracy: 0.3785 - val_loss: 0.7011 - val_accuracy: 0.3716\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.21 - ETA: 0s - loss: 0.7055 - accuracy: 0.36 - ETA: 0s - loss: 0.6909 - accuracy: 0.37 - ETA: 0s - loss: 0.6901 - accuracy: 0.37 - ETA: 0s - loss: 0.6918 - accuracy: 0.37 - ETA: 0s - loss: 0.6880 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6875 - accuracy: 0.3785 - val_loss: 0.6888 - val_accuracy: 0.3716\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.50 - ETA: 0s - loss: 0.6863 - accuracy: 0.34 - ETA: 0s - loss: 0.6808 - accuracy: 0.34 - ETA: 0s - loss: 0.6859 - accuracy: 0.37 - ETA: 0s - loss: 0.6878 - accuracy: 0.37 - ETA: 0s - loss: 0.6885 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6885 - accuracy: 0.3781 - val_loss: 0.6936 - val_accuracy: 0.3716\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.18 - ETA: 0s - loss: 0.6908 - accuracy: 0.38 - ETA: 0s - loss: 0.6924 - accuracy: 0.38 - ETA: 0s - loss: 0.6939 - accuracy: 0.38 - ETA: 0s - loss: 0.6923 - accuracy: 0.38 - ETA: 0s - loss: 0.6914 - accuracy: 0.38 - 0s 4ms/step - loss: 0.6879 - accuracy: 0.3789 - val_loss: 0.6904 - val_accuracy: 0.3716\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.31 - ETA: 0s - loss: 0.6813 - accuracy: 0.37 - ETA: 0s - loss: 0.6817 - accuracy: 0.39 - ETA: 0s - loss: 0.6932 - accuracy: 0.39 - ETA: 0s - loss: 0.6908 - accuracy: 0.38 - ETA: 0s - loss: 0.6876 - accuracy: 0.37 - 0s 4ms/step - loss: 0.6876 - accuracy: 0.3789 - val_loss: 0.6915 - val_accuracy: 0.3716\n",
      "Epoch 21/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6986 - accuracy: 0.43 - ETA: 0s - loss: 0.6859 - accuracy: 0.35 - ETA: 0s - loss: 0.6834 - accuracy: 0.35 - ETA: 0s - loss: 0.6902 - accuracy: 0.37 - ETA: 0s - loss: 0.6892 - accuracy: 0.37 - ETA: 0s - loss: 0.6876 - accuracy: 0.3785Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.3789 - val_loss: 0.6899 - val_accuracy: 0.3716\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.62 - ETA: 0s - loss: 1.2796 - accuracy: 0.61 - ETA: 0s - loss: 0.9255 - accuracy: 0.68 - ETA: 0s - loss: 0.8066 - accuracy: 0.69 - ETA: 0s - loss: 0.7544 - accuracy: 0.69 - ETA: 0s - loss: 0.7260 - accuracy: 0.69 - 0s 5ms/step - loss: 0.7167 - accuracy: 0.6999 - val_loss: 0.6035 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.81 - ETA: 0s - loss: 0.5405 - accuracy: 0.74 - ETA: 0s - loss: 0.5287 - accuracy: 0.75 - ETA: 0s - loss: 0.5257 - accuracy: 0.76 - ETA: 0s - loss: 0.5358 - accuracy: 0.77 - ETA: 0s - loss: 0.5346 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5385 - accuracy: 0.7648 - val_loss: 0.6160 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.87 - ETA: 0s - loss: 0.4699 - accuracy: 0.80 - ETA: 0s - loss: 0.4852 - accuracy: 0.78 - ETA: 0s - loss: 0.4912 - accuracy: 0.78 - ETA: 0s - loss: 0.4895 - accuracy: 0.79 - ETA: 0s - loss: 0.4900 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7951 - val_loss: 0.7264 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.4289 - accuracy: 0.81 - ETA: 0s - loss: 0.4340 - accuracy: 0.82 - ETA: 0s - loss: 0.4286 - accuracy: 0.83 - ETA: 0s - loss: 0.4080 - accuracy: 0.83 - ETA: 0s - loss: 0.4240 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8253 - val_loss: 0.5725 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.81 - ETA: 0s - loss: 0.4168 - accuracy: 0.86 - ETA: 0s - loss: 0.4403 - accuracy: 0.83 - ETA: 0s - loss: 0.4381 - accuracy: 0.83 - ETA: 0s - loss: 0.4372 - accuracy: 0.83 - ETA: 0s - loss: 0.4305 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8350 - val_loss: 0.6312 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.87 - ETA: 0s - loss: 0.3858 - accuracy: 0.84 - ETA: 0s - loss: 0.4006 - accuracy: 0.83 - ETA: 0s - loss: 0.3925 - accuracy: 0.83 - ETA: 0s - loss: 0.4001 - accuracy: 0.83 - ETA: 0s - loss: 0.3928 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3970 - accuracy: 0.8402 - val_loss: 0.6031 - val_accuracy: 0.7224\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.90 - ETA: 0s - loss: 0.3450 - accuracy: 0.86 - ETA: 0s - loss: 0.3434 - accuracy: 0.87 - ETA: 0s - loss: 0.3341 - accuracy: 0.88 - ETA: 0s - loss: 0.3493 - accuracy: 0.88 - ETA: 0s - loss: 0.3657 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8731 - val_loss: 0.6373 - val_accuracy: 0.7194\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.81 - ETA: 0s - loss: 0.3059 - accuracy: 0.87 - ETA: 0s - loss: 0.3463 - accuracy: 0.87 - ETA: 0s - loss: 0.3574 - accuracy: 0.86 - ETA: 0s - loss: 0.3630 - accuracy: 0.86 - ETA: 0s - loss: 0.3729 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8600 - val_loss: 0.6543 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.90 - ETA: 0s - loss: 0.3888 - accuracy: 0.86 - ETA: 0s - loss: 0.5097 - accuracy: 0.83 - ETA: 0s - loss: 0.4659 - accuracy: 0.84 - ETA: 0s - loss: 0.4452 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4534 - accuracy: 0.8451 - val_loss: 0.7208 - val_accuracy: 0.7358\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5977 - accuracy: 0.78 - ETA: 0s - loss: 0.3889 - accuracy: 0.86 - ETA: 0s - loss: 0.3713 - accuracy: 0.87 - ETA: 0s - loss: 0.3652 - accuracy: 0.87 - ETA: 0s - loss: 0.3743 - accuracy: 0.86 - ETA: 0s - loss: 0.3638 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3661 - accuracy: 0.8675 - val_loss: 0.9760 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.84 - ETA: 0s - loss: 0.2893 - accuracy: 0.89 - ETA: 0s - loss: 0.3188 - accuracy: 0.88 - ETA: 0s - loss: 0.3326 - accuracy: 0.88 - ETA: 0s - loss: 0.3993 - accuracy: 0.86 - ETA: 0s - loss: 0.3900 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8723 - val_loss: 0.8563 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.81 - ETA: 0s - loss: 0.3537 - accuracy: 0.87 - ETA: 0s - loss: 0.3277 - accuracy: 0.88 - ETA: 0s - loss: 0.3340 - accuracy: 0.89 - ETA: 0s - loss: 0.3287 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8925 - val_loss: 1.0605 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.90 - ETA: 0s - loss: 0.2387 - accuracy: 0.92 - ETA: 0s - loss: 0.2607 - accuracy: 0.91 - ETA: 0s - loss: 0.2811 - accuracy: 0.90 - ETA: 0s - loss: 0.2904 - accuracy: 0.90 - ETA: 0s - loss: 0.3005 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2999 - accuracy: 0.9059 - val_loss: 1.3845 - val_accuracy: 0.6970\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.90 - ETA: 0s - loss: 0.3192 - accuracy: 0.91 - ETA: 0s - loss: 0.3882 - accuracy: 0.89 - ETA: 0s - loss: 0.3734 - accuracy: 0.88 - ETA: 0s - loss: 0.3703 - accuracy: 0.87 - ETA: 0s - loss: 0.3675 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3631 - accuracy: 0.8809 - val_loss: 1.0462 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.1469 - accuracy: 0.96 - ETA: 0s - loss: 0.2532 - accuracy: 0.91 - ETA: 0s - loss: 0.2753 - accuracy: 0.90 - ETA: 0s - loss: 0.2669 - accuracy: 0.91 - ETA: 0s - loss: 0.2849 - accuracy: 0.90 - ETA: 0s - loss: 0.2963 - accuracy: 0.9016Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2963 - accuracy: 0.9029 - val_loss: 0.6498 - val_accuracy: 0.7164\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d97eb075ca757a4aef06b33cdad84c65</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7383084495862325</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.14111931385018994</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 310</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0052 - accuracy: 0.62 - ETA: 0s - loss: 1.7193 - accuracy: 0.63 - ETA: 0s - loss: 1.1653 - accuracy: 0.65 - ETA: 0s - loss: 0.9665 - accuracy: 0.66 - ETA: 0s - loss: 0.8642 - accuracy: 0.68 - ETA: 0s - loss: 0.8100 - accuracy: 0.68 - 0s 5ms/step - loss: 0.8100 - accuracy: 0.6891 - val_loss: 0.5687 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5365 - accuracy: 0.71 - ETA: 0s - loss: 0.5713 - accuracy: 0.71 - ETA: 0s - loss: 0.5524 - accuracy: 0.74 - ETA: 0s - loss: 0.5313 - accuracy: 0.75 - ETA: 0s - loss: 0.5353 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5456 - accuracy: 0.7551 - val_loss: 0.6019 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.90 - ETA: 0s - loss: 0.4044 - accuracy: 0.80 - ETA: 0s - loss: 0.4803 - accuracy: 0.79 - ETA: 0s - loss: 0.4853 - accuracy: 0.79 - ETA: 0s - loss: 0.4923 - accuracy: 0.78 - ETA: 0s - loss: 0.4955 - accuracy: 0.77 - 0s 4ms/step - loss: 0.4937 - accuracy: 0.7794 - val_loss: 0.5930 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3885 - accuracy: 0.84 - ETA: 0s - loss: 0.4504 - accuracy: 0.80 - ETA: 0s - loss: 0.4562 - accuracy: 0.81 - ETA: 0s - loss: 0.4586 - accuracy: 0.81 - ETA: 0s - loss: 0.4537 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4574 - accuracy: 0.8130 - val_loss: 0.6141 - val_accuracy: 0.7194\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.93 - ETA: 0s - loss: 0.3896 - accuracy: 0.81 - ETA: 0s - loss: 0.4274 - accuracy: 0.82 - ETA: 0s - loss: 0.4389 - accuracy: 0.81 - ETA: 0s - loss: 0.4445 - accuracy: 0.81 - ETA: 0s - loss: 0.4438 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4484 - accuracy: 0.8130 - val_loss: 0.6444 - val_accuracy: 0.6866\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.90 - ETA: 0s - loss: 0.5041 - accuracy: 0.80 - ETA: 0s - loss: 0.5870 - accuracy: 0.79 - ETA: 0s - loss: 0.5654 - accuracy: 0.80 - ETA: 0s - loss: 0.5340 - accuracy: 0.80 - ETA: 0s - loss: 0.5123 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5023 - accuracy: 0.8111 - val_loss: 0.9625 - val_accuracy: 0.7075\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.81 - ETA: 0s - loss: 0.4466 - accuracy: 0.80 - ETA: 0s - loss: 0.4355 - accuracy: 0.82 - ETA: 0s - loss: 0.4297 - accuracy: 0.82 - ETA: 0s - loss: 0.4306 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8283 - val_loss: 0.8527 - val_accuracy: 0.6970\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.93 - ETA: 0s - loss: 0.4394 - accuracy: 0.83 - ETA: 0s - loss: 0.4153 - accuracy: 0.84 - ETA: 0s - loss: 0.4102 - accuracy: 0.85 - ETA: 0s - loss: 0.4044 - accuracy: 0.84 - ETA: 0s - loss: 0.4031 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3985 - accuracy: 0.8458 - val_loss: 0.7993 - val_accuracy: 0.6925\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.87 - ETA: 0s - loss: 0.3859 - accuracy: 0.86 - ETA: 0s - loss: 0.3641 - accuracy: 0.86 - ETA: 0s - loss: 0.3789 - accuracy: 0.85 - ETA: 0s - loss: 0.3688 - accuracy: 0.86 - ETA: 0s - loss: 0.3625 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3614 - accuracy: 0.8589 - val_loss: 0.9792 - val_accuracy: 0.6687\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.81 - ETA: 0s - loss: 0.2684 - accuracy: 0.88 - ETA: 0s - loss: 0.2743 - accuracy: 0.89 - ETA: 0s - loss: 0.2978 - accuracy: 0.87 - ETA: 0s - loss: 0.3555 - accuracy: 0.86 - ETA: 0s - loss: 0.3979 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4120 - accuracy: 0.8466 - val_loss: 1.9344 - val_accuracy: 0.7328\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.84 - ETA: 0s - loss: 0.7196 - accuracy: 0.76 - ETA: 0s - loss: 0.8083 - accuracy: 0.73 - ETA: 0s - loss: 0.8083 - accuracy: 0.72 - ETA: 0s - loss: 0.7997 - accuracy: 0.71 - ETA: 0s - loss: 0.7792 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7728 - accuracy: 0.7074 - val_loss: 0.6560 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7390 - accuracy: 0.65 - ETA: 0s - loss: 0.7166 - accuracy: 0.67 - ETA: 0s - loss: 0.7098 - accuracy: 0.68 - ETA: 0s - loss: 0.7170 - accuracy: 0.70 - ETA: 0s - loss: 0.7151 - accuracy: 0.69 - ETA: 0s - loss: 0.7107 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7090 - accuracy: 0.6973 - val_loss: 0.6853 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7236 - accuracy: 0.65 - ETA: 0s - loss: 0.7092 - accuracy: 0.67 - ETA: 0s - loss: 0.6984 - accuracy: 0.55 - ETA: 0s - loss: 0.6995 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.46 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5495 - val_loss: 0.6890 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7651 - accuracy: 0.59 - ETA: 0s - loss: 0.6789 - accuracy: 0.72 - ETA: 0s - loss: 0.6939 - accuracy: 0.69 - ETA: 0s - loss: 0.6926 - accuracy: 0.70 - ETA: 0s - loss: 0.6913 - accuracy: 0.70 - ETA: 0s - loss: 0.6927 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.6876 - val_loss: 0.6941 - val_accuracy: 0.3045\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.28 - ETA: 0s - loss: 0.6981 - accuracy: 0.30 - ETA: 0s - loss: 0.6927 - accuracy: 0.30 - ETA: 0s - loss: 0.6976 - accuracy: 0.30 - ETA: 0s - loss: 0.6933 - accuracy: 0.30 - ETA: 0s - loss: 0.6928 - accuracy: 0.35 - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4020 - val_loss: 0.6915 - val_accuracy: 0.6955\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.68 - ETA: 0s - loss: 0.7060 - accuracy: 0.68 - ETA: 0s - loss: 0.7029 - accuracy: 0.50 - ETA: 0s - loss: 0.7005 - accuracy: 0.44 - ETA: 0s - loss: 0.6930 - accuracy: 0.39 - ETA: 0s - loss: 0.6905 - accuracy: 0.45 - 0s 4ms/step - loss: 0.6936 - accuracy: 0.4647 - val_loss: 0.6860 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.68 - ETA: 0s - loss: 0.6783 - accuracy: 0.72 - ETA: 0s - loss: 0.6895 - accuracy: 0.70 - ETA: 0s - loss: 0.6965 - accuracy: 0.69 - ETA: 0s - loss: 0.6946 - accuracy: 0.59 - ETA: 0s - loss: 0.6969 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5166 - val_loss: 0.6968 - val_accuracy: 0.3045\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7006 - accuracy: 0.31 - ETA: 0s - loss: 0.7008 - accuracy: 0.30 - ETA: 0s - loss: 0.7021 - accuracy: 0.30 - ETA: 0s - loss: 0.7618 - accuracy: 0.32 - ETA: 0s - loss: 0.7463 - accuracy: 0.31 - ETA: 0s - loss: 0.7313 - accuracy: 0.30 - 0s 4ms/step - loss: 0.7268 - accuracy: 0.3124 - val_loss: 0.6907 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7010 - accuracy: 0.68 - ETA: 0s - loss: 0.6848 - accuracy: 0.71 - ETA: 0s - loss: 0.6902 - accuracy: 0.70 - ETA: 0s - loss: 0.6953 - accuracy: 0.69 - ETA: 0s - loss: 0.6950 - accuracy: 0.66 - ETA: 0s - loss: 0.6956 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5976 - val_loss: 0.6923 - val_accuracy: 0.6955\n",
      "Epoch 20/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.7422 - accuracy: 0.62 - ETA: 0s - loss: 0.7069 - accuracy: 0.45 - ETA: 0s - loss: 0.6971 - accuracy: 0.37 - ETA: 0s - loss: 0.6935 - accuracy: 0.36 - ETA: 0s - loss: 0.6942 - accuracy: 0.45 - ETA: 0s - loss: 0.6917 - accuracy: 0.5078Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5140 - val_loss: 0.6896 - val_accuracy: 0.6955\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.75 - ETA: 0s - loss: 1.5918 - accuracy: 0.68 - ETA: 0s - loss: 1.1185 - accuracy: 0.69 - ETA: 0s - loss: 0.9340 - accuracy: 0.71 - ETA: 0s - loss: 0.8480 - accuracy: 0.72 - ETA: 0s - loss: 0.7974 - accuracy: 0.71 - 0s 5ms/step - loss: 0.7756 - accuracy: 0.7186 - val_loss: 0.6243 - val_accuracy: 0.6866\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.78 - ETA: 0s - loss: 0.5304 - accuracy: 0.76 - ETA: 0s - loss: 0.5184 - accuracy: 0.75 - ETA: 0s - loss: 0.5071 - accuracy: 0.76 - ETA: 0s - loss: 0.5214 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5269 - accuracy: 0.7619 - val_loss: 0.6675 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.87 - ETA: 0s - loss: 0.4405 - accuracy: 0.77 - ETA: 0s - loss: 0.4240 - accuracy: 0.79 - ETA: 0s - loss: 0.4428 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7981 - val_loss: 0.5766 - val_accuracy: 0.7164\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.87 - ETA: 0s - loss: 0.4137 - accuracy: 0.83 - ETA: 0s - loss: 0.3926 - accuracy: 0.84 - ETA: 0s - loss: 0.4204 - accuracy: 0.82 - ETA: 0s - loss: 0.4353 - accuracy: 0.81 - ETA: 0s - loss: 0.4387 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4387 - accuracy: 0.8167 - val_loss: 0.6364 - val_accuracy: 0.6657\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.96 - ETA: 0s - loss: 0.4096 - accuracy: 0.78 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4022 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5049 - accuracy: 0.8137 - val_loss: 0.7653 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.81 - ETA: 0s - loss: 0.5383 - accuracy: 0.80 - ETA: 0s - loss: 0.5524 - accuracy: 0.79 - ETA: 0s - loss: 0.5162 - accuracy: 0.80 - ETA: 0s - loss: 0.5076 - accuracy: 0.80 - ETA: 0s - loss: 0.5210 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5196 - accuracy: 0.8029 - val_loss: 2.3021 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4137 - accuracy: 0.75 - ETA: 0s - loss: 0.8486 - accuracy: 0.72 - ETA: 0s - loss: 0.7780 - accuracy: 0.74 - ETA: 0s - loss: 0.8014 - accuracy: 0.73 - ETA: 0s - loss: 0.7375 - accuracy: 0.75 - ETA: 0s - loss: 0.7018 - accuracy: 0.75 - 0s 4ms/step - loss: 0.7018 - accuracy: 0.7589 - val_loss: 0.7085 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4077 - accuracy: 0.87 - ETA: 0s - loss: 0.5143 - accuracy: 0.81 - ETA: 0s - loss: 0.5141 - accuracy: 0.83 - ETA: 0s - loss: 0.6181 - accuracy: 0.83 - ETA: 0s - loss: 0.6162 - accuracy: 0.82 - ETA: 0s - loss: 0.6024 - accuracy: 0.82 - 0s 4ms/step - loss: 0.6024 - accuracy: 0.8219 - val_loss: 0.7114 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.90 - ETA: 0s - loss: 0.4518 - accuracy: 0.83 - ETA: 0s - loss: 1.1621 - accuracy: 0.81 - ETA: 0s - loss: 1.1310 - accuracy: 0.80 - ETA: 0s - loss: 1.0353 - accuracy: 0.78 - ETA: 0s - loss: 0.9821 - accuracy: 0.77 - 0s 5ms/step - loss: 0.9574 - accuracy: 0.7779 - val_loss: 0.6876 - val_accuracy: 0.7478\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7445 - accuracy: 0.71 - ETA: 0s - loss: 0.6421 - accuracy: 0.76 - ETA: 0s - loss: 0.6139 - accuracy: 0.78 - ETA: 0s - loss: 0.6199 - accuracy: 0.78 - ETA: 0s - loss: 0.6139 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6149 - accuracy: 0.7869 - val_loss: 0.7047 - val_accuracy: 0.7522\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.84 - ETA: 0s - loss: 0.5734 - accuracy: 0.79 - ETA: 0s - loss: 0.5968 - accuracy: 0.78 - ETA: 0s - loss: 0.5951 - accuracy: 0.78 - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5855 - accuracy: 0.7891 - val_loss: 0.8081 - val_accuracy: 0.7642\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.90 - ETA: 0s - loss: 0.5869 - accuracy: 0.82 - ETA: 0s - loss: 0.5754 - accuracy: 0.81 - ETA: 0s - loss: 0.5901 - accuracy: 0.79 - ETA: 0s - loss: 0.5920 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5940 - accuracy: 0.7913 - val_loss: 0.6627 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6264 - accuracy: 0.75 - ETA: 0s - loss: 0.6599 - accuracy: 0.75 - ETA: 0s - loss: 0.6181 - accuracy: 0.77 - ETA: 0s - loss: 0.5909 - accuracy: 0.79 - ETA: 0s - loss: 0.5940 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5882 - accuracy: 0.7954 - val_loss: 0.7432 - val_accuracy: 0.7552\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.87 - ETA: 0s - loss: 0.6401 - accuracy: 0.81 - ETA: 0s - loss: 0.7123 - accuracy: 0.81 - ETA: 0s - loss: 0.6789 - accuracy: 0.82 - ETA: 0s - loss: 0.6254 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6163 - accuracy: 0.8294 - val_loss: 0.9632 - val_accuracy: 0.7104\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.71 - ETA: 0s - loss: 0.4778 - accuracy: 0.84 - ETA: 0s - loss: 0.5191 - accuracy: 0.83 - ETA: 0s - loss: 0.5145 - accuracy: 0.83 - ETA: 0s - loss: 0.5212 - accuracy: 0.82 - ETA: 0s - loss: 0.5190 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5185 - accuracy: 0.8302 - val_loss: 1.1196 - val_accuracy: 0.7433\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.84 - ETA: 0s - loss: 0.5170 - accuracy: 0.83 - ETA: 0s - loss: 0.5213 - accuracy: 0.84 - ETA: 0s - loss: 0.5057 - accuracy: 0.84 - ETA: 0s - loss: 0.5258 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5366 - accuracy: 0.8365 - val_loss: 0.6771 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.84 - ETA: 0s - loss: 0.8192 - accuracy: 0.82 - ETA: 0s - loss: 0.6690 - accuracy: 0.82 - ETA: 0s - loss: 0.5949 - accuracy: 0.83 - ETA: 0s - loss: 0.5668 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5610 - accuracy: 0.8365 - val_loss: 0.7841 - val_accuracy: 0.7313\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6428 - accuracy: 0.75 - ETA: 0s - loss: 0.5077 - accuracy: 0.82 - ETA: 0s - loss: 0.4869 - accuracy: 0.84 - ETA: 0s - loss: 0.4678 - accuracy: 0.85 - ETA: 0s - loss: 0.4678 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4685 - accuracy: 0.8488 - val_loss: 0.7203 - val_accuracy: 0.7507\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.96 - ETA: 0s - loss: 0.4396 - accuracy: 0.86 - ETA: 0s - loss: 0.5013 - accuracy: 0.85 - ETA: 0s - loss: 0.5020 - accuracy: 0.84 - ETA: 0s - loss: 0.4968 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4931 - accuracy: 0.8485 - val_loss: 0.7552 - val_accuracy: 0.7358\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.81 - ETA: 0s - loss: 0.4285 - accuracy: 0.86 - ETA: 0s - loss: 0.4397 - accuracy: 0.86 - ETA: 0s - loss: 0.4517 - accuracy: 0.85 - ETA: 0s - loss: 0.4481 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8593 - val_loss: 1.1350 - val_accuracy: 0.7313\n",
      "Epoch 21/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.3554 - accuracy: 0.90 - ETA: 0s - loss: 0.4078 - accuracy: 0.88 - ETA: 0s - loss: 0.4126 - accuracy: 0.89 - ETA: 0s - loss: 0.4157 - accuracy: 0.88 - ETA: 0s - loss: 0.4460 - accuracy: 0.8764Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.8746 - val_loss: 1.2804 - val_accuracy: 0.7343\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.59 - ETA: 0s - loss: 2.3225 - accuracy: 0.62 - ETA: 0s - loss: 1.4198 - accuracy: 0.67 - ETA: 0s - loss: 1.1384 - accuracy: 0.68 - ETA: 0s - loss: 1.0018 - accuracy: 0.70 - 0s 5ms/step - loss: 0.9128 - accuracy: 0.7122 - val_loss: 0.6185 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.65 - ETA: 0s - loss: 0.5291 - accuracy: 0.76 - ETA: 0s - loss: 0.5075 - accuracy: 0.76 - ETA: 0s - loss: 0.5151 - accuracy: 0.76 - ETA: 0s - loss: 0.5195 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5161 - accuracy: 0.7686 - val_loss: 0.6262 - val_accuracy: 0.6896\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4096 - accuracy: 0.81 - ETA: 0s - loss: 0.3928 - accuracy: 0.84 - ETA: 0s - loss: 0.4718 - accuracy: 0.79 - ETA: 0s - loss: 0.4620 - accuracy: 0.79 - ETA: 0s - loss: 0.4606 - accuracy: 0.80 - ETA: 0s - loss: 0.4587 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4555 - accuracy: 0.7996 - val_loss: 0.6312 - val_accuracy: 0.6940\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.3464 - accuracy: 0.85 - ETA: 0s - loss: 0.3690 - accuracy: 0.84 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.4080 - accuracy: 0.80 - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8122 - val_loss: 0.7702 - val_accuracy: 0.7134\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.87 - ETA: 0s - loss: 0.4848 - accuracy: 0.81 - ETA: 0s - loss: 0.5284 - accuracy: 0.80 - ETA: 0s - loss: 0.5132 - accuracy: 0.80 - ETA: 0s - loss: 0.5148 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7999 - val_loss: 1.6992 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.90 - ETA: 0s - loss: 0.6900 - accuracy: 0.77 - ETA: 0s - loss: 0.5863 - accuracy: 0.80 - ETA: 0s - loss: 0.5670 - accuracy: 0.80 - ETA: 0s - loss: 0.5630 - accuracy: 0.81 - ETA: 0s - loss: 0.5531 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5533 - accuracy: 0.8182 - val_loss: 0.7252 - val_accuracy: 0.6627\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.81 - ETA: 0s - loss: 0.4910 - accuracy: 0.78 - ETA: 0s - loss: 0.4490 - accuracy: 0.81 - ETA: 0s - loss: 0.4511 - accuracy: 0.82 - ETA: 0s - loss: 0.4407 - accuracy: 0.82 - ETA: 0s - loss: 0.4318 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8369 - val_loss: 0.7454 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.90 - ETA: 0s - loss: 0.4752 - accuracy: 0.84 - ETA: 0s - loss: 0.4321 - accuracy: 0.84 - ETA: 0s - loss: 0.4204 - accuracy: 0.85 - ETA: 0s - loss: 0.4229 - accuracy: 0.85 - ETA: 0s - loss: 0.5140 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5120 - accuracy: 0.8458 - val_loss: 1.0148 - val_accuracy: 0.6612\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.81 - ETA: 0s - loss: 0.4551 - accuracy: 0.82 - ETA: 0s - loss: 0.4376 - accuracy: 0.84 - ETA: 0s - loss: 0.4356 - accuracy: 0.85 - ETA: 0s - loss: 0.4181 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4107 - accuracy: 0.8593 - val_loss: 0.8963 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.84 - ETA: 0s - loss: 0.3834 - accuracy: 0.85 - ETA: 0s - loss: 0.3862 - accuracy: 0.87 - ETA: 0s - loss: 0.4033 - accuracy: 0.87 - ETA: 0s - loss: 0.4007 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8701 - val_loss: 1.1156 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.90 - ETA: 0s - loss: 0.3601 - accuracy: 0.89 - ETA: 0s - loss: 0.3677 - accuracy: 0.88 - ETA: 0s - loss: 0.3710 - accuracy: 0.88 - ETA: 0s - loss: 0.3756 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8850 - val_loss: 0.9838 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.93 - ETA: 0s - loss: 0.3169 - accuracy: 0.90 - ETA: 0s - loss: 0.3226 - accuracy: 0.89 - ETA: 0s - loss: 0.3335 - accuracy: 0.89 - ETA: 0s - loss: 0.3303 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8955 - val_loss: 0.8327 - val_accuracy: 0.7209\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.93 - ETA: 0s - loss: 0.3226 - accuracy: 0.89 - ETA: 0s - loss: 0.3196 - accuracy: 0.90 - ETA: 0s - loss: 0.3273 - accuracy: 0.89 - ETA: 0s - loss: 0.3189 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8970 - val_loss: 0.8057 - val_accuracy: 0.7209\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.93 - ETA: 0s - loss: 0.2842 - accuracy: 0.91 - ETA: 0s - loss: 0.3058 - accuracy: 0.90 - ETA: 0s - loss: 0.2957 - accuracy: 0.91 - ETA: 0s - loss: 0.2988 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3024 - accuracy: 0.9119 - val_loss: 1.3013 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.90 - ETA: 0s - loss: 0.3504 - accuracy: 0.88 - ETA: 0s - loss: 0.3737 - accuracy: 0.88 - ETA: 0s - loss: 0.3841 - accuracy: 0.87 - ETA: 0s - loss: 0.3706 - accuracy: 0.88 - ETA: 0s - loss: 0.3611 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3611 - accuracy: 0.8847 - val_loss: 1.6513 - val_accuracy: 0.6940\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.93 - ETA: 0s - loss: 0.3218 - accuracy: 0.90 - ETA: 0s - loss: 0.3217 - accuracy: 0.91 - ETA: 0s - loss: 0.3055 - accuracy: 0.91 - ETA: 0s - loss: 0.3131 - accuracy: 0.91 - ETA: 0s - loss: 0.3126 - accuracy: 0.91 - 0s 4ms/step - loss: 0.3127 - accuracy: 0.9160 - val_loss: 1.7122 - val_accuracy: 0.7134\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.84 - ETA: 0s - loss: 0.2918 - accuracy: 0.93 - ETA: 0s - loss: 0.2342 - accuracy: 0.94 - ETA: 0s - loss: 0.2907 - accuracy: 0.92 - ETA: 0s - loss: 0.3136 - accuracy: 0.91 - ETA: 0s - loss: 0.3106 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3086 - accuracy: 0.9108 - val_loss: 1.0524 - val_accuracy: 0.7299\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.84 - ETA: 0s - loss: 0.2518 - accuracy: 0.91 - ETA: 0s - loss: 0.2577 - accuracy: 0.91 - ETA: 0s - loss: 0.2649 - accuracy: 0.91 - ETA: 0s - loss: 0.2666 - accuracy: 0.91 - 0s 3ms/step - loss: 0.2645 - accuracy: 0.9194 - val_loss: 1.7581 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.87 - ETA: 0s - loss: 0.3724 - accuracy: 0.93 - ETA: 0s - loss: 0.2923 - accuracy: 0.94 - ETA: 0s - loss: 0.2534 - accuracy: 0.94 - ETA: 0s - loss: 0.2722 - accuracy: 0.93 - 0s 3ms/step - loss: 0.2869 - accuracy: 0.9295 - val_loss: 1.6180 - val_accuracy: 0.7090\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.84 - ETA: 0s - loss: 0.3099 - accuracy: 0.90 - ETA: 0s - loss: 0.2642 - accuracy: 0.91 - ETA: 0s - loss: 0.2626 - accuracy: 0.92 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - 0s 3ms/step - loss: 0.2637 - accuracy: 0.9186 - val_loss: 0.7175 - val_accuracy: 0.7373\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.90 - ETA: 0s - loss: 0.2784 - accuracy: 0.94 - ETA: 0s - loss: 0.2588 - accuracy: 0.93 - ETA: 0s - loss: 0.2491 - accuracy: 0.93 - ETA: 0s - loss: 0.2729 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2738 - accuracy: 0.9212 - val_loss: 0.9946 - val_accuracy: 0.7239\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 1.00 - ETA: 0s - loss: 0.2154 - accuracy: 0.94 - ETA: 0s - loss: 0.2533 - accuracy: 0.93 - ETA: 0s - loss: 0.2973 - accuracy: 0.93 - ETA: 0s - loss: 0.2878 - accuracy: 0.93 - ETA: 0s - loss: 0.2943 - accuracy: 0.92 - 0s 4ms/step - loss: 0.3111 - accuracy: 0.9261 - val_loss: 1.0125 - val_accuracy: 0.7209\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.96 - ETA: 0s - loss: 0.2289 - accuracy: 0.94 - ETA: 0s - loss: 0.4857 - accuracy: 0.91 - ETA: 0s - loss: 0.5262 - accuracy: 0.90 - ETA: 0s - loss: 0.5766 - accuracy: 0.88 - 0s 3ms/step - loss: 0.6898 - accuracy: 0.8843 - val_loss: 2.5492 - val_accuracy: 0.6955\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4418 - accuracy: 0.90 - ETA: 0s - loss: 0.5227 - accuracy: 0.87 - ETA: 0s - loss: 0.4076 - accuracy: 0.89 - ETA: 0s - loss: 0.4663 - accuracy: 0.89 - ETA: 0s - loss: 0.5034 - accuracy: 0.89 - 0s 3ms/step - loss: 0.5175 - accuracy: 0.8865 - val_loss: 1.0165 - val_accuracy: 0.7403\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.81 - ETA: 0s - loss: 0.3649 - accuracy: 0.87 - ETA: 0s - loss: 0.3604 - accuracy: 0.88 - ETA: 0s - loss: 0.3255 - accuracy: 0.89 - ETA: 0s - loss: 0.3143 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3065 - accuracy: 0.9056 - val_loss: 1.0920 - val_accuracy: 0.7224\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 1.00 - ETA: 0s - loss: 0.2288 - accuracy: 0.92 - ETA: 0s - loss: 0.2782 - accuracy: 0.92 - ETA: 0s - loss: 0.2949 - accuracy: 0.91 - ETA: 0s - loss: 0.3062 - accuracy: 0.91 - 0s 3ms/step - loss: 0.3136 - accuracy: 0.9141 - val_loss: 1.6189 - val_accuracy: 0.7149\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.93 - ETA: 0s - loss: 0.2625 - accuracy: 0.93 - ETA: 0s - loss: 0.2481 - accuracy: 0.93 - ETA: 0s - loss: 0.2560 - accuracy: 0.93 - ETA: 0s - loss: 0.2715 - accuracy: 0.93 - 0s 3ms/step - loss: 0.3264 - accuracy: 0.9287 - val_loss: 1.0511 - val_accuracy: 0.7209\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.90 - ETA: 0s - loss: 0.3838 - accuracy: 0.89 - ETA: 0s - loss: 0.3647 - accuracy: 0.90 - ETA: 0s - loss: 0.3821 - accuracy: 0.90 - ETA: 0s - loss: 0.3874 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3846 - accuracy: 0.9052 - val_loss: 3.8011 - val_accuracy: 0.7343\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0385 - accuracy: 0.78 - ETA: 0s - loss: 0.5339 - accuracy: 0.87 - ETA: 0s - loss: 0.5470 - accuracy: 0.85 - ETA: 0s - loss: 0.5066 - accuracy: 0.86 - ETA: 0s - loss: 0.5045 - accuracy: 0.86 - ETA: 0s - loss: 0.5180 - accuracy: 0.85 - 0s 4ms/step - loss: 0.5471 - accuracy: 0.8552 - val_loss: 1.6087 - val_accuracy: 0.7224\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7118 - accuracy: 0.71 - ETA: 0s - loss: 0.5905 - accuracy: 0.81 - ETA: 0s - loss: 0.5436 - accuracy: 0.82 - ETA: 0s - loss: 0.5272 - accuracy: 0.82 - ETA: 0s - loss: 0.5155 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5098 - accuracy: 0.8384 - val_loss: 1.2541 - val_accuracy: 0.7373\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.90 - ETA: 0s - loss: 0.4875 - accuracy: 0.84 - ETA: 0s - loss: 0.4462 - accuracy: 0.86 - ETA: 0s - loss: 0.4548 - accuracy: 0.86 - ETA: 0s - loss: 0.4576 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4680 - accuracy: 0.8600 - val_loss: 1.4163 - val_accuracy: 0.7448\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.87 - ETA: 0s - loss: 0.4990 - accuracy: 0.83 - ETA: 0s - loss: 0.4919 - accuracy: 0.84 - ETA: 0s - loss: 0.4885 - accuracy: 0.84 - ETA: 0s - loss: 0.4799 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4747 - accuracy: 0.8526 - val_loss: 0.9377 - val_accuracy: 0.7642\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.96 - ETA: 0s - loss: 0.4468 - accuracy: 0.86 - ETA: 0s - loss: 0.4341 - accuracy: 0.87 - ETA: 0s - loss: 0.4459 - accuracy: 0.87 - ETA: 0s - loss: 0.4450 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4667 - accuracy: 0.8705 - val_loss: 1.0488 - val_accuracy: 0.7388\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.90 - ETA: 0s - loss: 0.4785 - accuracy: 0.85 - ETA: 0s - loss: 0.4700 - accuracy: 0.85 - ETA: 0s - loss: 0.4723 - accuracy: 0.86 - ETA: 0s - loss: 0.4632 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8608 - val_loss: 2.4396 - val_accuracy: 0.7418\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.87 - ETA: 0s - loss: 0.4254 - accuracy: 0.87 - ETA: 0s - loss: 0.4516 - accuracy: 0.86 - ETA: 0s - loss: 0.4643 - accuracy: 0.85 - ETA: 0s - loss: 0.4839 - accuracy: 0.84 - ETA: 0s - loss: 0.4781 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4791 - accuracy: 0.8526 - val_loss: 0.6426 - val_accuracy: 0.7507\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.90 - ETA: 0s - loss: 0.4022 - accuracy: 0.88 - ETA: 0s - loss: 0.4000 - accuracy: 0.88 - ETA: 0s - loss: 0.3973 - accuracy: 0.89 - ETA: 0s - loss: 0.4083 - accuracy: 0.88 - ETA: 0s - loss: 0.4024 - accuracy: 0.89 - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8903 - val_loss: 1.1213 - val_accuracy: 0.7552\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.96 - ETA: 0s - loss: 0.3357 - accuracy: 0.91 - ETA: 0s - loss: 0.8654 - accuracy: 0.89 - ETA: 0s - loss: 0.8357 - accuracy: 0.83 - ETA: 0s - loss: 0.7805 - accuracy: 0.83 - ETA: 0s - loss: 0.7419 - accuracy: 0.83 - 0s 4ms/step - loss: 0.7419 - accuracy: 0.8283 - val_loss: 1.9347 - val_accuracy: 0.7164\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.81 - ETA: 0s - loss: 0.7189 - accuracy: 0.72 - ETA: 0s - loss: 0.7027 - accuracy: 0.71 - ETA: 0s - loss: 0.6928 - accuracy: 0.71 - ETA: 0s - loss: 0.6619 - accuracy: 0.74 - ETA: 0s - loss: 0.6747 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6747 - accuracy: 0.7533 - val_loss: 3.2259 - val_accuracy: 0.7164\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.75 - ETA: 0s - loss: 0.6820 - accuracy: 0.70 - ETA: 0s - loss: 0.6643 - accuracy: 0.72 - ETA: 0s - loss: 0.6552 - accuracy: 0.73 - ETA: 0s - loss: 0.6543 - accuracy: 0.73 - ETA: 0s - loss: 0.6590 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6581 - accuracy: 0.7301 - val_loss: 3.0775 - val_accuracy: 0.7075\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.81 - ETA: 0s - loss: 0.6774 - accuracy: 0.70 - ETA: 0s - loss: 0.7869 - accuracy: 0.71 - ETA: 0s - loss: 0.7219 - accuracy: 0.73 - ETA: 0s - loss: 0.6969 - accuracy: 0.74 - ETA: 0s - loss: 0.6780 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6758 - accuracy: 0.7521 - val_loss: 1.1453 - val_accuracy: 0.7358\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.75 - ETA: 0s - loss: 0.6284 - accuracy: 0.75 - ETA: 0s - loss: 0.6242 - accuracy: 0.75 - ETA: 0s - loss: 0.6198 - accuracy: 0.76 - ETA: 0s - loss: 0.6208 - accuracy: 0.75 - ETA: 0s - loss: 0.6123 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6110 - accuracy: 0.7682 - val_loss: 3.0526 - val_accuracy: 0.7493\n",
      "Epoch 42/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6314 - accuracy: 0.71 - ETA: 0s - loss: 0.6130 - accuracy: 0.77 - ETA: 0s - loss: 0.5865 - accuracy: 0.79 - ETA: 0s - loss: 0.5826 - accuracy: 0.79 - ETA: 0s - loss: 0.5827 - accuracy: 0.79 - ETA: 0s - loss: 0.5894 - accuracy: 0.79 - ETA: 0s - loss: 0.5891 - accuracy: 0.7937Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6018 - accuracy: 0.7962 - val_loss: 3.2099 - val_accuracy: 0.7313\n",
      "Epoch 00042: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 33d56ea1066d8ed8bb83135f91290df5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.753731350104014</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.24262330713561298</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7427 - accuracy: 0.75 - ETA: 0s - loss: 1.8702 - accuracy: 0.67 - ETA: 0s - loss: 1.2631 - accuracy: 0.69 - ETA: 0s - loss: 1.0679 - accuracy: 0.69 - 0s 4ms/step - loss: 1.0307 - accuracy: 0.7003 - val_loss: 0.6807 - val_accuracy: 0.6746\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.68 - ETA: 0s - loss: 0.6055 - accuracy: 0.75 - ETA: 0s - loss: 0.5929 - accuracy: 0.75 - ETA: 0s - loss: 0.5891 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5891 - accuracy: 0.7514 - val_loss: 0.5642 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.81 - ETA: 0s - loss: 0.5479 - accuracy: 0.78 - ETA: 0s - loss: 0.5393 - accuracy: 0.77 - ETA: 0s - loss: 0.5393 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5540 - accuracy: 0.7775 - val_loss: 0.5535 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.84 - ETA: 0s - loss: 0.5654 - accuracy: 0.78 - ETA: 0s - loss: 0.5809 - accuracy: 0.77 - ETA: 0s - loss: 0.5669 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7880 - val_loss: 0.6468 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.93 - ETA: 0s - loss: 0.5049 - accuracy: 0.81 - ETA: 0s - loss: 0.5333 - accuracy: 0.80 - ETA: 0s - loss: 0.5220 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5318 - accuracy: 0.8044 - val_loss: 0.6245 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.78 - ETA: 0s - loss: 0.4921 - accuracy: 0.81 - ETA: 0s - loss: 0.5485 - accuracy: 0.80 - ETA: 0s - loss: 0.5424 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5410 - accuracy: 0.8044 - val_loss: 0.6421 - val_accuracy: 0.7209\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.84 - ETA: 0s - loss: 0.4526 - accuracy: 0.84 - ETA: 0s - loss: 0.5208 - accuracy: 0.83 - ETA: 0s - loss: 0.5078 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5123 - accuracy: 0.8298 - val_loss: 0.5880 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.90 - ETA: 0s - loss: 0.4377 - accuracy: 0.84 - ETA: 0s - loss: 0.4578 - accuracy: 0.84 - ETA: 0s - loss: 0.4559 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4647 - accuracy: 0.8391 - val_loss: 0.5717 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4040 - accuracy: 0.87 - ETA: 0s - loss: 0.4113 - accuracy: 0.85 - ETA: 0s - loss: 0.4276 - accuracy: 0.84 - ETA: 0s - loss: 0.4384 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8384 - val_loss: 0.7212 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.81 - ETA: 0s - loss: 0.4217 - accuracy: 0.85 - ETA: 0s - loss: 0.4312 - accuracy: 0.85 - ETA: 0s - loss: 0.4411 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8593 - val_loss: 0.7831 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.87 - ETA: 0s - loss: 0.4547 - accuracy: 0.85 - ETA: 0s - loss: 0.4782 - accuracy: 0.84 - ETA: 0s - loss: 0.5094 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5367 - accuracy: 0.8343 - val_loss: 0.7090 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.87 - ETA: 0s - loss: 0.6267 - accuracy: 0.81 - ETA: 0s - loss: 0.6451 - accuracy: 0.81 - ETA: 0s - loss: 0.5815 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5701 - accuracy: 0.8302 - val_loss: 0.8667 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3660 - accuracy: 0.87 - ETA: 0s - loss: 0.4239 - accuracy: 0.86 - ETA: 0s - loss: 0.4719 - accuracy: 0.86 - ETA: 0s - loss: 0.4640 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4684 - accuracy: 0.8567 - val_loss: 0.9138 - val_accuracy: 0.7015\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.90 - ETA: 0s - loss: 0.3965 - accuracy: 0.87 - ETA: 0s - loss: 0.4045 - accuracy: 0.86 - ETA: 0s - loss: 0.4374 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8652 - val_loss: 0.9824 - val_accuracy: 0.7179\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.87 - ETA: 0s - loss: 0.4277 - accuracy: 0.86 - ETA: 0s - loss: 0.4238 - accuracy: 0.86 - ETA: 0s - loss: 0.4150 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8694 - val_loss: 1.1535 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.90 - ETA: 0s - loss: 0.3805 - accuracy: 0.88 - ETA: 0s - loss: 0.3731 - accuracy: 0.88 - ETA: 0s - loss: 0.4015 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4026 - accuracy: 0.8794 - val_loss: 0.6551 - val_accuracy: 0.7269\n",
      "Epoch 17/50\n",
      "67/84 [======================>.......] - ETA: 0s - loss: 0.1734 - accuracy: 1.00 - ETA: 0s - loss: 0.5488 - accuracy: 0.87 - ETA: 0s - loss: 0.4742 - accuracy: 0.87 - ETA: 0s - loss: 0.4750 - accuracy: 0.8666Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.8641 - val_loss: 0.8061 - val_accuracy: 0.7284\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.78 - ETA: 0s - loss: 1.3417 - accuracy: 0.65 - ETA: 0s - loss: 1.0078 - accuracy: 0.69 - ETA: 0s - loss: 0.8678 - accuracy: 0.71 - 0s 4ms/step - loss: 0.8615 - accuracy: 0.7092 - val_loss: 0.5985 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.78 - ETA: 0s - loss: 0.5927 - accuracy: 0.76 - ETA: 0s - loss: 0.5870 - accuracy: 0.76 - ETA: 0s - loss: 0.5968 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6016 - accuracy: 0.7570 - val_loss: 0.6362 - val_accuracy: 0.7448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.93 - ETA: 0s - loss: 0.5842 - accuracy: 0.79 - ETA: 0s - loss: 0.5709 - accuracy: 0.78 - ETA: 0s - loss: 0.5875 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5876 - accuracy: 0.7712 - val_loss: 0.5802 - val_accuracy: 0.7582\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.71 - ETA: 0s - loss: 0.5323 - accuracy: 0.80 - ETA: 0s - loss: 0.5688 - accuracy: 0.78 - ETA: 0s - loss: 0.5694 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5709 - accuracy: 0.7846 - val_loss: 0.7438 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.71 - ETA: 0s - loss: 0.5450 - accuracy: 0.80 - ETA: 0s - loss: 0.5409 - accuracy: 0.80 - ETA: 0s - loss: 0.5454 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5560 - accuracy: 0.7969 - val_loss: 0.5717 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5814 - accuracy: 0.87 - ETA: 0s - loss: 0.5188 - accuracy: 0.80 - ETA: 0s - loss: 0.5295 - accuracy: 0.80 - ETA: 0s - loss: 0.5630 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5500 - accuracy: 0.7999 - val_loss: 0.6384 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.96 - ETA: 0s - loss: 0.5407 - accuracy: 0.82 - ETA: 0s - loss: 0.5311 - accuracy: 0.82 - ETA: 0s - loss: 0.5266 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5220 - accuracy: 0.8242 - val_loss: 0.9668 - val_accuracy: 0.6910\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.81 - ETA: 0s - loss: 0.4985 - accuracy: 0.83 - ETA: 0s - loss: 0.5293 - accuracy: 0.83 - ETA: 0s - loss: 0.5219 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5235 - accuracy: 0.8294 - val_loss: 0.7782 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.87 - ETA: 0s - loss: 0.5017 - accuracy: 0.84 - ETA: 0s - loss: 0.4963 - accuracy: 0.83 - ETA: 0s - loss: 0.4834 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8417 - val_loss: 0.8846 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7380 - accuracy: 0.90 - ETA: 0s - loss: 0.5161 - accuracy: 0.82 - ETA: 0s - loss: 0.5105 - accuracy: 0.83 - ETA: 0s - loss: 0.5260 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5188 - accuracy: 0.8290 - val_loss: 0.7418 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.84 - ETA: 0s - loss: 0.4427 - accuracy: 0.86 - ETA: 0s - loss: 0.4567 - accuracy: 0.85 - ETA: 0s - loss: 0.4753 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4991 - accuracy: 0.8447 - val_loss: 0.7919 - val_accuracy: 0.7239\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.75 - ETA: 0s - loss: 0.5031 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - ETA: 0s - loss: 0.4710 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8425 - val_loss: 1.1679 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "72/84 [========================>.....] - ETA: 0s - loss: 0.6698 - accuracy: 0.78 - ETA: 0s - loss: 0.5651 - accuracy: 0.83 - ETA: 0s - loss: 0.5982 - accuracy: 0.83 - ETA: 0s - loss: 0.5807 - accuracy: 0.8312Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.8339 - val_loss: 0.7509 - val_accuracy: 0.7358\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2016 - accuracy: 0.53 - ETA: 0s - loss: 1.3315 - accuracy: 0.63 - ETA: 0s - loss: 0.9986 - accuracy: 0.66 - ETA: 0s - loss: 0.8787 - accuracy: 0.66 - 0s 4ms/step - loss: 0.8411 - accuracy: 0.6700 - val_loss: 0.5866 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7261 - accuracy: 0.50 - ETA: 0s - loss: 0.5625 - accuracy: 0.73 - ETA: 0s - loss: 0.5911 - accuracy: 0.73 - ETA: 0s - loss: 0.5921 - accuracy: 0.71 - 0s 2ms/step - loss: 0.5929 - accuracy: 0.7219 - val_loss: 0.6203 - val_accuracy: 0.7194\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.5046 - accuracy: 0.78 - ETA: 0s - loss: 0.5137 - accuracy: 0.78 - ETA: 0s - loss: 0.5300 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5387 - accuracy: 0.7809 - val_loss: 0.6636 - val_accuracy: 0.6716\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.93 - ETA: 0s - loss: 0.5474 - accuracy: 0.78 - ETA: 0s - loss: 0.5248 - accuracy: 0.79 - ETA: 0s - loss: 0.5486 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5407 - accuracy: 0.7824 - val_loss: 0.7828 - val_accuracy: 0.6672\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8801 - accuracy: 0.53 - ETA: 0s - loss: 0.4696 - accuracy: 0.80 - ETA: 0s - loss: 0.4811 - accuracy: 0.79 - ETA: 0s - loss: 0.5211 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5372 - accuracy: 0.7917 - val_loss: 0.6337 - val_accuracy: 0.7015\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.65 - ETA: 0s - loss: 0.6511 - accuracy: 0.75 - ETA: 0s - loss: 0.6178 - accuracy: 0.76 - ETA: 0s - loss: 0.6156 - accuracy: 0.76 - 0s 2ms/step - loss: 0.6157 - accuracy: 0.7693 - val_loss: 0.7197 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.84 - ETA: 0s - loss: 0.5790 - accuracy: 0.78 - ETA: 0s - loss: 0.5415 - accuracy: 0.80 - ETA: 0s - loss: 0.5326 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5301 - accuracy: 0.8066 - val_loss: 0.7202 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.65 - ETA: 0s - loss: 0.5868 - accuracy: 0.81 - ETA: 0s - loss: 0.5441 - accuracy: 0.81 - ETA: 0s - loss: 0.5013 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5053 - accuracy: 0.8261 - val_loss: 1.1578 - val_accuracy: 0.6836\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.78 - ETA: 0s - loss: 0.4553 - accuracy: 0.84 - ETA: 0s - loss: 0.4244 - accuracy: 0.84 - ETA: 0s - loss: 0.4551 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8391 - val_loss: 1.1866 - val_accuracy: 0.7194\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.93 - ETA: 0s - loss: 0.4968 - accuracy: 0.83 - ETA: 0s - loss: 0.5053 - accuracy: 0.83 - ETA: 0s - loss: 0.4963 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4847 - accuracy: 0.8387 - val_loss: 0.7700 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.93 - ETA: 0s - loss: 0.4844 - accuracy: 0.85 - ETA: 0s - loss: 0.5010 - accuracy: 0.85 - ETA: 0s - loss: 0.5029 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5041 - accuracy: 0.8421 - val_loss: 0.8772 - val_accuracy: 0.7164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.90 - ETA: 0s - loss: 0.4363 - accuracy: 0.85 - ETA: 0s - loss: 0.4627 - accuracy: 0.83 - ETA: 0s - loss: 0.5048 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4927 - accuracy: 0.8387 - val_loss: 0.7953 - val_accuracy: 0.7104\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.93 - ETA: 0s - loss: 0.4603 - accuracy: 0.86 - ETA: 0s - loss: 0.4329 - accuracy: 0.86 - ETA: 0s - loss: 0.4345 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8641 - val_loss: 0.7238 - val_accuracy: 0.7463\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.93 - ETA: 0s - loss: 0.3787 - accuracy: 0.87 - ETA: 0s - loss: 0.3868 - accuracy: 0.87 - ETA: 0s - loss: 0.3909 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8753 - val_loss: 0.9656 - val_accuracy: 0.7134\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.87 - ETA: 0s - loss: 0.5363 - accuracy: 0.86 - ETA: 0s - loss: 0.4795 - accuracy: 0.87 - ETA: 0s - loss: 0.4548 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4573 - accuracy: 0.8746 - val_loss: 0.9685 - val_accuracy: 0.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.87 - ETA: 0s - loss: 0.4897 - accuracy: 0.86 - ETA: 0s - loss: 0.4542 - accuracy: 0.86 - ETA: 0s - loss: 0.4422 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8664 - val_loss: 0.8543 - val_accuracy: 0.7224\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.90 - ETA: 0s - loss: 0.3608 - accuracy: 0.92 - ETA: 0s - loss: 0.3837 - accuracy: 0.90 - ETA: 0s - loss: 0.3927 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8817 - val_loss: 0.8032 - val_accuracy: 0.7179\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 1.00 - ETA: 0s - loss: 0.2763 - accuracy: 0.92 - ETA: 0s - loss: 0.3537 - accuracy: 0.89 - ETA: 0s - loss: 0.3474 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3517 - accuracy: 0.9000 - val_loss: 1.0972 - val_accuracy: 0.7224\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.6680 - accuracy: 0.93 - ETA: 0s - loss: 0.5021 - accuracy: 0.88 - ETA: 0s - loss: 0.4650 - accuracy: 0.88 - ETA: 0s - loss: 0.4427 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4395 - accuracy: 0.8813 - val_loss: 0.9511 - val_accuracy: 0.7179\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.96 - ETA: 0s - loss: 0.3801 - accuracy: 0.88 - ETA: 0s - loss: 0.4051 - accuracy: 0.88 - ETA: 0s - loss: 0.3999 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8813 - val_loss: 0.8851 - val_accuracy: 0.7149\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.81 - ETA: 0s - loss: 0.3630 - accuracy: 0.88 - ETA: 0s - loss: 0.3330 - accuracy: 0.89 - ETA: 0s - loss: 0.3224 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3226 - accuracy: 0.9059 - val_loss: 1.3152 - val_accuracy: 0.7045\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1686 - accuracy: 0.87 - ETA: 0s - loss: 0.3762 - accuracy: 0.91 - ETA: 0s - loss: 0.3533 - accuracy: 0.90 - ETA: 0s - loss: 0.3389 - accuracy: 0.90 - 0s 3ms/step - loss: 0.3323 - accuracy: 0.9056 - val_loss: 0.9093 - val_accuracy: 0.7343\n",
      "Epoch 23/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 3.1041 - accuracy: 0.93 - ETA: 0s - loss: 0.5304 - accuracy: 0.90 - ETA: 0s - loss: 0.4254 - accuracy: 0.90 - ETA: 0s - loss: 0.3783 - accuracy: 0.9140Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.9093 - val_loss: 1.5076 - val_accuracy: 0.6955\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 12ab03dcb08f35772986284efede47ca</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7482586900393168</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.558351623355178</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.71 - ETA: 0s - loss: 1.0272 - accuracy: 0.67 - ETA: 0s - loss: 0.8294 - accuracy: 0.66 - 0s 3ms/step - loss: 0.7607 - accuracy: 0.6820 - val_loss: 0.5328 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.75 - ETA: 0s - loss: 0.5491 - accuracy: 0.74 - ETA: 0s - loss: 0.5319 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7495 - val_loss: 0.5744 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.78 - ETA: 0s - loss: 0.4787 - accuracy: 0.77 - ETA: 0s - loss: 0.4773 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7854 - val_loss: 0.6639 - val_accuracy: 0.7015\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.81 - ETA: 0s - loss: 0.4232 - accuracy: 0.79 - ETA: 0s - loss: 0.4390 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4410 - accuracy: 0.7872 - val_loss: 0.6341 - val_accuracy: 0.7284\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.87 - ETA: 0s - loss: 0.3363 - accuracy: 0.82 - ETA: 0s - loss: 0.3545 - accuracy: 0.82 - ETA: 0s - loss: 0.3989 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8085 - val_loss: 0.8388 - val_accuracy: 0.6627\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.81 - ETA: 0s - loss: 0.3693 - accuracy: 0.81 - ETA: 0s - loss: 0.3919 - accuracy: 0.80 - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8119 - val_loss: 0.7602 - val_accuracy: 0.6597\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.78 - ETA: 0s - loss: 0.2940 - accuracy: 0.86 - ETA: 0s - loss: 0.3365 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8298 - val_loss: 1.2715 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.90 - ETA: 0s - loss: 0.3351 - accuracy: 0.84 - ETA: 0s - loss: 0.3601 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8384 - val_loss: 1.0357 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.90 - ETA: 0s - loss: 0.4276 - accuracy: 0.85 - ETA: 0s - loss: 0.4020 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8410 - val_loss: 1.0142 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.84 - ETA: 0s - loss: 0.3576 - accuracy: 0.85 - ETA: 0s - loss: 0.3846 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8317 - val_loss: 1.0569 - val_accuracy: 0.6821\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.93 - ETA: 0s - loss: 0.2913 - accuracy: 0.87 - ETA: 0s - loss: 0.3328 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8567 - val_loss: 1.2733 - val_accuracy: 0.7164\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.84 - ETA: 0s - loss: 0.3182 - accuracy: 0.87 - ETA: 0s - loss: 0.3411 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8600 - val_loss: 1.5231 - val_accuracy: 0.7194\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.78 - ETA: 0s - loss: 0.2988 - accuracy: 0.86 - ETA: 0s - loss: 0.3332 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8567 - val_loss: 2.1338 - val_accuracy: 0.7209\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.93 - ETA: 0s - loss: 0.3638 - accuracy: 0.86 - ETA: 0s - loss: 0.4270 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4754 - accuracy: 0.8559 - val_loss: 1.3159 - val_accuracy: 0.6866\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.84 - ETA: 0s - loss: 0.3843 - accuracy: 0.86 - ETA: 0s - loss: 0.4027 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8615 - val_loss: 1.0231 - val_accuracy: 0.7388\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.81 - ETA: 0s - loss: 0.3066 - accuracy: 0.90 - ETA: 0s - loss: 0.3383 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8835 - val_loss: 1.5358 - val_accuracy: 0.7075\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.78 - ETA: 0s - loss: 0.3211 - accuracy: 0.88 - ETA: 0s - loss: 0.3169 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8921 - val_loss: 3.8745 - val_accuracy: 0.7179\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.9332 - accuracy: 0.93 - ETA: 0s - loss: 0.4088 - accuracy: 0.87 - ETA: 0s - loss: 0.3206 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8903 - val_loss: 1.8605 - val_accuracy: 0.7030\n",
      "Epoch 19/50\n",
      "68/84 [=======================>......] - ETA: 0s - loss: 0.1592 - accuracy: 0.96 - ETA: 0s - loss: 0.2230 - accuracy: 0.92 - ETA: 0s - loss: 0.2333 - accuracy: 0.9196Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.9108 - val_loss: 3.4992 - val_accuracy: 0.7075\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8710 - accuracy: 0.56 - ETA: 0s - loss: 0.9378 - accuracy: 0.64 - ETA: 0s - loss: 0.7886 - accuracy: 0.67 - 0s 3ms/step - loss: 0.7432 - accuracy: 0.6805 - val_loss: 0.5881 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.78 - ETA: 0s - loss: 0.5210 - accuracy: 0.75 - ETA: 0s - loss: 0.5181 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7503 - val_loss: 0.5654 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.71 - ETA: 0s - loss: 0.4379 - accuracy: 0.82 - ETA: 0s - loss: 0.4542 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8081 - val_loss: 0.5713 - val_accuracy: 0.7149\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.81 - ETA: 0s - loss: 0.4277 - accuracy: 0.80 - ETA: 0s - loss: 0.4358 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7992 - val_loss: 0.6946 - val_accuracy: 0.6731\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4773 - accuracy: 0.78 - ETA: 0s - loss: 0.3755 - accuracy: 0.82 - ETA: 0s - loss: 0.3591 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8290 - val_loss: 0.8051 - val_accuracy: 0.6761\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.84 - ETA: 0s - loss: 0.3468 - accuracy: 0.84 - ETA: 0s - loss: 0.3552 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8417 - val_loss: 0.9023 - val_accuracy: 0.6806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.87 - ETA: 0s - loss: 0.3350 - accuracy: 0.83 - ETA: 0s - loss: 0.3175 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8470 - val_loss: 0.9140 - val_accuracy: 0.6657\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.84 - ETA: 0s - loss: 0.3083 - accuracy: 0.85 - ETA: 0s - loss: 0.3062 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8559 - val_loss: 1.2410 - val_accuracy: 0.6806\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.78 - ETA: 0s - loss: 0.3273 - accuracy: 0.85 - ETA: 0s - loss: 0.2905 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8682 - val_loss: 0.6985 - val_accuracy: 0.6687\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.93 - ETA: 0s - loss: 0.2584 - accuracy: 0.88 - ETA: 0s - loss: 0.2601 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8753 - val_loss: 1.6345 - val_accuracy: 0.7030\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0979 - accuracy: 0.84 - ETA: 0s - loss: 0.3055 - accuracy: 0.88 - ETA: 0s - loss: 0.2930 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8791 - val_loss: 1.3316 - val_accuracy: 0.6582\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.88 - ETA: 0s - loss: 0.2861 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8806 - val_loss: 1.8392 - val_accuracy: 0.6851\n",
      "Epoch 13/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.1271 - accuracy: 0.93 - ETA: 0s - loss: 0.3330 - accuracy: 0.88 - ETA: 0s - loss: 0.4021 - accuracy: 0.8446Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.8369 - val_loss: 1.5218 - val_accuracy: 0.6776\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1267 - accuracy: 0.59 - ETA: 0s - loss: 0.8360 - accuracy: 0.64 - ETA: 0s - loss: 0.7605 - accuracy: 0.65 - 0s 3ms/step - loss: 0.7305 - accuracy: 0.6644 - val_loss: 0.6611 - val_accuracy: 0.6657\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.59 - ETA: 0s - loss: 0.5132 - accuracy: 0.75 - ETA: 0s - loss: 0.5343 - accuracy: 0.75 - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7465 - val_loss: 0.6085 - val_accuracy: 0.7015\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.68 - ETA: 0s - loss: 0.4506 - accuracy: 0.80 - ETA: 0s - loss: 0.4851 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7842 - val_loss: 0.6207 - val_accuracy: 0.6925\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.87 - ETA: 0s - loss: 0.4364 - accuracy: 0.81 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4384 - accuracy: 0.8096 - val_loss: 0.6129 - val_accuracy: 0.6836\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.84 - ETA: 0s - loss: 0.3384 - accuracy: 0.82 - ETA: 0s - loss: 0.3662 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8201 - val_loss: 0.6562 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.81 - ETA: 0s - loss: 0.4382 - accuracy: 0.83 - ETA: 0s - loss: 0.4193 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8253 - val_loss: 0.9644 - val_accuracy: 0.6313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.68 - ETA: 0s - loss: 0.3374 - accuracy: 0.84 - ETA: 0s - loss: 0.3331 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8391 - val_loss: 0.7495 - val_accuracy: 0.7224\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.90 - ETA: 0s - loss: 0.3535 - accuracy: 0.86 - ETA: 0s - loss: 0.3488 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8462 - val_loss: 1.6542 - val_accuracy: 0.5791\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.75 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3649 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8425 - val_loss: 0.8041 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.81 - ETA: 0s - loss: 0.9230 - accuracy: 0.80 - ETA: 0s - loss: 0.7630 - accuracy: 0.80 - 0s 2ms/step - loss: 0.7251 - accuracy: 0.8033 - val_loss: 1.2773 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.84 - ETA: 0s - loss: 0.5181 - accuracy: 0.83 - ETA: 0s - loss: 0.5123 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5688 - accuracy: 0.8234 - val_loss: 1.0738 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.90 - ETA: 0s - loss: 0.4761 - accuracy: 0.83 - ETA: 0s - loss: 0.4641 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4583 - accuracy: 0.8462 - val_loss: 0.7713 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.87 - ETA: 0s - loss: 0.3434 - accuracy: 0.86 - ETA: 0s - loss: 0.3379 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8664 - val_loss: 1.5388 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.96 - ETA: 0s - loss: 0.3497 - accuracy: 0.88 - ETA: 0s - loss: 0.3122 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8914 - val_loss: 1.5945 - val_accuracy: 0.7224\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.96 - ETA: 0s - loss: 0.2734 - accuracy: 0.90 - ETA: 0s - loss: 0.5488 - accuracy: 0.88 - 0s 2ms/step - loss: 0.6289 - accuracy: 0.8731 - val_loss: 4.6328 - val_accuracy: 0.7507\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.93 - ETA: 0s - loss: 0.6264 - accuracy: 0.81 - ETA: 0s - loss: 0.6299 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6026 - accuracy: 0.8059 - val_loss: 0.7554 - val_accuracy: 0.7687\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.78 - ETA: 0s - loss: 0.4959 - accuracy: 0.82 - ETA: 0s - loss: 0.4636 - accuracy: 0.83 - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8384 - val_loss: 3.9996 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.96 - ETA: 0s - loss: 0.4332 - accuracy: 0.87 - ETA: 0s - loss: 0.4558 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4637 - accuracy: 0.8537 - val_loss: 1.0954 - val_accuracy: 0.7567\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3782 - accuracy: 0.84 - ETA: 0s - loss: 0.4800 - accuracy: 0.86 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8667 - val_loss: 1.4043 - val_accuracy: 0.7313\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.87 - ETA: 0s - loss: 0.3943 - accuracy: 0.88 - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8779 - val_loss: 0.7398 - val_accuracy: 0.7403\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.90 - ETA: 0s - loss: 0.3691 - accuracy: 0.87 - ETA: 0s - loss: 0.3677 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8779 - val_loss: 1.3589 - val_accuracy: 0.7045\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.87 - ETA: 0s - loss: 0.2965 - accuracy: 0.92 - ETA: 0s - loss: 0.3369 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8977 - val_loss: 1.1446 - val_accuracy: 0.7418\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.87 - ETA: 0s - loss: 0.3079 - accuracy: 0.90 - ETA: 0s - loss: 0.3077 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3153 - accuracy: 0.9029 - val_loss: 0.9224 - val_accuracy: 0.7328\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - ETA: 0s - loss: 0.3299 - accuracy: 0.91 - ETA: 0s - loss: 0.3157 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3123 - accuracy: 0.9097 - val_loss: 1.8503 - val_accuracy: 0.7254\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 1.00 - ETA: 0s - loss: 0.2706 - accuracy: 0.92 - ETA: 0s - loss: 0.2684 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2586 - accuracy: 0.9306 - val_loss: 1.4665 - val_accuracy: 0.7358\n",
      "Epoch 26/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.3203 - accuracy: 0.90 - ETA: 0s - loss: 0.2481 - accuracy: 0.93 - ETA: 0s - loss: 0.2554 - accuracy: 0.9325Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9295 - val_loss: 1.4408 - val_accuracy: 0.7373\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ef4848c83c15cc31843383044453ea05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7407960096995035</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.34561317959923044</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1617 - accuracy: 0.53 - ETA: 0s - loss: 1.2800 - accuracy: 0.62 - ETA: 0s - loss: 0.9245 - accuracy: 0.66 - ETA: 0s - loss: 0.7953 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7706 - accuracy: 0.6965 - val_loss: 0.7910 - val_accuracy: 0.6045\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.71 - ETA: 0s - loss: 0.5310 - accuracy: 0.74 - ETA: 0s - loss: 0.5034 - accuracy: 0.75 - ETA: 0s - loss: 0.5206 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5244 - accuracy: 0.7604 - val_loss: 0.6476 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4918 - accuracy: 0.81 - ETA: 0s - loss: 0.4303 - accuracy: 0.82 - ETA: 0s - loss: 0.4386 - accuracy: 0.81 - ETA: 0s - loss: 0.4514 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4546 - accuracy: 0.8048 - val_loss: 0.6693 - val_accuracy: 0.6627\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.75 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4237 - accuracy: 0.82 - ETA: 0s - loss: 0.4299 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8152 - val_loss: 0.7985 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.93 - ETA: 0s - loss: 0.5394 - accuracy: 0.79 - ETA: 0s - loss: 0.4753 - accuracy: 0.80 - ETA: 0s - loss: 0.4691 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8003 - val_loss: 0.7765 - val_accuracy: 0.6791\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.81 - ETA: 0s - loss: 0.3541 - accuracy: 0.85 - ETA: 0s - loss: 0.3753 - accuracy: 0.83 - ETA: 0s - loss: 0.3910 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3910 - accuracy: 0.8339 - val_loss: 0.6679 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.87 - ETA: 0s - loss: 0.3154 - accuracy: 0.85 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3613 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8294 - val_loss: 1.2058 - val_accuracy: 0.7015\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.3288 - accuracy: 0.85 - ETA: 0s - loss: 0.3281 - accuracy: 0.86 - ETA: 0s - loss: 0.3326 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8608 - val_loss: 1.5249 - val_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.93 - ETA: 0s - loss: 0.4381 - accuracy: 0.85 - ETA: 0s - loss: 0.4027 - accuracy: 0.84 - ETA: 0s - loss: 0.3888 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8488 - val_loss: 0.6651 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.90 - ETA: 0s - loss: 0.3932 - accuracy: 0.86 - ETA: 0s - loss: 0.3668 - accuracy: 0.85 - ETA: 0s - loss: 0.3503 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3490 - accuracy: 0.8518 - val_loss: 1.1088 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.90 - ETA: 0s - loss: 0.3309 - accuracy: 0.87 - ETA: 0s - loss: 0.4776 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8522 - val_loss: 2.8228 - val_accuracy: 0.6940\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.84 - ETA: 0s - loss: 0.2702 - accuracy: 0.90 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8731 - val_loss: 5.4886 - val_accuracy: 0.6716\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0586 - accuracy: 0.81 - ETA: 0s - loss: 0.3734 - accuracy: 0.85 - ETA: 0s - loss: 0.3341 - accuracy: 0.86 - ETA: 0s - loss: 0.3362 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3333 - accuracy: 0.8750 - val_loss: 4.2516 - val_accuracy: 0.7045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.93 - ETA: 0s - loss: 0.3208 - accuracy: 0.86 - ETA: 0s - loss: 0.3188 - accuracy: 0.87 - ETA: 0s - loss: 0.3196 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8791 - val_loss: 1.3601 - val_accuracy: 0.6925\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.87 - ETA: 0s - loss: 0.2891 - accuracy: 0.89 - ETA: 0s - loss: 0.3108 - accuracy: 0.88 - ETA: 0s - loss: 0.3099 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8910 - val_loss: 1.8200 - val_accuracy: 0.7269\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.96 - ETA: 0s - loss: 0.2754 - accuracy: 0.89 - ETA: 0s - loss: 0.2777 - accuracy: 0.90 - ETA: 0s - loss: 0.2843 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8955 - val_loss: 1.7893 - val_accuracy: 0.6866\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.90 - ETA: 0s - loss: 0.2781 - accuracy: 0.89 - ETA: 0s - loss: 0.3532 - accuracy: 0.88 - ETA: 0s - loss: 0.3302 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8891 - val_loss: 4.1337 - val_accuracy: 0.6896\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.93 - ETA: 0s - loss: 0.2118 - accuracy: 0.93 - ETA: 0s - loss: 0.3836 - accuracy: 0.87 - ETA: 0s - loss: 0.4014 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8682 - val_loss: 2.8773 - val_accuracy: 0.7164\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.90 - ETA: 0s - loss: 0.3509 - accuracy: 0.89 - ETA: 0s - loss: 0.3628 - accuracy: 0.89 - ETA: 0s - loss: 0.3491 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3518 - accuracy: 0.8962 - val_loss: 2.5731 - val_accuracy: 0.7299\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.90 - ETA: 0s - loss: 0.3046 - accuracy: 0.90 - ETA: 0s - loss: 0.3205 - accuracy: 0.89 - ETA: 0s - loss: 0.3720 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8783 - val_loss: 2.2490 - val_accuracy: 0.7403\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.87 - ETA: 0s - loss: 0.5079 - accuracy: 0.85 - ETA: 0s - loss: 0.4894 - accuracy: 0.86 - ETA: 0s - loss: 0.5116 - accuracy: 0.85 - 0s 2ms/step - loss: 0.5019 - accuracy: 0.8600 - val_loss: 3.7247 - val_accuracy: 0.6433\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.78 - ETA: 0s - loss: 0.4326 - accuracy: 0.84 - ETA: 0s - loss: 0.3701 - accuracy: 0.86 - ETA: 0s - loss: 0.3802 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8652 - val_loss: 12.0067 - val_accuracy: 0.7358\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.93 - ETA: 0s - loss: 0.2809 - accuracy: 0.90 - ETA: 0s - loss: 0.2581 - accuracy: 0.91 - ETA: 0s - loss: 0.3182 - accuracy: 0.89 - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8947 - val_loss: 8.5091 - val_accuracy: 0.7463\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.90 - ETA: 0s - loss: 0.2989 - accuracy: 0.88 - ETA: 0s - loss: 0.2779 - accuracy: 0.90 - ETA: 0s - loss: 0.2817 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2789 - accuracy: 0.9018 - val_loss: 10.8891 - val_accuracy: 0.7209\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - ETA: 0s - loss: 0.2050 - accuracy: 0.92 - ETA: 0s - loss: 0.2081 - accuracy: 0.92 - ETA: 0s - loss: 0.2405 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9227 - val_loss: 2.0193 - val_accuracy: 0.7328\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.96 - ETA: 0s - loss: 0.2114 - accuracy: 0.92 - ETA: 0s - loss: 0.2284 - accuracy: 0.92 - ETA: 0s - loss: 0.2218 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9257 - val_loss: 2.9800 - val_accuracy: 0.6985\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.87 - ETA: 0s - loss: 0.2201 - accuracy: 0.92 - ETA: 0s - loss: 0.2281 - accuracy: 0.92 - ETA: 0s - loss: 0.2204 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2129 - accuracy: 0.9280 - val_loss: 4.4579 - val_accuracy: 0.7104\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.90 - ETA: 0s - loss: 0.1812 - accuracy: 0.94 - ETA: 0s - loss: 0.1717 - accuracy: 0.94 - ETA: 0s - loss: 0.1646 - accuracy: 0.95 - 0s 2ms/step - loss: 0.2470 - accuracy: 0.9530 - val_loss: 4.0885 - val_accuracy: 0.6761\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.96 - ETA: 0s - loss: 0.2566 - accuracy: 0.92 - ETA: 0s - loss: 0.2038 - accuracy: 0.94 - ETA: 0s - loss: 0.1963 - accuracy: 0.94 - 0s 3ms/step - loss: 0.1987 - accuracy: 0.9436 - val_loss: 0.8555 - val_accuracy: 0.7358\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.96 - ETA: 0s - loss: 0.1553 - accuracy: 0.95 - ETA: 0s - loss: 0.1698 - accuracy: 0.95 - ETA: 0s - loss: 0.1716 - accuracy: 0.95 - 0s 2ms/step - loss: 0.1690 - accuracy: 0.9545 - val_loss: 1.6174 - val_accuracy: 0.7194\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.93 - ETA: 0s - loss: 0.1909 - accuracy: 0.93 - ETA: 0s - loss: 0.1731 - accuracy: 0.94 - ETA: 0s - loss: 0.1569 - accuracy: 0.95 - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9507 - val_loss: 1.4053 - val_accuracy: 0.7269\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.96 - ETA: 0s - loss: 0.1178 - accuracy: 0.96 - ETA: 0s - loss: 0.1085 - accuracy: 0.96 - ETA: 0s - loss: 0.1111 - accuracy: 0.96 - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9593 - val_loss: 2.4139 - val_accuracy: 0.7000\n",
      "Epoch 33/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.0429 - accuracy: 0.96 - ETA: 0s - loss: 0.0909 - accuracy: 0.97 - ETA: 0s - loss: 0.1569 - accuracy: 0.96 - ETA: 0s - loss: 0.1581 - accuracy: 0.9529Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9522 - val_loss: 2.1519 - val_accuracy: 0.6985\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.68 - ETA: 0s - loss: 1.4205 - accuracy: 0.64 - ETA: 0s - loss: 1.0030 - accuracy: 0.68 - ETA: 0s - loss: 0.8569 - accuracy: 0.69 - 0s 4ms/step - loss: 0.8382 - accuracy: 0.7003 - val_loss: 0.5568 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.78 - ETA: 0s - loss: 0.4827 - accuracy: 0.76 - ETA: 0s - loss: 0.4959 - accuracy: 0.75 - ETA: 0s - loss: 0.4934 - accuracy: 0.75 - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7600 - val_loss: 0.5976 - val_accuracy: 0.6851\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.75 - ETA: 0s - loss: 0.3902 - accuracy: 0.80 - ETA: 0s - loss: 0.4057 - accuracy: 0.80 - ETA: 0s - loss: 0.4124 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8029 - val_loss: 0.6081 - val_accuracy: 0.7090\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.71 - ETA: 0s - loss: 0.3612 - accuracy: 0.84 - ETA: 0s - loss: 0.3748 - accuracy: 0.83 - ETA: 0s - loss: 0.4029 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8201 - val_loss: 0.6441 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3804 - accuracy: 0.81 - ETA: 0s - loss: 0.3753 - accuracy: 0.83 - ETA: 0s - loss: 0.3889 - accuracy: 0.83 - ETA: 0s - loss: 0.4055 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8234 - val_loss: 0.6447 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.90 - ETA: 0s - loss: 0.4239 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - ETA: 0s - loss: 0.3643 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8361 - val_loss: 1.0076 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.90 - ETA: 0s - loss: 0.2782 - accuracy: 0.88 - ETA: 0s - loss: 0.3337 - accuracy: 0.86 - ETA: 0s - loss: 0.3651 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8425 - val_loss: 0.8295 - val_accuracy: 0.6761\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.78 - ETA: 0s - loss: 0.3475 - accuracy: 0.84 - ETA: 0s - loss: 0.3839 - accuracy: 0.82 - ETA: 0s - loss: 0.3834 - accuracy: 0.82 - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8242 - val_loss: 0.9012 - val_accuracy: 0.6925\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.90 - ETA: 0s - loss: 0.3678 - accuracy: 0.86 - ETA: 0s - loss: 0.3557 - accuracy: 0.86 - ETA: 0s - loss: 0.3504 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3497 - accuracy: 0.8623 - val_loss: 1.4368 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.93 - ETA: 0s - loss: 0.3482 - accuracy: 0.85 - ETA: 0s - loss: 0.3465 - accuracy: 0.85 - ETA: 0s - loss: 0.3298 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3298 - accuracy: 0.8589 - val_loss: 1.4868 - val_accuracy: 0.6672\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.90 - ETA: 0s - loss: 0.3139 - accuracy: 0.86 - ETA: 0s - loss: 0.3099 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8779 - val_loss: 1.3163 - val_accuracy: 0.6612\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.81 - ETA: 0s - loss: 0.3560 - accuracy: 0.85 - ETA: 0s - loss: 0.3378 - accuracy: 0.86 - ETA: 0s - loss: 0.3274 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8701 - val_loss: 1.3418 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.90 - ETA: 0s - loss: 0.2816 - accuracy: 0.90 - ETA: 0s - loss: 0.3449 - accuracy: 0.87 - ETA: 0s - loss: 0.3869 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8496 - val_loss: 2.9586 - val_accuracy: 0.7179\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.87 - ETA: 0s - loss: 0.3919 - accuracy: 0.86 - ETA: 0s - loss: 0.4089 - accuracy: 0.85 - ETA: 0s - loss: 0.4147 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8455 - val_loss: 2.2156 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.78 - ETA: 0s - loss: 0.3894 - accuracy: 0.87 - ETA: 0s - loss: 0.3763 - accuracy: 0.86 - ETA: 0s - loss: 0.3620 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8694 - val_loss: 0.9787 - val_accuracy: 0.7030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.93 - ETA: 0s - loss: 0.3033 - accuracy: 0.90 - ETA: 0s - loss: 0.3137 - accuracy: 0.89 - ETA: 0s - loss: 0.3589 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3589 - accuracy: 0.8932 - val_loss: 3.3716 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.90 - ETA: 0s - loss: 0.4693 - accuracy: 0.84 - ETA: 0s - loss: 0.4342 - accuracy: 0.84 - ETA: 0s - loss: 0.4085 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8526 - val_loss: 1.4857 - val_accuracy: 0.7119\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.90 - ETA: 0s - loss: 0.3347 - accuracy: 0.89 - ETA: 0s - loss: 0.3452 - accuracy: 0.89 - ETA: 0s - loss: 0.3295 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3260 - accuracy: 0.8932 - val_loss: 1.8665 - val_accuracy: 0.7179\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1907 - accuracy: 0.93 - ETA: 0s - loss: 0.2766 - accuracy: 0.89 - ETA: 0s - loss: 0.2739 - accuracy: 0.90 - ETA: 0s - loss: 0.2735 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2711 - accuracy: 0.9044 - val_loss: 1.5695 - val_accuracy: 0.7179\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 1.00 - ETA: 0s - loss: 0.2566 - accuracy: 0.91 - ETA: 0s - loss: 0.2535 - accuracy: 0.91 - ETA: 0s - loss: 0.2610 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3105 - accuracy: 0.9082 - val_loss: 3.6466 - val_accuracy: 0.7224\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.87 - ETA: 0s - loss: 0.2767 - accuracy: 0.88 - ETA: 0s - loss: 0.3637 - accuracy: 0.88 - ETA: 0s - loss: 0.3701 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8835 - val_loss: 1.1981 - val_accuracy: 0.7075\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.93 - ETA: 0s - loss: 0.3044 - accuracy: 0.90 - ETA: 0s - loss: 0.2956 - accuracy: 0.91 - ETA: 0s - loss: 0.3187 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3254 - accuracy: 0.9003 - val_loss: 3.0104 - val_accuracy: 0.7254\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.84 - ETA: 0s - loss: 0.4239 - accuracy: 0.87 - ETA: 0s - loss: 0.3807 - accuracy: 0.88 - ETA: 0s - loss: 0.3549 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8914 - val_loss: 2.5187 - val_accuracy: 0.7254\n",
      "Epoch 24/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3615 - accuracy: 0.87 - ETA: 0s - loss: 0.2883 - accuracy: 0.90 - ETA: 0s - loss: 0.2490 - accuracy: 0.92 - ETA: 0s - loss: 0.2408 - accuracy: 0.9271Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.9276 - val_loss: 2.3119 - val_accuracy: 0.7254\n",
      "Epoch 00024: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9727 - accuracy: 0.56 - ETA: 0s - loss: 1.5413 - accuracy: 0.63 - ETA: 0s - loss: 1.0203 - accuracy: 0.68 - ETA: 0s - loss: 0.8658 - accuracy: 0.69 - 0s 4ms/step - loss: 0.8463 - accuracy: 0.6991 - val_loss: 0.6566 - val_accuracy: 0.6910\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.78 - ETA: 0s - loss: 0.5125 - accuracy: 0.74 - ETA: 0s - loss: 0.5101 - accuracy: 0.74 - ETA: 0s - loss: 0.5067 - accuracy: 0.75 - ETA: 0s - loss: 0.5049 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5131 - accuracy: 0.7563 - val_loss: 0.5840 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.81 - ETA: 0s - loss: 0.4475 - accuracy: 0.79 - ETA: 0s - loss: 0.4577 - accuracy: 0.80 - ETA: 0s - loss: 0.4596 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7962 - val_loss: 0.6191 - val_accuracy: 0.7015\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.84 - ETA: 0s - loss: 0.4458 - accuracy: 0.82 - ETA: 0s - loss: 0.4308 - accuracy: 0.82 - ETA: 0s - loss: 0.4578 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8141 - val_loss: 0.8125 - val_accuracy: 0.6731\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4360 - accuracy: 0.87 - ETA: 0s - loss: 0.4037 - accuracy: 0.82 - ETA: 0s - loss: 0.4397 - accuracy: 0.82 - ETA: 0s - loss: 0.4430 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8149 - val_loss: 0.5935 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3436 - accuracy: 0.87 - ETA: 0s - loss: 0.4406 - accuracy: 0.84 - ETA: 0s - loss: 0.4103 - accuracy: 0.85 - ETA: 0s - loss: 0.4075 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8425 - val_loss: 0.8650 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.87 - ETA: 0s - loss: 0.3749 - accuracy: 0.85 - ETA: 0s - loss: 0.4359 - accuracy: 0.84 - ETA: 0s - loss: 0.4369 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8410 - val_loss: 1.0840 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.84 - ETA: 0s - loss: 0.3321 - accuracy: 0.86 - ETA: 0s - loss: 0.3281 - accuracy: 0.85 - ETA: 0s - loss: 0.3475 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8503 - val_loss: 0.6203 - val_accuracy: 0.7403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3458 - accuracy: 0.87 - ETA: 0s - loss: 0.3615 - accuracy: 0.88 - ETA: 0s - loss: 0.3685 - accuracy: 0.87 - ETA: 0s - loss: 0.4154 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4203 - accuracy: 0.8511 - val_loss: 1.0589 - val_accuracy: 0.6612\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.78 - ETA: 0s - loss: 0.3520 - accuracy: 0.86 - ETA: 0s - loss: 0.3834 - accuracy: 0.86 - ETA: 0s - loss: 0.3705 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8634 - val_loss: 4.1396 - val_accuracy: 0.6522\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.87 - ETA: 0s - loss: 0.4245 - accuracy: 0.84 - ETA: 0s - loss: 0.3644 - accuracy: 0.86 - ETA: 0s - loss: 0.3526 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8705 - val_loss: 1.4044 - val_accuracy: 0.6806\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.78 - ETA: 0s - loss: 0.3144 - accuracy: 0.88 - ETA: 0s - loss: 0.3375 - accuracy: 0.88 - ETA: 0s - loss: 0.3114 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8895 - val_loss: 1.8733 - val_accuracy: 0.6910\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 1.00 - ETA: 0s - loss: 0.4783 - accuracy: 0.88 - ETA: 0s - loss: 0.5075 - accuracy: 0.85 - ETA: 0s - loss: 0.5090 - accuracy: 0.84 - 0s 2ms/step - loss: 0.5090 - accuracy: 0.8466 - val_loss: 0.8954 - val_accuracy: 0.7194\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.87 - ETA: 0s - loss: 0.4779 - accuracy: 0.83 - ETA: 0s - loss: 0.5100 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4926 - accuracy: 0.8440 - val_loss: 0.8445 - val_accuracy: 0.7209\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.84 - ETA: 0s - loss: 0.4185 - accuracy: 0.86 - ETA: 0s - loss: 0.4038 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8664 - val_loss: 1.6646 - val_accuracy: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.87 - ETA: 0s - loss: 0.3980 - accuracy: 0.88 - ETA: 0s - loss: 0.3826 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8858 - val_loss: 0.9850 - val_accuracy: 0.7090\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.96 - ETA: 0s - loss: 0.2873 - accuracy: 0.91 - ETA: 0s - loss: 0.3031 - accuracy: 0.90 - ETA: 0s - loss: 0.3044 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3056 - accuracy: 0.9052 - val_loss: 0.6484 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "57/84 [===================>..........] - ETA: 0s - loss: 0.2852 - accuracy: 0.96 - ETA: 0s - loss: 0.2871 - accuracy: 0.93 - ETA: 0s - loss: 0.2772 - accuracy: 0.9254Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.9145 - val_loss: 0.9773 - val_accuracy: 0.7254\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 8715c14e714a91c2b83e0ee74bc034b2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7398009896278381</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.13138568584847732</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7707 - accuracy: 0.71 - ETA: 0s - loss: 2.5925 - accuracy: 0.66 - ETA: 0s - loss: 1.5868 - accuracy: 0.70 - ETA: 0s - loss: 1.2308 - accuracy: 0.71 - ETA: 0s - loss: 1.0680 - accuracy: 0.72 - ETA: 0s - loss: 0.9842 - accuracy: 0.72 - ETA: 0s - loss: 0.9283 - accuracy: 0.72 - 1s 6ms/step - loss: 0.9147 - accuracy: 0.7212 - val_loss: 0.6129 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4929 - accuracy: 0.84 - ETA: 0s - loss: 0.5371 - accuracy: 0.78 - ETA: 0s - loss: 0.5198 - accuracy: 0.78 - ETA: 0s - loss: 0.5395 - accuracy: 0.76 - ETA: 0s - loss: 0.5475 - accuracy: 0.75 - ETA: 0s - loss: 0.5434 - accuracy: 0.75 - ETA: 0s - loss: 0.5393 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5376 - accuracy: 0.7581 - val_loss: 0.6568 - val_accuracy: 0.6896\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.78 - ETA: 0s - loss: 0.4911 - accuracy: 0.77 - ETA: 0s - loss: 0.4784 - accuracy: 0.78 - ETA: 0s - loss: 0.4781 - accuracy: 0.78 - ETA: 0s - loss: 0.4773 - accuracy: 0.79 - ETA: 0s - loss: 0.4777 - accuracy: 0.78 - ETA: 0s - loss: 0.4652 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4675 - accuracy: 0.8007 - val_loss: 0.5549 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.71 - ETA: 0s - loss: 0.4887 - accuracy: 0.80 - ETA: 0s - loss: 0.5128 - accuracy: 0.80 - ETA: 0s - loss: 0.5257 - accuracy: 0.80 - ETA: 0s - loss: 0.5222 - accuracy: 0.81 - ETA: 0s - loss: 0.5110 - accuracy: 0.81 - ETA: 0s - loss: 0.5145 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5133 - accuracy: 0.8156 - val_loss: 0.6914 - val_accuracy: 0.7194\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.87 - ETA: 0s - loss: 0.4420 - accuracy: 0.82 - ETA: 0s - loss: 0.4238 - accuracy: 0.83 - ETA: 0s - loss: 0.4636 - accuracy: 0.81 - ETA: 0s - loss: 0.4694 - accuracy: 0.81 - ETA: 0s - loss: 0.5129 - accuracy: 0.79 - ETA: 0s - loss: 0.5233 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5205 - accuracy: 0.7981 - val_loss: 0.6682 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.84 - ETA: 0s - loss: 0.5318 - accuracy: 0.79 - ETA: 0s - loss: 0.5002 - accuracy: 0.79 - ETA: 0s - loss: 0.4911 - accuracy: 0.80 - ETA: 0s - loss: 0.5075 - accuracy: 0.80 - ETA: 0s - loss: 0.5023 - accuracy: 0.80 - ETA: 0s - loss: 0.5054 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5080 - accuracy: 0.8119 - val_loss: 1.0032 - val_accuracy: 0.7299\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.84 - ETA: 0s - loss: 0.5616 - accuracy: 0.84 - ETA: 0s - loss: 0.5279 - accuracy: 0.83 - ETA: 0s - loss: 0.5238 - accuracy: 0.83 - ETA: 0s - loss: 0.5122 - accuracy: 0.83 - ETA: 0s - loss: 0.5179 - accuracy: 0.82 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5244 - accuracy: 0.8201 - val_loss: 0.7903 - val_accuracy: 0.7104\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.84 - ETA: 0s - loss: 0.4397 - accuracy: 0.86 - ETA: 0s - loss: 0.4686 - accuracy: 0.84 - ETA: 0s - loss: 0.4577 - accuracy: 0.85 - ETA: 0s - loss: 0.4634 - accuracy: 0.85 - ETA: 0s - loss: 0.4615 - accuracy: 0.85 - ETA: 0s - loss: 0.4665 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4701 - accuracy: 0.8473 - val_loss: 0.6791 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.96 - ETA: 0s - loss: 0.4641 - accuracy: 0.86 - ETA: 0s - loss: 0.4858 - accuracy: 0.85 - ETA: 0s - loss: 0.4701 - accuracy: 0.85 - ETA: 0s - loss: 0.4771 - accuracy: 0.85 - ETA: 0s - loss: 0.4739 - accuracy: 0.85 - ETA: 0s - loss: 0.4679 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4659 - accuracy: 0.8537 - val_loss: 1.3135 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.78 - ETA: 0s - loss: 0.4160 - accuracy: 0.86 - ETA: 0s - loss: 0.4205 - accuracy: 0.85 - ETA: 0s - loss: 0.4132 - accuracy: 0.86 - ETA: 0s - loss: 0.4050 - accuracy: 0.86 - ETA: 0s - loss: 0.4048 - accuracy: 0.86 - ETA: 0s - loss: 0.4103 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4114 - accuracy: 0.8660 - val_loss: 1.2217 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.87 - ETA: 0s - loss: 0.4161 - accuracy: 0.87 - ETA: 0s - loss: 0.4121 - accuracy: 0.87 - ETA: 0s - loss: 0.4110 - accuracy: 0.87 - ETA: 0s - loss: 0.4263 - accuracy: 0.86 - ETA: 0s - loss: 0.4208 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4080 - accuracy: 0.8731 - val_loss: 0.8390 - val_accuracy: 0.7403\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.93 - ETA: 0s - loss: 0.2862 - accuracy: 0.93 - ETA: 0s - loss: 0.2991 - accuracy: 0.92 - ETA: 0s - loss: 0.3311 - accuracy: 0.91 - ETA: 0s - loss: 0.3444 - accuracy: 0.90 - ETA: 0s - loss: 0.3492 - accuracy: 0.89 - ETA: 0s - loss: 0.3686 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8903 - val_loss: 1.2334 - val_accuracy: 0.7209\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.93 - ETA: 0s - loss: 0.4235 - accuracy: 0.90 - ETA: 0s - loss: 0.3852 - accuracy: 0.89 - ETA: 0s - loss: 0.3755 - accuracy: 0.90 - ETA: 0s - loss: 0.3534 - accuracy: 0.91 - ETA: 0s - loss: 0.3600 - accuracy: 0.90 - ETA: 0s - loss: 0.3615 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3675 - accuracy: 0.8985 - val_loss: 0.8598 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.90 - ETA: 0s - loss: 0.3727 - accuracy: 0.87 - ETA: 0s - loss: 0.3477 - accuracy: 0.88 - ETA: 0s - loss: 0.3384 - accuracy: 0.89 - ETA: 0s - loss: 0.3497 - accuracy: 0.89 - ETA: 0s - loss: 0.3587 - accuracy: 0.89 - ETA: 0s - loss: 0.3681 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3733 - accuracy: 0.8914 - val_loss: 0.9713 - val_accuracy: 0.7134\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.90 - ETA: 0s - loss: 0.4405 - accuracy: 0.86 - ETA: 0s - loss: 0.3868 - accuracy: 0.88 - ETA: 0s - loss: 0.3742 - accuracy: 0.89 - ETA: 0s - loss: 0.3744 - accuracy: 0.89 - ETA: 0s - loss: 0.3905 - accuracy: 0.89 - ETA: 0s - loss: 0.4015 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4029 - accuracy: 0.8862 - val_loss: 1.4363 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.81 - ETA: 0s - loss: 0.3879 - accuracy: 0.90 - ETA: 0s - loss: 0.3552 - accuracy: 0.90 - ETA: 0s - loss: 0.3428 - accuracy: 0.91 - ETA: 0s - loss: 0.3478 - accuracy: 0.90 - ETA: 0s - loss: 0.3543 - accuracy: 0.89 - ETA: 0s - loss: 0.3630 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3663 - accuracy: 0.8884 - val_loss: 1.2275 - val_accuracy: 0.7104\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.84 - ETA: 0s - loss: 0.3523 - accuracy: 0.88 - ETA: 0s - loss: 0.3183 - accuracy: 0.90 - ETA: 0s - loss: 0.3098 - accuracy: 0.90 - ETA: 0s - loss: 0.4291 - accuracy: 0.89 - ETA: 0s - loss: 0.4546 - accuracy: 0.88 - ETA: 0s - loss: 0.4457 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4440 - accuracy: 0.8723 - val_loss: 0.6256 - val_accuracy: 0.7134\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.90 - ETA: 0s - loss: 0.3154 - accuracy: 0.87 - ETA: 0s - loss: 0.3532 - accuracy: 0.87 - ETA: 0s - loss: 0.3511 - accuracy: 0.87 - ETA: 0s - loss: 0.3720 - accuracy: 0.86 - ETA: 0s - loss: 0.3810 - accuracy: 0.87 - ETA: 0s - loss: 0.3918 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3948 - accuracy: 0.8723 - val_loss: 0.8981 - val_accuracy: 0.7104\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.90 - ETA: 0s - loss: 0.3652 - accuracy: 0.88 - ETA: 0s - loss: 0.3826 - accuracy: 0.88 - ETA: 0s - loss: 0.3743 - accuracy: 0.87 - ETA: 0s - loss: 0.3498 - accuracy: 0.88 - ETA: 0s - loss: 0.4251 - accuracy: 0.88 - ETA: 0s - loss: 0.4443 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4618 - accuracy: 0.8750 - val_loss: 0.6814 - val_accuracy: 0.6955\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3173 - accuracy: 0.59 - ETA: 0s - loss: 1.0412 - accuracy: 0.68 - ETA: 0s - loss: 0.9340 - accuracy: 0.69 - ETA: 0s - loss: 0.8718 - accuracy: 0.70 - ETA: 0s - loss: 0.8515 - accuracy: 0.69 - ETA: 0s - loss: 0.8210 - accuracy: 0.69 - ETA: 0s - loss: 0.8017 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7988 - accuracy: 0.6969 - val_loss: 0.6577 - val_accuracy: 0.6955\n",
      "Epoch 21/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6523 - accuracy: 0.75 - ETA: 0s - loss: 0.6813 - accuracy: 0.71 - ETA: 0s - loss: 0.7062 - accuracy: 0.69 - ETA: 0s - loss: 0.7079 - accuracy: 0.68 - ETA: 0s - loss: 0.6998 - accuracy: 0.65 - ETA: 0s - loss: 0.6963 - accuracy: 0.67 - ETA: 0s - loss: 0.6850 - accuracy: 0.6895Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6768 - accuracy: 0.7032 - val_loss: 1.0332 - val_accuracy: 0.7388\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0060 - accuracy: 0.59 - ETA: 0s - loss: 2.1569 - accuracy: 0.60 - ETA: 0s - loss: 1.4531 - accuracy: 0.62 - ETA: 0s - loss: 1.1503 - accuracy: 0.66 - ETA: 0s - loss: 1.0108 - accuracy: 0.67 - ETA: 0s - loss: 0.9287 - accuracy: 0.69 - ETA: 0s - loss: 0.8669 - accuracy: 0.69 - 0s 6ms/step - loss: 0.8404 - accuracy: 0.7010 - val_loss: 0.6063 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0501 - accuracy: 0.75 - ETA: 0s - loss: 0.5468 - accuracy: 0.75 - ETA: 0s - loss: 0.5396 - accuracy: 0.76 - ETA: 0s - loss: 0.5300 - accuracy: 0.76 - ETA: 0s - loss: 0.5218 - accuracy: 0.76 - ETA: 0s - loss: 0.5204 - accuracy: 0.76 - ETA: 0s - loss: 0.5274 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5320 - accuracy: 0.7615 - val_loss: 0.5828 - val_accuracy: 0.6985\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.75 - ETA: 0s - loss: 0.4313 - accuracy: 0.80 - ETA: 0s - loss: 0.4137 - accuracy: 0.81 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4571 - accuracy: 0.80 - ETA: 0s - loss: 0.4607 - accuracy: 0.79 - ETA: 0s - loss: 0.4680 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4667 - accuracy: 0.7928 - val_loss: 0.5784 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.71 - ETA: 0s - loss: 0.3924 - accuracy: 0.80 - ETA: 0s - loss: 0.4047 - accuracy: 0.78 - ETA: 0s - loss: 0.4084 - accuracy: 0.80 - ETA: 0s - loss: 0.4180 - accuracy: 0.80 - ETA: 0s - loss: 0.4119 - accuracy: 0.80 - ETA: 0s - loss: 0.4062 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8033 - val_loss: 0.9029 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.81 - ETA: 0s - loss: 0.3452 - accuracy: 0.83 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.3788 - accuracy: 0.83 - ETA: 0s - loss: 0.3784 - accuracy: 0.83 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.4049 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4049 - accuracy: 0.8246 - val_loss: 0.6054 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.75 - ETA: 0s - loss: 0.4623 - accuracy: 0.79 - ETA: 0s - loss: 0.5177 - accuracy: 0.81 - ETA: 0s - loss: 0.4933 - accuracy: 0.81 - ETA: 0s - loss: 0.4816 - accuracy: 0.81 - ETA: 0s - loss: 0.4700 - accuracy: 0.81 - ETA: 0s - loss: 0.4606 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4590 - accuracy: 0.8175 - val_loss: 0.9049 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5868 - accuracy: 0.87 - ETA: 0s - loss: 0.5275 - accuracy: 0.80 - ETA: 0s - loss: 0.4868 - accuracy: 0.82 - ETA: 0s - loss: 0.4739 - accuracy: 0.81 - ETA: 0s - loss: 0.4494 - accuracy: 0.82 - ETA: 0s - loss: 0.4460 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4478 - accuracy: 0.8242 - val_loss: 0.7290 - val_accuracy: 0.6731\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.87 - ETA: 0s - loss: 0.3911 - accuracy: 0.84 - ETA: 0s - loss: 0.5095 - accuracy: 0.78 - ETA: 0s - loss: 0.4776 - accuracy: 0.80 - ETA: 0s - loss: 0.4984 - accuracy: 0.80 - ETA: 0s - loss: 0.5474 - accuracy: 0.80 - ETA: 0s - loss: 0.6261 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6264 - accuracy: 0.7887 - val_loss: 0.9267 - val_accuracy: 0.7134\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6050 - accuracy: 0.78 - ETA: 0s - loss: 0.5487 - accuracy: 0.81 - ETA: 0s - loss: 0.5425 - accuracy: 0.82 - ETA: 0s - loss: 0.5738 - accuracy: 0.80 - ETA: 0s - loss: 0.5609 - accuracy: 0.81 - ETA: 0s - loss: 0.5581 - accuracy: 0.82 - ETA: 0s - loss: 0.5682 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5682 - accuracy: 0.8134 - val_loss: 0.6648 - val_accuracy: 0.7269\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.75 - ETA: 0s - loss: 0.4840 - accuracy: 0.82 - ETA: 0s - loss: 0.4492 - accuracy: 0.84 - ETA: 0s - loss: 0.4785 - accuracy: 0.85 - ETA: 0s - loss: 0.5235 - accuracy: 0.83 - ETA: 0s - loss: 0.5242 - accuracy: 0.83 - ETA: 0s - loss: 0.5450 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5514 - accuracy: 0.8201 - val_loss: 0.6151 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6064 - accuracy: 0.81 - ETA: 0s - loss: 0.7731 - accuracy: 0.69 - ETA: 0s - loss: 0.7740 - accuracy: 0.68 - ETA: 0s - loss: 0.7526 - accuracy: 0.69 - ETA: 0s - loss: 0.7146 - accuracy: 0.72 - ETA: 0s - loss: 0.6878 - accuracy: 0.73 - ETA: 0s - loss: 0.6754 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6754 - accuracy: 0.7507 - val_loss: 0.7566 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.68 - ETA: 0s - loss: 0.5902 - accuracy: 0.78 - ETA: 0s - loss: 0.5431 - accuracy: 0.82 - ETA: 0s - loss: 0.5365 - accuracy: 0.82 - ETA: 0s - loss: 0.5372 - accuracy: 0.82 - ETA: 0s - loss: 0.5301 - accuracy: 0.82 - ETA: 0s - loss: 0.5323 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5323 - accuracy: 0.8272 - val_loss: 0.9488 - val_accuracy: 0.6269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.81 - ETA: 0s - loss: 0.5012 - accuracy: 0.81 - ETA: 0s - loss: 0.4974 - accuracy: 0.82 - ETA: 0s - loss: 0.4657 - accuracy: 0.84 - ETA: 0s - loss: 0.4641 - accuracy: 0.84 - ETA: 0s - loss: 0.4662 - accuracy: 0.84 - ETA: 0s - loss: 0.4612 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4639 - accuracy: 0.8485 - val_loss: 2.1883 - val_accuracy: 0.6716\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 1.3949 - accuracy: 0.78 - ETA: 0s - loss: 0.6386 - accuracy: 0.83 - ETA: 0s - loss: 0.5495 - accuracy: 0.85 - ETA: 0s - loss: 0.5121 - accuracy: 0.85 - ETA: 0s - loss: 0.4895 - accuracy: 0.85 - ETA: 0s - loss: 0.4743 - accuracy: 0.86 - ETA: 0s - loss: 0.4717 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4675 - accuracy: 0.8641 - val_loss: 1.1538 - val_accuracy: 0.7134\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.81 - ETA: 0s - loss: 0.5189 - accuracy: 0.81 - ETA: 0s - loss: 0.5209 - accuracy: 0.82 - ETA: 0s - loss: 0.4826 - accuracy: 0.84 - ETA: 0s - loss: 0.4645 - accuracy: 0.85 - ETA: 0s - loss: 0.4497 - accuracy: 0.85 - ETA: 0s - loss: 0.4375 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4333 - accuracy: 0.8630 - val_loss: 0.9129 - val_accuracy: 0.7119\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.93 - ETA: 0s - loss: 0.3458 - accuracy: 0.90 - ETA: 0s - loss: 0.3374 - accuracy: 0.90 - ETA: 0s - loss: 0.3658 - accuracy: 0.89 - ETA: 0s - loss: 0.3626 - accuracy: 0.89 - ETA: 0s - loss: 0.3701 - accuracy: 0.88 - ETA: 0s - loss: 0.3747 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8835 - val_loss: 0.9845 - val_accuracy: 0.7328\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 1.00 - ETA: 0s - loss: 0.3506 - accuracy: 0.88 - ETA: 0s - loss: 0.3355 - accuracy: 0.89 - ETA: 0s - loss: 0.3281 - accuracy: 0.90 - ETA: 0s - loss: 0.3256 - accuracy: 0.90 - ETA: 0s - loss: 0.3236 - accuracy: 0.90 - ETA: 0s - loss: 0.3294 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3348 - accuracy: 0.9015 - val_loss: 2.5539 - val_accuracy: 0.6851\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.87 - ETA: 0s - loss: 0.3812 - accuracy: 0.88 - ETA: 0s - loss: 0.3710 - accuracy: 0.89 - ETA: 0s - loss: 0.3693 - accuracy: 0.88 - ETA: 0s - loss: 0.3499 - accuracy: 0.89 - ETA: 0s - loss: 0.3546 - accuracy: 0.89 - ETA: 0s - loss: 0.3946 - accuracy: 0.89 - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8858 - val_loss: 1.3703 - val_accuracy: 0.7149\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.75 - ETA: 0s - loss: 0.5034 - accuracy: 0.84 - ETA: 0s - loss: 0.5447 - accuracy: 0.83 - ETA: 0s - loss: 0.5730 - accuracy: 0.83 - ETA: 0s - loss: 0.5740 - accuracy: 0.83 - ETA: 0s - loss: 0.5730 - accuracy: 0.82 - ETA: 0s - loss: 0.5911 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5971 - accuracy: 0.8111 - val_loss: 0.7550 - val_accuracy: 0.7388\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6289 - accuracy: 0.75 - ETA: 0s - loss: 0.5857 - accuracy: 0.78 - ETA: 0s - loss: 0.5570 - accuracy: 0.80 - ETA: 0s - loss: 0.5598 - accuracy: 0.80 - ETA: 0s - loss: 0.5795 - accuracy: 0.79 - ETA: 0s - loss: 0.5755 - accuracy: 0.79 - ETA: 0s - loss: 0.5669 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5566 - accuracy: 0.8052 - val_loss: 0.7760 - val_accuracy: 0.7478\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.81 - ETA: 0s - loss: 0.5044 - accuracy: 0.83 - ETA: 0s - loss: 0.5052 - accuracy: 0.83 - ETA: 0s - loss: 0.5244 - accuracy: 0.82 - ETA: 0s - loss: 0.5211 - accuracy: 0.82 - ETA: 0s - loss: 0.5068 - accuracy: 0.83 - ETA: 0s - loss: 0.5042 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5129 - accuracy: 0.8384 - val_loss: 0.7892 - val_accuracy: 0.7328\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.78 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - ETA: 0s - loss: 0.4674 - accuracy: 0.86 - ETA: 0s - loss: 0.4607 - accuracy: 0.86 - ETA: 0s - loss: 0.4440 - accuracy: 0.86 - ETA: 0s - loss: 0.4510 - accuracy: 0.86 - ETA: 0s - loss: 0.4513 - accuracy: 0.86 - ETA: 0s - loss: 0.4490 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4490 - accuracy: 0.8626 - val_loss: 0.9016 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.81 - ETA: 0s - loss: 0.4271 - accuracy: 0.85 - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.3836 - accuracy: 0.88 - ETA: 0s - loss: 0.4033 - accuracy: 0.87 - ETA: 0s - loss: 0.4016 - accuracy: 0.87 - ETA: 0s - loss: 0.4048 - accuracy: 0.87 - ETA: 0s - loss: 0.4110 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4117 - accuracy: 0.8738 - val_loss: 0.9538 - val_accuracy: 0.7299\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.90 - ETA: 0s - loss: 0.4279 - accuracy: 0.87 - ETA: 0s - loss: 0.4641 - accuracy: 0.86 - ETA: 0s - loss: 0.4692 - accuracy: 0.85 - ETA: 0s - loss: 0.4752 - accuracy: 0.85 - ETA: 0s - loss: 0.4682 - accuracy: 0.85 - ETA: 0s - loss: 0.4591 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4567 - accuracy: 0.8585 - val_loss: 0.9408 - val_accuracy: 0.7313\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.93 - ETA: 0s - loss: 0.3659 - accuracy: 0.89 - ETA: 0s - loss: 0.4121 - accuracy: 0.87 - ETA: 0s - loss: 0.4084 - accuracy: 0.87 - ETA: 0s - loss: 0.4111 - accuracy: 0.87 - ETA: 0s - loss: 0.4024 - accuracy: 0.87 - ETA: 0s - loss: 0.4056 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8764 - val_loss: 0.9244 - val_accuracy: 0.7179\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.93 - ETA: 0s - loss: 0.3926 - accuracy: 0.87 - ETA: 0s - loss: 0.3896 - accuracy: 0.88 - ETA: 0s - loss: 0.4004 - accuracy: 0.87 - ETA: 0s - loss: 0.4120 - accuracy: 0.87 - ETA: 0s - loss: 0.4187 - accuracy: 0.87 - ETA: 0s - loss: 0.4203 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4221 - accuracy: 0.8727 - val_loss: 0.8193 - val_accuracy: 0.7299\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.81 - ETA: 0s - loss: 0.3829 - accuracy: 0.88 - ETA: 0s - loss: 0.3775 - accuracy: 0.89 - ETA: 0s - loss: 0.3906 - accuracy: 0.89 - ETA: 0s - loss: 0.4011 - accuracy: 0.88 - ETA: 0s - loss: 0.4102 - accuracy: 0.88 - ETA: 0s - loss: 0.4160 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4204 - accuracy: 0.8738 - val_loss: 0.8603 - val_accuracy: 0.7179\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8279 - accuracy: 0.75 - ETA: 0s - loss: 0.4822 - accuracy: 0.84 - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.3946 - accuracy: 0.88 - ETA: 0s - loss: 0.3986 - accuracy: 0.88 - ETA: 0s - loss: 0.4014 - accuracy: 0.88 - ETA: 0s - loss: 0.4191 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4245 - accuracy: 0.8723 - val_loss: 0.5983 - val_accuracy: 0.7463\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.87 - ETA: 0s - loss: 0.3989 - accuracy: 0.89 - ETA: 0s - loss: 0.4233 - accuracy: 0.87 - ETA: 0s - loss: 0.3959 - accuracy: 0.88 - ETA: 0s - loss: 0.4093 - accuracy: 0.87 - ETA: 0s - loss: 0.3995 - accuracy: 0.88 - ETA: 0s - loss: 0.3919 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8884 - val_loss: 1.3943 - val_accuracy: 0.7269\n",
      "Epoch 30/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3716 - accuracy: 0.84 - ETA: 0s - loss: 0.3189 - accuracy: 0.90 - ETA: 0s - loss: 0.3475 - accuracy: 0.89 - ETA: 0s - loss: 0.3408 - accuracy: 0.89 - ETA: 0s - loss: 0.3556 - accuracy: 0.89 - ETA: 0s - loss: 0.3588 - accuracy: 0.89 - ETA: 0s - loss: 0.3652 - accuracy: 0.8908Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3658 - accuracy: 0.8906 - val_loss: 0.9856 - val_accuracy: 0.7373\n",
      "Epoch 00030: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.75 - ETA: 0s - loss: 1.5149 - accuracy: 0.64 - ETA: 0s - loss: 1.0908 - accuracy: 0.66 - ETA: 0s - loss: 0.9329 - accuracy: 0.67 - ETA: 0s - loss: 0.8516 - accuracy: 0.68 - ETA: 0s - loss: 0.8021 - accuracy: 0.69 - ETA: 0s - loss: 0.7630 - accuracy: 0.69 - 0s 6ms/step - loss: 0.7425 - accuracy: 0.6954 - val_loss: 0.5620 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.75 - ETA: 0s - loss: 0.5480 - accuracy: 0.75 - ETA: 0s - loss: 0.5244 - accuracy: 0.74 - ETA: 0s - loss: 0.5401 - accuracy: 0.73 - ETA: 0s - loss: 0.5357 - accuracy: 0.74 - ETA: 0s - loss: 0.5349 - accuracy: 0.74 - ETA: 0s - loss: 0.5376 - accuracy: 0.74 - 0s 5ms/step - loss: 0.5366 - accuracy: 0.7521 - val_loss: 0.6158 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.93 - ETA: 0s - loss: 0.5655 - accuracy: 0.78 - ETA: 0s - loss: 0.5193 - accuracy: 0.80 - ETA: 0s - loss: 0.5226 - accuracy: 0.79 - ETA: 0s - loss: 0.5140 - accuracy: 0.79 - ETA: 0s - loss: 0.5064 - accuracy: 0.79 - ETA: 0s - loss: 0.5080 - accuracy: 0.79 - ETA: 0s - loss: 0.5108 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5098 - accuracy: 0.7943 - val_loss: 0.5560 - val_accuracy: 0.7149\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.84 - ETA: 0s - loss: 0.4903 - accuracy: 0.80 - ETA: 0s - loss: 0.5136 - accuracy: 0.81 - ETA: 0s - loss: 0.5218 - accuracy: 0.79 - ETA: 0s - loss: 0.5060 - accuracy: 0.80 - ETA: 0s - loss: 0.5070 - accuracy: 0.80 - ETA: 0s - loss: 0.4881 - accuracy: 0.81 - ETA: 0s - loss: 0.4810 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4805 - accuracy: 0.8167 - val_loss: 0.6678 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.81 - ETA: 0s - loss: 0.3816 - accuracy: 0.84 - ETA: 0s - loss: 0.4050 - accuracy: 0.84 - ETA: 0s - loss: 0.4194 - accuracy: 0.82 - ETA: 0s - loss: 0.4149 - accuracy: 0.82 - ETA: 0s - loss: 0.4274 - accuracy: 0.82 - ETA: 0s - loss: 0.4337 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8309 - val_loss: 0.5932 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.78 - ETA: 0s - loss: 0.4877 - accuracy: 0.82 - ETA: 0s - loss: 0.4625 - accuracy: 0.82 - ETA: 0s - loss: 0.4484 - accuracy: 0.83 - ETA: 0s - loss: 0.4397 - accuracy: 0.83 - ETA: 0s - loss: 0.4604 - accuracy: 0.82 - ETA: 0s - loss: 0.4587 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4628 - accuracy: 0.8257 - val_loss: 0.5520 - val_accuracy: 0.7343\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.87 - ETA: 0s - loss: 0.4046 - accuracy: 0.86 - ETA: 0s - loss: 0.3907 - accuracy: 0.87 - ETA: 0s - loss: 0.3765 - accuracy: 0.88 - ETA: 0s - loss: 0.4103 - accuracy: 0.87 - ETA: 0s - loss: 0.4464 - accuracy: 0.85 - ETA: 0s - loss: 0.4547 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4565 - accuracy: 0.8514 - val_loss: 0.6195 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.90 - ETA: 0s - loss: 0.4025 - accuracy: 0.84 - ETA: 0s - loss: 0.4047 - accuracy: 0.83 - ETA: 0s - loss: 0.4343 - accuracy: 0.83 - ETA: 0s - loss: 0.4647 - accuracy: 0.83 - ETA: 0s - loss: 0.4524 - accuracy: 0.84 - ETA: 0s - loss: 0.4532 - accuracy: 0.84 - ETA: 0s - loss: 0.4488 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4457 - accuracy: 0.8458 - val_loss: 0.7996 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.87 - ETA: 0s - loss: 0.3653 - accuracy: 0.87 - ETA: 0s - loss: 0.3772 - accuracy: 0.87 - ETA: 0s - loss: 0.3850 - accuracy: 0.86 - ETA: 0s - loss: 0.3674 - accuracy: 0.86 - ETA: 0s - loss: 0.3703 - accuracy: 0.87 - ETA: 0s - loss: 0.3674 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3707 - accuracy: 0.8716 - val_loss: 0.5802 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.84 - ETA: 0s - loss: 0.3484 - accuracy: 0.89 - ETA: 0s - loss: 0.3381 - accuracy: 0.89 - ETA: 0s - loss: 0.3279 - accuracy: 0.89 - ETA: 0s - loss: 0.3248 - accuracy: 0.89 - ETA: 0s - loss: 0.3370 - accuracy: 0.88 - ETA: 0s - loss: 0.3457 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3511 - accuracy: 0.8817 - val_loss: 0.9022 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.93 - ETA: 0s - loss: 0.3345 - accuracy: 0.87 - ETA: 0s - loss: 0.2939 - accuracy: 0.90 - ETA: 0s - loss: 0.3010 - accuracy: 0.90 - ETA: 0s - loss: 0.3169 - accuracy: 0.89 - ETA: 0s - loss: 0.3492 - accuracy: 0.89 - ETA: 0s - loss: 0.3477 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3481 - accuracy: 0.8895 - val_loss: 0.7508 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.81 - ETA: 0s - loss: 0.3116 - accuracy: 0.90 - ETA: 0s - loss: 0.3728 - accuracy: 0.87 - ETA: 0s - loss: 0.3672 - accuracy: 0.88 - ETA: 0s - loss: 0.3563 - accuracy: 0.88 - ETA: 0s - loss: 0.3500 - accuracy: 0.88 - ETA: 0s - loss: 0.3463 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3463 - accuracy: 0.8839 - val_loss: 1.2164 - val_accuracy: 0.6388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.68 - ETA: 0s - loss: 0.2888 - accuracy: 0.87 - ETA: 0s - loss: 0.3088 - accuracy: 0.88 - ETA: 0s - loss: 0.3075 - accuracy: 0.88 - ETA: 0s - loss: 0.3112 - accuracy: 0.88 - ETA: 0s - loss: 0.3209 - accuracy: 0.88 - ETA: 0s - loss: 0.3347 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8750 - val_loss: 0.5756 - val_accuracy: 0.7478\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.90 - ETA: 0s - loss: 0.4296 - accuracy: 0.83 - ETA: 0s - loss: 0.5460 - accuracy: 0.84 - ETA: 0s - loss: 0.5164 - accuracy: 0.84 - ETA: 0s - loss: 0.4845 - accuracy: 0.85 - ETA: 0s - loss: 0.4653 - accuracy: 0.86 - ETA: 0s - loss: 0.4744 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4788 - accuracy: 0.8563 - val_loss: 0.7550 - val_accuracy: 0.7119\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4627 - accuracy: 0.81 - ETA: 0s - loss: 0.4346 - accuracy: 0.85 - ETA: 0s - loss: 0.4630 - accuracy: 0.86 - ETA: 0s - loss: 0.4613 - accuracy: 0.85 - ETA: 0s - loss: 0.4502 - accuracy: 0.86 - ETA: 0s - loss: 0.4448 - accuracy: 0.86 - ETA: 0s - loss: 0.4345 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4373 - accuracy: 0.8641 - val_loss: 0.5656 - val_accuracy: 0.7269\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.87 - ETA: 0s - loss: 0.3744 - accuracy: 0.88 - ETA: 0s - loss: 0.3358 - accuracy: 0.89 - ETA: 0s - loss: 0.3394 - accuracy: 0.89 - ETA: 0s - loss: 0.3835 - accuracy: 0.88 - ETA: 0s - loss: 0.4147 - accuracy: 0.86 - ETA: 0s - loss: 0.4608 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4575 - accuracy: 0.8466 - val_loss: 0.9939 - val_accuracy: 0.6955\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.75 - ETA: 0s - loss: 0.3850 - accuracy: 0.85 - ETA: 0s - loss: 0.3861 - accuracy: 0.85 - ETA: 0s - loss: 0.3907 - accuracy: 0.85 - ETA: 0s - loss: 0.4011 - accuracy: 0.85 - ETA: 0s - loss: 0.3956 - accuracy: 0.86 - ETA: 0s - loss: 0.4460 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4747 - accuracy: 0.8455 - val_loss: 0.7096 - val_accuracy: 0.7224\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.87 - ETA: 0s - loss: 0.3862 - accuracy: 0.88 - ETA: 0s - loss: 0.4054 - accuracy: 0.87 - ETA: 0s - loss: 0.4013 - accuracy: 0.88 - ETA: 0s - loss: 0.4246 - accuracy: 0.87 - ETA: 0s - loss: 0.4300 - accuracy: 0.87 - ETA: 0s - loss: 0.4322 - accuracy: 0.87 - ETA: 0s - loss: 0.4415 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4415 - accuracy: 0.8664 - val_loss: 0.8043 - val_accuracy: 0.7194\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4571 - accuracy: 0.84 - ETA: 0s - loss: 0.3974 - accuracy: 0.87 - ETA: 0s - loss: 0.4091 - accuracy: 0.88 - ETA: 0s - loss: 0.4132 - accuracy: 0.88 - ETA: 0s - loss: 0.4154 - accuracy: 0.87 - ETA: 0s - loss: 0.4015 - accuracy: 0.88 - ETA: 0s - loss: 0.3980 - accuracy: 0.88 - ETA: 0s - loss: 0.4035 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4009 - accuracy: 0.8813 - val_loss: 0.7977 - val_accuracy: 0.7343\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.93 - ETA: 0s - loss: 0.7431 - accuracy: 0.90 - ETA: 0s - loss: 0.5555 - accuracy: 0.90 - ETA: 0s - loss: 0.4878 - accuracy: 0.90 - ETA: 0s - loss: 0.4813 - accuracy: 0.89 - ETA: 0s - loss: 0.4572 - accuracy: 0.89 - ETA: 0s - loss: 0.4583 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4418 - accuracy: 0.8899 - val_loss: 0.7588 - val_accuracy: 0.7478\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.90 - ETA: 0s - loss: 0.3575 - accuracy: 0.89 - ETA: 0s - loss: 0.3275 - accuracy: 0.90 - ETA: 0s - loss: 0.3465 - accuracy: 0.90 - ETA: 0s - loss: 0.3565 - accuracy: 0.89 - ETA: 0s - loss: 0.3501 - accuracy: 0.90 - ETA: 0s - loss: 0.3473 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3417 - accuracy: 0.9052 - val_loss: 0.9885 - val_accuracy: 0.7328\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.78 - ETA: 0s - loss: 0.2812 - accuracy: 0.92 - ETA: 0s - loss: 0.2996 - accuracy: 0.91 - ETA: 0s - loss: 0.3010 - accuracy: 0.91 - ETA: 0s - loss: 0.2946 - accuracy: 0.92 - ETA: 0s - loss: 0.3104 - accuracy: 0.91 - ETA: 0s - loss: 0.3051 - accuracy: 0.91 - 0s 5ms/step - loss: 0.3135 - accuracy: 0.9134 - val_loss: 1.0034 - val_accuracy: 0.7284\n",
      "Epoch 23/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.90 - ETA: 0s - loss: 0.2809 - accuracy: 0.93 - ETA: 0s - loss: 0.2849 - accuracy: 0.92 - ETA: 0s - loss: 0.3035 - accuracy: 0.92 - ETA: 0s - loss: 0.3132 - accuracy: 0.91 - ETA: 0s - loss: 0.3091 - accuracy: 0.92 - ETA: 0s - loss: 0.3218 - accuracy: 0.91 - ETA: 0s - loss: 0.3291 - accuracy: 0.91 - ETA: 0s - loss: 0.3241 - accuracy: 0.9164Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9160 - val_loss: 1.2735 - val_accuracy: 0.7209\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 12a4c28c4ce4237ab0a80465a9113d53</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7452736298243204</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.18906402645033937</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1854 - accuracy: 0.34 - ETA: 0s - loss: 4.9299 - accuracy: 0.60 - ETA: 0s - loss: 4.4929 - accuracy: 0.59 - ETA: 0s - loss: 3.8325 - accuracy: 0.59 - ETA: 0s - loss: 3.2047 - accuracy: 0.60 - ETA: 0s - loss: 2.8224 - accuracy: 0.61 - ETA: 0s - loss: 2.5827 - accuracy: 0.60 - 1s 7ms/step - loss: 2.4420 - accuracy: 0.6140 - val_loss: 0.6333 - val_accuracy: 0.6627\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.81 - ETA: 0s - loss: 0.6694 - accuracy: 0.71 - ETA: 0s - loss: 0.7155 - accuracy: 0.69 - ETA: 0s - loss: 0.6831 - accuracy: 0.69 - ETA: 0s - loss: 0.7028 - accuracy: 0.69 - ETA: 0s - loss: 0.6927 - accuracy: 0.69 - ETA: 0s - loss: 0.6926 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6973 - accuracy: 0.6883 - val_loss: 0.6769 - val_accuracy: 0.6134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6734 - accuracy: 0.56 - ETA: 0s - loss: 0.6640 - accuracy: 0.66 - ETA: 0s - loss: 0.6402 - accuracy: 0.68 - ETA: 0s - loss: 0.6527 - accuracy: 0.71 - ETA: 0s - loss: 0.6899 - accuracy: 0.71 - ETA: 0s - loss: 0.6774 - accuracy: 0.71 - ETA: 0s - loss: 0.6723 - accuracy: 0.71 - ETA: 0s - loss: 0.6635 - accuracy: 0.71 - 0s 6ms/step - loss: 0.6616 - accuracy: 0.7208 - val_loss: 0.6104 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.68 - ETA: 0s - loss: 0.6218 - accuracy: 0.74 - ETA: 0s - loss: 0.6085 - accuracy: 0.75 - ETA: 0s - loss: 0.6060 - accuracy: 0.75 - ETA: 0s - loss: 0.6117 - accuracy: 0.74 - ETA: 0s - loss: 0.6050 - accuracy: 0.74 - ETA: 0s - loss: 0.6038 - accuracy: 0.74 - ETA: 0s - loss: 0.5920 - accuracy: 0.75 - ETA: 0s - loss: 0.5860 - accuracy: 0.75 - 1s 6ms/step - loss: 0.5871 - accuracy: 0.7563 - val_loss: 0.5936 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.71 - ETA: 0s - loss: 0.5516 - accuracy: 0.78 - ETA: 0s - loss: 0.5818 - accuracy: 0.77 - ETA: 0s - loss: 0.5962 - accuracy: 0.76 - ETA: 0s - loss: 0.5858 - accuracy: 0.77 - ETA: 0s - loss: 0.5851 - accuracy: 0.77 - ETA: 0s - loss: 0.5864 - accuracy: 0.77 - ETA: 0s - loss: 0.5866 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5866 - accuracy: 0.7727 - val_loss: 0.5818 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.78 - ETA: 0s - loss: 0.5344 - accuracy: 0.82 - ETA: 0s - loss: 0.5587 - accuracy: 0.79 - ETA: 0s - loss: 0.5727 - accuracy: 0.78 - ETA: 0s - loss: 0.5635 - accuracy: 0.78 - ETA: 0s - loss: 0.5702 - accuracy: 0.78 - ETA: 0s - loss: 0.5740 - accuracy: 0.78 - ETA: 0s - loss: 0.5718 - accuracy: 0.78 - 0s 6ms/step - loss: 0.5733 - accuracy: 0.7824 - val_loss: 0.5777 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.84 - ETA: 0s - loss: 0.5039 - accuracy: 0.81 - ETA: 0s - loss: 0.5501 - accuracy: 0.79 - ETA: 0s - loss: 0.5546 - accuracy: 0.78 - ETA: 0s - loss: 0.5653 - accuracy: 0.78 - ETA: 0s - loss: 0.5516 - accuracy: 0.79 - ETA: 0s - loss: 0.5471 - accuracy: 0.79 - ETA: 0s - loss: 0.5419 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5409 - accuracy: 0.7999 - val_loss: 0.5689 - val_accuracy: 0.7149\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.78 - ETA: 0s - loss: 0.5226 - accuracy: 0.82 - ETA: 0s - loss: 0.5207 - accuracy: 0.82 - ETA: 0s - loss: 0.5207 - accuracy: 0.82 - ETA: 0s - loss: 0.5311 - accuracy: 0.82 - ETA: 0s - loss: 0.5449 - accuracy: 0.81 - ETA: 0s - loss: 0.5452 - accuracy: 0.81 - ETA: 0s - loss: 0.5418 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5426 - accuracy: 0.8160 - val_loss: 0.5646 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.84 - ETA: 0s - loss: 0.5247 - accuracy: 0.80 - ETA: 0s - loss: 0.5392 - accuracy: 0.81 - ETA: 0s - loss: 0.5264 - accuracy: 0.81 - ETA: 0s - loss: 0.5150 - accuracy: 0.81 - ETA: 0s - loss: 0.5138 - accuracy: 0.81 - ETA: 0s - loss: 0.5134 - accuracy: 0.82 - ETA: 0s - loss: 0.5189 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5201 - accuracy: 0.8197 - val_loss: 0.5556 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.81 - ETA: 0s - loss: 0.5810 - accuracy: 0.77 - ETA: 0s - loss: 0.5406 - accuracy: 0.79 - ETA: 0s - loss: 0.5233 - accuracy: 0.80 - ETA: 0s - loss: 0.5170 - accuracy: 0.80 - ETA: 0s - loss: 0.4997 - accuracy: 0.81 - ETA: 0s - loss: 0.5122 - accuracy: 0.82 - ETA: 0s - loss: 0.5284 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5237 - accuracy: 0.8257 - val_loss: 0.5531 - val_accuracy: 0.7507\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6729 - accuracy: 0.68 - ETA: 0s - loss: 0.4891 - accuracy: 0.81 - ETA: 0s - loss: 0.4952 - accuracy: 0.82 - ETA: 0s - loss: 0.5078 - accuracy: 0.81 - ETA: 0s - loss: 0.5078 - accuracy: 0.82 - ETA: 0s - loss: 0.5184 - accuracy: 0.81 - ETA: 0s - loss: 0.5252 - accuracy: 0.82 - ETA: 0s - loss: 0.5239 - accuracy: 0.82 - 0s 6ms/step - loss: 0.5265 - accuracy: 0.8212 - val_loss: 0.5588 - val_accuracy: 0.7672\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.90 - ETA: 0s - loss: 0.4727 - accuracy: 0.84 - ETA: 0s - loss: 0.5715 - accuracy: 0.82 - ETA: 0s - loss: 0.5745 - accuracy: 0.82 - ETA: 0s - loss: 0.6434 - accuracy: 0.81 - ETA: 0s - loss: 0.6424 - accuracy: 0.79 - ETA: 0s - loss: 0.6415 - accuracy: 0.80 - ETA: 0s - loss: 0.6436 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6459 - accuracy: 0.7969 - val_loss: 0.5963 - val_accuracy: 0.7269\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.81 - ETA: 0s - loss: 0.6735 - accuracy: 0.75 - ETA: 0s - loss: 0.6285 - accuracy: 0.79 - ETA: 0s - loss: 0.6467 - accuracy: 0.78 - ETA: 0s - loss: 0.6474 - accuracy: 0.77 - ETA: 0s - loss: 0.6397 - accuracy: 0.77 - ETA: 0s - loss: 0.6359 - accuracy: 0.77 - ETA: 0s - loss: 0.6355 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6333 - accuracy: 0.7757 - val_loss: 0.6343 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.65 - ETA: 0s - loss: 0.6692 - accuracy: 0.78 - ETA: 0s - loss: 0.6209 - accuracy: 0.78 - ETA: 0s - loss: 0.6148 - accuracy: 0.79 - ETA: 0s - loss: 0.6125 - accuracy: 0.80 - ETA: 0s - loss: 0.6100 - accuracy: 0.79 - ETA: 0s - loss: 0.6101 - accuracy: 0.79 - ETA: 0s - loss: 0.6114 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6074 - accuracy: 0.7891 - val_loss: 0.7673 - val_accuracy: 0.7060\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.87 - ETA: 0s - loss: 0.7304 - accuracy: 0.79 - ETA: 0s - loss: 0.6605 - accuracy: 0.78 - ETA: 0s - loss: 0.6699 - accuracy: 0.78 - ETA: 0s - loss: 0.7003 - accuracy: 0.79 - ETA: 0s - loss: 0.6753 - accuracy: 0.79 - ETA: 0s - loss: 0.7793 - accuracy: 0.78 - ETA: 0s - loss: 0.7689 - accuracy: 0.78 - 0s 5ms/step - loss: 0.7513 - accuracy: 0.7846 - val_loss: 1.4933 - val_accuracy: 0.6567\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.4574 - accuracy: 0.59 - ETA: 0s - loss: 0.7389 - accuracy: 0.72 - ETA: 0s - loss: 0.7837 - accuracy: 0.73 - ETA: 0s - loss: 0.7230 - accuracy: 0.74 - ETA: 0s - loss: 0.8101 - accuracy: 0.76 - ETA: 0s - loss: 0.7926 - accuracy: 0.76 - ETA: 0s - loss: 0.7846 - accuracy: 0.75 - ETA: 0s - loss: 0.7590 - accuracy: 0.76 - 0s 5ms/step - loss: 0.7929 - accuracy: 0.7652 - val_loss: 0.8177 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.78 - ETA: 0s - loss: 1.4504 - accuracy: 0.75 - ETA: 0s - loss: 1.0807 - accuracy: 0.74 - ETA: 0s - loss: 0.9348 - accuracy: 0.75 - ETA: 0s - loss: 0.8643 - accuracy: 0.75 - ETA: 0s - loss: 0.8578 - accuracy: 0.75 - ETA: 0s - loss: 0.8280 - accuracy: 0.75 - ETA: 0s - loss: 0.8168 - accuracy: 0.74 - 0s 6ms/step - loss: 0.8053 - accuracy: 0.7458 - val_loss: 0.6760 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6652 - accuracy: 0.71 - ETA: 0s - loss: 0.7014 - accuracy: 0.69 - ETA: 0s - loss: 0.6728 - accuracy: 0.71 - ETA: 0s - loss: 0.6513 - accuracy: 0.73 - ETA: 0s - loss: 0.6526 - accuracy: 0.73 - ETA: 0s - loss: 0.6463 - accuracy: 0.74 - ETA: 0s - loss: 0.6394 - accuracy: 0.75 - ETA: 0s - loss: 0.6439 - accuracy: 0.74 - ETA: 0s - loss: 0.6376 - accuracy: 0.75 - 1s 6ms/step - loss: 0.6405 - accuracy: 0.7488 - val_loss: 0.6531 - val_accuracy: 0.7448\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.84 - ETA: 0s - loss: 0.6911 - accuracy: 0.70 - ETA: 0s - loss: 0.6524 - accuracy: 0.74 - ETA: 0s - loss: 0.6474 - accuracy: 0.75 - ETA: 0s - loss: 0.6410 - accuracy: 0.75 - ETA: 0s - loss: 0.6444 - accuracy: 0.75 - ETA: 0s - loss: 0.6504 - accuracy: 0.75 - ETA: 0s - loss: 0.6439 - accuracy: 0.75 - ETA: 0s - loss: 0.6420 - accuracy: 0.75 - 0s 6ms/step - loss: 0.6403 - accuracy: 0.7555 - val_loss: 0.6847 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.71 - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.6119 - accuracy: 0.77 - ETA: 0s - loss: 0.6169 - accuracy: 0.77 - ETA: 0s - loss: 0.6147 - accuracy: 0.77 - ETA: 0s - loss: 0.6239 - accuracy: 0.76 - ETA: 0s - loss: 0.6244 - accuracy: 0.76 - ETA: 0s - loss: 0.6351 - accuracy: 0.75 - ETA: 0s - loss: 0.6347 - accuracy: 0.75 - 0s 6ms/step - loss: 0.6348 - accuracy: 0.7592 - val_loss: 0.6623 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.71 - ETA: 0s - loss: 0.6199 - accuracy: 0.75 - ETA: 0s - loss: 0.6143 - accuracy: 0.76 - ETA: 0s - loss: 0.6131 - accuracy: 0.77 - ETA: 0s - loss: 0.6167 - accuracy: 0.76 - ETA: 0s - loss: 0.6168 - accuracy: 0.76 - ETA: 0s - loss: 0.6163 - accuracy: 0.76 - ETA: 0s - loss: 0.6140 - accuracy: 0.7716Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6140 - accuracy: 0.7716 - val_loss: 0.6636 - val_accuracy: 0.7358\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2974 - accuracy: 0.53 - ETA: 0s - loss: 5.8210 - accuracy: 0.59 - ETA: 0s - loss: 4.6917 - accuracy: 0.60 - ETA: 0s - loss: 3.8745 - accuracy: 0.60 - ETA: 0s - loss: 3.3105 - accuracy: 0.61 - ETA: 0s - loss: 2.9946 - accuracy: 0.61 - ETA: 0s - loss: 2.6674 - accuracy: 0.62 - ETA: 0s - loss: 2.4424 - accuracy: 0.62 - 1s 6ms/step - loss: 2.4424 - accuracy: 0.6215 - val_loss: 0.5544 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.71 - ETA: 0s - loss: 0.7033 - accuracy: 0.70 - ETA: 0s - loss: 0.9122 - accuracy: 0.67 - ETA: 0s - loss: 0.8795 - accuracy: 0.65 - ETA: 0s - loss: 0.8223 - accuracy: 0.67 - ETA: 0s - loss: 0.7902 - accuracy: 0.68 - ETA: 0s - loss: 0.7533 - accuracy: 0.69 - ETA: 0s - loss: 0.7347 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7341 - accuracy: 0.6995 - val_loss: 0.5928 - val_accuracy: 0.7418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6211 - accuracy: 0.78 - ETA: 0s - loss: 0.6122 - accuracy: 0.76 - ETA: 0s - loss: 0.6241 - accuracy: 0.73 - ETA: 0s - loss: 0.6108 - accuracy: 0.74 - ETA: 0s - loss: 0.6091 - accuracy: 0.74 - ETA: 0s - loss: 0.6077 - accuracy: 0.74 - ETA: 0s - loss: 0.6145 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6134 - accuracy: 0.7380 - val_loss: 0.5894 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.81 - ETA: 0s - loss: 0.5812 - accuracy: 0.76 - ETA: 0s - loss: 0.5990 - accuracy: 0.77 - ETA: 0s - loss: 0.5970 - accuracy: 0.76 - ETA: 0s - loss: 0.5996 - accuracy: 0.76 - ETA: 0s - loss: 0.5998 - accuracy: 0.76 - ETA: 0s - loss: 0.5985 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5998 - accuracy: 0.7671 - val_loss: 0.5899 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.81 - ETA: 0s - loss: 0.5637 - accuracy: 0.79 - ETA: 0s - loss: 0.6153 - accuracy: 0.76 - ETA: 0s - loss: 0.6089 - accuracy: 0.76 - ETA: 0s - loss: 0.5978 - accuracy: 0.77 - ETA: 0s - loss: 0.5864 - accuracy: 0.77 - ETA: 0s - loss: 0.5759 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5837 - accuracy: 0.7798 - val_loss: 0.5680 - val_accuracy: 0.7552\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.65 - ETA: 0s - loss: 0.5337 - accuracy: 0.80 - ETA: 0s - loss: 0.5407 - accuracy: 0.80 - ETA: 0s - loss: 0.5567 - accuracy: 0.79 - ETA: 0s - loss: 0.5742 - accuracy: 0.79 - ETA: 0s - loss: 0.5773 - accuracy: 0.79 - ETA: 0s - loss: 0.5764 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5871 - accuracy: 0.7895 - val_loss: 0.5881 - val_accuracy: 0.7015\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.71 - ETA: 0s - loss: 0.6189 - accuracy: 0.76 - ETA: 0s - loss: 0.6293 - accuracy: 0.78 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.6434 - accuracy: 0.79 - ETA: 0s - loss: 0.6549 - accuracy: 0.79 - ETA: 0s - loss: 0.7354 - accuracy: 0.79 - 0s 4ms/step - loss: 0.7628 - accuracy: 0.7902 - val_loss: 0.6133 - val_accuracy: 0.7209\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.62 - ETA: 0s - loss: 0.6388 - accuracy: 0.75 - ETA: 0s - loss: 0.6455 - accuracy: 0.75 - ETA: 0s - loss: 0.6424 - accuracy: 0.75 - ETA: 0s - loss: 0.8967 - accuracy: 0.76 - ETA: 0s - loss: 0.8770 - accuracy: 0.75 - ETA: 0s - loss: 0.8866 - accuracy: 0.76 - 0s 4ms/step - loss: 0.8885 - accuracy: 0.7641 - val_loss: 0.5962 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.81 - ETA: 0s - loss: 0.7228 - accuracy: 0.76 - ETA: 0s - loss: 0.7008 - accuracy: 0.75 - ETA: 0s - loss: 0.6604 - accuracy: 0.76 - ETA: 0s - loss: 0.6437 - accuracy: 0.76 - ETA: 0s - loss: 0.6464 - accuracy: 0.77 - ETA: 0s - loss: 0.6481 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6474 - accuracy: 0.7675 - val_loss: 0.5924 - val_accuracy: 0.7463\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.71 - ETA: 0s - loss: 0.7788 - accuracy: 0.75 - ETA: 0s - loss: 0.6844 - accuracy: 0.78 - ETA: 0s - loss: 0.6463 - accuracy: 0.78 - ETA: 0s - loss: 0.6427 - accuracy: 0.78 - ETA: 0s - loss: 0.6244 - accuracy: 0.78 - ETA: 0s - loss: 0.6972 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6827 - accuracy: 0.7884 - val_loss: 0.6192 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.75 - ETA: 0s - loss: 0.6708 - accuracy: 0.75 - ETA: 0s - loss: 0.6270 - accuracy: 0.78 - ETA: 0s - loss: 0.6214 - accuracy: 0.77 - ETA: 0s - loss: 0.6162 - accuracy: 0.77 - ETA: 0s - loss: 0.6095 - accuracy: 0.78 - ETA: 0s - loss: 0.6212 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6205 - accuracy: 0.7828 - val_loss: 0.5939 - val_accuracy: 0.7463\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.71 - ETA: 0s - loss: 0.6085 - accuracy: 0.79 - ETA: 0s - loss: 0.6020 - accuracy: 0.79 - ETA: 0s - loss: 0.6144 - accuracy: 0.79 - ETA: 0s - loss: 0.6212 - accuracy: 0.79 - ETA: 0s - loss: 0.6205 - accuracy: 0.79 - ETA: 0s - loss: 0.6217 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6209 - accuracy: 0.7928 - val_loss: 0.5764 - val_accuracy: 0.7567\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.71 - ETA: 0s - loss: 0.6088 - accuracy: 0.78 - ETA: 0s - loss: 0.5903 - accuracy: 0.79 - ETA: 0s - loss: 0.5835 - accuracy: 0.79 - ETA: 0s - loss: 0.6421 - accuracy: 0.80 - ETA: 0s - loss: 0.6650 - accuracy: 0.79 - ETA: 0s - loss: 0.6572 - accuracy: 0.79 - ETA: 0s - loss: 0.6840 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6825 - accuracy: 0.7913 - val_loss: 2.2104 - val_accuracy: 0.6836\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.2797 - accuracy: 0.84 - ETA: 0s - loss: 1.2671 - accuracy: 0.80 - ETA: 0s - loss: 1.0342 - accuracy: 0.78 - ETA: 0s - loss: 0.9191 - accuracy: 0.78 - ETA: 0s - loss: 0.8997 - accuracy: 0.77 - ETA: 0s - loss: 0.8469 - accuracy: 0.78 - ETA: 0s - loss: 0.8730 - accuracy: 0.78 - 0s 5ms/step - loss: 0.8573 - accuracy: 0.7772 - val_loss: 0.6026 - val_accuracy: 0.7433\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.90 - ETA: 0s - loss: 0.6033 - accuracy: 0.78 - ETA: 0s - loss: 0.6102 - accuracy: 0.78 - ETA: 0s - loss: 0.6121 - accuracy: 0.78 - ETA: 0s - loss: 0.6132 - accuracy: 0.78 - ETA: 0s - loss: 0.6138 - accuracy: 0.77 - ETA: 0s - loss: 0.6155 - accuracy: 0.77 - ETA: 0s - loss: 0.6095 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6118 - accuracy: 0.7813 - val_loss: 0.6369 - val_accuracy: 0.7164\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.75 - ETA: 0s - loss: 0.6220 - accuracy: 0.77 - ETA: 0s - loss: 0.5976 - accuracy: 0.79 - ETA: 0s - loss: 0.5995 - accuracy: 0.78 - ETA: 0s - loss: 0.5938 - accuracy: 0.78 - ETA: 0s - loss: 0.5990 - accuracy: 0.79 - ETA: 0s - loss: 0.6062 - accuracy: 0.79 - ETA: 0s - loss: 0.5995 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5989 - accuracy: 0.7951 - val_loss: 0.7779 - val_accuracy: 0.7448\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.81 - ETA: 0s - loss: 0.9511 - accuracy: 0.80 - ETA: 0s - loss: 0.7961 - accuracy: 0.78 - ETA: 0s - loss: 0.7176 - accuracy: 0.79 - ETA: 0s - loss: 0.6839 - accuracy: 0.79 - ETA: 0s - loss: 0.6649 - accuracy: 0.79 - ETA: 0s - loss: 0.6496 - accuracy: 0.79 - ETA: 0s - loss: 0.6377 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6394 - accuracy: 0.7962 - val_loss: 0.5948 - val_accuracy: 0.7627\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.84 - ETA: 0s - loss: 0.5834 - accuracy: 0.80 - ETA: 0s - loss: 0.5763 - accuracy: 0.80 - ETA: 0s - loss: 0.5642 - accuracy: 0.80 - ETA: 0s - loss: 0.5494 - accuracy: 0.81 - ETA: 0s - loss: 0.5674 - accuracy: 0.80 - ETA: 0s - loss: 0.5767 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5736 - accuracy: 0.8018 - val_loss: 0.6070 - val_accuracy: 0.7552\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.96 - ETA: 0s - loss: 0.5570 - accuracy: 0.81 - ETA: 0s - loss: 0.5643 - accuracy: 0.80 - ETA: 0s - loss: 0.5502 - accuracy: 0.81 - ETA: 0s - loss: 0.5619 - accuracy: 0.80 - ETA: 0s - loss: 0.5706 - accuracy: 0.79 - ETA: 0s - loss: 0.5884 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5920 - accuracy: 0.7988 - val_loss: 0.5866 - val_accuracy: 0.7657\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8078 - accuracy: 0.78 - ETA: 0s - loss: 0.6174 - accuracy: 0.78 - ETA: 0s - loss: 0.6274 - accuracy: 0.78 - ETA: 0s - loss: 0.6483 - accuracy: 0.78 - ETA: 0s - loss: 0.6383 - accuracy: 0.78 - ETA: 0s - loss: 0.6252 - accuracy: 0.78 - ETA: 0s - loss: 0.6266 - accuracy: 0.77 - ETA: 0s - loss: 0.6188 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6193 - accuracy: 0.7842 - val_loss: 0.6713 - val_accuracy: 0.7537\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.68 - ETA: 0s - loss: 0.5890 - accuracy: 0.77 - ETA: 0s - loss: 0.5609 - accuracy: 0.79 - ETA: 0s - loss: 0.5613 - accuracy: 0.80 - ETA: 0s - loss: 0.6511 - accuracy: 0.79 - ETA: 0s - loss: 0.6339 - accuracy: 0.79 - ETA: 0s - loss: 0.6277 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6185 - accuracy: 0.7981 - val_loss: 0.7333 - val_accuracy: 0.7537\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4648 - accuracy: 0.87 - ETA: 0s - loss: 0.9030 - accuracy: 0.79 - ETA: 0s - loss: 0.7831 - accuracy: 0.78 - ETA: 0s - loss: 0.7462 - accuracy: 0.78 - ETA: 0s - loss: 0.7204 - accuracy: 0.78 - ETA: 0s - loss: 0.6955 - accuracy: 0.78 - ETA: 0s - loss: 0.6751 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6891 - accuracy: 0.7902 - val_loss: 0.6768 - val_accuracy: 0.7433\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.78 - ETA: 0s - loss: 0.6097 - accuracy: 0.77 - ETA: 0s - loss: 0.5985 - accuracy: 0.79 - ETA: 0s - loss: 0.6084 - accuracy: 0.79 - ETA: 0s - loss: 0.8202 - accuracy: 0.78 - ETA: 0s - loss: 0.7956 - accuracy: 0.77 - ETA: 0s - loss: 0.7672 - accuracy: 0.77 - 0s 5ms/step - loss: 0.7579 - accuracy: 0.7779 - val_loss: 0.6852 - val_accuracy: 0.7299\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7488 - accuracy: 0.71 - ETA: 0s - loss: 0.7070 - accuracy: 0.75 - ETA: 0s - loss: 0.6405 - accuracy: 0.77 - ETA: 0s - loss: 0.6436 - accuracy: 0.77 - ETA: 0s - loss: 0.6416 - accuracy: 0.77 - ETA: 0s - loss: 0.6307 - accuracy: 0.77 - ETA: 0s - loss: 0.6478 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6460 - accuracy: 0.7660 - val_loss: 0.6503 - val_accuracy: 0.7537\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7328 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.74 - ETA: 0s - loss: 0.6391 - accuracy: 0.75 - ETA: 0s - loss: 0.6309 - accuracy: 0.75 - ETA: 0s - loss: 0.6442 - accuracy: 0.75 - ETA: 0s - loss: 0.6539 - accuracy: 0.75 - ETA: 0s - loss: 0.6530 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6491 - accuracy: 0.7544 - val_loss: 0.6967 - val_accuracy: 0.7463\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.71 - ETA: 0s - loss: 0.6429 - accuracy: 0.74 - ETA: 0s - loss: 0.6358 - accuracy: 0.75 - ETA: 0s - loss: 0.6296 - accuracy: 0.75 - ETA: 0s - loss: 0.6276 - accuracy: 0.75 - ETA: 0s - loss: 0.6302 - accuracy: 0.75 - ETA: 0s - loss: 0.6316 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6319 - accuracy: 0.7548 - val_loss: 0.7040 - val_accuracy: 0.7537\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.81 - ETA: 0s - loss: 0.6369 - accuracy: 0.75 - ETA: 0s - loss: 0.6661 - accuracy: 0.74 - ETA: 0s - loss: 0.6603 - accuracy: 0.74 - ETA: 0s - loss: 0.6630 - accuracy: 0.74 - ETA: 0s - loss: 0.6539 - accuracy: 0.74 - ETA: 0s - loss: 0.6469 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6453 - accuracy: 0.7521 - val_loss: 0.6717 - val_accuracy: 0.7478\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.90 - ETA: 0s - loss: 0.7383 - accuracy: 0.78 - ETA: 0s - loss: 0.7293 - accuracy: 0.77 - ETA: 0s - loss: 0.7108 - accuracy: 0.75 - ETA: 0s - loss: 0.6859 - accuracy: 0.75 - ETA: 0s - loss: 0.6870 - accuracy: 0.75 - ETA: 0s - loss: 0.6859 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6826 - accuracy: 0.7533 - val_loss: 0.6457 - val_accuracy: 0.7448\n",
      "Epoch 29/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.6524 - accuracy: 0.75 - ETA: 0s - loss: 0.6429 - accuracy: 0.74 - ETA: 0s - loss: 0.6389 - accuracy: 0.74 - ETA: 0s - loss: 0.6439 - accuracy: 0.73 - ETA: 0s - loss: 0.6329 - accuracy: 0.75 - ETA: 0s - loss: 0.6308 - accuracy: 0.75 - ETA: 0s - loss: 0.6248 - accuracy: 0.7588Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6235 - accuracy: 0.7589 - val_loss: 1.1696 - val_accuracy: 0.7254\n",
      "Epoch 00029: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0591 - accuracy: 0.68 - ETA: 0s - loss: 4.9860 - accuracy: 0.63 - ETA: 0s - loss: 4.5191 - accuracy: 0.60 - ETA: 0s - loss: 3.9925 - accuracy: 0.62 - ETA: 0s - loss: 3.6180 - accuracy: 0.61 - ETA: 0s - loss: 3.2138 - accuracy: 0.61 - ETA: 0s - loss: 2.8698 - accuracy: 0.61 - ETA: 0s - loss: 2.6046 - accuracy: 0.61 - 1s 6ms/step - loss: 2.5874 - accuracy: 0.6208 - val_loss: 0.6109 - val_accuracy: 0.6672\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.0551 - accuracy: 0.53 - ETA: 0s - loss: 0.7880 - accuracy: 0.65 - ETA: 0s - loss: 0.7221 - accuracy: 0.69 - ETA: 0s - loss: 0.7098 - accuracy: 0.71 - ETA: 0s - loss: 0.7005 - accuracy: 0.71 - ETA: 0s - loss: 0.6952 - accuracy: 0.71 - ETA: 0s - loss: 0.6892 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6824 - accuracy: 0.7163 - val_loss: 0.6168 - val_accuracy: 0.7209\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5348 - accuracy: 0.75 - ETA: 0s - loss: 0.6586 - accuracy: 0.69 - ETA: 0s - loss: 0.6420 - accuracy: 0.72 - ETA: 0s - loss: 0.6189 - accuracy: 0.73 - ETA: 0s - loss: 0.6168 - accuracy: 0.73 - ETA: 0s - loss: 0.6165 - accuracy: 0.73 - ETA: 0s - loss: 0.6123 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6133 - accuracy: 0.7383 - val_loss: 0.5830 - val_accuracy: 0.7522\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.75 - ETA: 0s - loss: 0.5704 - accuracy: 0.78 - ETA: 0s - loss: 0.6162 - accuracy: 0.76 - ETA: 0s - loss: 0.6024 - accuracy: 0.76 - ETA: 0s - loss: 0.5946 - accuracy: 0.76 - ETA: 0s - loss: 0.6027 - accuracy: 0.75 - ETA: 0s - loss: 0.6120 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6174 - accuracy: 0.7551 - val_loss: 0.6185 - val_accuracy: 0.6851\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.78 - ETA: 0s - loss: 0.6084 - accuracy: 0.76 - ETA: 0s - loss: 0.6224 - accuracy: 0.77 - ETA: 0s - loss: 0.6081 - accuracy: 0.77 - ETA: 0s - loss: 0.6075 - accuracy: 0.77 - ETA: 0s - loss: 0.6518 - accuracy: 0.76 - ETA: 0s - loss: 0.6677 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6772 - accuracy: 0.7660 - val_loss: 1.0053 - val_accuracy: 0.7209\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.3455 - accuracy: 0.78 - ETA: 0s - loss: 0.7999 - accuracy: 0.75 - ETA: 0s - loss: 0.8968 - accuracy: 0.76 - ETA: 0s - loss: 1.0299 - accuracy: 0.76 - ETA: 0s - loss: 0.9872 - accuracy: 0.76 - ETA: 0s - loss: 0.9269 - accuracy: 0.76 - ETA: 0s - loss: 0.8811 - accuracy: 0.76 - 0s 5ms/step - loss: 0.8709 - accuracy: 0.7622 - val_loss: 2.2176 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.78 - ETA: 0s - loss: 1.3251 - accuracy: 0.77 - ETA: 0s - loss: 1.0556 - accuracy: 0.75 - ETA: 0s - loss: 0.9278 - accuracy: 0.74 - ETA: 0s - loss: 0.8665 - accuracy: 0.74 - ETA: 0s - loss: 0.8275 - accuracy: 0.74 - ETA: 0s - loss: 0.8029 - accuracy: 0.74 - 0s 5ms/step - loss: 0.7969 - accuracy: 0.7402 - val_loss: 0.6396 - val_accuracy: 0.7388\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.72 - ETA: 0s - loss: 0.6675 - accuracy: 0.72 - ETA: 0s - loss: 0.6605 - accuracy: 0.73 - ETA: 0s - loss: 0.6671 - accuracy: 0.72 - ETA: 0s - loss: 0.6660 - accuracy: 0.72 - ETA: 0s - loss: 0.6621 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6662 - accuracy: 0.7238 - val_loss: 0.6558 - val_accuracy: 0.7567\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6013 - accuracy: 0.78 - ETA: 0s - loss: 0.6641 - accuracy: 0.73 - ETA: 0s - loss: 0.6680 - accuracy: 0.72 - ETA: 0s - loss: 0.6575 - accuracy: 0.73 - ETA: 0s - loss: 0.6625 - accuracy: 0.73 - ETA: 0s - loss: 0.6579 - accuracy: 0.73 - ETA: 0s - loss: 0.6579 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6569 - accuracy: 0.7357 - val_loss: 0.6310 - val_accuracy: 0.7507\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.78 - ETA: 0s - loss: 0.6308 - accuracy: 0.76 - ETA: 0s - loss: 0.6813 - accuracy: 0.72 - ETA: 0s - loss: 0.6846 - accuracy: 0.73 - ETA: 0s - loss: 0.6858 - accuracy: 0.74 - ETA: 0s - loss: 0.6705 - accuracy: 0.74 - ETA: 0s - loss: 0.6894 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6874 - accuracy: 0.7566 - val_loss: 0.6110 - val_accuracy: 0.7448\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.78 - ETA: 0s - loss: 1.7786 - accuracy: 0.77 - ETA: 0s - loss: 1.2689 - accuracy: 0.76 - ETA: 0s - loss: 1.0479 - accuracy: 0.76 - ETA: 0s - loss: 0.9490 - accuracy: 0.76 - ETA: 0s - loss: 0.8835 - accuracy: 0.76 - ETA: 0s - loss: 0.8453 - accuracy: 0.75 - 0s 4ms/step - loss: 0.8396 - accuracy: 0.7596 - val_loss: 0.6072 - val_accuracy: 0.7478\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.87 - ETA: 0s - loss: 0.5845 - accuracy: 0.78 - ETA: 0s - loss: 0.6148 - accuracy: 0.76 - ETA: 0s - loss: 0.6223 - accuracy: 0.75 - ETA: 0s - loss: 0.6170 - accuracy: 0.76 - ETA: 0s - loss: 0.6146 - accuracy: 0.76 - ETA: 0s - loss: 0.6152 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6178 - accuracy: 0.7633 - val_loss: 0.6124 - val_accuracy: 0.7448\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.78 - ETA: 0s - loss: 0.6520 - accuracy: 0.73 - ETA: 0s - loss: 0.6459 - accuracy: 0.75 - ETA: 0s - loss: 0.6341 - accuracy: 0.76 - ETA: 0s - loss: 0.6261 - accuracy: 0.76 - ETA: 0s - loss: 0.6332 - accuracy: 0.77 - ETA: 0s - loss: 0.6287 - accuracy: 0.77 - ETA: 0s - loss: 0.6266 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6243 - accuracy: 0.7738 - val_loss: 0.6115 - val_accuracy: 0.7358\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7218 - accuracy: 0.65 - ETA: 0s - loss: 1.2064 - accuracy: 0.78 - ETA: 0s - loss: 0.9236 - accuracy: 0.77 - ETA: 0s - loss: 0.8218 - accuracy: 0.77 - ETA: 0s - loss: 0.7555 - accuracy: 0.78 - ETA: 0s - loss: 0.7362 - accuracy: 0.77 - ETA: 0s - loss: 0.7301 - accuracy: 0.78 - 0s 5ms/step - loss: 0.7166 - accuracy: 0.7809 - val_loss: 0.6352 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5118 - accuracy: 0.84 - ETA: 0s - loss: 0.6030 - accuracy: 0.78 - ETA: 0s - loss: 0.5967 - accuracy: 0.79 - ETA: 0s - loss: 0.6037 - accuracy: 0.78 - ETA: 0s - loss: 0.6042 - accuracy: 0.78 - ETA: 0s - loss: 0.6050 - accuracy: 0.78 - ETA: 0s - loss: 0.6018 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5977 - accuracy: 0.7865 - val_loss: 0.6766 - val_accuracy: 0.7164\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.71 - ETA: 0s - loss: 0.6105 - accuracy: 0.79 - ETA: 0s - loss: 0.6052 - accuracy: 0.79 - ETA: 0s - loss: 0.6121 - accuracy: 0.77 - ETA: 0s - loss: 0.5992 - accuracy: 0.79 - ETA: 0s - loss: 0.5895 - accuracy: 0.79 - ETA: 0s - loss: 0.5987 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6142 - accuracy: 0.7966 - val_loss: 0.6617 - val_accuracy: 0.7358\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.90 - ETA: 0s - loss: 0.5473 - accuracy: 0.82 - ETA: 0s - loss: 0.7890 - accuracy: 0.81 - ETA: 0s - loss: 0.7946 - accuracy: 0.79 - ETA: 0s - loss: 0.8096 - accuracy: 0.79 - ETA: 0s - loss: 1.0660 - accuracy: 0.78 - ETA: 0s - loss: 1.0084 - accuracy: 0.77 - 0s 5ms/step - loss: 1.0199 - accuracy: 0.7775 - val_loss: 0.6187 - val_accuracy: 0.7537\n",
      "Epoch 18/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.5959 - accuracy: 0.78 - ETA: 0s - loss: 0.6413 - accuracy: 0.76 - ETA: 0s - loss: 0.6299 - accuracy: 0.77 - ETA: 0s - loss: 0.6368 - accuracy: 0.76 - ETA: 0s - loss: 0.6434 - accuracy: 0.75 - ETA: 0s - loss: 0.6394 - accuracy: 0.75 - ETA: 0s - loss: 0.6389 - accuracy: 0.7598Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6369 - accuracy: 0.7626 - val_loss: 0.6023 - val_accuracy: 0.7209\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cc31ce1c31e660768b7ec9d3570fb875</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7631840904553732</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8001698624704281</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 175</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9070 - accuracy: 0.62 - ETA: 0s - loss: 2.6205 - accuracy: 0.59 - ETA: 0s - loss: 1.5903 - accuracy: 0.64 - ETA: 0s - loss: 1.2845 - accuracy: 0.66 - ETA: 0s - loss: 1.1355 - accuracy: 0.67 - ETA: 0s - loss: 1.0394 - accuracy: 0.67 - ETA: 0s - loss: 0.9562 - accuracy: 0.68 - ETA: 0s - loss: 0.9032 - accuracy: 0.70 - 1s 7ms/step - loss: 0.8851 - accuracy: 0.7036 - val_loss: 0.6405 - val_accuracy: 0.6612\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.71 - ETA: 0s - loss: 0.5457 - accuracy: 0.70 - ETA: 0s - loss: 0.5216 - accuracy: 0.72 - ETA: 0s - loss: 0.5198 - accuracy: 0.73 - ETA: 0s - loss: 0.5223 - accuracy: 0.73 - ETA: 0s - loss: 0.5246 - accuracy: 0.74 - ETA: 0s - loss: 0.5219 - accuracy: 0.75 - ETA: 0s - loss: 0.5118 - accuracy: 0.76 - 0s 6ms/step - loss: 0.5120 - accuracy: 0.7596 - val_loss: 0.5916 - val_accuracy: 0.6821\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.81 - ETA: 0s - loss: 0.4046 - accuracy: 0.79 - ETA: 0s - loss: 0.4108 - accuracy: 0.80 - ETA: 0s - loss: 0.4181 - accuracy: 0.80 - ETA: 0s - loss: 0.4085 - accuracy: 0.80 - ETA: 0s - loss: 0.4205 - accuracy: 0.80 - ETA: 0s - loss: 0.4310 - accuracy: 0.80 - ETA: 0s - loss: 0.4262 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8048 - val_loss: 0.6119 - val_accuracy: 0.6642\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.84 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.3513 - accuracy: 0.83 - ETA: 0s - loss: 0.3652 - accuracy: 0.83 - ETA: 0s - loss: 0.3745 - accuracy: 0.82 - ETA: 0s - loss: 0.3845 - accuracy: 0.82 - ETA: 0s - loss: 0.3858 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - 0s 5ms/step - loss: 0.3927 - accuracy: 0.8231 - val_loss: 1.0332 - val_accuracy: 0.6239\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3767 - accuracy: 0.65 - ETA: 0s - loss: 0.3961 - accuracy: 0.81 - ETA: 0s - loss: 0.4142 - accuracy: 0.82 - ETA: 0s - loss: 0.4468 - accuracy: 0.82 - ETA: 0s - loss: 0.4722 - accuracy: 0.82 - ETA: 0s - loss: 0.4754 - accuracy: 0.82 - ETA: 0s - loss: 0.5186 - accuracy: 0.81 - ETA: 0s - loss: 0.5636 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5654 - accuracy: 0.7925 - val_loss: 0.6782 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9751 - accuracy: 0.75 - ETA: 0s - loss: 0.7683 - accuracy: 0.67 - ETA: 0s - loss: 0.7576 - accuracy: 0.67 - ETA: 0s - loss: 0.7443 - accuracy: 0.68 - ETA: 0s - loss: 0.7345 - accuracy: 0.68 - ETA: 0s - loss: 0.7310 - accuracy: 0.68 - ETA: 0s - loss: 0.7263 - accuracy: 0.68 - ETA: 0s - loss: 0.7200 - accuracy: 0.69 - 0s 5ms/step - loss: 0.7196 - accuracy: 0.6902 - val_loss: 0.6805 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.71 - ETA: 0s - loss: 0.7316 - accuracy: 0.68 - ETA: 0s - loss: 0.7198 - accuracy: 0.68 - ETA: 0s - loss: 0.7121 - accuracy: 0.68 - ETA: 0s - loss: 0.7027 - accuracy: 0.69 - ETA: 0s - loss: 0.7011 - accuracy: 0.69 - ETA: 0s - loss: 0.7229 - accuracy: 0.69 - ETA: 0s - loss: 0.7175 - accuracy: 0.69 - 0s 5ms/step - loss: 0.7178 - accuracy: 0.6973 - val_loss: 0.6897 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7858 - accuracy: 0.56 - ETA: 0s - loss: 0.6958 - accuracy: 0.69 - ETA: 0s - loss: 0.7044 - accuracy: 0.55 - ETA: 0s - loss: 0.7078 - accuracy: 0.48 - ETA: 0s - loss: 0.7018 - accuracy: 0.43 - ETA: 0s - loss: 0.6993 - accuracy: 0.40 - ETA: 0s - loss: 0.6959 - accuracy: 0.38 - ETA: 0s - loss: 0.6938 - accuracy: 0.42 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4289 - val_loss: 0.6853 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7016 - accuracy: 0.68 - ETA: 0s - loss: 0.6777 - accuracy: 0.72 - ETA: 0s - loss: 0.6888 - accuracy: 0.70 - ETA: 0s - loss: 0.6950 - accuracy: 0.69 - ETA: 0s - loss: 0.6878 - accuracy: 0.70 - ETA: 0s - loss: 0.6898 - accuracy: 0.70 - ETA: 0s - loss: 0.6888 - accuracy: 0.70 - ETA: 0s - loss: 0.6936 - accuracy: 0.69 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.6991 - val_loss: 0.6909 - val_accuracy: 0.6955\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6591 - accuracy: 0.75 - ETA: 0s - loss: 0.7006 - accuracy: 0.43 - ETA: 0s - loss: 0.6984 - accuracy: 0.37 - ETA: 0s - loss: 0.6921 - accuracy: 0.34 - ETA: 0s - loss: 0.6958 - accuracy: 0.33 - ETA: 0s - loss: 0.6981 - accuracy: 0.33 - ETA: 0s - loss: 0.6929 - accuracy: 0.32 - ETA: 0s - loss: 0.6938 - accuracy: 0.33 - 0s 5ms/step - loss: 0.6934 - accuracy: 0.3423 - val_loss: 0.6926 - val_accuracy: 0.6955\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.68 - ETA: 0s - loss: 0.6801 - accuracy: 0.71 - ETA: 0s - loss: 0.6800 - accuracy: 0.71 - ETA: 0s - loss: 0.6906 - accuracy: 0.70 - ETA: 0s - loss: 0.6946 - accuracy: 0.67 - ETA: 0s - loss: 0.6961 - accuracy: 0.60 - ETA: 0s - loss: 0.6962 - accuracy: 0.56 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - 0s 5ms/step - loss: 0.6937 - accuracy: 0.5312 - val_loss: 0.6919 - val_accuracy: 0.6955\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8046 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.37 - ETA: 0s - loss: 0.6911 - accuracy: 0.47 - ETA: 0s - loss: 0.6952 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.48 - ETA: 0s - loss: 0.6976 - accuracy: 0.45 - ETA: 0s - loss: 0.6970 - accuracy: 0.43 - ETA: 0s - loss: 0.6927 - accuracy: 0.40 - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4061 - val_loss: 0.6938 - val_accuracy: 0.3045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.21 - ETA: 0s - loss: 0.6572 - accuracy: 0.68 - ETA: 0s - loss: 0.6851 - accuracy: 0.67 - ETA: 0s - loss: 0.6940 - accuracy: 0.67 - ETA: 0s - loss: 0.6944 - accuracy: 0.62 - ETA: 0s - loss: 0.6918 - accuracy: 0.55 - ETA: 0s - loss: 0.6938 - accuracy: 0.54 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5039 - val_loss: 0.6945 - val_accuracy: 0.3045\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7412 - accuracy: 0.37 - ETA: 0s - loss: 0.7057 - accuracy: 0.32 - ETA: 0s - loss: 0.6904 - accuracy: 0.29 - ETA: 0s - loss: 0.6871 - accuracy: 0.39 - ETA: 0s - loss: 0.6896 - accuracy: 0.47 - ETA: 0s - loss: 0.6952 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6926 - accuracy: 0.45 - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4464 - val_loss: 0.6947 - val_accuracy: 0.3045\n",
      "Epoch 15/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6805 - accuracy: 0.28 - ETA: 0s - loss: 0.6788 - accuracy: 0.32 - ETA: 0s - loss: 0.6850 - accuracy: 0.50 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.60 - ETA: 0s - loss: 0.6900 - accuracy: 0.61 - ETA: 0s - loss: 0.6883 - accuracy: 0.63 - ETA: 0s - loss: 0.6907 - accuracy: 0.6414Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.6417 - val_loss: 0.6932 - val_accuracy: 0.3045\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.8613 - accuracy: 0.53 - ETA: 0s - loss: 4.9320 - accuracy: 0.52 - ETA: 0s - loss: 2.7342 - accuracy: 0.59 - ETA: 0s - loss: 1.9681 - accuracy: 0.62 - ETA: 0s - loss: 1.6375 - accuracy: 0.64 - ETA: 0s - loss: 1.4416 - accuracy: 0.65 - ETA: 0s - loss: 1.3128 - accuracy: 0.66 - ETA: 0s - loss: 1.2064 - accuracy: 0.66 - 1s 7ms/step - loss: 1.1467 - accuracy: 0.6753 - val_loss: 0.5530 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4593 - accuracy: 0.84 - ETA: 0s - loss: 0.5073 - accuracy: 0.78 - ETA: 0s - loss: 0.5163 - accuracy: 0.78 - ETA: 0s - loss: 0.5289 - accuracy: 0.77 - ETA: 0s - loss: 0.5485 - accuracy: 0.78 - ETA: 0s - loss: 0.5629 - accuracy: 0.77 - ETA: 0s - loss: 0.5632 - accuracy: 0.77 - ETA: 0s - loss: 0.5599 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5592 - accuracy: 0.7719 - val_loss: 0.5782 - val_accuracy: 0.7313\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.84 - ETA: 0s - loss: 0.5246 - accuracy: 0.78 - ETA: 0s - loss: 0.4917 - accuracy: 0.80 - ETA: 0s - loss: 0.4923 - accuracy: 0.80 - ETA: 0s - loss: 0.4942 - accuracy: 0.80 - ETA: 0s - loss: 0.4840 - accuracy: 0.81 - ETA: 0s - loss: 0.4822 - accuracy: 0.81 - ETA: 0s - loss: 0.4868 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4859 - accuracy: 0.8074 - val_loss: 0.5959 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.84 - ETA: 0s - loss: 0.4452 - accuracy: 0.82 - ETA: 0s - loss: 0.4248 - accuracy: 0.82 - ETA: 0s - loss: 0.4414 - accuracy: 0.82 - ETA: 0s - loss: 0.4384 - accuracy: 0.82 - ETA: 0s - loss: 0.4503 - accuracy: 0.81 - ETA: 0s - loss: 0.4694 - accuracy: 0.80 - ETA: 0s - loss: 0.4717 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4708 - accuracy: 0.8093 - val_loss: 0.9800 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.84 - ETA: 0s - loss: 0.6002 - accuracy: 0.82 - ETA: 0s - loss: 0.5192 - accuracy: 0.83 - ETA: 0s - loss: 0.4953 - accuracy: 0.83 - ETA: 0s - loss: 0.4905 - accuracy: 0.82 - ETA: 0s - loss: 0.4789 - accuracy: 0.82 - ETA: 0s - loss: 0.4713 - accuracy: 0.82 - ETA: 0s - loss: 0.4554 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4554 - accuracy: 0.8268 - val_loss: 0.5942 - val_accuracy: 0.7194\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.81 - ETA: 0s - loss: 0.3755 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.83 - ETA: 0s - loss: 0.3861 - accuracy: 0.83 - ETA: 0s - loss: 0.3958 - accuracy: 0.83 - ETA: 0s - loss: 0.4409 - accuracy: 0.82 - ETA: 0s - loss: 0.4396 - accuracy: 0.82 - ETA: 0s - loss: 0.4447 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4447 - accuracy: 0.8261 - val_loss: 0.7957 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.90 - ETA: 0s - loss: 0.4036 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.84 - ETA: 0s - loss: 0.3853 - accuracy: 0.84 - ETA: 0s - loss: 0.3717 - accuracy: 0.85 - ETA: 0s - loss: 0.3592 - accuracy: 0.85 - ETA: 0s - loss: 0.3593 - accuracy: 0.85 - ETA: 0s - loss: 0.3708 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3822 - accuracy: 0.8440 - val_loss: 0.6078 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.93 - ETA: 0s - loss: 0.3809 - accuracy: 0.85 - ETA: 0s - loss: 0.3417 - accuracy: 0.87 - ETA: 0s - loss: 0.3354 - accuracy: 0.87 - ETA: 0s - loss: 0.3356 - accuracy: 0.87 - ETA: 0s - loss: 0.3468 - accuracy: 0.86 - ETA: 0s - loss: 0.3550 - accuracy: 0.86 - ETA: 0s - loss: 0.3681 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3765 - accuracy: 0.8611 - val_loss: 0.8014 - val_accuracy: 0.7164\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.90 - ETA: 0s - loss: 0.3792 - accuracy: 0.87 - ETA: 0s - loss: 0.3591 - accuracy: 0.87 - ETA: 0s - loss: 0.3361 - accuracy: 0.87 - ETA: 0s - loss: 0.3659 - accuracy: 0.87 - ETA: 0s - loss: 0.3657 - accuracy: 0.87 - ETA: 0s - loss: 0.3705 - accuracy: 0.86 - ETA: 0s - loss: 0.3773 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3850 - accuracy: 0.8626 - val_loss: 0.6306 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.84 - ETA: 0s - loss: 0.4400 - accuracy: 0.84 - ETA: 0s - loss: 0.4333 - accuracy: 0.83 - ETA: 0s - loss: 0.3986 - accuracy: 0.85 - ETA: 0s - loss: 0.3775 - accuracy: 0.86 - ETA: 0s - loss: 0.7328 - accuracy: 0.86 - ETA: 0s - loss: 0.6902 - accuracy: 0.86 - ETA: 0s - loss: 0.6566 - accuracy: 0.85 - 0s 5ms/step - loss: 0.6385 - accuracy: 0.8567 - val_loss: 0.7743 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.3009 - accuracy: 0.93 - ETA: 0s - loss: 0.4104 - accuracy: 0.86 - ETA: 0s - loss: 0.4008 - accuracy: 0.87 - ETA: 0s - loss: 0.4141 - accuracy: 0.87 - ETA: 0s - loss: 0.4240 - accuracy: 0.87 - ETA: 0s - loss: 0.4167 - accuracy: 0.87 - ETA: 0s - loss: 0.4192 - accuracy: 0.86 - ETA: 0s - loss: 0.4162 - accuracy: 0.8640Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4223 - accuracy: 0.8600 - val_loss: 0.8674 - val_accuracy: 0.6910\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9395 - accuracy: 0.59 - ETA: 0s - loss: 2.0820 - accuracy: 0.55 - ETA: 0s - loss: 1.3604 - accuracy: 0.62 - ETA: 0s - loss: 1.0826 - accuracy: 0.67 - ETA: 0s - loss: 0.9759 - accuracy: 0.67 - ETA: 0s - loss: 0.9047 - accuracy: 0.67 - ETA: 0s - loss: 0.8528 - accuracy: 0.68 - ETA: 0s - loss: 0.8201 - accuracy: 0.68 - 1s 7ms/step - loss: 0.7961 - accuracy: 0.6924 - val_loss: 0.5923 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6465 - accuracy: 0.71 - ETA: 0s - loss: 0.5726 - accuracy: 0.75 - ETA: 0s - loss: 0.5531 - accuracy: 0.74 - ETA: 0s - loss: 0.5369 - accuracy: 0.76 - ETA: 0s - loss: 0.5460 - accuracy: 0.75 - ETA: 0s - loss: 0.5647 - accuracy: 0.74 - ETA: 0s - loss: 0.5775 - accuracy: 0.74 - ETA: 0s - loss: 0.5744 - accuracy: 0.74 - 0s 6ms/step - loss: 0.5760 - accuracy: 0.7458 - val_loss: 0.5400 - val_accuracy: 0.7537\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.93 - ETA: 0s - loss: 0.5234 - accuracy: 0.80 - ETA: 0s - loss: 0.5186 - accuracy: 0.78 - ETA: 0s - loss: 0.5037 - accuracy: 0.80 - ETA: 0s - loss: 0.5086 - accuracy: 0.80 - ETA: 0s - loss: 0.5204 - accuracy: 0.80 - ETA: 0s - loss: 0.5108 - accuracy: 0.80 - ETA: 0s - loss: 0.5280 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5302 - accuracy: 0.7910 - val_loss: 0.5655 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.78 - ETA: 0s - loss: 0.4764 - accuracy: 0.83 - ETA: 0s - loss: 0.5314 - accuracy: 0.82 - ETA: 0s - loss: 0.5271 - accuracy: 0.81 - ETA: 0s - loss: 0.5067 - accuracy: 0.82 - ETA: 0s - loss: 0.4873 - accuracy: 0.82 - ETA: 0s - loss: 0.4929 - accuracy: 0.82 - ETA: 0s - loss: 0.4914 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4891 - accuracy: 0.8193 - val_loss: 0.6375 - val_accuracy: 0.6925\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.84 - ETA: 0s - loss: 0.3851 - accuracy: 0.88 - ETA: 0s - loss: 0.4037 - accuracy: 0.86 - ETA: 0s - loss: 0.4164 - accuracy: 0.85 - ETA: 0s - loss: 0.4163 - accuracy: 0.84 - ETA: 0s - loss: 0.4227 - accuracy: 0.83 - ETA: 0s - loss: 0.4184 - accuracy: 0.84 - ETA: 0s - loss: 0.4320 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4320 - accuracy: 0.8387 - val_loss: 0.7383 - val_accuracy: 0.7075\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.84 - ETA: 0s - loss: 0.4885 - accuracy: 0.82 - ETA: 0s - loss: 0.4533 - accuracy: 0.84 - ETA: 0s - loss: 0.4371 - accuracy: 0.83 - ETA: 0s - loss: 0.4072 - accuracy: 0.84 - ETA: 0s - loss: 0.4005 - accuracy: 0.85 - ETA: 0s - loss: 0.4070 - accuracy: 0.84 - ETA: 0s - loss: 0.4078 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4244 - accuracy: 0.8481 - val_loss: 0.5608 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.90 - ETA: 0s - loss: 0.4475 - accuracy: 0.87 - ETA: 0s - loss: 0.4337 - accuracy: 0.86 - ETA: 0s - loss: 0.4341 - accuracy: 0.85 - ETA: 0s - loss: 0.4248 - accuracy: 0.85 - ETA: 0s - loss: 0.4192 - accuracy: 0.85 - ETA: 0s - loss: 0.4151 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4207 - accuracy: 0.8570 - val_loss: 0.6718 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.90 - ETA: 0s - loss: 0.3395 - accuracy: 0.88 - ETA: 0s - loss: 0.3626 - accuracy: 0.87 - ETA: 0s - loss: 0.3699 - accuracy: 0.87 - ETA: 0s - loss: 0.3737 - accuracy: 0.87 - ETA: 0s - loss: 0.3729 - accuracy: 0.87 - ETA: 0s - loss: 0.3776 - accuracy: 0.87 - ETA: 0s - loss: 0.3810 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3810 - accuracy: 0.8738 - val_loss: 0.7149 - val_accuracy: 0.7209\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5958 - accuracy: 0.75 - ETA: 0s - loss: 0.3252 - accuracy: 0.88 - ETA: 0s - loss: 0.3114 - accuracy: 0.89 - ETA: 0s - loss: 0.3231 - accuracy: 0.89 - ETA: 0s - loss: 0.3318 - accuracy: 0.89 - ETA: 0s - loss: 0.3343 - accuracy: 0.89 - ETA: 0s - loss: 0.3437 - accuracy: 0.89 - ETA: 0s - loss: 0.3517 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3581 - accuracy: 0.8839 - val_loss: 0.6674 - val_accuracy: 0.7119\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.87 - ETA: 0s - loss: 0.3424 - accuracy: 0.89 - ETA: 0s - loss: 0.3360 - accuracy: 0.90 - ETA: 0s - loss: 0.3207 - accuracy: 0.90 - ETA: 0s - loss: 0.3430 - accuracy: 0.89 - ETA: 0s - loss: 0.3512 - accuracy: 0.88 - ETA: 0s - loss: 0.3585 - accuracy: 0.88 - ETA: 0s - loss: 0.3671 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3684 - accuracy: 0.8791 - val_loss: 0.8268 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.93 - ETA: 0s - loss: 0.3330 - accuracy: 0.88 - ETA: 0s - loss: 0.3250 - accuracy: 0.88 - ETA: 0s - loss: 0.3182 - accuracy: 0.89 - ETA: 0s - loss: 0.3231 - accuracy: 0.89 - ETA: 0s - loss: 0.3183 - accuracy: 0.89 - ETA: 0s - loss: 0.3309 - accuracy: 0.89 - ETA: 0s - loss: 0.3270 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8947 - val_loss: 0.9079 - val_accuracy: 0.7179\n",
      "Epoch 12/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.1638 - accuracy: 0.93 - ETA: 0s - loss: 0.3729 - accuracy: 0.89 - ETA: 0s - loss: 0.3731 - accuracy: 0.88 - ETA: 0s - loss: 0.3798 - accuracy: 0.88 - ETA: 0s - loss: 0.3888 - accuracy: 0.87 - ETA: 0s - loss: 0.3715 - accuracy: 0.88 - ETA: 0s - loss: 0.3656 - accuracy: 0.88 - ETA: 0s - loss: 0.3762 - accuracy: 0.8816Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8839 - val_loss: 0.9334 - val_accuracy: 0.7194\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: efcacd681ad2fd9d0e72a8ce05ef0774</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7298507491747538</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.027967003701402926</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 35</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.78 - ETA: 0s - loss: 4.3814 - accuracy: 0.58 - ETA: 0s - loss: 2.5827 - accuracy: 0.62 - ETA: 0s - loss: 1.8713 - accuracy: 0.65 - ETA: 0s - loss: 1.5345 - accuracy: 0.67 - ETA: 0s - loss: 1.3508 - accuracy: 0.68 - ETA: 0s - loss: 1.2183 - accuracy: 0.69 - 1s 6ms/step - loss: 1.1826 - accuracy: 0.6935 - val_loss: 0.6130 - val_accuracy: 0.7045\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.71 - ETA: 0s - loss: 0.5712 - accuracy: 0.72 - ETA: 0s - loss: 0.5714 - accuracy: 0.73 - ETA: 0s - loss: 0.5692 - accuracy: 0.74 - ETA: 0s - loss: 0.5603 - accuracy: 0.75 - ETA: 0s - loss: 0.5496 - accuracy: 0.75 - ETA: 0s - loss: 0.5434 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5420 - accuracy: 0.7604 - val_loss: 0.6020 - val_accuracy: 0.7075\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.68 - ETA: 0s - loss: 0.4883 - accuracy: 0.77 - ETA: 0s - loss: 0.4700 - accuracy: 0.79 - ETA: 0s - loss: 0.4967 - accuracy: 0.79 - ETA: 0s - loss: 0.4901 - accuracy: 0.80 - ETA: 0s - loss: 0.4971 - accuracy: 0.79 - ETA: 0s - loss: 0.4896 - accuracy: 0.79 - 0s 5ms/step - loss: 0.4892 - accuracy: 0.7958 - val_loss: 0.5810 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.84 - ETA: 0s - loss: 0.3701 - accuracy: 0.81 - ETA: 0s - loss: 0.3799 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.83 - ETA: 0s - loss: 0.4163 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8216 - val_loss: 0.6810 - val_accuracy: 0.6940\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.87 - ETA: 0s - loss: 0.3029 - accuracy: 0.86 - ETA: 0s - loss: 0.3351 - accuracy: 0.84 - ETA: 0s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3710 - accuracy: 0.83 - ETA: 0s - loss: 0.3921 - accuracy: 0.83 - ETA: 0s - loss: 0.3994 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4004 - accuracy: 0.8324 - val_loss: 0.6500 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.75 - ETA: 0s - loss: 0.3711 - accuracy: 0.83 - ETA: 0s - loss: 0.3499 - accuracy: 0.84 - ETA: 0s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3688 - accuracy: 0.83 - ETA: 0s - loss: 0.3751 - accuracy: 0.83 - ETA: 0s - loss: 0.3755 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8294 - val_loss: 1.0352 - val_accuracy: 0.6642\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.78 - ETA: 0s - loss: 0.4039 - accuracy: 0.84 - ETA: 0s - loss: 0.4448 - accuracy: 0.85 - ETA: 0s - loss: 0.4569 - accuracy: 0.83 - ETA: 0s - loss: 0.4504 - accuracy: 0.82 - ETA: 0s - loss: 0.4593 - accuracy: 0.82 - ETA: 0s - loss: 0.4480 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4480 - accuracy: 0.8287 - val_loss: 1.0895 - val_accuracy: 0.7164\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.93 - ETA: 0s - loss: 0.4554 - accuracy: 0.80 - ETA: 0s - loss: 0.4491 - accuracy: 0.80 - ETA: 0s - loss: 0.4226 - accuracy: 0.81 - ETA: 0s - loss: 0.4193 - accuracy: 0.82 - ETA: 0s - loss: 0.4139 - accuracy: 0.83 - ETA: 0s - loss: 0.4067 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4059 - accuracy: 0.8365 - val_loss: 1.4785 - val_accuracy: 0.6687\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.78 - ETA: 0s - loss: 0.3313 - accuracy: 0.85 - ETA: 0s - loss: 0.3586 - accuracy: 0.84 - ETA: 0s - loss: 0.3972 - accuracy: 0.84 - ETA: 0s - loss: 0.3939 - accuracy: 0.84 - ETA: 0s - loss: 0.4009 - accuracy: 0.84 - ETA: 0s - loss: 0.4005 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4005 - accuracy: 0.8417 - val_loss: 2.7250 - val_accuracy: 0.6552\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.84 - ETA: 0s - loss: 0.3902 - accuracy: 0.84 - ETA: 0s - loss: 0.8449 - accuracy: 0.83 - ETA: 0s - loss: 0.7124 - accuracy: 0.81 - ETA: 0s - loss: 0.6795 - accuracy: 0.81 - ETA: 0s - loss: 0.6292 - accuracy: 0.81 - ETA: 0s - loss: 0.5864 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5820 - accuracy: 0.8160 - val_loss: 1.0349 - val_accuracy: 0.6567\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.84 - ETA: 0s - loss: 0.3870 - accuracy: 0.84 - ETA: 0s - loss: 0.4162 - accuracy: 0.84 - ETA: 0s - loss: 0.4317 - accuracy: 0.84 - ETA: 0s - loss: 0.4329 - accuracy: 0.84 - ETA: 0s - loss: 0.4432 - accuracy: 0.84 - ETA: 0s - loss: 0.4597 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4634 - accuracy: 0.8380 - val_loss: 1.2456 - val_accuracy: 0.7507\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.93 - ETA: 0s - loss: 0.3896 - accuracy: 0.85 - ETA: 0s - loss: 0.5309 - accuracy: 0.85 - ETA: 0s - loss: 0.5247 - accuracy: 0.84 - ETA: 0s - loss: 0.5118 - accuracy: 0.84 - ETA: 0s - loss: 0.5377 - accuracy: 0.84 - ETA: 0s - loss: 0.5405 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5405 - accuracy: 0.8387 - val_loss: 0.5539 - val_accuracy: 0.7627\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4053 - accuracy: 0.93 - ETA: 0s - loss: 0.4992 - accuracy: 0.83 - ETA: 0s - loss: 0.4848 - accuracy: 0.83 - ETA: 0s - loss: 0.4741 - accuracy: 0.83 - ETA: 0s - loss: 0.4722 - accuracy: 0.83 - ETA: 0s - loss: 0.4740 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4625 - accuracy: 0.8417 - val_loss: 0.6288 - val_accuracy: 0.7313\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.90 - ETA: 0s - loss: 0.3966 - accuracy: 0.85 - ETA: 0s - loss: 0.4103 - accuracy: 0.86 - ETA: 0s - loss: 0.4154 - accuracy: 0.86 - ETA: 0s - loss: 0.4436 - accuracy: 0.85 - ETA: 0s - loss: 0.4569 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4722 - accuracy: 0.8384 - val_loss: 5.9092 - val_accuracy: 0.7254\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.81 - ETA: 0s - loss: 0.6413 - accuracy: 0.84 - ETA: 0s - loss: 0.6026 - accuracy: 0.82 - ETA: 0s - loss: 0.8580 - accuracy: 0.82 - ETA: 0s - loss: 0.7788 - accuracy: 0.82 - ETA: 0s - loss: 0.7619 - accuracy: 0.82 - ETA: 0s - loss: 0.7226 - accuracy: 0.83 - 0s 4ms/step - loss: 0.7226 - accuracy: 0.8313 - val_loss: 0.6630 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.78 - ETA: 0s - loss: 0.5719 - accuracy: 0.80 - ETA: 0s - loss: 0.6093 - accuracy: 0.78 - ETA: 0s - loss: 0.6077 - accuracy: 0.78 - ETA: 0s - loss: 0.6183 - accuracy: 0.77 - ETA: 0s - loss: 0.6020 - accuracy: 0.78 - ETA: 0s - loss: 0.5840 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7992 - val_loss: 0.7194 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.68 - ETA: 0s - loss: 0.5794 - accuracy: 0.80 - ETA: 0s - loss: 0.5378 - accuracy: 0.81 - ETA: 0s - loss: 0.5338 - accuracy: 0.82 - ETA: 0s - loss: 0.5111 - accuracy: 0.83 - ETA: 0s - loss: 0.5053 - accuracy: 0.83 - ETA: 0s - loss: 0.4950 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4943 - accuracy: 0.8410 - val_loss: 0.6784 - val_accuracy: 0.7493\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.87 - ETA: 0s - loss: 0.4247 - accuracy: 0.87 - ETA: 0s - loss: 0.4376 - accuracy: 0.87 - ETA: 0s - loss: 0.4520 - accuracy: 0.85 - ETA: 0s - loss: 0.4541 - accuracy: 0.85 - ETA: 0s - loss: 0.4650 - accuracy: 0.85 - ETA: 0s - loss: 0.4667 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4661 - accuracy: 0.8518 - val_loss: 0.8615 - val_accuracy: 0.7343\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.81 - ETA: 0s - loss: 0.4371 - accuracy: 0.84 - ETA: 0s - loss: 0.4016 - accuracy: 0.87 - ETA: 0s - loss: 0.4137 - accuracy: 0.87 - ETA: 0s - loss: 0.4995 - accuracy: 0.86 - ETA: 0s - loss: 0.4898 - accuracy: 0.86 - ETA: 0s - loss: 0.4922 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4942 - accuracy: 0.8600 - val_loss: 0.9046 - val_accuracy: 0.7134\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.87 - ETA: 0s - loss: 0.4696 - accuracy: 0.84 - ETA: 0s - loss: 0.4579 - accuracy: 0.84 - ETA: 0s - loss: 0.4594 - accuracy: 0.85 - ETA: 0s - loss: 0.4505 - accuracy: 0.86 - ETA: 0s - loss: 0.4507 - accuracy: 0.85 - ETA: 0s - loss: 0.4459 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4417 - accuracy: 0.8649 - val_loss: 1.0589 - val_accuracy: 0.7373\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.75 - ETA: 0s - loss: 0.4438 - accuracy: 0.86 - ETA: 0s - loss: 0.4655 - accuracy: 0.85 - ETA: 0s - loss: 0.4572 - accuracy: 0.85 - ETA: 0s - loss: 0.4567 - accuracy: 0.85 - ETA: 0s - loss: 0.4408 - accuracy: 0.86 - ETA: 0s - loss: 0.4284 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8679 - val_loss: 0.9485 - val_accuracy: 0.7269\n",
      "Epoch 22/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3033 - accuracy: 0.93 - ETA: 0s - loss: 0.4180 - accuracy: 0.88 - ETA: 0s - loss: 0.4038 - accuracy: 0.88 - ETA: 0s - loss: 0.4121 - accuracy: 0.87 - ETA: 0s - loss: 0.3984 - accuracy: 0.88 - ETA: 0s - loss: 0.3917 - accuracy: 0.88 - ETA: 0s - loss: 0.3939 - accuracy: 0.8833Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3925 - accuracy: 0.8839 - val_loss: 1.0585 - val_accuracy: 0.7358\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7658 - accuracy: 0.68 - ETA: 0s - loss: 1.9178 - accuracy: 0.61 - ETA: 0s - loss: 1.2500 - accuracy: 0.66 - ETA: 0s - loss: 1.0356 - accuracy: 0.67 - ETA: 0s - loss: 0.9203 - accuracy: 0.69 - ETA: 0s - loss: 0.8613 - accuracy: 0.70 - ETA: 0s - loss: 0.8239 - accuracy: 0.70 - 0s 6ms/step - loss: 0.8091 - accuracy: 0.7088 - val_loss: 0.5981 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.75 - ETA: 0s - loss: 0.5117 - accuracy: 0.78 - ETA: 0s - loss: 0.5270 - accuracy: 0.76 - ETA: 0s - loss: 0.5199 - accuracy: 0.77 - ETA: 0s - loss: 0.5229 - accuracy: 0.77 - ETA: 0s - loss: 0.5362 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5448 - accuracy: 0.7607 - val_loss: 0.6357 - val_accuracy: 0.6940\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.75 - ETA: 0s - loss: 0.4470 - accuracy: 0.80 - ETA: 0s - loss: 0.4771 - accuracy: 0.81 - ETA: 0s - loss: 0.4789 - accuracy: 0.80 - ETA: 0s - loss: 0.4998 - accuracy: 0.79 - ETA: 0s - loss: 0.5029 - accuracy: 0.79 - ETA: 0s - loss: 0.5108 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5112 - accuracy: 0.7887 - val_loss: 0.5957 - val_accuracy: 0.7239\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.84 - ETA: 0s - loss: 0.4583 - accuracy: 0.82 - ETA: 0s - loss: 0.4927 - accuracy: 0.82 - ETA: 0s - loss: 0.4886 - accuracy: 0.82 - ETA: 0s - loss: 0.4745 - accuracy: 0.82 - ETA: 0s - loss: 0.4848 - accuracy: 0.82 - ETA: 0s - loss: 0.4782 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4782 - accuracy: 0.8246 - val_loss: 0.6836 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.90 - ETA: 0s - loss: 0.3906 - accuracy: 0.85 - ETA: 0s - loss: 0.4401 - accuracy: 0.83 - ETA: 0s - loss: 0.4412 - accuracy: 0.83 - ETA: 0s - loss: 0.4431 - accuracy: 0.83 - ETA: 0s - loss: 0.4376 - accuracy: 0.84 - ETA: 0s - loss: 0.4472 - accuracy: 0.83 - ETA: 0s - loss: 0.4727 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4750 - accuracy: 0.8350 - val_loss: 0.7590 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.81 - ETA: 0s - loss: 0.4338 - accuracy: 0.85 - ETA: 0s - loss: 0.4416 - accuracy: 0.85 - ETA: 0s - loss: 0.4432 - accuracy: 0.84 - ETA: 0s - loss: 0.4499 - accuracy: 0.84 - ETA: 0s - loss: 0.4470 - accuracy: 0.84 - ETA: 0s - loss: 0.4451 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4388 - accuracy: 0.8466 - val_loss: 0.8279 - val_accuracy: 0.7134\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.84 - ETA: 0s - loss: 0.4286 - accuracy: 0.87 - ETA: 0s - loss: 0.3948 - accuracy: 0.88 - ETA: 0s - loss: 0.4336 - accuracy: 0.87 - ETA: 0s - loss: 0.4135 - accuracy: 0.87 - ETA: 0s - loss: 0.4201 - accuracy: 0.86 - ETA: 0s - loss: 0.4211 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4299 - accuracy: 0.8593 - val_loss: 0.7183 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.93 - ETA: 0s - loss: 0.4049 - accuracy: 0.84 - ETA: 0s - loss: 0.4170 - accuracy: 0.85 - ETA: 0s - loss: 0.4086 - accuracy: 0.86 - ETA: 0s - loss: 0.4086 - accuracy: 0.87 - ETA: 0s - loss: 0.4119 - accuracy: 0.86 - ETA: 0s - loss: 0.4112 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4111 - accuracy: 0.8712 - val_loss: 1.2059 - val_accuracy: 0.6806\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.84 - ETA: 0s - loss: 0.4319 - accuracy: 0.86 - ETA: 0s - loss: 0.4305 - accuracy: 0.87 - ETA: 0s - loss: 0.4185 - accuracy: 0.87 - ETA: 0s - loss: 0.4119 - accuracy: 0.87 - ETA: 0s - loss: 0.4072 - accuracy: 0.86 - ETA: 0s - loss: 0.4193 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8585 - val_loss: 0.6181 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.90 - ETA: 0s - loss: 0.3461 - accuracy: 0.89 - ETA: 0s - loss: 0.3873 - accuracy: 0.88 - ETA: 0s - loss: 0.3955 - accuracy: 0.88 - ETA: 0s - loss: 0.4095 - accuracy: 0.87 - ETA: 0s - loss: 0.4176 - accuracy: 0.87 - ETA: 0s - loss: 0.4281 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8768 - val_loss: 0.8817 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3660 - accuracy: 0.93 - ETA: 0s - loss: 0.5160 - accuracy: 0.82 - ETA: 0s - loss: 0.4719 - accuracy: 0.85 - ETA: 0s - loss: 0.4897 - accuracy: 0.84 - ETA: 0s - loss: 0.4792 - accuracy: 0.84 - ETA: 0s - loss: 0.4689 - accuracy: 0.85 - ETA: 0s - loss: 0.5031 - accuracy: 0.85 - ETA: 0s - loss: 0.5309 - accuracy: 0.84 - ETA: 0s - loss: 0.5252 - accuracy: 0.84 - 0s 6ms/step - loss: 0.5237 - accuracy: 0.8473 - val_loss: 0.7435 - val_accuracy: 0.7269\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.84 - ETA: 0s - loss: 0.4397 - accuracy: 0.84 - ETA: 0s - loss: 0.4944 - accuracy: 0.84 - ETA: 0s - loss: 0.4783 - accuracy: 0.85 - ETA: 0s - loss: 0.4624 - accuracy: 0.85 - ETA: 0s - loss: 0.4507 - accuracy: 0.86 - ETA: 0s - loss: 0.4574 - accuracy: 0.85 - ETA: 0s - loss: 0.4705 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4717 - accuracy: 0.8511 - val_loss: 1.5947 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.96 - ETA: 0s - loss: 0.4263 - accuracy: 0.87 - ETA: 0s - loss: 0.4444 - accuracy: 0.86 - ETA: 0s - loss: 0.4352 - accuracy: 0.86 - ETA: 0s - loss: 0.4424 - accuracy: 0.86 - ETA: 0s - loss: 0.4452 - accuracy: 0.86 - ETA: 0s - loss: 0.4372 - accuracy: 0.86 - ETA: 0s - loss: 0.4396 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4396 - accuracy: 0.8626 - val_loss: 1.3589 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.84 - ETA: 0s - loss: 0.3620 - accuracy: 0.89 - ETA: 0s - loss: 0.3522 - accuracy: 0.90 - ETA: 0s - loss: 0.3559 - accuracy: 0.89 - ETA: 0s - loss: 0.3437 - accuracy: 0.89 - ETA: 0s - loss: 0.3661 - accuracy: 0.89 - ETA: 0s - loss: 0.3677 - accuracy: 0.89 - ETA: 0s - loss: 0.3773 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3781 - accuracy: 0.8906 - val_loss: 1.2105 - val_accuracy: 0.7388\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.93 - ETA: 0s - loss: 0.3253 - accuracy: 0.89 - ETA: 0s - loss: 0.3246 - accuracy: 0.90 - ETA: 0s - loss: 0.3425 - accuracy: 0.90 - ETA: 0s - loss: 0.3707 - accuracy: 0.89 - ETA: 0s - loss: 0.3745 - accuracy: 0.89 - ETA: 0s - loss: 0.3728 - accuracy: 0.89 - ETA: 0s - loss: 0.3728 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3725 - accuracy: 0.8921 - val_loss: 1.2644 - val_accuracy: 0.7418\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.93 - ETA: 0s - loss: 0.3242 - accuracy: 0.90 - ETA: 0s - loss: 0.3258 - accuracy: 0.90 - ETA: 0s - loss: 0.3374 - accuracy: 0.90 - ETA: 0s - loss: 0.3211 - accuracy: 0.91 - ETA: 0s - loss: 0.3324 - accuracy: 0.90 - ETA: 0s - loss: 0.3317 - accuracy: 0.90 - ETA: 0s - loss: 0.3358 - accuracy: 0.90 - ETA: 0s - loss: 0.3385 - accuracy: 0.90 - 0s 6ms/step - loss: 0.3384 - accuracy: 0.9059 - val_loss: 1.1729 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.90 - ETA: 0s - loss: 0.3011 - accuracy: 0.91 - ETA: 0s - loss: 0.3487 - accuracy: 0.90 - ETA: 0s - loss: 0.3490 - accuracy: 0.90 - ETA: 0s - loss: 0.3546 - accuracy: 0.90 - ETA: 0s - loss: 0.3661 - accuracy: 0.89 - ETA: 0s - loss: 0.3710 - accuracy: 0.89 - ETA: 0s - loss: 0.3604 - accuracy: 0.90 - ETA: 0s - loss: 0.3524 - accuracy: 0.90 - 1s 6ms/step - loss: 0.3500 - accuracy: 0.9033 - val_loss: 1.2815 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.93 - ETA: 0s - loss: 0.2795 - accuracy: 0.93 - ETA: 0s - loss: 0.3120 - accuracy: 0.91 - ETA: 0s - loss: 0.3133 - accuracy: 0.91 - ETA: 0s - loss: 0.2993 - accuracy: 0.92 - ETA: 0s - loss: 0.3091 - accuracy: 0.92 - ETA: 0s - loss: 0.3439 - accuracy: 0.91 - ETA: 0s - loss: 0.3477 - accuracy: 0.90 - ETA: 0s - loss: 0.3541 - accuracy: 0.90 - 0s 6ms/step - loss: 0.3581 - accuracy: 0.9048 - val_loss: 0.8982 - val_accuracy: 0.7075\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.84 - ETA: 0s - loss: 0.3221 - accuracy: 0.90 - ETA: 0s - loss: 0.3223 - accuracy: 0.90 - ETA: 0s - loss: 0.3078 - accuracy: 0.91 - ETA: 0s - loss: 0.3146 - accuracy: 0.90 - ETA: 0s - loss: 0.3284 - accuracy: 0.91 - ETA: 0s - loss: 0.3262 - accuracy: 0.91 - ETA: 0s - loss: 0.3553 - accuracy: 0.90 - ETA: 0s - loss: 0.4063 - accuracy: 0.90 - 0s 6ms/step - loss: 0.4063 - accuracy: 0.9007 - val_loss: 1.6994 - val_accuracy: 0.6060\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8475 - accuracy: 0.75 - ETA: 0s - loss: 0.4564 - accuracy: 0.87 - ETA: 0s - loss: 0.4600 - accuracy: 0.86 - ETA: 0s - loss: 0.4576 - accuracy: 0.87 - ETA: 0s - loss: 0.4728 - accuracy: 0.86 - ETA: 0s - loss: 0.4931 - accuracy: 0.85 - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.4762 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4652 - accuracy: 0.8585 - val_loss: 0.7122 - val_accuracy: 0.7328\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6870 - accuracy: 0.78 - ETA: 0s - loss: 0.3701 - accuracy: 0.90 - ETA: 0s - loss: 0.3536 - accuracy: 0.90 - ETA: 0s - loss: 0.3617 - accuracy: 0.90 - ETA: 0s - loss: 0.3514 - accuracy: 0.90 - ETA: 0s - loss: 0.3510 - accuracy: 0.90 - ETA: 0s - loss: 0.3594 - accuracy: 0.90 - ETA: 0s - loss: 0.3602 - accuracy: 0.90 - ETA: 0s - loss: 0.3672 - accuracy: 0.90 - 0s 6ms/step - loss: 0.3667 - accuracy: 0.9011 - val_loss: 0.6490 - val_accuracy: 0.7224\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.90 - ETA: 0s - loss: 0.3502 - accuracy: 0.89 - ETA: 0s - loss: 0.3494 - accuracy: 0.89 - ETA: 0s - loss: 0.3663 - accuracy: 0.88 - ETA: 0s - loss: 0.3495 - accuracy: 0.89 - ETA: 0s - loss: 0.3537 - accuracy: 0.89 - ETA: 0s - loss: 0.3512 - accuracy: 0.89 - ETA: 0s - loss: 0.3566 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3515 - accuracy: 0.8988 - val_loss: 1.0200 - val_accuracy: 0.7299\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.93 - ETA: 0s - loss: 0.3171 - accuracy: 0.91 - ETA: 0s - loss: 0.3363 - accuracy: 0.90 - ETA: 0s - loss: 0.3278 - accuracy: 0.90 - ETA: 0s - loss: 0.3219 - accuracy: 0.90 - ETA: 0s - loss: 0.3175 - accuracy: 0.91 - ETA: 0s - loss: 0.3151 - accuracy: 0.91 - 0s 5ms/step - loss: 0.3250 - accuracy: 0.9082 - val_loss: 1.8063 - val_accuracy: 0.7164\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.81 - ETA: 0s - loss: 0.3564 - accuracy: 0.90 - ETA: 0s - loss: 0.3196 - accuracy: 0.91 - ETA: 0s - loss: 0.3166 - accuracy: 0.91 - ETA: 0s - loss: 0.3374 - accuracy: 0.91 - ETA: 0s - loss: 0.3824 - accuracy: 0.89 - ETA: 0s - loss: 0.4196 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4313 - accuracy: 0.8764 - val_loss: 0.7200 - val_accuracy: 0.7657\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.87 - ETA: 0s - loss: 0.4999 - accuracy: 0.84 - ETA: 0s - loss: 0.5040 - accuracy: 0.84 - ETA: 0s - loss: 0.5067 - accuracy: 0.84 - ETA: 0s - loss: 0.4969 - accuracy: 0.84 - ETA: 0s - loss: 0.5116 - accuracy: 0.84 - ETA: 0s - loss: 0.5225 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5214 - accuracy: 0.8414 - val_loss: 0.9786 - val_accuracy: 0.7478\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.90 - ETA: 0s - loss: 0.4544 - accuracy: 0.85 - ETA: 0s - loss: 0.5041 - accuracy: 0.84 - ETA: 0s - loss: 0.5000 - accuracy: 0.84 - ETA: 0s - loss: 0.4873 - accuracy: 0.85 - ETA: 0s - loss: 0.4880 - accuracy: 0.86 - ETA: 0s - loss: 0.4776 - accuracy: 0.86 - ETA: 0s - loss: 0.4674 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4650 - accuracy: 0.8671 - val_loss: 1.6332 - val_accuracy: 0.7463\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.96 - ETA: 0s - loss: 0.3624 - accuracy: 0.91 - ETA: 0s - loss: 0.3510 - accuracy: 0.91 - ETA: 0s - loss: 0.3549 - accuracy: 0.91 - ETA: 0s - loss: 0.4186 - accuracy: 0.89 - ETA: 0s - loss: 0.4998 - accuracy: 0.87 - ETA: 0s - loss: 0.4953 - accuracy: 0.87 - 0s 5ms/step - loss: 0.5007 - accuracy: 0.8660 - val_loss: 0.6346 - val_accuracy: 0.7567\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.84 - ETA: 0s - loss: 0.5201 - accuracy: 0.84 - ETA: 0s - loss: 0.4721 - accuracy: 0.86 - ETA: 0s - loss: 0.5054 - accuracy: 0.84 - ETA: 0s - loss: 0.5007 - accuracy: 0.84 - ETA: 0s - loss: 0.4698 - accuracy: 0.85 - ETA: 0s - loss: 0.4644 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4637 - accuracy: 0.8604 - val_loss: 0.7690 - val_accuracy: 0.7507\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.90 - ETA: 0s - loss: 0.4182 - accuracy: 0.87 - ETA: 0s - loss: 0.4193 - accuracy: 0.88 - ETA: 0s - loss: 0.4148 - accuracy: 0.88 - ETA: 0s - loss: 0.3985 - accuracy: 0.88 - ETA: 0s - loss: 0.4047 - accuracy: 0.88 - ETA: 0s - loss: 0.4783 - accuracy: 0.87 - ETA: 0s - loss: 0.4727 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4727 - accuracy: 0.8712 - val_loss: 0.8006 - val_accuracy: 0.7209\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4073 - accuracy: 0.87 - ETA: 0s - loss: 0.4243 - accuracy: 0.85 - ETA: 0s - loss: 0.5815 - accuracy: 0.87 - ETA: 0s - loss: 0.5269 - accuracy: 0.87 - ETA: 0s - loss: 0.5086 - accuracy: 0.87 - ETA: 0s - loss: 0.4816 - accuracy: 0.88 - ETA: 0s - loss: 0.4772 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4715 - accuracy: 0.8813 - val_loss: 1.3229 - val_accuracy: 0.7284\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.75 - ETA: 0s - loss: 0.3979 - accuracy: 0.88 - ETA: 0s - loss: 0.3750 - accuracy: 0.89 - ETA: 0s - loss: 0.3791 - accuracy: 0.89 - ETA: 0s - loss: 0.3771 - accuracy: 0.89 - ETA: 0s - loss: 0.3786 - accuracy: 0.89 - ETA: 0s - loss: 0.3780 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3748 - accuracy: 0.8951 - val_loss: 1.2110 - val_accuracy: 0.7493\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.84 - ETA: 0s - loss: 0.3779 - accuracy: 0.89 - ETA: 0s - loss: 0.3707 - accuracy: 0.90 - ETA: 0s - loss: 0.3667 - accuracy: 0.90 - ETA: 0s - loss: 0.3694 - accuracy: 0.90 - ETA: 0s - loss: 0.3610 - accuracy: 0.90 - ETA: 0s - loss: 0.3615 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3579 - accuracy: 0.9071 - val_loss: 1.7393 - val_accuracy: 0.7179\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.93 - ETA: 0s - loss: 0.3089 - accuracy: 0.91 - ETA: 0s - loss: 0.4475 - accuracy: 0.90 - ETA: 0s - loss: 0.4319 - accuracy: 0.90 - ETA: 0s - loss: 0.4054 - accuracy: 0.90 - ETA: 0s - loss: 0.3922 - accuracy: 0.90 - ETA: 0s - loss: 0.3856 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3830 - accuracy: 0.9048 - val_loss: 1.9910 - val_accuracy: 0.7164\n",
      "Epoch 34/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.4468 - accuracy: 0.87 - ETA: 0s - loss: 0.4373 - accuracy: 0.88 - ETA: 0s - loss: 0.4090 - accuracy: 0.88 - ETA: 0s - loss: 0.4046 - accuracy: 0.88 - ETA: 0s - loss: 0.4092 - accuracy: 0.88 - ETA: 0s - loss: 0.3984 - accuracy: 0.8908Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8955 - val_loss: 1.7400 - val_accuracy: 0.7104\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.68 - ETA: 0s - loss: 2.5199 - accuracy: 0.61 - ETA: 0s - loss: 1.6800 - accuracy: 0.64 - ETA: 0s - loss: 1.3202 - accuracy: 0.67 - ETA: 0s - loss: 1.1494 - accuracy: 0.67 - ETA: 0s - loss: 1.0417 - accuracy: 0.68 - ETA: 0s - loss: 0.9659 - accuracy: 0.69 - 0s 6ms/step - loss: 0.9359 - accuracy: 0.6988 - val_loss: 0.6876 - val_accuracy: 0.6194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.68 - ETA: 0s - loss: 0.5424 - accuracy: 0.73 - ETA: 0s - loss: 0.5310 - accuracy: 0.74 - ETA: 0s - loss: 0.5032 - accuracy: 0.76 - ETA: 0s - loss: 0.5110 - accuracy: 0.75 - ETA: 0s - loss: 0.5206 - accuracy: 0.76 - ETA: 0s - loss: 0.5275 - accuracy: 0.76 - 0s 5ms/step - loss: 0.5373 - accuracy: 0.7548 - val_loss: 0.6389 - val_accuracy: 0.6522\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6230 - accuracy: 0.65 - ETA: 0s - loss: 0.5054 - accuracy: 0.76 - ETA: 0s - loss: 0.4902 - accuracy: 0.78 - ETA: 0s - loss: 0.4787 - accuracy: 0.79 - ETA: 0s - loss: 0.4842 - accuracy: 0.79 - ETA: 0s - loss: 0.4826 - accuracy: 0.79 - ETA: 0s - loss: 0.4886 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4886 - accuracy: 0.7966 - val_loss: 0.6582 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.93 - ETA: 0s - loss: 0.4578 - accuracy: 0.83 - ETA: 0s - loss: 0.4663 - accuracy: 0.83 - ETA: 0s - loss: 0.4963 - accuracy: 0.82 - ETA: 0s - loss: 0.4825 - accuracy: 0.82 - ETA: 0s - loss: 0.4747 - accuracy: 0.83 - ETA: 0s - loss: 0.5013 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5033 - accuracy: 0.8178 - val_loss: 0.5941 - val_accuracy: 0.7075\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.68 - ETA: 0s - loss: 0.4654 - accuracy: 0.80 - ETA: 0s - loss: 0.4493 - accuracy: 0.81 - ETA: 0s - loss: 0.4582 - accuracy: 0.81 - ETA: 0s - loss: 0.4397 - accuracy: 0.82 - ETA: 0s - loss: 0.4531 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4642 - accuracy: 0.8238 - val_loss: 0.6030 - val_accuracy: 0.7075\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.87 - ETA: 0s - loss: 0.4473 - accuracy: 0.83 - ETA: 0s - loss: 0.4568 - accuracy: 0.83 - ETA: 0s - loss: 0.4341 - accuracy: 0.84 - ETA: 0s - loss: 0.4431 - accuracy: 0.84 - ETA: 0s - loss: 0.4459 - accuracy: 0.83 - ETA: 0s - loss: 0.4432 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4438 - accuracy: 0.8373 - val_loss: 0.7851 - val_accuracy: 0.7045\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.87 - ETA: 0s - loss: 0.4468 - accuracy: 0.83 - ETA: 0s - loss: 0.4233 - accuracy: 0.85 - ETA: 0s - loss: 0.4265 - accuracy: 0.85 - ETA: 0s - loss: 0.4277 - accuracy: 0.84 - ETA: 0s - loss: 0.4195 - accuracy: 0.85 - ETA: 0s - loss: 0.4237 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4223 - accuracy: 0.8507 - val_loss: 0.6141 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.71 - ETA: 0s - loss: 0.4483 - accuracy: 0.86 - ETA: 0s - loss: 0.4150 - accuracy: 0.86 - ETA: 0s - loss: 0.4130 - accuracy: 0.86 - ETA: 0s - loss: 0.3993 - accuracy: 0.86 - ETA: 0s - loss: 0.4013 - accuracy: 0.86 - ETA: 0s - loss: 0.4051 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4028 - accuracy: 0.8660 - val_loss: 0.9510 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.93 - ETA: 0s - loss: 0.3168 - accuracy: 0.89 - ETA: 0s - loss: 0.3677 - accuracy: 0.88 - ETA: 0s - loss: 0.3711 - accuracy: 0.88 - ETA: 0s - loss: 0.3724 - accuracy: 0.87 - ETA: 0s - loss: 0.3843 - accuracy: 0.87 - ETA: 0s - loss: 0.3915 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3910 - accuracy: 0.8679 - val_loss: 0.8351 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.81 - ETA: 0s - loss: 0.3925 - accuracy: 0.87 - ETA: 0s - loss: 0.3865 - accuracy: 0.86 - ETA: 0s - loss: 0.3650 - accuracy: 0.88 - ETA: 0s - loss: 0.3593 - accuracy: 0.88 - ETA: 0s - loss: 0.3641 - accuracy: 0.88 - ETA: 0s - loss: 0.3596 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3596 - accuracy: 0.8865 - val_loss: 0.8995 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.93 - ETA: 0s - loss: 0.3120 - accuracy: 0.90 - ETA: 0s - loss: 0.3262 - accuracy: 0.89 - ETA: 0s - loss: 0.3494 - accuracy: 0.89 - ETA: 0s - loss: 0.3603 - accuracy: 0.88 - ETA: 0s - loss: 0.3587 - accuracy: 0.88 - ETA: 0s - loss: 0.3692 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8828 - val_loss: 1.0377 - val_accuracy: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1746 - accuracy: 0.84 - ETA: 0s - loss: 0.5738 - accuracy: 0.85 - ETA: 0s - loss: 0.4936 - accuracy: 0.87 - ETA: 0s - loss: 0.4584 - accuracy: 0.86 - ETA: 0s - loss: 0.4587 - accuracy: 0.86 - ETA: 0s - loss: 0.4972 - accuracy: 0.86 - ETA: 0s - loss: 0.4736 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4663 - accuracy: 0.8682 - val_loss: 0.6577 - val_accuracy: 0.7433\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.96 - ETA: 0s - loss: 0.3047 - accuracy: 0.89 - ETA: 0s - loss: 0.3107 - accuracy: 0.90 - ETA: 0s - loss: 0.3191 - accuracy: 0.90 - ETA: 0s - loss: 0.3402 - accuracy: 0.89 - ETA: 0s - loss: 0.3534 - accuracy: 0.89 - ETA: 0s - loss: 0.3719 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8832 - val_loss: 0.7046 - val_accuracy: 0.7164\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.90 - ETA: 0s - loss: 0.3565 - accuracy: 0.88 - ETA: 0s - loss: 0.3917 - accuracy: 0.87 - ETA: 0s - loss: 0.3920 - accuracy: 0.88 - ETA: 0s - loss: 0.3945 - accuracy: 0.87 - ETA: 0s - loss: 0.3955 - accuracy: 0.87 - ETA: 0s - loss: 0.4223 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4171 - accuracy: 0.8813 - val_loss: 0.7045 - val_accuracy: 0.7194\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.2935 - accuracy: 0.90 - ETA: 0s - loss: 0.2921 - accuracy: 0.91 - ETA: 0s - loss: 0.3051 - accuracy: 0.90 - ETA: 0s - loss: 0.3318 - accuracy: 0.89 - ETA: 0s - loss: 0.3430 - accuracy: 0.89 - ETA: 0s - loss: 0.3473 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8888 - val_loss: 0.6208 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.93 - ETA: 0s - loss: 0.3437 - accuracy: 0.89 - ETA: 0s - loss: 0.3365 - accuracy: 0.89 - ETA: 0s - loss: 0.3121 - accuracy: 0.90 - ETA: 0s - loss: 0.3083 - accuracy: 0.90 - ETA: 0s - loss: 0.3028 - accuracy: 0.90 - ETA: 0s - loss: 0.3126 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3134 - accuracy: 0.9033 - val_loss: 0.8648 - val_accuracy: 0.7104\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 1.00 - ETA: 0s - loss: 0.2699 - accuracy: 0.92 - ETA: 0s - loss: 0.2662 - accuracy: 0.92 - ETA: 0s - loss: 0.2664 - accuracy: 0.91 - ETA: 0s - loss: 0.2817 - accuracy: 0.91 - ETA: 0s - loss: 0.2752 - accuracy: 0.91 - ETA: 0s - loss: 0.2743 - accuracy: 0.91 - ETA: 0s - loss: 0.2775 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2773 - accuracy: 0.9194 - val_loss: 1.2040 - val_accuracy: 0.7194\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.87 - ETA: 0s - loss: 0.2794 - accuracy: 0.92 - ETA: 0s - loss: 0.2604 - accuracy: 0.93 - ETA: 0s - loss: 0.2624 - accuracy: 0.93 - ETA: 0s - loss: 0.2648 - accuracy: 0.93 - ETA: 0s - loss: 0.2581 - accuracy: 0.93 - ETA: 0s - loss: 0.2522 - accuracy: 0.93 - 0s 5ms/step - loss: 0.2525 - accuracy: 0.9354 - val_loss: 1.1187 - val_accuracy: 0.7284\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.93 - ETA: 0s - loss: 0.3083 - accuracy: 0.92 - ETA: 0s - loss: 0.3045 - accuracy: 0.91 - ETA: 0s - loss: 0.3434 - accuracy: 0.90 - ETA: 0s - loss: 0.3516 - accuracy: 0.89 - ETA: 0s - loss: 0.3527 - accuracy: 0.89 - ETA: 0s - loss: 0.3420 - accuracy: 0.90 - ETA: 0s - loss: 0.3444 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3453 - accuracy: 0.9022 - val_loss: 0.6620 - val_accuracy: 0.7030\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.84 - ETA: 0s - loss: 0.3739 - accuracy: 0.87 - ETA: 0s - loss: 0.3639 - accuracy: 0.88 - ETA: 0s - loss: 0.3385 - accuracy: 0.89 - ETA: 0s - loss: 0.3371 - accuracy: 0.90 - ETA: 0s - loss: 0.3299 - accuracy: 0.90 - ETA: 0s - loss: 0.3186 - accuracy: 0.90 - 0s 5ms/step - loss: 0.3214 - accuracy: 0.9082 - val_loss: 1.2099 - val_accuracy: 0.6970\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.87 - ETA: 0s - loss: 0.3089 - accuracy: 0.90 - ETA: 0s - loss: 0.2981 - accuracy: 0.92 - ETA: 0s - loss: 0.3021 - accuracy: 0.91 - ETA: 0s - loss: 0.3044 - accuracy: 0.91 - ETA: 0s - loss: 0.2967 - accuracy: 0.91 - ETA: 0s - loss: 0.2891 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2824 - accuracy: 0.9205 - val_loss: 1.3249 - val_accuracy: 0.6896\n",
      "Epoch 22/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.4043 - accuracy: 0.87 - ETA: 0s - loss: 0.2500 - accuracy: 0.93 - ETA: 0s - loss: 0.2477 - accuracy: 0.93 - ETA: 0s - loss: 0.2681 - accuracy: 0.93 - ETA: 0s - loss: 0.2500 - accuracy: 0.94 - ETA: 0s - loss: 0.2573 - accuracy: 0.93 - ETA: 0s - loss: 0.2634 - accuracy: 0.9350Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.9321 - val_loss: 0.8804 - val_accuracy: 0.7224\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a01299b5504e976ee739ae826e783dee</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7572139302889506</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.2858706245181789</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.65 - ETA: 0s - loss: 1.0649 - accuracy: 0.61 - ETA: 0s - loss: 0.9712 - accuracy: 0.61 - ETA: 0s - loss: 0.8982 - accuracy: 0.61 - ETA: 0s - loss: 0.8377 - accuracy: 0.64 - ETA: 0s - loss: 0.7842 - accuracy: 0.65 - 0s 6ms/step - loss: 0.7692 - accuracy: 0.6641 - val_loss: 0.7041 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4683 - accuracy: 0.81 - ETA: 0s - loss: 0.5403 - accuracy: 0.75 - ETA: 0s - loss: 0.5375 - accuracy: 0.74 - ETA: 0s - loss: 0.5257 - accuracy: 0.75 - ETA: 0s - loss: 0.5270 - accuracy: 0.75 - ETA: 0s - loss: 0.5369 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5366 - accuracy: 0.7473 - val_loss: 0.6262 - val_accuracy: 0.6910\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.87 - ETA: 0s - loss: 0.4631 - accuracy: 0.78 - ETA: 0s - loss: 0.4443 - accuracy: 0.78 - ETA: 0s - loss: 0.4371 - accuracy: 0.79 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4524 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7932 - val_loss: 0.6349 - val_accuracy: 0.6925\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.87 - ETA: 0s - loss: 0.3491 - accuracy: 0.85 - ETA: 0s - loss: 0.3659 - accuracy: 0.85 - ETA: 0s - loss: 0.3703 - accuracy: 0.84 - ETA: 0s - loss: 0.3830 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3995 - accuracy: 0.8212 - val_loss: 0.7729 - val_accuracy: 0.6582\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.90 - ETA: 0s - loss: 0.3324 - accuracy: 0.85 - ETA: 0s - loss: 0.3251 - accuracy: 0.85 - ETA: 0s - loss: 0.3418 - accuracy: 0.84 - ETA: 0s - loss: 0.3545 - accuracy: 0.84 - ETA: 0s - loss: 0.3645 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8373 - val_loss: 0.7602 - val_accuracy: 0.6836\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.84 - ETA: 0s - loss: 0.2916 - accuracy: 0.87 - ETA: 0s - loss: 0.3253 - accuracy: 0.85 - ETA: 0s - loss: 0.3134 - accuracy: 0.86 - ETA: 0s - loss: 0.3187 - accuracy: 0.85 - ETA: 0s - loss: 0.3323 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3371 - accuracy: 0.8429 - val_loss: 0.6815 - val_accuracy: 0.7030\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.90 - ETA: 0s - loss: 0.2909 - accuracy: 0.85 - ETA: 0s - loss: 0.2851 - accuracy: 0.87 - ETA: 0s - loss: 0.2938 - accuracy: 0.86 - ETA: 0s - loss: 0.3040 - accuracy: 0.86 - ETA: 0s - loss: 0.3064 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3036 - accuracy: 0.8652 - val_loss: 0.9458 - val_accuracy: 0.6761\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.87 - ETA: 0s - loss: 0.2961 - accuracy: 0.86 - ETA: 0s - loss: 0.2930 - accuracy: 0.87 - ETA: 0s - loss: 0.2857 - accuracy: 0.88 - ETA: 0s - loss: 0.2867 - accuracy: 0.88 - ETA: 0s - loss: 0.3070 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3101 - accuracy: 0.8723 - val_loss: 0.9175 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.90 - ETA: 0s - loss: 0.2637 - accuracy: 0.87 - ETA: 0s - loss: 0.2620 - accuracy: 0.87 - ETA: 0s - loss: 0.2712 - accuracy: 0.88 - ETA: 0s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2897 - accuracy: 0.88 - 0s 4ms/step - loss: 0.2915 - accuracy: 0.8865 - val_loss: 1.2308 - val_accuracy: 0.6716\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.84 - ETA: 0s - loss: 0.2567 - accuracy: 0.90 - ETA: 0s - loss: 0.2748 - accuracy: 0.89 - ETA: 0s - loss: 0.2644 - accuracy: 0.89 - ETA: 0s - loss: 0.2643 - accuracy: 0.89 - ETA: 0s - loss: 0.2634 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2634 - accuracy: 0.8932 - val_loss: 1.0510 - val_accuracy: 0.6851\n",
      "Epoch 11/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3354 - accuracy: 0.84 - ETA: 0s - loss: 0.2151 - accuracy: 0.89 - ETA: 0s - loss: 0.1930 - accuracy: 0.90 - ETA: 0s - loss: 0.2006 - accuracy: 0.90 - ETA: 0s - loss: 0.2088 - accuracy: 0.91 - ETA: 0s - loss: 0.2250 - accuracy: 0.9105Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2276 - accuracy: 0.9089 - val_loss: 1.2694 - val_accuracy: 0.6836\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8292 - accuracy: 0.53 - ETA: 0s - loss: 2.0446 - accuracy: 0.59 - ETA: 0s - loss: 1.4198 - accuracy: 0.61 - ETA: 0s - loss: 1.1948 - accuracy: 0.62 - ETA: 0s - loss: 1.0646 - accuracy: 0.64 - ETA: 0s - loss: 0.9787 - accuracy: 0.65 - ETA: 0s - loss: 0.9172 - accuracy: 0.66 - 0s 5ms/step - loss: 0.9172 - accuracy: 0.6611 - val_loss: 0.7258 - val_accuracy: 0.6493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.71 - ETA: 0s - loss: 0.5612 - accuracy: 0.74 - ETA: 0s - loss: 0.5533 - accuracy: 0.73 - ETA: 0s - loss: 0.5567 - accuracy: 0.74 - ETA: 0s - loss: 0.5533 - accuracy: 0.73 - ETA: 0s - loss: 0.5517 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7432 - val_loss: 0.6405 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.90 - ETA: 0s - loss: 0.5119 - accuracy: 0.76 - ETA: 0s - loss: 0.4713 - accuracy: 0.78 - ETA: 0s - loss: 0.4797 - accuracy: 0.78 - ETA: 0s - loss: 0.4895 - accuracy: 0.78 - ETA: 0s - loss: 0.4913 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4883 - accuracy: 0.7921 - val_loss: 0.5875 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.68 - ETA: 0s - loss: 0.4741 - accuracy: 0.78 - ETA: 0s - loss: 0.4426 - accuracy: 0.80 - ETA: 0s - loss: 0.4444 - accuracy: 0.80 - ETA: 0s - loss: 0.4463 - accuracy: 0.80 - ETA: 0s - loss: 0.4430 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4459 - accuracy: 0.8096 - val_loss: 0.6069 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.68 - ETA: 0s - loss: 0.3520 - accuracy: 0.84 - ETA: 0s - loss: 0.3444 - accuracy: 0.83 - ETA: 0s - loss: 0.3600 - accuracy: 0.83 - ETA: 0s - loss: 0.3669 - accuracy: 0.83 - ETA: 0s - loss: 0.3801 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3839 - accuracy: 0.8331 - val_loss: 0.6277 - val_accuracy: 0.7299\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.87 - ETA: 0s - loss: 0.2803 - accuracy: 0.88 - ETA: 0s - loss: 0.3547 - accuracy: 0.86 - ETA: 0s - loss: 0.3659 - accuracy: 0.85 - ETA: 0s - loss: 0.3761 - accuracy: 0.84 - ETA: 0s - loss: 0.3819 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8406 - val_loss: 0.6856 - val_accuracy: 0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.93 - ETA: 0s - loss: 0.3102 - accuracy: 0.87 - ETA: 0s - loss: 0.2993 - accuracy: 0.87 - ETA: 0s - loss: 0.3175 - accuracy: 0.87 - ETA: 0s - loss: 0.3224 - accuracy: 0.86 - ETA: 0s - loss: 0.3282 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3327 - accuracy: 0.8667 - val_loss: 0.6422 - val_accuracy: 0.7060\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.93 - ETA: 0s - loss: 0.2957 - accuracy: 0.86 - ETA: 0s - loss: 0.2797 - accuracy: 0.87 - ETA: 0s - loss: 0.2873 - accuracy: 0.87 - ETA: 0s - loss: 0.2946 - accuracy: 0.87 - ETA: 0s - loss: 0.3045 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3033 - accuracy: 0.8697 - val_loss: 0.9515 - val_accuracy: 0.6881\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.84 - ETA: 0s - loss: 0.2484 - accuracy: 0.91 - ETA: 0s - loss: 0.2716 - accuracy: 0.88 - ETA: 0s - loss: 0.2765 - accuracy: 0.88 - ETA: 0s - loss: 0.2697 - accuracy: 0.88 - ETA: 0s - loss: 0.2696 - accuracy: 0.88 - 0s 4ms/step - loss: 0.2678 - accuracy: 0.8806 - val_loss: 1.0652 - val_accuracy: 0.6731\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 1.00 - ETA: 0s - loss: 0.2017 - accuracy: 0.92 - ETA: 0s - loss: 0.2143 - accuracy: 0.91 - ETA: 0s - loss: 0.2344 - accuracy: 0.90 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2468 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2584 - accuracy: 0.8940 - val_loss: 0.7163 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.84 - ETA: 0s - loss: 0.2919 - accuracy: 0.88 - ETA: 0s - loss: 0.2529 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.88 - ETA: 0s - loss: 0.2539 - accuracy: 0.89 - ETA: 0s - loss: 0.2522 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2573 - accuracy: 0.8932 - val_loss: 1.1590 - val_accuracy: 0.6881\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 1.00 - ETA: 0s - loss: 0.2209 - accuracy: 0.91 - ETA: 0s - loss: 0.2357 - accuracy: 0.90 - ETA: 0s - loss: 0.2268 - accuracy: 0.90 - ETA: 0s - loss: 0.2516 - accuracy: 0.90 - ETA: 0s - loss: 0.2614 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3049 - accuracy: 0.8966 - val_loss: 0.8066 - val_accuracy: 0.6806\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.75 - ETA: 0s - loss: 0.4838 - accuracy: 0.79 - ETA: 0s - loss: 0.4699 - accuracy: 0.82 - ETA: 0s - loss: 0.4763 - accuracy: 0.82 - ETA: 0s - loss: 0.4435 - accuracy: 0.82 - ETA: 0s - loss: 0.4895 - accuracy: 0.82 - ETA: 0s - loss: 0.4994 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4984 - accuracy: 0.8182 - val_loss: 0.9191 - val_accuracy: 0.6373\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3768 - accuracy: 0.87 - ETA: 0s - loss: 0.3387 - accuracy: 0.87 - ETA: 0s - loss: 0.4505 - accuracy: 0.86 - ETA: 0s - loss: 0.4459 - accuracy: 0.86 - ETA: 0s - loss: 0.4309 - accuracy: 0.86 - ETA: 0s - loss: 0.4211 - accuracy: 0.85 - ETA: 0s - loss: 0.4496 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4492 - accuracy: 0.8410 - val_loss: 0.7489 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3660 - accuracy: 0.84 - ETA: 0s - loss: 0.3685 - accuracy: 0.82 - ETA: 0s - loss: 0.3813 - accuracy: 0.84 - ETA: 0s - loss: 0.3499 - accuracy: 0.86 - ETA: 0s - loss: 0.3554 - accuracy: 0.87 - ETA: 0s - loss: 0.3597 - accuracy: 0.86 - ETA: 0s - loss: 0.3499 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3480 - accuracy: 0.8712 - val_loss: 1.9484 - val_accuracy: 0.6940\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.84 - ETA: 0s - loss: 0.3043 - accuracy: 0.85 - ETA: 0s - loss: 0.2815 - accuracy: 0.87 - ETA: 0s - loss: 0.2746 - accuracy: 0.87 - ETA: 0s - loss: 0.2992 - accuracy: 0.88 - ETA: 0s - loss: 0.3604 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8690 - val_loss: 0.8772 - val_accuracy: 0.6970\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.90 - ETA: 0s - loss: 0.2851 - accuracy: 0.88 - ETA: 0s - loss: 0.2788 - accuracy: 0.89 - ETA: 0s - loss: 0.2942 - accuracy: 0.89 - ETA: 0s - loss: 0.3206 - accuracy: 0.89 - ETA: 0s - loss: 0.3178 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8891 - val_loss: 0.9271 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.90 - ETA: 0s - loss: 0.2763 - accuracy: 0.87 - ETA: 0s - loss: 0.2978 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.89 - ETA: 0s - loss: 0.3092 - accuracy: 0.89 - ETA: 0s - loss: 0.3646 - accuracy: 0.87 - ETA: 0s - loss: 0.3738 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3738 - accuracy: 0.8761 - val_loss: 1.3245 - val_accuracy: 0.7358\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.93 - ETA: 0s - loss: 0.4096 - accuracy: 0.88 - ETA: 0s - loss: 0.3935 - accuracy: 0.87 - ETA: 0s - loss: 0.3980 - accuracy: 0.86 - ETA: 0s - loss: 0.4823 - accuracy: 0.85 - ETA: 0s - loss: 0.4844 - accuracy: 0.84 - ETA: 0s - loss: 0.4685 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4651 - accuracy: 0.8451 - val_loss: 0.7928 - val_accuracy: 0.7179\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.87 - ETA: 0s - loss: 0.3259 - accuracy: 0.88 - ETA: 0s - loss: 0.3840 - accuracy: 0.84 - ETA: 0s - loss: 0.4002 - accuracy: 0.83 - ETA: 0s - loss: 0.3997 - accuracy: 0.83 - ETA: 0s - loss: 0.3991 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3855 - accuracy: 0.8429 - val_loss: 1.0404 - val_accuracy: 0.6507\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.81 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2832 - accuracy: 0.88 - ETA: 0s - loss: 0.3121 - accuracy: 0.88 - ETA: 0s - loss: 0.3069 - accuracy: 0.88 - ETA: 0s - loss: 0.3065 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3093 - accuracy: 0.8858 - val_loss: 0.9844 - val_accuracy: 0.7284\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.96 - ETA: 0s - loss: 0.2899 - accuracy: 0.91 - ETA: 0s - loss: 0.2596 - accuracy: 0.92 - ETA: 0s - loss: 0.2743 - accuracy: 0.92 - ETA: 0s - loss: 0.2951 - accuracy: 0.90 - ETA: 0s - loss: 0.2901 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2918 - accuracy: 0.9026 - val_loss: 1.4625 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.93 - ETA: 0s - loss: 0.2447 - accuracy: 0.91 - ETA: 0s - loss: 0.2678 - accuracy: 0.89 - ETA: 0s - loss: 0.2695 - accuracy: 0.89 - ETA: 0s - loss: 0.2744 - accuracy: 0.89 - ETA: 0s - loss: 0.2681 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2705 - accuracy: 0.8985 - val_loss: 1.6028 - val_accuracy: 0.6716\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.81 - ETA: 0s - loss: 0.2764 - accuracy: 0.89 - ETA: 0s - loss: 0.2412 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.92 - ETA: 0s - loss: 0.2984 - accuracy: 0.91 - ETA: 0s - loss: 0.3112 - accuracy: 0.90 - ETA: 0s - loss: 0.3412 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3412 - accuracy: 0.8962 - val_loss: 3.4583 - val_accuracy: 0.7090\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.96 - ETA: 0s - loss: 0.2377 - accuracy: 0.93 - ETA: 0s - loss: 0.2630 - accuracy: 0.91 - ETA: 0s - loss: 0.3379 - accuracy: 0.90 - ETA: 0s - loss: 0.3739 - accuracy: 0.88 - ETA: 0s - loss: 0.4031 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4238 - accuracy: 0.8757 - val_loss: 1.5464 - val_accuracy: 0.7075\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.84 - ETA: 0s - loss: 0.4437 - accuracy: 0.85 - ETA: 0s - loss: 0.3930 - accuracy: 0.87 - ETA: 0s - loss: 0.3909 - accuracy: 0.87 - ETA: 0s - loss: 0.3642 - accuracy: 0.88 - ETA: 0s - loss: 0.4529 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4409 - accuracy: 0.8876 - val_loss: 1.1777 - val_accuracy: 0.7299\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.81 - ETA: 0s - loss: 0.3478 - accuracy: 0.89 - ETA: 0s - loss: 0.3435 - accuracy: 0.89 - ETA: 0s - loss: 0.3460 - accuracy: 0.89 - ETA: 0s - loss: 0.3284 - accuracy: 0.90 - ETA: 0s - loss: 0.3204 - accuracy: 0.91 - 0s 4ms/step - loss: 0.3613 - accuracy: 0.9104 - val_loss: 2.7311 - val_accuracy: 0.6866\n",
      "Epoch 28/50\n",
      "71/84 [========================>.....] - ETA: 0s - loss: 0.2002 - accuracy: 0.90 - ETA: 0s - loss: 0.3165 - accuracy: 0.90 - ETA: 0s - loss: 0.3210 - accuracy: 0.90 - ETA: 0s - loss: 0.3128 - accuracy: 0.90 - ETA: 0s - loss: 0.3094 - accuracy: 0.90 - ETA: 0s - loss: 0.3597 - accuracy: 0.8996Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8977 - val_loss: 1.0975 - val_accuracy: 0.6896\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8786 - accuracy: 0.53 - ETA: 0s - loss: 1.7392 - accuracy: 0.62 - ETA: 0s - loss: 1.3314 - accuracy: 0.65 - ETA: 0s - loss: 1.1195 - accuracy: 0.65 - ETA: 0s - loss: 1.0330 - accuracy: 0.66 - ETA: 0s - loss: 0.9478 - accuracy: 0.67 - 0s 5ms/step - loss: 0.9108 - accuracy: 0.6820 - val_loss: 0.5942 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7280 - accuracy: 0.68 - ETA: 0s - loss: 0.5830 - accuracy: 0.70 - ETA: 0s - loss: 0.5696 - accuracy: 0.73 - ETA: 0s - loss: 0.5713 - accuracy: 0.72 - ETA: 0s - loss: 0.5581 - accuracy: 0.73 - ETA: 0s - loss: 0.5447 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5490 - accuracy: 0.7507 - val_loss: 0.6591 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.80 - ETA: 0s - loss: 0.4756 - accuracy: 0.80 - ETA: 0s - loss: 0.4880 - accuracy: 0.79 - ETA: 0s - loss: 0.4728 - accuracy: 0.79 - ETA: 0s - loss: 0.4759 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4781 - accuracy: 0.7962 - val_loss: 0.7550 - val_accuracy: 0.6493\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.81 - ETA: 0s - loss: 0.3844 - accuracy: 0.84 - ETA: 0s - loss: 0.3751 - accuracy: 0.84 - ETA: 0s - loss: 0.4005 - accuracy: 0.83 - ETA: 0s - loss: 0.4154 - accuracy: 0.82 - ETA: 0s - loss: 0.4227 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4240 - accuracy: 0.8249 - val_loss: 0.6633 - val_accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.81 - ETA: 0s - loss: 0.3929 - accuracy: 0.83 - ETA: 0s - loss: 0.3723 - accuracy: 0.83 - ETA: 0s - loss: 0.3630 - accuracy: 0.83 - ETA: 0s - loss: 0.3727 - accuracy: 0.84 - ETA: 0s - loss: 0.3750 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3743 - accuracy: 0.8395 - val_loss: 0.7584 - val_accuracy: 0.6851\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.84 - ETA: 0s - loss: 0.3316 - accuracy: 0.86 - ETA: 0s - loss: 0.3408 - accuracy: 0.85 - ETA: 0s - loss: 0.3370 - accuracy: 0.85 - ETA: 0s - loss: 0.3440 - accuracy: 0.85 - ETA: 0s - loss: 0.3537 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3506 - accuracy: 0.8492 - val_loss: 0.7784 - val_accuracy: 0.7060\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.90 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - ETA: 0s - loss: 0.3230 - accuracy: 0.85 - ETA: 0s - loss: 0.3083 - accuracy: 0.86 - ETA: 0s - loss: 0.2962 - accuracy: 0.87 - ETA: 0s - loss: 0.3103 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8675 - val_loss: 0.6961 - val_accuracy: 0.7060\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.84 - ETA: 0s - loss: 0.2459 - accuracy: 0.91 - ETA: 0s - loss: 0.2933 - accuracy: 0.89 - ETA: 0s - loss: 0.3047 - accuracy: 0.88 - ETA: 0s - loss: 0.2967 - accuracy: 0.87 - ETA: 0s - loss: 0.3223 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3221 - accuracy: 0.8720 - val_loss: 0.7368 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.87 - ETA: 0s - loss: 0.2744 - accuracy: 0.87 - ETA: 0s - loss: 0.2753 - accuracy: 0.87 - ETA: 0s - loss: 0.2683 - accuracy: 0.87 - ETA: 0s - loss: 0.2730 - accuracy: 0.86 - ETA: 0s - loss: 0.2792 - accuracy: 0.86 - ETA: 0s - loss: 0.2850 - accuracy: 0.87 - 0s 4ms/step - loss: 0.2833 - accuracy: 0.8716 - val_loss: 0.7477 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.87 - ETA: 0s - loss: 0.2200 - accuracy: 0.92 - ETA: 0s - loss: 0.2700 - accuracy: 0.90 - ETA: 0s - loss: 0.2811 - accuracy: 0.90 - ETA: 0s - loss: 0.2812 - accuracy: 0.89 - ETA: 0s - loss: 0.2759 - accuracy: 0.89 - ETA: 0s - loss: 0.2740 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2737 - accuracy: 0.8936 - val_loss: 0.9924 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.96 - ETA: 0s - loss: 0.1743 - accuracy: 0.93 - ETA: 0s - loss: 0.2470 - accuracy: 0.92 - ETA: 0s - loss: 0.2621 - accuracy: 0.90 - ETA: 0s - loss: 0.2716 - accuracy: 0.90 - ETA: 0s - loss: 0.2721 - accuracy: 0.90 - ETA: 0s - loss: 0.2706 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2718 - accuracy: 0.8955 - val_loss: 0.9659 - val_accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.93 - ETA: 0s - loss: 0.1845 - accuracy: 0.91 - ETA: 0s - loss: 0.1977 - accuracy: 0.91 - ETA: 0s - loss: 0.1894 - accuracy: 0.91 - ETA: 0s - loss: 0.1971 - accuracy: 0.91 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2344 - accuracy: 0.9022 - val_loss: 0.8253 - val_accuracy: 0.6701\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.90 - ETA: 0s - loss: 0.2476 - accuracy: 0.89 - ETA: 0s - loss: 0.2891 - accuracy: 0.89 - ETA: 0s - loss: 0.3064 - accuracy: 0.87 - ETA: 0s - loss: 0.3037 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - 0s 4ms/step - loss: 0.2927 - accuracy: 0.8839 - val_loss: 1.5590 - val_accuracy: 0.6731\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.81 - ETA: 0s - loss: 0.2720 - accuracy: 0.89 - ETA: 0s - loss: 0.2574 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2589 - accuracy: 0.90 - ETA: 0s - loss: 0.2588 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2574 - accuracy: 0.9063 - val_loss: 0.9746 - val_accuracy: 0.7164\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.93 - ETA: 0s - loss: 0.2150 - accuracy: 0.92 - ETA: 0s - loss: 0.2158 - accuracy: 0.92 - ETA: 0s - loss: 0.2068 - accuracy: 0.92 - ETA: 0s - loss: 0.1945 - accuracy: 0.92 - ETA: 0s - loss: 0.1965 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2050 - accuracy: 0.9231 - val_loss: 1.2158 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.87 - ETA: 0s - loss: 0.1884 - accuracy: 0.93 - ETA: 0s - loss: 0.2207 - accuracy: 0.91 - ETA: 0s - loss: 0.2270 - accuracy: 0.91 - ETA: 0s - loss: 0.2235 - accuracy: 0.91 - ETA: 0s - loss: 0.2245 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9130 - val_loss: 1.2156 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.87 - ETA: 0s - loss: 0.2328 - accuracy: 0.90 - ETA: 0s - loss: 0.3375 - accuracy: 0.87 - ETA: 0s - loss: 0.3548 - accuracy: 0.86 - ETA: 0s - loss: 0.3514 - accuracy: 0.86 - ETA: 0s - loss: 0.3526 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3534 - accuracy: 0.8727 - val_loss: 0.6734 - val_accuracy: 0.7134\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.93 - ETA: 0s - loss: 0.2921 - accuracy: 0.90 - ETA: 0s - loss: 0.2978 - accuracy: 0.91 - ETA: 0s - loss: 0.3647 - accuracy: 0.90 - ETA: 0s - loss: 0.3337 - accuracy: 0.91 - ETA: 0s - loss: 0.3727 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3667 - accuracy: 0.8970 - val_loss: 1.1952 - val_accuracy: 0.6925\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.87 - ETA: 0s - loss: 0.2292 - accuracy: 0.90 - ETA: 0s - loss: 0.2309 - accuracy: 0.90 - ETA: 0s - loss: 0.2612 - accuracy: 0.89 - ETA: 0s - loss: 0.2736 - accuracy: 0.88 - ETA: 0s - loss: 0.2649 - accuracy: 0.89 - ETA: 0s - loss: 0.2617 - accuracy: 0.89 - 0s 4ms/step - loss: 0.2696 - accuracy: 0.8959 - val_loss: 1.5015 - val_accuracy: 0.7119\n",
      "Epoch 20/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.1438 - accuracy: 1.00 - ETA: 0s - loss: 0.2121 - accuracy: 0.93 - ETA: 0s - loss: 0.2083 - accuracy: 0.93 - ETA: 0s - loss: 0.1762 - accuracy: 0.94 - ETA: 0s - loss: 0.2020 - accuracy: 0.93 - ETA: 0s - loss: 0.2106 - accuracy: 0.93 - ETA: 0s - loss: 0.2362 - accuracy: 0.9236Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2348 - accuracy: 0.9227 - val_loss: 1.1134 - val_accuracy: 0.7209\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 525c08456e7dd9624b9a9fe564b80b1a</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7243781089782715</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.22107639659872136</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 430</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 70</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8134 - accuracy: 0.65 - ETA: 0s - loss: 2.7050 - accuracy: 0.58 - ETA: 0s - loss: 1.7505 - accuracy: 0.66 - ETA: 0s - loss: 1.3993 - accuracy: 0.67 - ETA: 0s - loss: 1.2190 - accuracy: 0.68 - ETA: 0s - loss: 1.1000 - accuracy: 0.68 - ETA: 0s - loss: 1.0070 - accuracy: 0.68 - 1s 6ms/step - loss: 0.9739 - accuracy: 0.6932 - val_loss: 0.5985 - val_accuracy: 0.7060\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6761 - accuracy: 0.65 - ETA: 0s - loss: 0.5716 - accuracy: 0.73 - ETA: 0s - loss: 0.5495 - accuracy: 0.74 - ETA: 0s - loss: 0.5513 - accuracy: 0.74 - ETA: 0s - loss: 0.5493 - accuracy: 0.74 - ETA: 0s - loss: 0.5377 - accuracy: 0.75 - ETA: 0s - loss: 0.5363 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5363 - accuracy: 0.7559 - val_loss: 0.5736 - val_accuracy: 0.7134\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.75 - ETA: 0s - loss: 0.4707 - accuracy: 0.79 - ETA: 0s - loss: 0.4879 - accuracy: 0.77 - ETA: 0s - loss: 0.4796 - accuracy: 0.78 - ETA: 0s - loss: 0.4631 - accuracy: 0.79 - ETA: 0s - loss: 0.4582 - accuracy: 0.79 - ETA: 0s - loss: 0.4743 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4715 - accuracy: 0.8033 - val_loss: 0.7837 - val_accuracy: 0.6955\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.84 - ETA: 0s - loss: 0.4513 - accuracy: 0.82 - ETA: 0s - loss: 0.4209 - accuracy: 0.82 - ETA: 0s - loss: 0.4465 - accuracy: 0.81 - ETA: 0s - loss: 0.4382 - accuracy: 0.81 - ETA: 0s - loss: 0.4444 - accuracy: 0.80 - ETA: 0s - loss: 0.4471 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4460 - accuracy: 0.8040 - val_loss: 0.5926 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.78 - ETA: 0s - loss: 0.3899 - accuracy: 0.85 - ETA: 0s - loss: 0.4117 - accuracy: 0.85 - ETA: 0s - loss: 0.4166 - accuracy: 0.84 - ETA: 0s - loss: 0.4297 - accuracy: 0.83 - ETA: 0s - loss: 0.4246 - accuracy: 0.83 - ETA: 0s - loss: 0.4470 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4531 - accuracy: 0.8294 - val_loss: 1.4258 - val_accuracy: 0.6119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6593 - accuracy: 0.75 - ETA: 0s - loss: 0.5987 - accuracy: 0.74 - ETA: 0s - loss: 0.6028 - accuracy: 0.75 - ETA: 0s - loss: 0.6318 - accuracy: 0.76 - ETA: 0s - loss: 0.6291 - accuracy: 0.77 - ETA: 0s - loss: 0.6415 - accuracy: 0.76 - ETA: 0s - loss: 0.6355 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6374 - accuracy: 0.7667 - val_loss: 0.5994 - val_accuracy: 0.7463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5514 - accuracy: 0.81 - ETA: 0s - loss: 0.5674 - accuracy: 0.79 - ETA: 0s - loss: 0.5707 - accuracy: 0.79 - ETA: 0s - loss: 0.6169 - accuracy: 0.78 - ETA: 0s - loss: 0.6358 - accuracy: 0.78 - ETA: 0s - loss: 0.6324 - accuracy: 0.77 - ETA: 0s - loss: 0.6233 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6221 - accuracy: 0.7809 - val_loss: 0.6315 - val_accuracy: 0.7403\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.87 - ETA: 0s - loss: 0.6690 - accuracy: 0.77 - ETA: 0s - loss: 0.6215 - accuracy: 0.78 - ETA: 0s - loss: 0.5876 - accuracy: 0.80 - ETA: 0s - loss: 0.6193 - accuracy: 0.79 - ETA: 0s - loss: 0.6205 - accuracy: 0.79 - ETA: 0s - loss: 0.6144 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6140 - accuracy: 0.7943 - val_loss: 0.5998 - val_accuracy: 0.7373\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.71 - ETA: 0s - loss: 0.5619 - accuracy: 0.79 - ETA: 0s - loss: 0.5602 - accuracy: 0.79 - ETA: 0s - loss: 0.5502 - accuracy: 0.80 - ETA: 0s - loss: 0.5736 - accuracy: 0.80 - ETA: 0s - loss: 0.5646 - accuracy: 0.80 - ETA: 0s - loss: 0.5766 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5766 - accuracy: 0.8066 - val_loss: 0.7044 - val_accuracy: 0.7552\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.84 - ETA: 0s - loss: 0.5498 - accuracy: 0.81 - ETA: 0s - loss: 0.5352 - accuracy: 0.81 - ETA: 0s - loss: 0.5286 - accuracy: 0.81 - ETA: 0s - loss: 0.5245 - accuracy: 0.81 - ETA: 0s - loss: 0.5223 - accuracy: 0.82 - ETA: 0s - loss: 0.5149 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5180 - accuracy: 0.8268 - val_loss: 0.7109 - val_accuracy: 0.7075\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.87 - ETA: 0s - loss: 0.3966 - accuracy: 0.89 - ETA: 0s - loss: 0.4301 - accuracy: 0.87 - ETA: 0s - loss: 0.4688 - accuracy: 0.85 - ETA: 0s - loss: 0.4854 - accuracy: 0.84 - ETA: 0s - loss: 0.4947 - accuracy: 0.83 - ETA: 0s - loss: 0.4882 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4881 - accuracy: 0.8395 - val_loss: 0.7754 - val_accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.93 - ETA: 0s - loss: 0.5142 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.84 - ETA: 0s - loss: 0.4786 - accuracy: 0.84 - ETA: 0s - loss: 0.4677 - accuracy: 0.84 - ETA: 0s - loss: 0.4586 - accuracy: 0.85 - ETA: 0s - loss: 0.4629 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4597 - accuracy: 0.8511 - val_loss: 0.6329 - val_accuracy: 0.7478\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.87 - ETA: 0s - loss: 0.4153 - accuracy: 0.87 - ETA: 0s - loss: 0.4308 - accuracy: 0.86 - ETA: 0s - loss: 0.4863 - accuracy: 0.86 - ETA: 0s - loss: 0.4812 - accuracy: 0.85 - ETA: 0s - loss: 0.4681 - accuracy: 0.85 - ETA: 0s - loss: 0.4747 - accuracy: 0.85 - ETA: 0s - loss: 0.4751 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4751 - accuracy: 0.8537 - val_loss: 0.8777 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.81 - ETA: 0s - loss: 0.5506 - accuracy: 0.83 - ETA: 0s - loss: 0.5171 - accuracy: 0.84 - ETA: 0s - loss: 0.5408 - accuracy: 0.85 - ETA: 0s - loss: 0.5480 - accuracy: 0.84 - ETA: 0s - loss: 0.5543 - accuracy: 0.84 - ETA: 0s - loss: 0.5719 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5675 - accuracy: 0.8313 - val_loss: 0.7655 - val_accuracy: 0.7284\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.78 - ETA: 0s - loss: 0.5282 - accuracy: 0.81 - ETA: 0s - loss: 0.5026 - accuracy: 0.83 - ETA: 0s - loss: 0.5147 - accuracy: 0.82 - ETA: 0s - loss: 0.5204 - accuracy: 0.81 - ETA: 0s - loss: 0.5121 - accuracy: 0.81 - ETA: 0s - loss: 0.5103 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5061 - accuracy: 0.8253 - val_loss: 0.6220 - val_accuracy: 0.7433\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.87 - ETA: 0s - loss: 0.4321 - accuracy: 0.85 - ETA: 0s - loss: 0.4545 - accuracy: 0.85 - ETA: 0s - loss: 0.4609 - accuracy: 0.84 - ETA: 0s - loss: 0.4627 - accuracy: 0.85 - ETA: 0s - loss: 0.5300 - accuracy: 0.84 - ETA: 0s - loss: 0.5206 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5180 - accuracy: 0.8522 - val_loss: 1.8728 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.6975 - accuracy: 0.93 - ETA: 0s - loss: 0.7504 - accuracy: 0.84 - ETA: 0s - loss: 0.5947 - accuracy: 0.85 - ETA: 0s - loss: 0.5795 - accuracy: 0.85 - ETA: 0s - loss: 0.5719 - accuracy: 0.84 - ETA: 0s - loss: 0.5687 - accuracy: 0.84 - ETA: 0s - loss: 0.5665 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5627 - accuracy: 0.8346 - val_loss: 0.9065 - val_accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.84 - ETA: 0s - loss: 0.4834 - accuracy: 0.84 - ETA: 0s - loss: 0.4748 - accuracy: 0.85 - ETA: 0s - loss: 0.4863 - accuracy: 0.84 - ETA: 0s - loss: 0.4858 - accuracy: 0.84 - ETA: 0s - loss: 0.4937 - accuracy: 0.84 - ETA: 0s - loss: 0.4925 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4987 - accuracy: 0.8455 - val_loss: 0.8319 - val_accuracy: 0.7537\n",
      "Epoch 19/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.5116 - accuracy: 0.84 - ETA: 0s - loss: 0.4724 - accuracy: 0.85 - ETA: 0s - loss: 0.4570 - accuracy: 0.86 - ETA: 0s - loss: 0.4736 - accuracy: 0.85 - ETA: 0s - loss: 0.4785 - accuracy: 0.84 - ETA: 0s - loss: 0.4887 - accuracy: 0.84 - ETA: 0s - loss: 0.4899 - accuracy: 0.8450Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4899 - accuracy: 0.8458 - val_loss: 0.9534 - val_accuracy: 0.7448\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.75 - ETA: 0s - loss: 3.6541 - accuracy: 0.62 - ETA: 0s - loss: 2.1359 - accuracy: 0.66 - ETA: 0s - loss: 1.6772 - accuracy: 0.67 - ETA: 0s - loss: 1.4328 - accuracy: 0.68 - ETA: 0s - loss: 1.2578 - accuracy: 0.69 - ETA: 0s - loss: 1.1354 - accuracy: 0.69 - ETA: 0s - loss: 1.0588 - accuracy: 0.70 - 1s 6ms/step - loss: 1.0502 - accuracy: 0.7014 - val_loss: 0.5488 - val_accuracy: 0.7104\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.71 - ETA: 0s - loss: 0.6095 - accuracy: 0.72 - ETA: 0s - loss: 0.5926 - accuracy: 0.74 - ETA: 0s - loss: 0.5623 - accuracy: 0.76 - ETA: 0s - loss: 0.5632 - accuracy: 0.76 - ETA: 0s - loss: 0.5572 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5531 - accuracy: 0.7682 - val_loss: 0.6959 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 0.87 - ETA: 0s - loss: 0.4721 - accuracy: 0.80 - ETA: 0s - loss: 0.4648 - accuracy: 0.79 - ETA: 0s - loss: 0.4968 - accuracy: 0.79 - ETA: 0s - loss: 0.4775 - accuracy: 0.80 - ETA: 0s - loss: 0.4926 - accuracy: 0.79 - ETA: 0s - loss: 0.5006 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5029 - accuracy: 0.7954 - val_loss: 0.5900 - val_accuracy: 0.7388\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.87 - ETA: 0s - loss: 0.4758 - accuracy: 0.81 - ETA: 0s - loss: 0.4771 - accuracy: 0.81 - ETA: 0s - loss: 0.5274 - accuracy: 0.81 - ETA: 0s - loss: 0.5132 - accuracy: 0.81 - ETA: 0s - loss: 0.4967 - accuracy: 0.81 - ETA: 0s - loss: 0.4960 - accuracy: 0.81 - ETA: 0s - loss: 0.4959 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4959 - accuracy: 0.8126 - val_loss: 0.6971 - val_accuracy: 0.7134\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4137 - accuracy: 0.84 - ETA: 0s - loss: 0.4544 - accuracy: 0.81 - ETA: 0s - loss: 0.4508 - accuracy: 0.81 - ETA: 0s - loss: 0.4550 - accuracy: 0.81 - ETA: 0s - loss: 0.4356 - accuracy: 0.82 - ETA: 0s - loss: 0.4368 - accuracy: 0.82 - ETA: 0s - loss: 0.4471 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4467 - accuracy: 0.8171 - val_loss: 0.6158 - val_accuracy: 0.7239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.90 - ETA: 0s - loss: 0.3413 - accuracy: 0.86 - ETA: 0s - loss: 0.3955 - accuracy: 0.83 - ETA: 0s - loss: 0.3940 - accuracy: 0.83 - ETA: 0s - loss: 0.4041 - accuracy: 0.83 - ETA: 0s - loss: 0.4177 - accuracy: 0.83 - ETA: 0s - loss: 0.4153 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4110 - accuracy: 0.8369 - val_loss: 0.9931 - val_accuracy: 0.7104\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.93 - ETA: 0s - loss: 0.4321 - accuracy: 0.85 - ETA: 0s - loss: 0.4626 - accuracy: 0.82 - ETA: 0s - loss: 0.4580 - accuracy: 0.82 - ETA: 0s - loss: 0.4393 - accuracy: 0.82 - ETA: 0s - loss: 0.4644 - accuracy: 0.82 - ETA: 0s - loss: 0.4674 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4653 - accuracy: 0.8216 - val_loss: 0.7332 - val_accuracy: 0.6881\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.90 - ETA: 0s - loss: 0.3598 - accuracy: 0.86 - ETA: 0s - loss: 0.4737 - accuracy: 0.86 - ETA: 0s - loss: 0.4966 - accuracy: 0.84 - ETA: 0s - loss: 0.5181 - accuracy: 0.84 - ETA: 0s - loss: 0.5263 - accuracy: 0.83 - ETA: 0s - loss: 0.5265 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5382 - accuracy: 0.8175 - val_loss: 1.0868 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.93 - ETA: 0s - loss: 0.3894 - accuracy: 0.84 - ETA: 0s - loss: 0.4247 - accuracy: 0.83 - ETA: 0s - loss: 0.4864 - accuracy: 0.81 - ETA: 0s - loss: 0.5276 - accuracy: 0.81 - ETA: 0s - loss: 0.5219 - accuracy: 0.81 - ETA: 0s - loss: 0.5282 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5461 - accuracy: 0.8070 - val_loss: 0.6582 - val_accuracy: 0.7507\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.78 - ETA: 0s - loss: 0.5705 - accuracy: 0.78 - ETA: 0s - loss: 0.5208 - accuracy: 0.79 - ETA: 0s - loss: 0.4853 - accuracy: 0.80 - ETA: 0s - loss: 0.4653 - accuracy: 0.80 - ETA: 0s - loss: 0.4405 - accuracy: 0.81 - ETA: 0s - loss: 0.4304 - accuracy: 0.82 - ETA: 0s - loss: 0.4643 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4840 - accuracy: 0.8279 - val_loss: 0.7456 - val_accuracy: 0.6821\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.78 - ETA: 0s - loss: 0.3862 - accuracy: 0.87 - ETA: 0s - loss: 0.4531 - accuracy: 0.85 - ETA: 0s - loss: 0.4786 - accuracy: 0.83 - ETA: 0s - loss: 0.4895 - accuracy: 0.82 - ETA: 0s - loss: 0.4795 - accuracy: 0.82 - ETA: 0s - loss: 0.4856 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4811 - accuracy: 0.8290 - val_loss: 0.8089 - val_accuracy: 0.7194\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5489 - accuracy: 0.75 - ETA: 0s - loss: 0.6282 - accuracy: 0.84 - ETA: 0s - loss: 0.5906 - accuracy: 0.82 - ETA: 0s - loss: 0.5735 - accuracy: 0.81 - ETA: 0s - loss: 0.5455 - accuracy: 0.82 - ETA: 0s - loss: 0.5398 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5067 - accuracy: 0.8283 - val_loss: 0.8141 - val_accuracy: 0.7015\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.93 - ETA: 0s - loss: 0.4327 - accuracy: 0.84 - ETA: 0s - loss: 0.4034 - accuracy: 0.85 - ETA: 0s - loss: 0.3893 - accuracy: 0.85 - ETA: 0s - loss: 0.3821 - accuracy: 0.86 - ETA: 0s - loss: 0.3786 - accuracy: 0.86 - ETA: 0s - loss: 0.3758 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3696 - accuracy: 0.8626 - val_loss: 1.4036 - val_accuracy: 0.7284\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.84 - ETA: 0s - loss: 0.3122 - accuracy: 0.89 - ETA: 0s - loss: 0.3852 - accuracy: 0.85 - ETA: 0s - loss: 0.4140 - accuracy: 0.84 - ETA: 0s - loss: 0.4258 - accuracy: 0.84 - ETA: 0s - loss: 0.4341 - accuracy: 0.84 - ETA: 0s - loss: 0.4494 - accuracy: 0.83 - ETA: 0s - loss: 0.4519 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4489 - accuracy: 0.8395 - val_loss: 1.6043 - val_accuracy: 0.6358\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.90 - ETA: 0s - loss: 0.5649 - accuracy: 0.82 - ETA: 0s - loss: 0.5025 - accuracy: 0.82 - ETA: 0s - loss: 0.4717 - accuracy: 0.83 - ETA: 0s - loss: 0.4642 - accuracy: 0.83 - ETA: 0s - loss: 0.4726 - accuracy: 0.83 - ETA: 0s - loss: 0.4797 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4820 - accuracy: 0.8339 - val_loss: 0.6236 - val_accuracy: 0.7403\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3849 - accuracy: 0.90 - ETA: 0s - loss: 0.4289 - accuracy: 0.86 - ETA: 0s - loss: 0.4588 - accuracy: 0.83 - ETA: 0s - loss: 0.4790 - accuracy: 0.82 - ETA: 0s - loss: 0.4757 - accuracy: 0.82 - ETA: 0s - loss: 0.4763 - accuracy: 0.82 - ETA: 0s - loss: 0.4884 - accuracy: 0.82 - ETA: 0s - loss: 0.4873 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4875 - accuracy: 0.8216 - val_loss: 0.6021 - val_accuracy: 0.7358\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.78 - ETA: 0s - loss: 0.5559 - accuracy: 0.81 - ETA: 0s - loss: 0.5275 - accuracy: 0.81 - ETA: 0s - loss: 0.4922 - accuracy: 0.82 - ETA: 0s - loss: 0.4787 - accuracy: 0.83 - ETA: 0s - loss: 0.4837 - accuracy: 0.82 - ETA: 0s - loss: 0.4900 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4951 - accuracy: 0.8264 - val_loss: 0.6820 - val_accuracy: 0.7418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.78 - ETA: 0s - loss: 0.6136 - accuracy: 0.82 - ETA: 0s - loss: 0.5932 - accuracy: 0.79 - ETA: 0s - loss: 0.5691 - accuracy: 0.81 - ETA: 0s - loss: 0.5902 - accuracy: 0.80 - ETA: 0s - loss: 0.6082 - accuracy: 0.78 - ETA: 0s - loss: 0.6127 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6078 - accuracy: 0.7764 - val_loss: 0.8452 - val_accuracy: 0.6925\n",
      "Epoch 19/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.5556 - accuracy: 0.68 - ETA: 0s - loss: 0.5959 - accuracy: 0.75 - ETA: 0s - loss: 0.5914 - accuracy: 0.74 - ETA: 0s - loss: 0.5760 - accuracy: 0.66 - ETA: 0s - loss: 0.5730 - accuracy: 0.64 - ETA: 0s - loss: 0.5860 - accuracy: 0.66 - ETA: 0s - loss: 0.6216 - accuracy: 0.6477Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6192 - accuracy: 0.6592 - val_loss: 0.8909 - val_accuracy: 0.7299\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8055 - accuracy: 0.68 - ETA: 0s - loss: 1.9220 - accuracy: 0.59 - ETA: 0s - loss: 1.3323 - accuracy: 0.63 - ETA: 0s - loss: 1.1093 - accuracy: 0.66 - ETA: 0s - loss: 0.9855 - accuracy: 0.67 - ETA: 0s - loss: 0.9223 - accuracy: 0.66 - ETA: 0s - loss: 0.8747 - accuracy: 0.67 - 1s 6ms/step - loss: 0.8374 - accuracy: 0.6838 - val_loss: 0.6461 - val_accuracy: 0.6537\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.78 - ETA: 0s - loss: 0.4993 - accuracy: 0.75 - ETA: 0s - loss: 0.5151 - accuracy: 0.74 - ETA: 0s - loss: 0.5194 - accuracy: 0.73 - ETA: 0s - loss: 0.5223 - accuracy: 0.74 - ETA: 0s - loss: 0.5311 - accuracy: 0.74 - ETA: 0s - loss: 0.5324 - accuracy: 0.74 - ETA: 0s - loss: 0.5455 - accuracy: 0.75 - 0s 5ms/step - loss: 0.5455 - accuracy: 0.7525 - val_loss: 0.6091 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.87 - ETA: 0s - loss: 0.5139 - accuracy: 0.80 - ETA: 0s - loss: 0.5214 - accuracy: 0.79 - ETA: 0s - loss: 0.5033 - accuracy: 0.80 - ETA: 0s - loss: 0.4960 - accuracy: 0.79 - ETA: 0s - loss: 0.4946 - accuracy: 0.79 - ETA: 0s - loss: 0.4950 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5028 - accuracy: 0.7925 - val_loss: 0.6415 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7228 - accuracy: 0.81 - ETA: 0s - loss: 0.5217 - accuracy: 0.79 - ETA: 0s - loss: 0.5244 - accuracy: 0.79 - ETA: 0s - loss: 0.5083 - accuracy: 0.79 - ETA: 0s - loss: 0.5470 - accuracy: 0.79 - ETA: 0s - loss: 0.5412 - accuracy: 0.79 - ETA: 0s - loss: 0.5497 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5434 - accuracy: 0.7921 - val_loss: 0.8142 - val_accuracy: 0.6836\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.87 - ETA: 0s - loss: 0.4901 - accuracy: 0.81 - ETA: 0s - loss: 0.4465 - accuracy: 0.82 - ETA: 0s - loss: 0.4248 - accuracy: 0.82 - ETA: 0s - loss: 0.4378 - accuracy: 0.81 - ETA: 0s - loss: 0.4347 - accuracy: 0.81 - ETA: 0s - loss: 0.4367 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4391 - accuracy: 0.8186 - val_loss: 0.9062 - val_accuracy: 0.6925\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5122 - accuracy: 0.71 - ETA: 0s - loss: 0.4742 - accuracy: 0.83 - ETA: 0s - loss: 0.4783 - accuracy: 0.83 - ETA: 0s - loss: 0.4736 - accuracy: 0.82 - ETA: 0s - loss: 0.4942 - accuracy: 0.82 - ETA: 0s - loss: 0.4930 - accuracy: 0.82 - ETA: 0s - loss: 0.5056 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5111 - accuracy: 0.8219 - val_loss: 0.6589 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.75 - ETA: 0s - loss: 0.5406 - accuracy: 0.79 - ETA: 0s - loss: 0.5420 - accuracy: 0.78 - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.5747 - accuracy: 0.78 - ETA: 0s - loss: 0.5381 - accuracy: 0.80 - ETA: 0s - loss: 0.5939 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5933 - accuracy: 0.7966 - val_loss: 0.9618 - val_accuracy: 0.6955\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.78 - ETA: 0s - loss: 0.5906 - accuracy: 0.77 - ETA: 0s - loss: 0.5419 - accuracy: 0.80 - ETA: 0s - loss: 0.5572 - accuracy: 0.80 - ETA: 0s - loss: 0.5737 - accuracy: 0.80 - ETA: 0s - loss: 0.5718 - accuracy: 0.80 - ETA: 0s - loss: 0.5861 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5823 - accuracy: 0.8066 - val_loss: 0.6655 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.81 - ETA: 0s - loss: 0.5432 - accuracy: 0.82 - ETA: 0s - loss: 0.5320 - accuracy: 0.83 - ETA: 0s - loss: 0.5521 - accuracy: 0.82 - ETA: 0s - loss: 0.5406 - accuracy: 0.82 - ETA: 0s - loss: 0.5412 - accuracy: 0.82 - ETA: 0s - loss: 0.5338 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5344 - accuracy: 0.8141 - val_loss: 0.6506 - val_accuracy: 0.7090\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.65 - ETA: 0s - loss: 0.5458 - accuracy: 0.80 - ETA: 0s - loss: 0.6230 - accuracy: 0.79 - ETA: 0s - loss: 0.6443 - accuracy: 0.78 - ETA: 0s - loss: 0.6468 - accuracy: 0.79 - ETA: 0s - loss: 0.6266 - accuracy: 0.79 - ETA: 0s - loss: 0.6170 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6104 - accuracy: 0.8003 - val_loss: 0.6913 - val_accuracy: 0.7328\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.93 - ETA: 0s - loss: 0.4956 - accuracy: 0.83 - ETA: 0s - loss: 0.4744 - accuracy: 0.85 - ETA: 0s - loss: 0.4871 - accuracy: 0.84 - ETA: 0s - loss: 0.5505 - accuracy: 0.83 - ETA: 0s - loss: 0.5723 - accuracy: 0.82 - ETA: 0s - loss: 0.5916 - accuracy: 0.82 - ETA: 0s - loss: 0.5875 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5875 - accuracy: 0.8141 - val_loss: 0.6313 - val_accuracy: 0.7343\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.78 - ETA: 0s - loss: 0.5224 - accuracy: 0.78 - ETA: 0s - loss: 0.5152 - accuracy: 0.79 - ETA: 0s - loss: 0.5003 - accuracy: 0.80 - ETA: 0s - loss: 0.5017 - accuracy: 0.80 - ETA: 0s - loss: 0.4988 - accuracy: 0.81 - ETA: 0s - loss: 0.4909 - accuracy: 0.82 - ETA: 0s - loss: 0.4907 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4907 - accuracy: 0.8205 - val_loss: 0.6406 - val_accuracy: 0.7522\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.81 - ETA: 0s - loss: 0.3722 - accuracy: 0.87 - ETA: 0s - loss: 0.4089 - accuracy: 0.86 - ETA: 0s - loss: 0.4133 - accuracy: 0.86 - ETA: 0s - loss: 0.4452 - accuracy: 0.85 - ETA: 0s - loss: 0.4632 - accuracy: 0.84 - ETA: 0s - loss: 0.4778 - accuracy: 0.83 - ETA: 0s - loss: 0.4836 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4836 - accuracy: 0.8350 - val_loss: 0.6730 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.90 - ETA: 0s - loss: 0.4639 - accuracy: 0.83 - ETA: 0s - loss: 0.5003 - accuracy: 0.84 - ETA: 0s - loss: 0.4919 - accuracy: 0.84 - ETA: 0s - loss: 0.5126 - accuracy: 0.83 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.5110 - accuracy: 0.83 - ETA: 0s - loss: 0.5495 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5478 - accuracy: 0.8328 - val_loss: 0.6494 - val_accuracy: 0.7313\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.96 - ETA: 0s - loss: 0.6591 - accuracy: 0.79 - ETA: 0s - loss: 0.5684 - accuracy: 0.82 - ETA: 0s - loss: 0.5451 - accuracy: 0.83 - ETA: 0s - loss: 0.5386 - accuracy: 0.82 - ETA: 0s - loss: 0.5278 - accuracy: 0.83 - ETA: 0s - loss: 0.5312 - accuracy: 0.83 - ETA: 0s - loss: 0.5325 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5333 - accuracy: 0.8358 - val_loss: 0.8400 - val_accuracy: 0.7343\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.90 - ETA: 0s - loss: 0.4242 - accuracy: 0.86 - ETA: 0s - loss: 0.4673 - accuracy: 0.83 - ETA: 0s - loss: 0.4658 - accuracy: 0.84 - ETA: 0s - loss: 0.4538 - accuracy: 0.84 - ETA: 0s - loss: 0.4532 - accuracy: 0.84 - ETA: 0s - loss: 0.4427 - accuracy: 0.84 - ETA: 0s - loss: 0.4386 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4386 - accuracy: 0.8514 - val_loss: 1.0089 - val_accuracy: 0.6985\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.90 - ETA: 0s - loss: 0.4615 - accuracy: 0.84 - ETA: 0s - loss: 0.4706 - accuracy: 0.85 - ETA: 0s - loss: 0.4539 - accuracy: 0.86 - ETA: 0s - loss: 0.4452 - accuracy: 0.86 - ETA: 0s - loss: 0.4503 - accuracy: 0.86 - ETA: 0s - loss: 0.4474 - accuracy: 0.86 - ETA: 0s - loss: 0.4695 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4695 - accuracy: 0.8619 - val_loss: 0.6647 - val_accuracy: 0.7418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4467 - accuracy: 0.84 - ETA: 0s - loss: 0.4274 - accuracy: 0.88 - ETA: 0s - loss: 0.4334 - accuracy: 0.86 - ETA: 0s - loss: 0.4593 - accuracy: 0.86 - ETA: 0s - loss: 0.4392 - accuracy: 0.87 - ETA: 0s - loss: 0.4461 - accuracy: 0.87 - ETA: 0s - loss: 0.4451 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4324 - accuracy: 0.8712 - val_loss: 0.8070 - val_accuracy: 0.7045\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.90 - ETA: 0s - loss: 0.4330 - accuracy: 0.87 - ETA: 0s - loss: 0.4041 - accuracy: 0.87 - ETA: 0s - loss: 0.4173 - accuracy: 0.87 - ETA: 0s - loss: 0.4118 - accuracy: 0.87 - ETA: 0s - loss: 0.4137 - accuracy: 0.87 - ETA: 0s - loss: 0.4154 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4100 - accuracy: 0.8772 - val_loss: 0.8029 - val_accuracy: 0.7224\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.84 - ETA: 0s - loss: 0.3892 - accuracy: 0.87 - ETA: 0s - loss: 0.3856 - accuracy: 0.87 - ETA: 0s - loss: 0.3892 - accuracy: 0.88 - ETA: 0s - loss: 0.4014 - accuracy: 0.87 - ETA: 0s - loss: 0.4018 - accuracy: 0.87 - ETA: 0s - loss: 0.4078 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4352 - accuracy: 0.8768 - val_loss: 1.7400 - val_accuracy: 0.7090\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.81 - ETA: 0s - loss: 0.7357 - accuracy: 0.87 - ETA: 0s - loss: 0.7380 - accuracy: 0.85 - ETA: 0s - loss: 0.7249 - accuracy: 0.83 - ETA: 0s - loss: 0.6811 - accuracy: 0.83 - ETA: 0s - loss: 0.6882 - accuracy: 0.83 - ETA: 0s - loss: 0.6950 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6941 - accuracy: 0.8145 - val_loss: 0.5729 - val_accuracy: 0.7313\n",
      "Epoch 22/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7330 - accuracy: 0.71 - ETA: 0s - loss: 0.7257 - accuracy: 0.72 - ETA: 0s - loss: 0.8307 - accuracy: 0.78 - ETA: 0s - loss: 0.7269 - accuracy: 0.80 - ETA: 0s - loss: 0.7948 - accuracy: 0.80 - ETA: 0s - loss: 0.8411 - accuracy: 0.80 - ETA: 0s - loss: 0.7847 - accuracy: 0.8157Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7639 - accuracy: 0.8182 - val_loss: 0.9623 - val_accuracy: 0.7478\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a6472da7d2748e6cb194d6d9c3b532c2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7527363101641337</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.3413646476119552</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 470</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 220</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7857 - accuracy: 0.46 - ETA: 0s - loss: 2.1908 - accuracy: 0.59 - ETA: 0s - loss: 1.7426 - accuracy: 0.59 - ETA: 0s - loss: 1.5138 - accuracy: 0.61 - ETA: 0s - loss: 1.3192 - accuracy: 0.63 - ETA: 0s - loss: 1.1718 - accuracy: 0.64 - 0s 5ms/step - loss: 1.1564 - accuracy: 0.6487 - val_loss: 0.5659 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.75 - ETA: 0s - loss: 0.6205 - accuracy: 0.69 - ETA: 0s - loss: 0.6040 - accuracy: 0.70 - ETA: 0s - loss: 0.6102 - accuracy: 0.71 - ETA: 0s - loss: 0.6069 - accuracy: 0.71 - ETA: 0s - loss: 0.5896 - accuracy: 0.72 - 0s 4ms/step - loss: 0.5891 - accuracy: 0.7219 - val_loss: 0.6853 - val_accuracy: 0.6627\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.78 - ETA: 0s - loss: 0.5546 - accuracy: 0.77 - ETA: 0s - loss: 0.5447 - accuracy: 0.76 - ETA: 0s - loss: 0.5409 - accuracy: 0.75 - ETA: 0s - loss: 0.5289 - accuracy: 0.75 - ETA: 0s - loss: 0.5338 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7633 - val_loss: 0.5897 - val_accuracy: 0.6836\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.81 - ETA: 0s - loss: 0.4623 - accuracy: 0.79 - ETA: 0s - loss: 0.4445 - accuracy: 0.80 - ETA: 0s - loss: 0.4421 - accuracy: 0.81 - ETA: 0s - loss: 0.4519 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8048 - val_loss: 0.6694 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.78 - ETA: 0s - loss: 0.5415 - accuracy: 0.79 - ETA: 0s - loss: 0.4767 - accuracy: 0.81 - ETA: 0s - loss: 0.4595 - accuracy: 0.81 - ETA: 0s - loss: 0.4626 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4647 - accuracy: 0.8052 - val_loss: 0.6483 - val_accuracy: 0.6925\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.78 - ETA: 0s - loss: 0.4564 - accuracy: 0.81 - ETA: 0s - loss: 0.4867 - accuracy: 0.80 - ETA: 0s - loss: 0.4862 - accuracy: 0.79 - ETA: 0s - loss: 0.4857 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4893 - accuracy: 0.7936 - val_loss: 0.6793 - val_accuracy: 0.6791\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.75 - ETA: 0s - loss: 0.4593 - accuracy: 0.81 - ETA: 0s - loss: 0.4205 - accuracy: 0.82 - ETA: 0s - loss: 0.4322 - accuracy: 0.82 - ETA: 0s - loss: 0.4444 - accuracy: 0.82 - ETA: 0s - loss: 0.4463 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4449 - accuracy: 0.8219 - val_loss: 0.8614 - val_accuracy: 0.7060\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.93 - ETA: 0s - loss: 0.5213 - accuracy: 0.82 - ETA: 0s - loss: 0.4757 - accuracy: 0.82 - ETA: 0s - loss: 0.4822 - accuracy: 0.82 - ETA: 0s - loss: 0.4632 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5031 - accuracy: 0.8178 - val_loss: 0.6804 - val_accuracy: 0.6687\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.87 - ETA: 0s - loss: 0.4964 - accuracy: 0.83 - ETA: 0s - loss: 0.5000 - accuracy: 0.81 - ETA: 0s - loss: 0.6170 - accuracy: 0.79 - ETA: 0s - loss: 0.6030 - accuracy: 0.79 - ETA: 0s - loss: 0.5965 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5970 - accuracy: 0.7910 - val_loss: 0.6941 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.78 - ETA: 0s - loss: 0.4124 - accuracy: 0.84 - ETA: 0s - loss: 0.4976 - accuracy: 0.82 - ETA: 0s - loss: 0.5193 - accuracy: 0.81 - ETA: 0s - loss: 0.5354 - accuracy: 0.81 - ETA: 0s - loss: 0.6075 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6062 - accuracy: 0.7910 - val_loss: 0.7469 - val_accuracy: 0.6657\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.75 - ETA: 0s - loss: 0.5481 - accuracy: 0.80 - ETA: 0s - loss: 0.5276 - accuracy: 0.81 - ETA: 0s - loss: 0.4976 - accuracy: 0.82 - ETA: 0s - loss: 0.5228 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5160 - accuracy: 0.8108 - val_loss: 0.8289 - val_accuracy: 0.7030\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.84 - ETA: 0s - loss: 0.4291 - accuracy: 0.81 - ETA: 0s - loss: 0.5161 - accuracy: 0.82 - ETA: 0s - loss: 0.4797 - accuracy: 0.82 - ETA: 0s - loss: 0.4580 - accuracy: 0.82 - ETA: 0s - loss: 0.4676 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4672 - accuracy: 0.8205 - val_loss: 0.8060 - val_accuracy: 0.6687\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.87 - ETA: 0s - loss: 0.3865 - accuracy: 0.85 - ETA: 0s - loss: 0.3846 - accuracy: 0.84 - ETA: 0s - loss: 0.3948 - accuracy: 0.84 - ETA: 0s - loss: 0.4040 - accuracy: 0.84 - ETA: 0s - loss: 0.3881 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3926 - accuracy: 0.8499 - val_loss: 0.8840 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.93 - ETA: 0s - loss: 0.3155 - accuracy: 0.88 - ETA: 0s - loss: 0.3209 - accuracy: 0.87 - ETA: 0s - loss: 0.3395 - accuracy: 0.87 - ETA: 0s - loss: 0.3764 - accuracy: 0.86 - ETA: 0s - loss: 0.4063 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4046 - accuracy: 0.8615 - val_loss: 1.5446 - val_accuracy: 0.6522\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.90 - ETA: 0s - loss: 0.4362 - accuracy: 0.86 - ETA: 0s - loss: 0.3894 - accuracy: 0.86 - ETA: 0s - loss: 0.3520 - accuracy: 0.87 - ETA: 0s - loss: 0.3845 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3786 - accuracy: 0.8589 - val_loss: 1.1565 - val_accuracy: 0.6925\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.87 - ETA: 0s - loss: 0.4754 - accuracy: 0.82 - ETA: 0s - loss: 0.4109 - accuracy: 0.84 - ETA: 0s - loss: 0.3877 - accuracy: 0.85 - ETA: 0s - loss: 0.4736 - accuracy: 0.85 - ETA: 0s - loss: 0.4921 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4942 - accuracy: 0.8485 - val_loss: 1.6111 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4333 - accuracy: 0.84 - ETA: 0s - loss: 0.4497 - accuracy: 0.84 - ETA: 0s - loss: 0.4514 - accuracy: 0.84 - ETA: 0s - loss: 0.4325 - accuracy: 0.85 - ETA: 0s - loss: 0.4281 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4266 - accuracy: 0.8511 - val_loss: 1.2357 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.93 - ETA: 0s - loss: 0.4981 - accuracy: 0.86 - ETA: 0s - loss: 0.4673 - accuracy: 0.85 - ETA: 0s - loss: 0.4470 - accuracy: 0.86 - ETA: 0s - loss: 0.4340 - accuracy: 0.86 - ETA: 0s - loss: 0.4868 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4801 - accuracy: 0.8526 - val_loss: 3.4028 - val_accuracy: 0.6940\n",
      "Epoch 19/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.7620 - accuracy: 0.81 - ETA: 0s - loss: 0.4452 - accuracy: 0.83 - ETA: 0s - loss: 0.4188 - accuracy: 0.83 - ETA: 0s - loss: 0.4188 - accuracy: 0.85 - ETA: 0s - loss: 0.4074 - accuracy: 0.85 - ETA: 0s - loss: 0.4452 - accuracy: 0.8555Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.8541 - val_loss: 1.6282 - val_accuracy: 0.7343\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.50 - ETA: 0s - loss: 2.3801 - accuracy: 0.60 - ETA: 0s - loss: 1.5467 - accuracy: 0.65 - ETA: 0s - loss: 1.2643 - accuracy: 0.65 - ETA: 0s - loss: 1.1367 - accuracy: 0.65 - ETA: 0s - loss: 1.0410 - accuracy: 0.66 - 0s 5ms/step - loss: 1.0055 - accuracy: 0.6678 - val_loss: 0.5795 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4180 - accuracy: 0.84 - ETA: 0s - loss: 0.5161 - accuracy: 0.77 - ETA: 0s - loss: 0.5586 - accuracy: 0.75 - ETA: 0s - loss: 0.5731 - accuracy: 0.74 - ETA: 0s - loss: 0.5732 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5773 - accuracy: 0.7391 - val_loss: 0.5643 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.84 - ETA: 0s - loss: 0.5146 - accuracy: 0.76 - ETA: 0s - loss: 0.5161 - accuracy: 0.78 - ETA: 0s - loss: 0.5188 - accuracy: 0.77 - ETA: 0s - loss: 0.5225 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5233 - accuracy: 0.7708 - val_loss: 0.5818 - val_accuracy: 0.7030\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.65 - ETA: 0s - loss: 0.5051 - accuracy: 0.80 - ETA: 0s - loss: 0.5165 - accuracy: 0.79 - ETA: 0s - loss: 0.5062 - accuracy: 0.79 - ETA: 0s - loss: 0.4920 - accuracy: 0.79 - ETA: 0s - loss: 0.4943 - accuracy: 0.79 - 0s 4ms/step - loss: 0.4943 - accuracy: 0.7906 - val_loss: 0.5636 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.78 - ETA: 0s - loss: 0.4496 - accuracy: 0.81 - ETA: 0s - loss: 0.4227 - accuracy: 0.82 - ETA: 0s - loss: 0.4314 - accuracy: 0.81 - ETA: 0s - loss: 0.4639 - accuracy: 0.81 - ETA: 0s - loss: 0.4837 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4810 - accuracy: 0.8018 - val_loss: 0.6538 - val_accuracy: 0.7045\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4763 - accuracy: 0.78 - ETA: 0s - loss: 0.5499 - accuracy: 0.77 - ETA: 0s - loss: 0.5455 - accuracy: 0.77 - ETA: 0s - loss: 0.5377 - accuracy: 0.77 - ETA: 0s - loss: 0.5280 - accuracy: 0.78 - ETA: 0s - loss: 0.5120 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5120 - accuracy: 0.7932 - val_loss: 0.9371 - val_accuracy: 0.6522\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.87 - ETA: 0s - loss: 0.4492 - accuracy: 0.85 - ETA: 0s - loss: 0.4562 - accuracy: 0.83 - ETA: 0s - loss: 0.4980 - accuracy: 0.82 - ETA: 0s - loss: 0.4892 - accuracy: 0.82 - ETA: 0s - loss: 0.5038 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5106 - accuracy: 0.8193 - val_loss: 0.5895 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.81 - ETA: 0s - loss: 0.4534 - accuracy: 0.80 - ETA: 0s - loss: 0.4989 - accuracy: 0.81 - ETA: 0s - loss: 0.5082 - accuracy: 0.81 - ETA: 0s - loss: 0.5143 - accuracy: 0.80 - ETA: 0s - loss: 0.5133 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5067 - accuracy: 0.8122 - val_loss: 1.0868 - val_accuracy: 0.6910\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1046 - accuracy: 0.68 - ETA: 0s - loss: 0.5741 - accuracy: 0.80 - ETA: 0s - loss: 0.5191 - accuracy: 0.83 - ETA: 0s - loss: 0.5278 - accuracy: 0.83 - ETA: 0s - loss: 0.5203 - accuracy: 0.82 - ETA: 0s - loss: 0.4952 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4987 - accuracy: 0.8302 - val_loss: 0.9455 - val_accuracy: 0.6761\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.75 - ETA: 0s - loss: 0.5754 - accuracy: 0.78 - ETA: 0s - loss: 0.5896 - accuracy: 0.79 - ETA: 0s - loss: 0.5774 - accuracy: 0.80 - ETA: 0s - loss: 0.5446 - accuracy: 0.81 - ETA: 0s - loss: 0.5393 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5342 - accuracy: 0.8208 - val_loss: 0.7373 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.87 - ETA: 0s - loss: 0.4470 - accuracy: 0.83 - ETA: 0s - loss: 0.4901 - accuracy: 0.84 - ETA: 0s - loss: 0.5167 - accuracy: 0.83 - ETA: 0s - loss: 0.5096 - accuracy: 0.83 - ETA: 0s - loss: 0.4942 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4986 - accuracy: 0.8242 - val_loss: 0.7856 - val_accuracy: 0.7149\n",
      "Epoch 12/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.2905 - accuracy: 0.93 - ETA: 0s - loss: 0.3673 - accuracy: 0.87 - ETA: 0s - loss: 0.3865 - accuracy: 0.85 - ETA: 0s - loss: 0.3766 - accuracy: 0.85 - ETA: 0s - loss: 0.3917 - accuracy: 0.85 - ETA: 0s - loss: 0.3956 - accuracy: 0.8569Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4027 - accuracy: 0.8544 - val_loss: 0.7020 - val_accuracy: 0.7194\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7773 - accuracy: 0.71 - ETA: 0s - loss: 2.6375 - accuracy: 0.63 - ETA: 0s - loss: 1.8687 - accuracy: 0.62 - ETA: 0s - loss: 1.4913 - accuracy: 0.64 - ETA: 0s - loss: 1.3096 - accuracy: 0.65 - ETA: 0s - loss: 1.2100 - accuracy: 0.65 - 0s 5ms/step - loss: 1.1852 - accuracy: 0.6585 - val_loss: 0.5903 - val_accuracy: 0.7179\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.71 - ETA: 0s - loss: 0.6226 - accuracy: 0.68 - ETA: 0s - loss: 0.6166 - accuracy: 0.68 - ETA: 0s - loss: 0.6110 - accuracy: 0.70 - ETA: 0s - loss: 0.6210 - accuracy: 0.69 - ETA: 0s - loss: 0.6100 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6089 - accuracy: 0.7126 - val_loss: 0.6009 - val_accuracy: 0.7269\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.84 - ETA: 0s - loss: 0.5003 - accuracy: 0.81 - ETA: 0s - loss: 0.5261 - accuracy: 0.77 - ETA: 0s - loss: 0.5287 - accuracy: 0.76 - ETA: 0s - loss: 0.5088 - accuracy: 0.77 - ETA: 0s - loss: 0.5203 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5211 - accuracy: 0.7775 - val_loss: 0.6016 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.81 - ETA: 0s - loss: 0.4170 - accuracy: 0.82 - ETA: 0s - loss: 0.4441 - accuracy: 0.81 - ETA: 0s - loss: 0.4427 - accuracy: 0.81 - ETA: 0s - loss: 0.4597 - accuracy: 0.80 - ETA: 0s - loss: 0.4667 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4697 - accuracy: 0.8040 - val_loss: 0.5867 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.84 - ETA: 0s - loss: 0.4110 - accuracy: 0.85 - ETA: 0s - loss: 0.4267 - accuracy: 0.84 - ETA: 0s - loss: 0.4087 - accuracy: 0.84 - ETA: 0s - loss: 0.4260 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8234 - val_loss: 0.6787 - val_accuracy: 0.7060\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.81 - ETA: 0s - loss: 0.4202 - accuracy: 0.83 - ETA: 0s - loss: 0.4079 - accuracy: 0.84 - ETA: 0s - loss: 0.4080 - accuracy: 0.84 - ETA: 0s - loss: 0.4184 - accuracy: 0.83 - ETA: 0s - loss: 0.4389 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4380 - accuracy: 0.8275 - val_loss: 0.6477 - val_accuracy: 0.7015\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.75 - ETA: 0s - loss: 0.4169 - accuracy: 0.83 - ETA: 0s - loss: 0.3939 - accuracy: 0.83 - ETA: 0s - loss: 0.3933 - accuracy: 0.83 - ETA: 0s - loss: 0.4242 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4450 - accuracy: 0.8212 - val_loss: 0.5469 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.75 - ETA: 0s - loss: 0.6031 - accuracy: 0.82 - ETA: 0s - loss: 0.5555 - accuracy: 0.81 - ETA: 0s - loss: 0.5451 - accuracy: 0.81 - ETA: 0s - loss: 0.5116 - accuracy: 0.81 - ETA: 0s - loss: 0.5152 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5279 - accuracy: 0.8066 - val_loss: 2.6577 - val_accuracy: 0.6642\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.87 - ETA: 0s - loss: 0.7636 - accuracy: 0.80 - ETA: 0s - loss: 0.6236 - accuracy: 0.81 - ETA: 0s - loss: 0.5975 - accuracy: 0.80 - ETA: 0s - loss: 0.5590 - accuracy: 0.81 - ETA: 0s - loss: 0.5572 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5465 - accuracy: 0.8066 - val_loss: 0.8396 - val_accuracy: 0.6836\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.90 - ETA: 0s - loss: 0.3790 - accuracy: 0.84 - ETA: 0s - loss: 0.3941 - accuracy: 0.83 - ETA: 0s - loss: 0.4031 - accuracy: 0.83 - ETA: 0s - loss: 0.4119 - accuracy: 0.82 - ETA: 0s - loss: 0.4122 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4118 - accuracy: 0.8294 - val_loss: 0.7457 - val_accuracy: 0.6881\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.84 - ETA: 0s - loss: 0.5320 - accuracy: 0.87 - ETA: 0s - loss: 0.5627 - accuracy: 0.83 - ETA: 0s - loss: 0.5702 - accuracy: 0.83 - ETA: 0s - loss: 0.5703 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5733 - accuracy: 0.8257 - val_loss: 0.6457 - val_accuracy: 0.7269\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.93 - ETA: 0s - loss: 0.7532 - accuracy: 0.80 - ETA: 0s - loss: 0.7040 - accuracy: 0.81 - ETA: 0s - loss: 0.7437 - accuracy: 0.80 - ETA: 0s - loss: 0.7389 - accuracy: 0.79 - ETA: 0s - loss: 0.7783 - accuracy: 0.79 - 0s 4ms/step - loss: 0.7681 - accuracy: 0.7887 - val_loss: 0.6269 - val_accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.75 - ETA: 0s - loss: 0.7180 - accuracy: 0.78 - ETA: 0s - loss: 0.7213 - accuracy: 0.78 - ETA: 0s - loss: 0.6736 - accuracy: 0.78 - ETA: 0s - loss: 0.6874 - accuracy: 0.78 - ETA: 0s - loss: 0.6746 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6802 - accuracy: 0.7824 - val_loss: 0.6217 - val_accuracy: 0.7493\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.62 - ETA: 0s - loss: 0.5996 - accuracy: 0.78 - ETA: 0s - loss: 0.5776 - accuracy: 0.79 - ETA: 0s - loss: 0.6104 - accuracy: 0.80 - ETA: 0s - loss: 0.5994 - accuracy: 0.79 - ETA: 0s - loss: 0.6043 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6015 - accuracy: 0.7943 - val_loss: 0.6113 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.81 - ETA: 0s - loss: 0.5537 - accuracy: 0.81 - ETA: 0s - loss: 0.5589 - accuracy: 0.81 - ETA: 0s - loss: 0.5455 - accuracy: 0.82 - ETA: 0s - loss: 0.5375 - accuracy: 0.82 - ETA: 0s - loss: 0.5258 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5280 - accuracy: 0.8257 - val_loss: 0.6120 - val_accuracy: 0.7507\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3864 - accuracy: 0.96 - ETA: 0s - loss: 0.4652 - accuracy: 0.84 - ETA: 0s - loss: 0.4622 - accuracy: 0.85 - ETA: 0s - loss: 0.4803 - accuracy: 0.84 - ETA: 0s - loss: 0.4897 - accuracy: 0.84 - ETA: 0s - loss: 0.4879 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4838 - accuracy: 0.8429 - val_loss: 0.8674 - val_accuracy: 0.7358\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.65 - ETA: 0s - loss: 0.4540 - accuracy: 0.85 - ETA: 0s - loss: 0.4692 - accuracy: 0.84 - ETA: 0s - loss: 0.4844 - accuracy: 0.84 - ETA: 0s - loss: 0.5118 - accuracy: 0.83 - ETA: 0s - loss: 0.5272 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5309 - accuracy: 0.8294 - val_loss: 0.7811 - val_accuracy: 0.7612\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.71 - ETA: 0s - loss: 0.5399 - accuracy: 0.81 - ETA: 0s - loss: 0.5265 - accuracy: 0.82 - ETA: 0s - loss: 0.6657 - accuracy: 0.83 - ETA: 0s - loss: 0.6228 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6067 - accuracy: 0.8305 - val_loss: 0.6192 - val_accuracy: 0.7582\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.90 - ETA: 0s - loss: 0.5331 - accuracy: 0.85 - ETA: 0s - loss: 0.5452 - accuracy: 0.84 - ETA: 0s - loss: 0.5906 - accuracy: 0.84 - ETA: 0s - loss: 0.5715 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5839 - accuracy: 0.8369 - val_loss: 0.7522 - val_accuracy: 0.7507\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.84 - ETA: 0s - loss: 0.5150 - accuracy: 0.83 - ETA: 0s - loss: 0.5225 - accuracy: 0.82 - ETA: 0s - loss: 0.5022 - accuracy: 0.83 - ETA: 0s - loss: 0.5035 - accuracy: 0.83 - ETA: 0s - loss: 0.5022 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5018 - accuracy: 0.8391 - val_loss: 0.6774 - val_accuracy: 0.7463\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.90 - ETA: 0s - loss: 0.4535 - accuracy: 0.85 - ETA: 0s - loss: 0.4496 - accuracy: 0.85 - ETA: 0s - loss: 0.4624 - accuracy: 0.84 - ETA: 0s - loss: 0.4539 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4951 - accuracy: 0.8503 - val_loss: 0.6362 - val_accuracy: 0.7463\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.84 - ETA: 0s - loss: 0.4795 - accuracy: 0.85 - ETA: 0s - loss: 0.4893 - accuracy: 0.84 - ETA: 0s - loss: 0.4918 - accuracy: 0.84 - ETA: 0s - loss: 0.4822 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4840 - accuracy: 0.8529 - val_loss: 0.5617 - val_accuracy: 0.7433\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.81 - ETA: 0s - loss: 0.4719 - accuracy: 0.84 - ETA: 0s - loss: 0.4614 - accuracy: 0.85 - ETA: 0s - loss: 0.5617 - accuracy: 0.84 - ETA: 0s - loss: 0.5508 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5438 - accuracy: 0.8473 - val_loss: 0.8411 - val_accuracy: 0.7239\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3838 - accuracy: 0.90 - ETA: 0s - loss: 0.5199 - accuracy: 0.83 - ETA: 0s - loss: 0.5073 - accuracy: 0.84 - ETA: 0s - loss: 0.5005 - accuracy: 0.84 - ETA: 0s - loss: 0.4955 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4901 - accuracy: 0.8541 - val_loss: 0.9119 - val_accuracy: 0.7612\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.96 - ETA: 0s - loss: 0.4449 - accuracy: 0.86 - ETA: 0s - loss: 0.4524 - accuracy: 0.85 - ETA: 0s - loss: 0.5197 - accuracy: 0.85 - ETA: 0s - loss: 0.5075 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4973 - accuracy: 0.8626 - val_loss: 0.7338 - val_accuracy: 0.7299\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.84 - ETA: 0s - loss: 0.7471 - accuracy: 0.86 - ETA: 0s - loss: 0.8051 - accuracy: 0.84 - ETA: 0s - loss: 0.7371 - accuracy: 0.83 - ETA: 0s - loss: 0.7294 - accuracy: 0.81 - ETA: 0s - loss: 0.7224 - accuracy: 0.80 - 0s 4ms/step - loss: 0.7279 - accuracy: 0.7951 - val_loss: 0.6197 - val_accuracy: 0.7045\n",
      "Epoch 27/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.7338 - accuracy: 0.68 - ETA: 0s - loss: 0.7621 - accuracy: 0.71 - ETA: 0s - loss: 0.7201 - accuracy: 0.71 - ETA: 0s - loss: 0.6948 - accuracy: 0.72 - ETA: 0s - loss: 0.6990 - accuracy: 0.71 - ETA: 0s - loss: 0.6930 - accuracy: 0.7163Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.7182 - val_loss: 0.7141 - val_accuracy: 0.7373\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f446d22fc9eedd816930b8a85d0f6d67</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.746268649895986</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5452876706494295</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0214 - accuracy: 0.56 - ETA: 0s - loss: 1.9405 - accuracy: 0.63 - ETA: 0s - loss: 1.3160 - accuracy: 0.65 - ETA: 0s - loss: 1.0593 - accuracy: 0.66 - ETA: 0s - loss: 0.9382 - accuracy: 0.67 - ETA: 0s - loss: 0.8644 - accuracy: 0.68 - 0s 6ms/step - loss: 0.8409 - accuracy: 0.6831 - val_loss: 0.5588 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.71 - ETA: 0s - loss: 0.5132 - accuracy: 0.77 - ETA: 0s - loss: 0.4942 - accuracy: 0.78 - ETA: 0s - loss: 0.5044 - accuracy: 0.77 - ETA: 0s - loss: 0.5123 - accuracy: 0.77 - ETA: 0s - loss: 0.5124 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5120 - accuracy: 0.7786 - val_loss: 0.6031 - val_accuracy: 0.6955\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.75 - ETA: 0s - loss: 0.4278 - accuracy: 0.80 - ETA: 0s - loss: 0.4255 - accuracy: 0.80 - ETA: 0s - loss: 0.4418 - accuracy: 0.79 - ETA: 0s - loss: 0.4439 - accuracy: 0.79 - ETA: 0s - loss: 0.4423 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4446 - accuracy: 0.8010 - val_loss: 0.8464 - val_accuracy: 0.6896\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.71 - ETA: 0s - loss: 0.4502 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.82 - ETA: 0s - loss: 0.4453 - accuracy: 0.82 - ETA: 0s - loss: 0.4371 - accuracy: 0.83 - ETA: 0s - loss: 0.4403 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4402 - accuracy: 0.8197 - val_loss: 0.6232 - val_accuracy: 0.6836\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.90 - ETA: 0s - loss: 0.3232 - accuracy: 0.86 - ETA: 0s - loss: 0.3425 - accuracy: 0.84 - ETA: 0s - loss: 0.3553 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3663 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8425 - val_loss: 0.7605 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.87 - ETA: 0s - loss: 0.3050 - accuracy: 0.86 - ETA: 0s - loss: 0.3245 - accuracy: 0.85 - ETA: 0s - loss: 0.3255 - accuracy: 0.85 - ETA: 0s - loss: 0.3307 - accuracy: 0.85 - ETA: 0s - loss: 0.3338 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3327 - accuracy: 0.8574 - val_loss: 0.6570 - val_accuracy: 0.7328\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.90 - ETA: 0s - loss: 0.3750 - accuracy: 0.86 - ETA: 0s - loss: 0.3323 - accuracy: 0.87 - ETA: 0s - loss: 0.3432 - accuracy: 0.87 - ETA: 0s - loss: 0.3534 - accuracy: 0.86 - ETA: 0s - loss: 0.3712 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8544 - val_loss: 0.7985 - val_accuracy: 0.7149\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3263 - accuracy: 0.87 - ETA: 0s - loss: 0.3113 - accuracy: 0.88 - ETA: 0s - loss: 0.3019 - accuracy: 0.88 - ETA: 0s - loss: 0.3218 - accuracy: 0.87 - ETA: 0s - loss: 0.3323 - accuracy: 0.87 - ETA: 0s - loss: 0.3429 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3518 - accuracy: 0.8589 - val_loss: 0.7761 - val_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.90 - ETA: 0s - loss: 0.2986 - accuracy: 0.87 - ETA: 0s - loss: 0.3602 - accuracy: 0.86 - ETA: 0s - loss: 0.3330 - accuracy: 0.87 - ETA: 0s - loss: 0.3114 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8764 - val_loss: 1.1290 - val_accuracy: 0.6701\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.78 - ETA: 0s - loss: 0.2429 - accuracy: 0.90 - ETA: 0s - loss: 0.2534 - accuracy: 0.90 - ETA: 0s - loss: 0.3263 - accuracy: 0.88 - ETA: 0s - loss: 0.3353 - accuracy: 0.88 - ETA: 0s - loss: 0.3316 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3442 - accuracy: 0.8705 - val_loss: 1.9521 - val_accuracy: 0.7328\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3813 - accuracy: 0.84 - ETA: 0s - loss: 0.3766 - accuracy: 0.85 - ETA: 0s - loss: 0.4458 - accuracy: 0.84 - ETA: 0s - loss: 0.4539 - accuracy: 0.83 - ETA: 0s - loss: 0.4451 - accuracy: 0.83 - ETA: 0s - loss: 0.4717 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4668 - accuracy: 0.8324 - val_loss: 3.0517 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.87 - ETA: 0s - loss: 0.6419 - accuracy: 0.85 - ETA: 0s - loss: 0.6900 - accuracy: 0.83 - ETA: 0s - loss: 0.6368 - accuracy: 0.82 - ETA: 0s - loss: 0.5895 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5680 - accuracy: 0.8268 - val_loss: 0.6363 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.87 - ETA: 0s - loss: 0.3471 - accuracy: 0.88 - ETA: 0s - loss: 0.3699 - accuracy: 0.87 - ETA: 0s - loss: 0.3975 - accuracy: 0.86 - ETA: 0s - loss: 0.4077 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8641 - val_loss: 0.5926 - val_accuracy: 0.7343\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9490 - accuracy: 0.75 - ETA: 0s - loss: 0.4648 - accuracy: 0.85 - ETA: 0s - loss: 0.5042 - accuracy: 0.83 - ETA: 0s - loss: 0.4793 - accuracy: 0.84 - ETA: 0s - loss: 0.4764 - accuracy: 0.85 - ETA: 0s - loss: 0.4498 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4434 - accuracy: 0.8608 - val_loss: 0.6946 - val_accuracy: 0.7254\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.96 - ETA: 0s - loss: 0.2717 - accuracy: 0.91 - ETA: 0s - loss: 0.3191 - accuracy: 0.90 - ETA: 0s - loss: 0.3224 - accuracy: 0.90 - ETA: 0s - loss: 0.3350 - accuracy: 0.90 - ETA: 0s - loss: 0.3412 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3462 - accuracy: 0.8951 - val_loss: 0.7434 - val_accuracy: 0.7179\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.90 - ETA: 0s - loss: 0.2927 - accuracy: 0.90 - ETA: 0s - loss: 0.2909 - accuracy: 0.90 - ETA: 0s - loss: 0.2912 - accuracy: 0.90 - ETA: 0s - loss: 0.2956 - accuracy: 0.90 - ETA: 0s - loss: 0.2929 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2901 - accuracy: 0.9063 - val_loss: 0.9871 - val_accuracy: 0.7030\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.90 - ETA: 0s - loss: 0.2514 - accuracy: 0.91 - ETA: 0s - loss: 0.2488 - accuracy: 0.91 - ETA: 0s - loss: 0.2495 - accuracy: 0.91 - ETA: 0s - loss: 0.2461 - accuracy: 0.91 - ETA: 0s - loss: 0.2577 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2537 - accuracy: 0.9108 - val_loss: 0.8772 - val_accuracy: 0.7060\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.96 - ETA: 0s - loss: 0.2306 - accuracy: 0.92 - ETA: 0s - loss: 0.2127 - accuracy: 0.93 - ETA: 0s - loss: 0.2203 - accuracy: 0.93 - ETA: 0s - loss: 0.2096 - accuracy: 0.93 - ETA: 0s - loss: 0.2036 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2053 - accuracy: 0.9395 - val_loss: 1.2679 - val_accuracy: 0.7164\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.90 - ETA: 0s - loss: 0.2867 - accuracy: 0.94 - ETA: 0s - loss: 0.2212 - accuracy: 0.95 - ETA: 0s - loss: 0.2194 - accuracy: 0.94 - ETA: 0s - loss: 0.2850 - accuracy: 0.93 - ETA: 0s - loss: 0.2776 - accuracy: 0.93 - 0s 4ms/step - loss: 0.2735 - accuracy: 0.9291 - val_loss: 1.1482 - val_accuracy: 0.6881\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.96 - ETA: 0s - loss: 0.1280 - accuracy: 0.96 - ETA: 0s - loss: 0.1603 - accuracy: 0.95 - ETA: 0s - loss: 0.1650 - accuracy: 0.95 - ETA: 0s - loss: 0.1705 - accuracy: 0.94 - ETA: 0s - loss: 0.1855 - accuracy: 0.94 - 0s 4ms/step - loss: 0.1857 - accuracy: 0.9436 - val_loss: 1.7951 - val_accuracy: 0.7075\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.96 - ETA: 0s - loss: 0.2820 - accuracy: 0.93 - ETA: 0s - loss: 0.2743 - accuracy: 0.91 - ETA: 0s - loss: 0.2555 - accuracy: 0.91 - ETA: 0s - loss: 0.3445 - accuracy: 0.91 - ETA: 0s - loss: 0.3684 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8981 - val_loss: 0.8997 - val_accuracy: 0.7299\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.81 - ETA: 0s - loss: 0.4232 - accuracy: 0.86 - ETA: 0s - loss: 0.3393 - accuracy: 0.88 - ETA: 0s - loss: 0.3053 - accuracy: 0.89 - ETA: 0s - loss: 0.2911 - accuracy: 0.90 - ETA: 0s - loss: 0.2820 - accuracy: 0.90 - 0s 4ms/step - loss: 0.2791 - accuracy: 0.9048 - val_loss: 1.3876 - val_accuracy: 0.7045\n",
      "Epoch 23/50\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.1082 - accuracy: 1.00 - ETA: 0s - loss: 0.1617 - accuracy: 0.95 - ETA: 0s - loss: 0.1765 - accuracy: 0.94 - ETA: 0s - loss: 0.1803 - accuracy: 0.94 - ETA: 0s - loss: 0.1882 - accuracy: 0.93 - ETA: 0s - loss: 0.1858 - accuracy: 0.9399Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9403 - val_loss: 1.5655 - val_accuracy: 0.7134\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0284 - accuracy: 0.56 - ETA: 0s - loss: 2.0308 - accuracy: 0.60 - ETA: 0s - loss: 1.2791 - accuracy: 0.63 - ETA: 0s - loss: 1.0841 - accuracy: 0.63 - ETA: 0s - loss: 0.9632 - accuracy: 0.65 - ETA: 0s - loss: 0.8882 - accuracy: 0.66 - 0s 5ms/step - loss: 0.8377 - accuracy: 0.6753 - val_loss: 0.6069 - val_accuracy: 0.6821\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.78 - ETA: 0s - loss: 0.5527 - accuracy: 0.69 - ETA: 0s - loss: 0.5398 - accuracy: 0.73 - ETA: 0s - loss: 0.5249 - accuracy: 0.74 - ETA: 0s - loss: 0.5228 - accuracy: 0.75 - ETA: 0s - loss: 0.5308 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5337 - accuracy: 0.7540 - val_loss: 0.6886 - val_accuracy: 0.6657\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.78 - ETA: 0s - loss: 0.4460 - accuracy: 0.80 - ETA: 0s - loss: 0.4349 - accuracy: 0.80 - ETA: 0s - loss: 0.4647 - accuracy: 0.79 - ETA: 0s - loss: 0.4716 - accuracy: 0.79 - ETA: 0s - loss: 0.4887 - accuracy: 0.78 - 0s 4ms/step - loss: 0.4869 - accuracy: 0.7857 - val_loss: 0.6626 - val_accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.87 - ETA: 0s - loss: 0.4779 - accuracy: 0.78 - ETA: 0s - loss: 0.4597 - accuracy: 0.80 - ETA: 0s - loss: 0.4568 - accuracy: 0.80 - ETA: 0s - loss: 0.4448 - accuracy: 0.80 - ETA: 0s - loss: 0.4526 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4505 - accuracy: 0.8014 - val_loss: 0.7463 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.75 - ETA: 0s - loss: 0.4533 - accuracy: 0.79 - ETA: 0s - loss: 0.4229 - accuracy: 0.81 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - ETA: 0s - loss: 0.4067 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8216 - val_loss: 0.5874 - val_accuracy: 0.7478\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.90 - ETA: 0s - loss: 0.3512 - accuracy: 0.86 - ETA: 0s - loss: 0.3620 - accuracy: 0.86 - ETA: 0s - loss: 0.3743 - accuracy: 0.85 - ETA: 0s - loss: 0.3755 - accuracy: 0.85 - ETA: 0s - loss: 0.3856 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3829 - accuracy: 0.8488 - val_loss: 0.7426 - val_accuracy: 0.6284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.90 - ETA: 0s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3724 - accuracy: 0.83 - ETA: 0s - loss: 0.3664 - accuracy: 0.84 - ETA: 0s - loss: 0.3832 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3847 - accuracy: 0.8309 - val_loss: 0.6550 - val_accuracy: 0.7104\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.81 - ETA: 0s - loss: 0.3394 - accuracy: 0.85 - ETA: 0s - loss: 0.3457 - accuracy: 0.84 - ETA: 0s - loss: 0.3533 - accuracy: 0.84 - ETA: 0s - loss: 0.3405 - accuracy: 0.84 - ETA: 0s - loss: 0.3297 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3240 - accuracy: 0.8526 - val_loss: 1.0488 - val_accuracy: 0.6925\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.90 - ETA: 0s - loss: 0.2546 - accuracy: 0.86 - ETA: 0s - loss: 0.2690 - accuracy: 0.86 - ETA: 0s - loss: 0.2885 - accuracy: 0.86 - ETA: 0s - loss: 0.2942 - accuracy: 0.87 - ETA: 0s - loss: 0.2892 - accuracy: 0.87 - 0s 4ms/step - loss: 0.2926 - accuracy: 0.8727 - val_loss: 0.9131 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.84 - ETA: 0s - loss: 0.2204 - accuracy: 0.89 - ETA: 0s - loss: 0.2398 - accuracy: 0.87 - ETA: 0s - loss: 0.2861 - accuracy: 0.87 - ETA: 0s - loss: 0.3400 - accuracy: 0.86 - ETA: 0s - loss: 0.3770 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8499 - val_loss: 0.8779 - val_accuracy: 0.6627\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.81 - ETA: 0s - loss: 0.3269 - accuracy: 0.86 - ETA: 0s - loss: 0.3380 - accuracy: 0.85 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.3506 - accuracy: 0.87 - ETA: 0s - loss: 0.3615 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8686 - val_loss: 0.9130 - val_accuracy: 0.6776\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.90 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2782 - accuracy: 0.90 - ETA: 0s - loss: 0.3214 - accuracy: 0.88 - ETA: 0s - loss: 0.3193 - accuracy: 0.88 - ETA: 0s - loss: 0.3251 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3356 - accuracy: 0.8791 - val_loss: 1.1604 - val_accuracy: 0.6896\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.96 - ETA: 0s - loss: 0.3096 - accuracy: 0.88 - ETA: 0s - loss: 0.2918 - accuracy: 0.90 - ETA: 0s - loss: 0.4008 - accuracy: 0.88 - ETA: 0s - loss: 0.3922 - accuracy: 0.88 - ETA: 0s - loss: 0.3843 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3805 - accuracy: 0.8802 - val_loss: 1.8949 - val_accuracy: 0.6537\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.90 - ETA: 0s - loss: 0.2881 - accuracy: 0.89 - ETA: 0s - loss: 0.3861 - accuracy: 0.89 - ETA: 0s - loss: 0.3972 - accuracy: 0.89 - ETA: 0s - loss: 0.4121 - accuracy: 0.87 - ETA: 0s - loss: 0.3971 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8794 - val_loss: 1.6097 - val_accuracy: 0.7299\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5333 - accuracy: 0.90 - ETA: 0s - loss: 0.2698 - accuracy: 0.90 - ETA: 0s - loss: 0.2718 - accuracy: 0.89 - ETA: 0s - loss: 0.2505 - accuracy: 0.90 - ETA: 0s - loss: 0.2590 - accuracy: 0.90 - ETA: 0s - loss: 0.3152 - accuracy: 0.8839Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8820 - val_loss: 1.2193 - val_accuracy: 0.7164\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8225 - accuracy: 0.68 - ETA: 0s - loss: 2.4077 - accuracy: 0.58 - ETA: 0s - loss: 1.5610 - accuracy: 0.62 - ETA: 0s - loss: 1.2135 - accuracy: 0.66 - ETA: 0s - loss: 1.0523 - accuracy: 0.68 - ETA: 0s - loss: 0.9608 - accuracy: 0.69 - 0s 5ms/step - loss: 0.9049 - accuracy: 0.6988 - val_loss: 0.5862 - val_accuracy: 0.6866\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.68 - ETA: 0s - loss: 0.5158 - accuracy: 0.73 - ETA: 0s - loss: 0.5112 - accuracy: 0.73 - ETA: 0s - loss: 0.5042 - accuracy: 0.75 - ETA: 0s - loss: 0.5015 - accuracy: 0.75 - ETA: 0s - loss: 0.5038 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5077 - accuracy: 0.7533 - val_loss: 0.6014 - val_accuracy: 0.6866\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.78 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - ETA: 0s - loss: 0.4295 - accuracy: 0.82 - ETA: 0s - loss: 0.4201 - accuracy: 0.82 - ETA: 0s - loss: 0.4195 - accuracy: 0.81 - ETA: 0s - loss: 0.4293 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4362 - accuracy: 0.8055 - val_loss: 0.5791 - val_accuracy: 0.7179\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4267 - accuracy: 0.81 - ETA: 0s - loss: 0.3502 - accuracy: 0.83 - ETA: 0s - loss: 0.3784 - accuracy: 0.82 - ETA: 0s - loss: 0.3797 - accuracy: 0.82 - ETA: 0s - loss: 0.3778 - accuracy: 0.82 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3972 - accuracy: 0.8234 - val_loss: 1.0112 - val_accuracy: 0.6224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.81 - ETA: 0s - loss: 0.3421 - accuracy: 0.82 - ETA: 0s - loss: 0.3579 - accuracy: 0.82 - ETA: 0s - loss: 0.3471 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3777 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8335 - val_loss: 1.1191 - val_accuracy: 0.7090\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.81 - ETA: 0s - loss: 0.2913 - accuracy: 0.88 - ETA: 0s - loss: 0.3494 - accuracy: 0.85 - ETA: 0s - loss: 0.3456 - accuracy: 0.85 - ETA: 0s - loss: 0.3407 - accuracy: 0.85 - ETA: 0s - loss: 0.3433 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3528 - accuracy: 0.8552 - val_loss: 0.9108 - val_accuracy: 0.6642\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.78 - ETA: 0s - loss: 0.3300 - accuracy: 0.83 - ETA: 0s - loss: 0.3783 - accuracy: 0.84 - ETA: 0s - loss: 0.3884 - accuracy: 0.84 - ETA: 0s - loss: 0.4494 - accuracy: 0.83 - ETA: 0s - loss: 0.4470 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4521 - accuracy: 0.8242 - val_loss: 0.7674 - val_accuracy: 0.6851\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.87 - ETA: 0s - loss: 0.3905 - accuracy: 0.84 - ETA: 0s - loss: 0.3679 - accuracy: 0.85 - ETA: 0s - loss: 0.3813 - accuracy: 0.84 - ETA: 0s - loss: 0.3944 - accuracy: 0.83 - ETA: 0s - loss: 0.3918 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8384 - val_loss: 1.4155 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.96 - ETA: 0s - loss: 0.4394 - accuracy: 0.86 - ETA: 0s - loss: 0.3985 - accuracy: 0.84 - ETA: 0s - loss: 0.3824 - accuracy: 0.84 - ETA: 0s - loss: 0.3806 - accuracy: 0.84 - ETA: 0s - loss: 0.3947 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3958 - accuracy: 0.8436 - val_loss: 0.8699 - val_accuracy: 0.7060\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.5081 - accuracy: 0.84 - ETA: 0s - loss: 0.4466 - accuracy: 0.84 - ETA: 0s - loss: 0.4241 - accuracy: 0.83 - ETA: 0s - loss: 0.4016 - accuracy: 0.84 - ETA: 0s - loss: 0.3921 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8443 - val_loss: 1.1187 - val_accuracy: 0.6866\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.87 - ETA: 0s - loss: 0.2874 - accuracy: 0.87 - ETA: 0s - loss: 0.3218 - accuracy: 0.87 - ETA: 0s - loss: 0.3373 - accuracy: 0.86 - ETA: 0s - loss: 0.3363 - accuracy: 0.86 - ETA: 0s - loss: 0.3327 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3273 - accuracy: 0.8660 - val_loss: 1.7345 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.90 - ETA: 0s - loss: 0.4488 - accuracy: 0.85 - ETA: 0s - loss: 0.3897 - accuracy: 0.87 - ETA: 0s - loss: 0.3911 - accuracy: 0.86 - ETA: 0s - loss: 0.4523 - accuracy: 0.86 - ETA: 0s - loss: 0.4368 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8589 - val_loss: 3.0415 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.93 - ETA: 0s - loss: 0.5582 - accuracy: 0.83 - ETA: 0s - loss: 0.5432 - accuracy: 0.82 - ETA: 0s - loss: 0.4922 - accuracy: 0.84 - ETA: 0s - loss: 0.5037 - accuracy: 0.84 - ETA: 0s - loss: 0.5081 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4937 - accuracy: 0.8417 - val_loss: 2.1346 - val_accuracy: 0.7358\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.90 - ETA: 0s - loss: 0.4226 - accuracy: 0.86 - ETA: 0s - loss: 0.4055 - accuracy: 0.86 - ETA: 0s - loss: 0.4855 - accuracy: 0.85 - ETA: 0s - loss: 0.4798 - accuracy: 0.85 - ETA: 0s - loss: 0.4750 - accuracy: 0.84 - ETA: 0s - loss: 0.4792 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4792 - accuracy: 0.8410 - val_loss: 1.6316 - val_accuracy: 0.7537\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.90 - ETA: 0s - loss: 0.6479 - accuracy: 0.86 - ETA: 0s - loss: 0.5303 - accuracy: 0.84 - ETA: 0s - loss: 0.5530 - accuracy: 0.84 - ETA: 0s - loss: 0.5168 - accuracy: 0.84 - ETA: 0s - loss: 0.5163 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5104 - accuracy: 0.8447 - val_loss: 0.7150 - val_accuracy: 0.7328\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5383 - accuracy: 0.81 - ETA: 0s - loss: 0.4443 - accuracy: 0.84 - ETA: 0s - loss: 0.4188 - accuracy: 0.85 - ETA: 0s - loss: 0.4031 - accuracy: 0.86 - ETA: 0s - loss: 0.3837 - accuracy: 0.87 - ETA: 0s - loss: 0.3704 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3704 - accuracy: 0.8727 - val_loss: 5.7800 - val_accuracy: 0.7090\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.87 - ETA: 0s - loss: 1.7106 - accuracy: 0.89 - ETA: 0s - loss: 1.1271 - accuracy: 0.86 - ETA: 0s - loss: 0.9236 - accuracy: 0.85 - ETA: 0s - loss: 0.8448 - accuracy: 0.84 - ETA: 0s - loss: 0.7638 - accuracy: 0.84 - 0s 4ms/step - loss: 0.7315 - accuracy: 0.8522 - val_loss: 1.1129 - val_accuracy: 0.7239\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.78 - ETA: 0s - loss: 0.3907 - accuracy: 0.86 - ETA: 0s - loss: 0.3682 - accuracy: 0.87 - ETA: 0s - loss: 0.3618 - accuracy: 0.88 - ETA: 0s - loss: 0.3616 - accuracy: 0.88 - ETA: 0s - loss: 0.3626 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3529 - accuracy: 0.8884 - val_loss: 1.1008 - val_accuracy: 0.7075\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.93 - ETA: 0s - loss: 0.3142 - accuracy: 0.89 - ETA: 0s - loss: 0.3154 - accuracy: 0.90 - ETA: 0s - loss: 0.3022 - accuracy: 0.90 - ETA: 0s - loss: 0.2941 - accuracy: 0.90 - ETA: 0s - loss: 0.3286 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3497 - accuracy: 0.8932 - val_loss: 0.6179 - val_accuracy: 0.7463\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.93 - ETA: 0s - loss: 0.3846 - accuracy: 0.88 - ETA: 0s - loss: 0.3617 - accuracy: 0.89 - ETA: 0s - loss: 0.3409 - accuracy: 0.89 - ETA: 0s - loss: 0.3175 - accuracy: 0.90 - ETA: 0s - loss: 0.3138 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3086 - accuracy: 0.9097 - val_loss: 0.9153 - val_accuracy: 0.7149\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.96 - ETA: 0s - loss: 0.2833 - accuracy: 0.92 - ETA: 0s - loss: 0.3256 - accuracy: 0.91 - ETA: 0s - loss: 0.3087 - accuracy: 0.91 - ETA: 0s - loss: 0.3492 - accuracy: 0.90 - ETA: 0s - loss: 0.3465 - accuracy: 0.90 - 0s 4ms/step - loss: 0.3533 - accuracy: 0.9052 - val_loss: 1.4086 - val_accuracy: 0.7164\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.90 - ETA: 0s - loss: 0.4272 - accuracy: 0.87 - ETA: 0s - loss: 0.4022 - accuracy: 0.88 - ETA: 0s - loss: 0.3558 - accuracy: 0.89 - ETA: 0s - loss: 0.3474 - accuracy: 0.90 - ETA: 0s - loss: 0.3580 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3494 - accuracy: 0.9000 - val_loss: 1.4200 - val_accuracy: 0.7030\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.90 - ETA: 0s - loss: 0.2643 - accuracy: 0.91 - ETA: 0s - loss: 0.2589 - accuracy: 0.92 - ETA: 0s - loss: 0.2639 - accuracy: 0.92 - ETA: 0s - loss: 0.2694 - accuracy: 0.92 - ETA: 0s - loss: 0.2674 - accuracy: 0.92 - 0s 4ms/step - loss: 0.2735 - accuracy: 0.9239 - val_loss: 1.3711 - val_accuracy: 0.7194\n",
      "Epoch 24/50\n",
      "73/84 [=========================>....] - ETA: 0s - loss: 0.0905 - accuracy: 1.00 - ETA: 0s - loss: 0.2508 - accuracy: 0.94 - ETA: 0s - loss: 0.2573 - accuracy: 0.93 - ETA: 0s - loss: 0.2651 - accuracy: 0.92 - ETA: 0s - loss: 0.2653 - accuracy: 0.92 - ETA: 0s - loss: 0.2581 - accuracy: 0.9268Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.9268 - val_loss: 1.0579 - val_accuracy: 0.7537\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cc59dc0bbf4b170f64ca9723d93a67b1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7452736298243204</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.09083652040639834</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 360</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8305 - accuracy: 0.68 - ETA: 0s - loss: 1.0160 - accuracy: 0.61 - ETA: 0s - loss: 0.7987 - accuracy: 0.67 - ETA: 0s - loss: 0.7123 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7010 - accuracy: 0.6976 - val_loss: 0.6189 - val_accuracy: 0.6687\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4831 - accuracy: 0.71 - ETA: 0s - loss: 0.5431 - accuracy: 0.73 - ETA: 0s - loss: 0.5324 - accuracy: 0.74 - ETA: 0s - loss: 0.5252 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5298 - accuracy: 0.7495 - val_loss: 0.5441 - val_accuracy: 0.7418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4804 - accuracy: 0.87 - ETA: 0s - loss: 0.4739 - accuracy: 0.80 - ETA: 0s - loss: 0.4746 - accuracy: 0.80 - ETA: 0s - loss: 0.4679 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8052 - val_loss: 0.7603 - val_accuracy: 0.6687\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.93 - ETA: 0s - loss: 0.4232 - accuracy: 0.82 - ETA: 0s - loss: 0.4217 - accuracy: 0.83 - ETA: 0s - loss: 0.4150 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8309 - val_loss: 0.6065 - val_accuracy: 0.6970\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.87 - ETA: 0s - loss: 0.3838 - accuracy: 0.86 - ETA: 0s - loss: 0.3839 - accuracy: 0.85 - ETA: 0s - loss: 0.3950 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8537 - val_loss: 0.6205 - val_accuracy: 0.7164\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.87 - ETA: 0s - loss: 0.3637 - accuracy: 0.85 - ETA: 0s - loss: 0.3782 - accuracy: 0.85 - ETA: 0s - loss: 0.3824 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8470 - val_loss: 0.8629 - val_accuracy: 0.6761\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.87 - ETA: 0s - loss: 0.3535 - accuracy: 0.87 - ETA: 0s - loss: 0.3617 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8630 - val_loss: 0.6936 - val_accuracy: 0.7179\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.93 - ETA: 0s - loss: 0.3728 - accuracy: 0.88 - ETA: 0s - loss: 0.3722 - accuracy: 0.87 - ETA: 0s - loss: 0.3786 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8645 - val_loss: 0.8494 - val_accuracy: 0.6881\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.90 - ETA: 0s - loss: 0.3322 - accuracy: 0.87 - ETA: 0s - loss: 0.3183 - accuracy: 0.88 - ETA: 0s - loss: 0.3208 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8888 - val_loss: 0.9356 - val_accuracy: 0.6940\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.93 - ETA: 0s - loss: 0.3134 - accuracy: 0.89 - ETA: 0s - loss: 0.3130 - accuracy: 0.89 - ETA: 0s - loss: 0.3106 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8955 - val_loss: 0.8967 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.93 - ETA: 0s - loss: 0.3050 - accuracy: 0.90 - ETA: 0s - loss: 0.2898 - accuracy: 0.90 - ETA: 0s - loss: 0.2904 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2982 - accuracy: 0.9074 - val_loss: 0.7937 - val_accuracy: 0.7269\n",
      "Epoch 12/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2522 - accuracy: 0.93 - ETA: 0s - loss: 0.2815 - accuracy: 0.91 - ETA: 0s - loss: 0.2761 - accuracy: 0.90 - ETA: 0s - loss: 0.2908 - accuracy: 0.8999Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8973 - val_loss: 0.8962 - val_accuracy: 0.7119\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.81 - ETA: 0s - loss: 0.9132 - accuracy: 0.64 - ETA: 0s - loss: 0.7193 - accuracy: 0.70 - ETA: 0s - loss: 0.6675 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6632 - accuracy: 0.7047 - val_loss: 0.6470 - val_accuracy: 0.6507\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5998 - accuracy: 0.65 - ETA: 0s - loss: 0.5164 - accuracy: 0.76 - ETA: 0s - loss: 0.5011 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7686 - val_loss: 0.5568 - val_accuracy: 0.7269\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.81 - ETA: 0s - loss: 0.4458 - accuracy: 0.80 - ETA: 0s - loss: 0.4381 - accuracy: 0.80 - ETA: 0s - loss: 0.4687 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7954 - val_loss: 0.6038 - val_accuracy: 0.7104\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.90 - ETA: 0s - loss: 0.5096 - accuracy: 0.78 - ETA: 0s - loss: 0.4613 - accuracy: 0.80 - ETA: 0s - loss: 0.4542 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8025 - val_loss: 0.6989 - val_accuracy: 0.6925\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.78 - ETA: 0s - loss: 0.3764 - accuracy: 0.83 - ETA: 0s - loss: 0.3846 - accuracy: 0.83 - ETA: 0s - loss: 0.4712 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4859 - accuracy: 0.8055 - val_loss: 0.7258 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.78 - ETA: 0s - loss: 0.4756 - accuracy: 0.78 - ETA: 0s - loss: 0.4856 - accuracy: 0.79 - ETA: 0s - loss: 0.4924 - accuracy: 0.79 - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7977 - val_loss: 1.4735 - val_accuracy: 0.6955\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.90 - ETA: 0s - loss: 0.5477 - accuracy: 0.79 - ETA: 0s - loss: 0.5002 - accuracy: 0.82 - ETA: 0s - loss: 0.4826 - accuracy: 0.82 - 0s 2ms/step - loss: 0.4830 - accuracy: 0.8216 - val_loss: 0.7707 - val_accuracy: 0.6716\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.87 - ETA: 0s - loss: 0.3392 - accuracy: 0.85 - ETA: 0s - loss: 0.3407 - accuracy: 0.85 - ETA: 0s - loss: 0.3709 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8451 - val_loss: 0.9549 - val_accuracy: 0.7239\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 1.00 - ETA: 0s - loss: 0.3429 - accuracy: 0.87 - ETA: 0s - loss: 0.3670 - accuracy: 0.86 - ETA: 0s - loss: 0.3791 - accuracy: 0.86 - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8537 - val_loss: 1.1883 - val_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.96 - ETA: 0s - loss: 0.3590 - accuracy: 0.87 - ETA: 0s - loss: 0.3588 - accuracy: 0.87 - ETA: 0s - loss: 0.3433 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8708 - val_loss: 1.2621 - val_accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.96 - ETA: 0s - loss: 0.2585 - accuracy: 0.90 - ETA: 0s - loss: 0.2610 - accuracy: 0.89 - ETA: 0s - loss: 0.2839 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2844 - accuracy: 0.8906 - val_loss: 1.4502 - val_accuracy: 0.6776\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.84 - ETA: 0s - loss: 0.2796 - accuracy: 0.86 - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2720 - accuracy: 0.88 - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8876 - val_loss: 0.9738 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.93 - ETA: 0s - loss: 0.2484 - accuracy: 0.90 - ETA: 0s - loss: 0.2584 - accuracy: 0.90 - ETA: 0s - loss: 0.2721 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9003 - val_loss: 1.4037 - val_accuracy: 0.7104\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.93 - ETA: 0s - loss: 0.2268 - accuracy: 0.90 - ETA: 0s - loss: 0.2433 - accuracy: 0.90 - ETA: 0s - loss: 0.2449 - accuracy: 0.90 - 0s 2ms/step - loss: 0.2446 - accuracy: 0.9052 - val_loss: 1.3546 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.93 - ETA: 0s - loss: 0.2002 - accuracy: 0.92 - ETA: 0s - loss: 0.2115 - accuracy: 0.92 - ETA: 0s - loss: 0.2161 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2208 - accuracy: 0.9168 - val_loss: 1.2022 - val_accuracy: 0.7254\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.93 - ETA: 0s - loss: 0.2060 - accuracy: 0.92 - ETA: 0s - loss: 0.1958 - accuracy: 0.93 - ETA: 0s - loss: 0.1900 - accuracy: 0.93 - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9309 - val_loss: 0.7954 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.93 - ETA: 0s - loss: 0.1979 - accuracy: 0.92 - ETA: 0s - loss: 0.2092 - accuracy: 0.92 - ETA: 0s - loss: 0.3078 - accuracy: 0.90 - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8977 - val_loss: 0.8807 - val_accuracy: 0.7418\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.96 - ETA: 0s - loss: 0.2571 - accuracy: 0.91 - ETA: 0s - loss: 0.2520 - accuracy: 0.90 - ETA: 0s - loss: 0.3230 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8791 - val_loss: 3.6317 - val_accuracy: 0.7493\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.9832 - accuracy: 0.68 - ETA: 0s - loss: 0.6220 - accuracy: 0.81 - ETA: 0s - loss: 0.5583 - accuracy: 0.82 - ETA: 0s - loss: 0.5356 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5295 - accuracy: 0.8358 - val_loss: 1.1182 - val_accuracy: 0.6463\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.84 - ETA: 0s - loss: 0.3792 - accuracy: 0.87 - ETA: 0s - loss: 0.3435 - accuracy: 0.88 - ETA: 0s - loss: 0.3518 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8862 - val_loss: 0.9930 - val_accuracy: 0.7134\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5658 - accuracy: 0.71 - ETA: 0s - loss: 0.2668 - accuracy: 0.90 - ETA: 0s - loss: 0.2703 - accuracy: 0.90 - ETA: 0s - loss: 0.2905 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2861 - accuracy: 0.9003 - val_loss: 1.1539 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3163 - accuracy: 0.90 - ETA: 0s - loss: 0.2142 - accuracy: 0.94 - ETA: 0s - loss: 0.2546 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2845 - accuracy: 0.9209 - val_loss: 1.0774 - val_accuracy: 0.7224\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1834 - accuracy: 0.90 - ETA: 0s - loss: 0.3163 - accuracy: 0.91 - ETA: 0s - loss: 0.3056 - accuracy: 0.91 - 0s 2ms/step - loss: 0.2719 - accuracy: 0.9224 - val_loss: 0.9445 - val_accuracy: 0.7254\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.90 - ETA: 0s - loss: 0.2599 - accuracy: 0.92 - ETA: 0s - loss: 0.2628 - accuracy: 0.92 - ETA: 0s - loss: 0.2623 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2545 - accuracy: 0.9280 - val_loss: 1.4886 - val_accuracy: 0.7209\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 1.00 - ETA: 0s - loss: 0.2229 - accuracy: 0.94 - ETA: 0s - loss: 0.2705 - accuracy: 0.92 - ETA: 0s - loss: 0.2800 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2923 - accuracy: 0.9250 - val_loss: 1.7725 - val_accuracy: 0.7567\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.96 - ETA: 0s - loss: 0.5862 - accuracy: 0.89 - ETA: 0s - loss: 0.4991 - accuracy: 0.88 - ETA: 0s - loss: 0.4765 - accuracy: 0.87 - ETA: 0s - loss: 0.4536 - accuracy: 0.88 - 0s 3ms/step - loss: 0.4692 - accuracy: 0.8794 - val_loss: 0.8338 - val_accuracy: 0.7149\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.93 - ETA: 0s - loss: 0.3354 - accuracy: 0.91 - ETA: 0s - loss: 0.3172 - accuracy: 0.91 - ETA: 0s - loss: 0.3005 - accuracy: 0.92 - 0s 2ms/step - loss: 0.3005 - accuracy: 0.9216 - val_loss: 1.1457 - val_accuracy: 0.7090\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.93 - ETA: 0s - loss: 0.2141 - accuracy: 0.94 - ETA: 0s - loss: 0.2399 - accuracy: 0.93 - ETA: 0s - loss: 0.2561 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2557 - accuracy: 0.9306 - val_loss: 1.8858 - val_accuracy: 0.7000\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.96 - ETA: 0s - loss: 0.2722 - accuracy: 0.92 - ETA: 0s - loss: 0.3556 - accuracy: 0.92 - ETA: 0s - loss: 0.3377 - accuracy: 0.92 - 0s 2ms/step - loss: 0.3377 - accuracy: 0.9212 - val_loss: 2.0561 - val_accuracy: 0.7060\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.96 - ETA: 0s - loss: 0.2789 - accuracy: 0.92 - ETA: 0s - loss: 0.2777 - accuracy: 0.92 - ETA: 0s - loss: 0.2730 - accuracy: 0.93 - 0s 2ms/step - loss: 0.2743 - accuracy: 0.9313 - val_loss: 0.9117 - val_accuracy: 0.7418\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.90 - ETA: 0s - loss: 0.2428 - accuracy: 0.93 - ETA: 0s - loss: 0.2474 - accuracy: 0.93 - ETA: 0s - loss: 0.2375 - accuracy: 0.93 - 0s 3ms/step - loss: 0.2425 - accuracy: 0.9384 - val_loss: 1.2989 - val_accuracy: 0.7254\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.96 - ETA: 0s - loss: 0.2352 - accuracy: 0.93 - ETA: 0s - loss: 0.2289 - accuracy: 0.94 - ETA: 0s - loss: 0.2498 - accuracy: 0.94 - 0s 3ms/step - loss: 0.2400 - accuracy: 0.9414 - val_loss: 1.0832 - val_accuracy: 0.7343\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3966 - accuracy: 0.90 - ETA: 0s - loss: 0.2067 - accuracy: 0.95 - ETA: 0s - loss: 0.2089 - accuracy: 0.95 - ETA: 0s - loss: 0.2087 - accuracy: 0.95 - 0s 3ms/step - loss: 0.2021 - accuracy: 0.9515 - val_loss: 1.6860 - val_accuracy: 0.7179\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.96 - ETA: 0s - loss: 0.1523 - accuracy: 0.97 - ETA: 0s - loss: 0.1641 - accuracy: 0.96 - 0s 2ms/step - loss: 0.1777 - accuracy: 0.9574 - val_loss: 1.9019 - val_accuracy: 0.7194\n",
      "Epoch 35/50\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.1698 - accuracy: 0.96 - ETA: 0s - loss: 0.1440 - accuracy: 0.96 - ETA: 0s - loss: 0.1628 - accuracy: 0.96 - ETA: 0s - loss: 0.1931 - accuracy: 0.9533Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9526 - val_loss: 2.0712 - val_accuracy: 0.6925\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.75 - ETA: 0s - loss: 0.9728 - accuracy: 0.63 - ETA: 0s - loss: 0.7686 - accuracy: 0.68 - ETA: 0s - loss: 0.7061 - accuracy: 0.70 - 0s 4ms/step - loss: 0.7041 - accuracy: 0.7025 - val_loss: 0.6782 - val_accuracy: 0.6373\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.71 - ETA: 0s - loss: 0.5070 - accuracy: 0.74 - ETA: 0s - loss: 0.5125 - accuracy: 0.74 - ETA: 0s - loss: 0.5152 - accuracy: 0.74 - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7469 - val_loss: 0.5970 - val_accuracy: 0.7179\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.84 - ETA: 0s - loss: 0.4200 - accuracy: 0.80 - ETA: 0s - loss: 0.4363 - accuracy: 0.80 - 0s 2ms/step - loss: 0.4442 - accuracy: 0.7913 - val_loss: 0.6819 - val_accuracy: 0.6881\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.84 - ETA: 0s - loss: 0.3535 - accuracy: 0.83 - ETA: 0s - loss: 0.3671 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8223 - val_loss: 0.6049 - val_accuracy: 0.6776\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.87 - ETA: 0s - loss: 0.3351 - accuracy: 0.84 - ETA: 0s - loss: 0.3484 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8339 - val_loss: 0.9059 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.96 - ETA: 0s - loss: 0.3583 - accuracy: 0.85 - ETA: 0s - loss: 0.3573 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3739 - accuracy: 0.8346 - val_loss: 0.8761 - val_accuracy: 0.6657\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.84 - ETA: 0s - loss: 0.3702 - accuracy: 0.84 - ETA: 0s - loss: 0.3855 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8399 - val_loss: 1.3328 - val_accuracy: 0.6493\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.93 - ETA: 0s - loss: 0.3267 - accuracy: 0.85 - ETA: 0s - loss: 0.3503 - accuracy: 0.84 - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8410 - val_loss: 1.2185 - val_accuracy: 0.6985\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.81 - ETA: 0s - loss: 0.4120 - accuracy: 0.84 - ETA: 0s - loss: 0.3988 - accuracy: 0.83 - ETA: 0s - loss: 0.3820 - accuracy: 0.83 - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8395 - val_loss: 2.1111 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.84 - ETA: 0s - loss: 0.3625 - accuracy: 0.86 - ETA: 0s - loss: 0.3968 - accuracy: 0.84 - ETA: 0s - loss: 0.4149 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8522 - val_loss: 1.1843 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.84 - ETA: 0s - loss: 0.3466 - accuracy: 0.86 - ETA: 0s - loss: 0.3555 - accuracy: 0.85 - ETA: 0s - loss: 0.3406 - accuracy: 0.85 - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8582 - val_loss: 1.7755 - val_accuracy: 0.6881\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.93 - ETA: 0s - loss: 0.2992 - accuracy: 0.87 - ETA: 0s - loss: 0.3032 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8802 - val_loss: 2.6456 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.93 - ETA: 0s - loss: 0.3763 - accuracy: 0.86 - ETA: 0s - loss: 0.3984 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8541 - val_loss: 2.3696 - val_accuracy: 0.7478\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.90 - ETA: 0s - loss: 0.3071 - accuracy: 0.87 - ETA: 0s - loss: 0.2812 - accuracy: 0.88 - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8735 - val_loss: 3.7264 - val_accuracy: 0.7090\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.90 - ETA: 0s - loss: 0.3007 - accuracy: 0.88 - ETA: 0s - loss: 0.4230 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8764 - val_loss: 1.4499 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.84 - ETA: 0s - loss: 0.4010 - accuracy: 0.89 - ETA: 0s - loss: 0.4603 - accuracy: 0.85 - ETA: 0s - loss: 0.4294 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8526 - val_loss: 4.0279 - val_accuracy: 0.7060\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.96 - ETA: 0s - loss: 0.4329 - accuracy: 0.89 - ETA: 0s - loss: 0.4361 - accuracy: 0.87 - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8738 - val_loss: 1.5298 - val_accuracy: 0.7015\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.84 - ETA: 0s - loss: 0.4248 - accuracy: 0.87 - ETA: 0s - loss: 0.3559 - accuracy: 0.89 - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8932 - val_loss: 1.4106 - val_accuracy: 0.6806\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.87 - ETA: 0s - loss: 0.2348 - accuracy: 0.91 - ETA: 0s - loss: 0.2927 - accuracy: 0.92 - 0s 2ms/step - loss: 0.3306 - accuracy: 0.9044 - val_loss: 1.7391 - val_accuracy: 0.7239\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.93 - ETA: 0s - loss: 0.2732 - accuracy: 0.89 - ETA: 0s - loss: 0.2943 - accuracy: 0.89 - 0s 2ms/step - loss: 0.2886 - accuracy: 0.9000 - val_loss: 6.5152 - val_accuracy: 0.6985\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.96 - ETA: 0s - loss: 0.2535 - accuracy: 0.91 - ETA: 0s - loss: 0.3915 - accuracy: 0.92 - ETA: 0s - loss: 0.3581 - accuracy: 0.91 - 0s 2ms/step - loss: 0.3558 - accuracy: 0.9134 - val_loss: 1.8622 - val_accuracy: 0.7388\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.93 - ETA: 0s - loss: 0.2459 - accuracy: 0.92 - ETA: 0s - loss: 0.2397 - accuracy: 0.92 - ETA: 0s - loss: 0.2371 - accuracy: 0.92 - 0s 2ms/step - loss: 0.2378 - accuracy: 0.9231 - val_loss: 5.0717 - val_accuracy: 0.6642\n",
      "Epoch 23/50\n",
      "57/84 [===================>..........] - ETA: 0s - loss: 0.3532 - accuracy: 0.90 - ETA: 0s - loss: 0.4128 - accuracy: 0.87 - ETA: 0s - loss: 0.3514 - accuracy: 0.8838Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8962 - val_loss: 1.5353 - val_accuracy: 0.7254\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d82beb61ef1c874f038f3c17b2d48c2c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7487562298774719</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.009279595326486034</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 80</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7526 - accuracy: 0.50 - ETA: 0s - loss: 1.1224 - accuracy: 0.57 - ETA: 0s - loss: 0.9516 - accuracy: 0.63 - ETA: 0s - loss: 0.8784 - accuracy: 0.65 - ETA: 0s - loss: 0.8072 - accuracy: 0.66 - ETA: 0s - loss: 0.7790 - accuracy: 0.66 - 0s 6ms/step - loss: 0.7695 - accuracy: 0.6685 - val_loss: 0.6104 - val_accuracy: 0.7164\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.71 - ETA: 0s - loss: 0.6030 - accuracy: 0.71 - ETA: 0s - loss: 0.6099 - accuracy: 0.71 - ETA: 0s - loss: 0.5998 - accuracy: 0.72 - ETA: 0s - loss: 0.6035 - accuracy: 0.72 - ETA: 0s - loss: 0.5947 - accuracy: 0.72 - 0s 4ms/step - loss: 0.6009 - accuracy: 0.7227 - val_loss: 0.6111 - val_accuracy: 0.6418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4820 - accuracy: 0.68 - ETA: 0s - loss: 0.5115 - accuracy: 0.76 - ETA: 0s - loss: 0.4937 - accuracy: 0.78 - ETA: 0s - loss: 0.5136 - accuracy: 0.78 - ETA: 0s - loss: 0.5243 - accuracy: 0.77 - ETA: 0s - loss: 0.5128 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5145 - accuracy: 0.7790 - val_loss: 0.6567 - val_accuracy: 0.6836\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.78 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4588 - accuracy: 0.80 - ETA: 0s - loss: 0.4628 - accuracy: 0.81 - ETA: 0s - loss: 0.4733 - accuracy: 0.80 - ETA: 0s - loss: 0.4699 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4649 - accuracy: 0.8126 - val_loss: 0.5648 - val_accuracy: 0.7537\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.96 - ETA: 0s - loss: 0.3933 - accuracy: 0.83 - ETA: 0s - loss: 0.3919 - accuracy: 0.83 - ETA: 0s - loss: 0.4039 - accuracy: 0.83 - ETA: 0s - loss: 0.4406 - accuracy: 0.82 - ETA: 0s - loss: 0.4427 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4421 - accuracy: 0.8182 - val_loss: 0.6376 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.84 - ETA: 0s - loss: 0.4254 - accuracy: 0.85 - ETA: 0s - loss: 0.4349 - accuracy: 0.84 - ETA: 0s - loss: 0.4225 - accuracy: 0.85 - ETA: 0s - loss: 0.4223 - accuracy: 0.84 - ETA: 0s - loss: 0.4105 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4073 - accuracy: 0.8402 - val_loss: 0.7080 - val_accuracy: 0.7090\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.81 - ETA: 0s - loss: 0.3296 - accuracy: 0.83 - ETA: 0s - loss: 0.4280 - accuracy: 0.82 - ETA: 0s - loss: 0.4147 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.83 - ETA: 0s - loss: 0.3970 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3995 - accuracy: 0.8384 - val_loss: 0.5659 - val_accuracy: 0.7254\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3063 - accuracy: 0.90 - ETA: 0s - loss: 0.2856 - accuracy: 0.89 - ETA: 0s - loss: 0.3462 - accuracy: 0.87 - ETA: 0s - loss: 0.3420 - accuracy: 0.88 - ETA: 0s - loss: 0.3321 - accuracy: 0.87 - ETA: 0s - loss: 0.3311 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3391 - accuracy: 0.8686 - val_loss: 0.6849 - val_accuracy: 0.6403\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.75 - ETA: 0s - loss: 0.3708 - accuracy: 0.82 - ETA: 0s - loss: 0.3572 - accuracy: 0.84 - ETA: 0s - loss: 0.3417 - accuracy: 0.85 - ETA: 0s - loss: 0.3529 - accuracy: 0.85 - ETA: 0s - loss: 0.3376 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3453 - accuracy: 0.8600 - val_loss: 1.7804 - val_accuracy: 0.6821\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.84 - ETA: 0s - loss: 0.3144 - accuracy: 0.86 - ETA: 0s - loss: 0.3215 - accuracy: 0.86 - ETA: 0s - loss: 0.3726 - accuracy: 0.86 - ETA: 0s - loss: 0.3763 - accuracy: 0.86 - ETA: 0s - loss: 0.3810 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3817 - accuracy: 0.8585 - val_loss: 0.7202 - val_accuracy: 0.7015\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.81 - ETA: 0s - loss: 0.3588 - accuracy: 0.84 - ETA: 0s - loss: 0.4455 - accuracy: 0.84 - ETA: 0s - loss: 0.4047 - accuracy: 0.85 - ETA: 0s - loss: 0.3984 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8559 - val_loss: 1.1921 - val_accuracy: 0.6940\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.2851 - accuracy: 0.88 - ETA: 0s - loss: 0.3570 - accuracy: 0.87 - ETA: 0s - loss: 0.4244 - accuracy: 0.84 - ETA: 0s - loss: 0.4373 - accuracy: 0.84 - ETA: 0s - loss: 0.4267 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4247 - accuracy: 0.8395 - val_loss: 1.4083 - val_accuracy: 0.6881\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.75 - ETA: 0s - loss: 0.6461 - accuracy: 0.81 - ETA: 0s - loss: 0.5463 - accuracy: 0.81 - ETA: 0s - loss: 0.4936 - accuracy: 0.82 - ETA: 0s - loss: 0.4490 - accuracy: 0.83 - ETA: 0s - loss: 0.4389 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4404 - accuracy: 0.8335 - val_loss: 1.7766 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.3228 - accuracy: 0.87 - ETA: 0s - loss: 0.5374 - accuracy: 0.82 - ETA: 0s - loss: 0.5073 - accuracy: 0.82 - ETA: 0s - loss: 0.4552 - accuracy: 0.83 - ETA: 0s - loss: 0.5077 - accuracy: 0.84 - ETA: 0s - loss: 0.5247 - accuracy: 0.8429Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5171 - accuracy: 0.8432 - val_loss: 0.5918 - val_accuracy: 0.7403\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.68 - ETA: 0s - loss: 1.3173 - accuracy: 0.61 - ETA: 0s - loss: 1.0387 - accuracy: 0.62 - ETA: 0s - loss: 0.9080 - accuracy: 0.64 - ETA: 0s - loss: 0.8291 - accuracy: 0.65 - ETA: 0s - loss: 0.7846 - accuracy: 0.66 - 0s 5ms/step - loss: 0.7803 - accuracy: 0.6685 - val_loss: 0.6264 - val_accuracy: 0.6582\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.81 - ETA: 0s - loss: 0.5385 - accuracy: 0.73 - ETA: 0s - loss: 0.5535 - accuracy: 0.72 - ETA: 0s - loss: 0.5493 - accuracy: 0.73 - ETA: 0s - loss: 0.5554 - accuracy: 0.73 - ETA: 0s - loss: 0.5527 - accuracy: 0.73 - 0s 4ms/step - loss: 0.5524 - accuracy: 0.7353 - val_loss: 0.5426 - val_accuracy: 0.7224\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.84 - ETA: 0s - loss: 0.5296 - accuracy: 0.76 - ETA: 0s - loss: 0.5102 - accuracy: 0.77 - ETA: 0s - loss: 0.5037 - accuracy: 0.77 - ETA: 0s - loss: 0.5120 - accuracy: 0.77 - ETA: 0s - loss: 0.4995 - accuracy: 0.77 - 0s 4ms/step - loss: 0.4987 - accuracy: 0.7738 - val_loss: 0.5427 - val_accuracy: 0.7328\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.84 - ETA: 0s - loss: 0.3918 - accuracy: 0.83 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.4108 - accuracy: 0.80 - ETA: 0s - loss: 0.4155 - accuracy: 0.80 - ETA: 0s - loss: 0.4110 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8059 - val_loss: 0.6991 - val_accuracy: 0.7119\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.84 - ETA: 0s - loss: 0.3697 - accuracy: 0.85 - ETA: 0s - loss: 0.3532 - accuracy: 0.85 - ETA: 0s - loss: 0.3624 - accuracy: 0.85 - ETA: 0s - loss: 0.3687 - accuracy: 0.84 - ETA: 0s - loss: 0.3825 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3807 - accuracy: 0.8387 - val_loss: 0.7975 - val_accuracy: 0.6239\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.84 - ETA: 0s - loss: 0.3426 - accuracy: 0.83 - ETA: 0s - loss: 0.3780 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3807 - accuracy: 0.83 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - 0s 4ms/step - loss: 0.3917 - accuracy: 0.8246 - val_loss: 0.8918 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.87 - ETA: 0s - loss: 0.3286 - accuracy: 0.82 - ETA: 0s - loss: 0.3599 - accuracy: 0.81 - ETA: 0s - loss: 0.3899 - accuracy: 0.81 - ETA: 0s - loss: 0.3873 - accuracy: 0.81 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - 0s 4ms/step - loss: 0.4230 - accuracy: 0.8089 - val_loss: 0.7356 - val_accuracy: 0.6552\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.87 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3457 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.83 - 0s 4ms/step - loss: 0.3510 - accuracy: 0.8384 - val_loss: 1.1163 - val_accuracy: 0.6328\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.78 - ETA: 0s - loss: 0.3411 - accuracy: 0.84 - ETA: 0s - loss: 0.3287 - accuracy: 0.85 - ETA: 0s - loss: 0.3140 - accuracy: 0.86 - ETA: 0s - loss: 0.3066 - accuracy: 0.86 - ETA: 0s - loss: 0.3091 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3091 - accuracy: 0.8638 - val_loss: 1.0097 - val_accuracy: 0.6806\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.87 - ETA: 0s - loss: 0.2548 - accuracy: 0.87 - ETA: 0s - loss: 0.2482 - accuracy: 0.89 - ETA: 0s - loss: 0.2792 - accuracy: 0.88 - ETA: 0s - loss: 0.2968 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3013 - accuracy: 0.8750 - val_loss: 1.0188 - val_accuracy: 0.6821\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.93 - ETA: 0s - loss: 0.3308 - accuracy: 0.88 - ETA: 0s - loss: 0.3298 - accuracy: 0.87 - ETA: 0s - loss: 0.3544 - accuracy: 0.86 - ETA: 0s - loss: 0.3521 - accuracy: 0.86 - ETA: 0s - loss: 0.3668 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3668 - accuracy: 0.8582 - val_loss: 1.0798 - val_accuracy: 0.6925\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 1.00 - ETA: 0s - loss: 0.3088 - accuracy: 0.88 - ETA: 0s - loss: 0.3883 - accuracy: 0.85 - ETA: 0s - loss: 0.4507 - accuracy: 0.83 - ETA: 0s - loss: 0.4855 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4935 - accuracy: 0.8063 - val_loss: 0.9853 - val_accuracy: 0.6672\n",
      "Epoch 13/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.71 - ETA: 0s - loss: 0.5413 - accuracy: 0.82 - ETA: 0s - loss: 0.5310 - accuracy: 0.81 - ETA: 0s - loss: 0.4776 - accuracy: 0.82 - ETA: 0s - loss: 0.4513 - accuracy: 0.83 - ETA: 0s - loss: 0.4494 - accuracy: 0.8366Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.8369 - val_loss: 0.7009 - val_accuracy: 0.7045\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8320 - accuracy: 0.65 - ETA: 0s - loss: 1.4858 - accuracy: 0.61 - ETA: 0s - loss: 1.0546 - accuracy: 0.65 - ETA: 0s - loss: 0.9472 - accuracy: 0.66 - ETA: 0s - loss: 0.8700 - accuracy: 0.67 - ETA: 0s - loss: 0.8194 - accuracy: 0.67 - 0s 5ms/step - loss: 0.8059 - accuracy: 0.6816 - val_loss: 0.5722 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4851 - accuracy: 0.78 - ETA: 0s - loss: 0.5585 - accuracy: 0.75 - ETA: 0s - loss: 0.5471 - accuracy: 0.75 - ETA: 0s - loss: 0.5745 - accuracy: 0.72 - ETA: 0s - loss: 0.5644 - accuracy: 0.73 - ETA: 0s - loss: 0.5615 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5561 - accuracy: 0.7424 - val_loss: 0.6456 - val_accuracy: 0.6881\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4757 - accuracy: 0.78 - ETA: 0s - loss: 0.4668 - accuracy: 0.79 - ETA: 0s - loss: 0.4928 - accuracy: 0.79 - ETA: 0s - loss: 0.5011 - accuracy: 0.79 - ETA: 0s - loss: 0.5028 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7869 - val_loss: 0.6856 - val_accuracy: 0.7075\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.65 - ETA: 0s - loss: 0.4652 - accuracy: 0.79 - ETA: 0s - loss: 0.4636 - accuracy: 0.81 - ETA: 0s - loss: 0.4681 - accuracy: 0.80 - ETA: 0s - loss: 0.4563 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4603 - accuracy: 0.8093 - val_loss: 0.5876 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.84 - ETA: 0s - loss: 0.3962 - accuracy: 0.85 - ETA: 0s - loss: 0.3926 - accuracy: 0.84 - ETA: 0s - loss: 0.4010 - accuracy: 0.84 - ETA: 0s - loss: 0.4230 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4311 - accuracy: 0.8268 - val_loss: 0.6042 - val_accuracy: 0.7104\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.68 - ETA: 0s - loss: 0.4361 - accuracy: 0.84 - ETA: 0s - loss: 0.4004 - accuracy: 0.85 - ETA: 0s - loss: 0.4019 - accuracy: 0.83 - ETA: 0s - loss: 0.3979 - accuracy: 0.84 - ETA: 0s - loss: 0.3957 - accuracy: 0.84 - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8451 - val_loss: 0.6288 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.96 - ETA: 0s - loss: 0.3358 - accuracy: 0.86 - ETA: 0s - loss: 0.3131 - accuracy: 0.87 - ETA: 0s - loss: 0.3463 - accuracy: 0.87 - ETA: 0s - loss: 0.3636 - accuracy: 0.86 - ETA: 0s - loss: 0.3668 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3661 - accuracy: 0.8638 - val_loss: 1.1107 - val_accuracy: 0.6791\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.90 - ETA: 0s - loss: 0.4083 - accuracy: 0.82 - ETA: 0s - loss: 0.3599 - accuracy: 0.85 - ETA: 0s - loss: 0.3444 - accuracy: 0.86 - ETA: 0s - loss: 0.3350 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3442 - accuracy: 0.8593 - val_loss: 0.6450 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.90 - ETA: 0s - loss: 0.2621 - accuracy: 0.89 - ETA: 0s - loss: 0.2865 - accuracy: 0.88 - ETA: 0s - loss: 0.3104 - accuracy: 0.87 - ETA: 0s - loss: 0.3175 - accuracy: 0.86 - ETA: 0s - loss: 0.3224 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3234 - accuracy: 0.8697 - val_loss: 1.4326 - val_accuracy: 0.6612\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.87 - ETA: 0s - loss: 0.2957 - accuracy: 0.86 - ETA: 0s - loss: 0.3011 - accuracy: 0.87 - ETA: 0s - loss: 0.3192 - accuracy: 0.87 - ETA: 0s - loss: 0.5191 - accuracy: 0.85 - ETA: 0s - loss: 0.5660 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5660 - accuracy: 0.8443 - val_loss: 1.0554 - val_accuracy: 0.6910\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.87 - ETA: 0s - loss: 0.4184 - accuracy: 0.84 - ETA: 0s - loss: 0.4135 - accuracy: 0.84 - ETA: 0s - loss: 0.4028 - accuracy: 0.85 - ETA: 0s - loss: 0.4347 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4359 - accuracy: 0.8496 - val_loss: 0.8532 - val_accuracy: 0.7522\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.87 - ETA: 0s - loss: 0.6175 - accuracy: 0.80 - ETA: 0s - loss: 0.5594 - accuracy: 0.80 - ETA: 0s - loss: 0.5380 - accuracy: 0.81 - ETA: 0s - loss: 0.5291 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5109 - accuracy: 0.8122 - val_loss: 0.6384 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.87 - ETA: 0s - loss: 0.3333 - accuracy: 0.85 - ETA: 0s - loss: 0.3398 - accuracy: 0.85 - ETA: 0s - loss: 0.3485 - accuracy: 0.85 - ETA: 0s - loss: 0.3525 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8582 - val_loss: 0.9313 - val_accuracy: 0.7090\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.93 - ETA: 0s - loss: 0.5524 - accuracy: 0.89 - ETA: 0s - loss: 0.4387 - accuracy: 0.88 - ETA: 0s - loss: 0.4123 - accuracy: 0.87 - ETA: 0s - loss: 0.3949 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8787 - val_loss: 1.1663 - val_accuracy: 0.6761\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.90 - ETA: 0s - loss: 0.4238 - accuracy: 0.83 - ETA: 0s - loss: 0.3975 - accuracy: 0.84 - ETA: 0s - loss: 0.3782 - accuracy: 0.85 - ETA: 0s - loss: 0.3673 - accuracy: 0.85 - ETA: 0s - loss: 0.3823 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3823 - accuracy: 0.8522 - val_loss: 0.9897 - val_accuracy: 0.6940\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.84 - ETA: 0s - loss: 0.2983 - accuracy: 0.87 - ETA: 0s - loss: 0.2911 - accuracy: 0.89 - ETA: 0s - loss: 0.3030 - accuracy: 0.88 - ETA: 0s - loss: 0.3043 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8675 - val_loss: 0.6607 - val_accuracy: 0.7373\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.78 - ETA: 0s - loss: 0.4481 - accuracy: 0.86 - ETA: 0s - loss: 0.4107 - accuracy: 0.87 - ETA: 0s - loss: 0.4194 - accuracy: 0.86 - ETA: 0s - loss: 0.3939 - accuracy: 0.87 - ETA: 0s - loss: 0.3903 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3882 - accuracy: 0.8652 - val_loss: 0.8426 - val_accuracy: 0.7254\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.96 - ETA: 0s - loss: 0.3536 - accuracy: 0.87 - ETA: 0s - loss: 0.3208 - accuracy: 0.88 - ETA: 0s - loss: 0.3277 - accuracy: 0.88 - ETA: 0s - loss: 0.3247 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8888 - val_loss: 1.0125 - val_accuracy: 0.7104\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.93 - ETA: 0s - loss: 0.2686 - accuracy: 0.91 - ETA: 0s - loss: 0.2570 - accuracy: 0.91 - ETA: 0s - loss: 0.2685 - accuracy: 0.91 - ETA: 0s - loss: 0.2740 - accuracy: 0.91 - ETA: 0s - loss: 0.2776 - accuracy: 0.91 - 0s 4ms/step - loss: 0.2807 - accuracy: 0.9115 - val_loss: 1.4439 - val_accuracy: 0.7239\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.93 - ETA: 0s - loss: 0.4857 - accuracy: 0.85 - ETA: 0s - loss: 0.3978 - accuracy: 0.86 - ETA: 0s - loss: 0.4688 - accuracy: 0.84 - ETA: 0s - loss: 0.4683 - accuracy: 0.84 - ETA: 0s - loss: 0.4585 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4514 - accuracy: 0.8429 - val_loss: 0.8548 - val_accuracy: 0.7507\n",
      "Epoch 21/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2277 - accuracy: 0.93 - ETA: 0s - loss: 0.4445 - accuracy: 0.85 - ETA: 0s - loss: 0.4328 - accuracy: 0.84 - ETA: 0s - loss: 0.4588 - accuracy: 0.82 - ETA: 0s - loss: 0.4577 - accuracy: 0.82 - ETA: 0s - loss: 0.4537 - accuracy: 0.8318Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.8339 - val_loss: 1.1090 - val_accuracy: 0.7328\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b504b90b62eb3ba82d99818605612249</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7462686697642008</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.31253660273302675</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 200</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.68 - ETA: 0s - loss: 4.1333 - accuracy: 0.59 - ETA: 0s - loss: 2.5054 - accuracy: 0.62 - ETA: 0s - loss: 1.8973 - accuracy: 0.65 - ETA: 0s - loss: 1.6125 - accuracy: 0.65 - ETA: 0s - loss: 1.4176 - accuracy: 0.66 - ETA: 0s - loss: 1.2842 - accuracy: 0.68 - ETA: 0s - loss: 1.1927 - accuracy: 0.69 - 1s 7ms/step - loss: 1.1744 - accuracy: 0.6939 - val_loss: 0.6003 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7451 - accuracy: 0.59 - ETA: 0s - loss: 0.5824 - accuracy: 0.75 - ETA: 0s - loss: 0.6641 - accuracy: 0.73 - ETA: 0s - loss: 0.6640 - accuracy: 0.71 - ETA: 0s - loss: 0.6446 - accuracy: 0.73 - ETA: 0s - loss: 0.6367 - accuracy: 0.73 - ETA: 0s - loss: 0.6469 - accuracy: 0.74 - ETA: 0s - loss: 0.6415 - accuracy: 0.74 - 0s 6ms/step - loss: 0.6408 - accuracy: 0.7428 - val_loss: 0.5830 - val_accuracy: 0.7418\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7441 - accuracy: 0.71 - ETA: 0s - loss: 0.5705 - accuracy: 0.79 - ETA: 0s - loss: 0.5596 - accuracy: 0.80 - ETA: 0s - loss: 0.5582 - accuracy: 0.79 - ETA: 0s - loss: 0.5651 - accuracy: 0.77 - ETA: 0s - loss: 0.5673 - accuracy: 0.77 - ETA: 0s - loss: 0.5655 - accuracy: 0.77 - ETA: 0s - loss: 0.5681 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5688 - accuracy: 0.7798 - val_loss: 0.5712 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.81 - ETA: 0s - loss: 0.5425 - accuracy: 0.80 - ETA: 0s - loss: 0.5956 - accuracy: 0.79 - ETA: 0s - loss: 0.5772 - accuracy: 0.79 - ETA: 0s - loss: 0.5798 - accuracy: 0.79 - ETA: 0s - loss: 0.5699 - accuracy: 0.79 - ETA: 0s - loss: 0.5711 - accuracy: 0.79 - ETA: 0s - loss: 0.5730 - accuracy: 0.78 - 1s 6ms/step - loss: 0.5755 - accuracy: 0.7876 - val_loss: 0.5554 - val_accuracy: 0.7701\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.87 - ETA: 0s - loss: 0.5438 - accuracy: 0.82 - ETA: 0s - loss: 0.5718 - accuracy: 0.81 - ETA: 0s - loss: 0.5684 - accuracy: 0.80 - ETA: 0s - loss: 0.5641 - accuracy: 0.79 - ETA: 0s - loss: 0.5600 - accuracy: 0.79 - ETA: 0s - loss: 0.5664 - accuracy: 0.79 - ETA: 0s - loss: 0.5602 - accuracy: 0.79 - ETA: 0s - loss: 0.5702 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5698 - accuracy: 0.7954 - val_loss: 0.5944 - val_accuracy: 0.7433\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.87 - ETA: 0s - loss: 0.5753 - accuracy: 0.82 - ETA: 0s - loss: 0.5884 - accuracy: 0.79 - ETA: 0s - loss: 0.5850 - accuracy: 0.79 - ETA: 0s - loss: 0.5721 - accuracy: 0.79 - ETA: 0s - loss: 0.5725 - accuracy: 0.80 - ETA: 0s - loss: 0.5706 - accuracy: 0.79 - ETA: 0s - loss: 0.5644 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5569 - accuracy: 0.8059 - val_loss: 0.7060 - val_accuracy: 0.7403\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.75 - ETA: 0s - loss: 0.5009 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.83 - ETA: 0s - loss: 0.5080 - accuracy: 0.83 - ETA: 0s - loss: 0.5400 - accuracy: 0.81 - ETA: 0s - loss: 0.5433 - accuracy: 0.80 - ETA: 0s - loss: 0.5509 - accuracy: 0.81 - ETA: 0s - loss: 0.5576 - accuracy: 0.80 - ETA: 0s - loss: 0.5576 - accuracy: 0.80 - 1s 6ms/step - loss: 0.5612 - accuracy: 0.8025 - val_loss: 0.7665 - val_accuracy: 0.7075\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8358 - accuracy: 0.65 - ETA: 0s - loss: 0.5932 - accuracy: 0.80 - ETA: 0s - loss: 0.5739 - accuracy: 0.80 - ETA: 0s - loss: 0.5703 - accuracy: 0.80 - ETA: 0s - loss: 0.5720 - accuracy: 0.80 - ETA: 0s - loss: 0.5828 - accuracy: 0.80 - ETA: 0s - loss: 0.5911 - accuracy: 0.79 - ETA: 0s - loss: 0.5772 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5785 - accuracy: 0.8010 - val_loss: 0.6591 - val_accuracy: 0.7343\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.87 - ETA: 0s - loss: 0.5476 - accuracy: 0.80 - ETA: 0s - loss: 0.5355 - accuracy: 0.80 - ETA: 0s - loss: 0.5624 - accuracy: 0.80 - ETA: 0s - loss: 0.6071 - accuracy: 0.81 - ETA: 0s - loss: 0.6004 - accuracy: 0.80 - ETA: 0s - loss: 0.5957 - accuracy: 0.81 - ETA: 0s - loss: 0.5953 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5891 - accuracy: 0.8085 - val_loss: 0.7028 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.65 - ETA: 0s - loss: 0.6248 - accuracy: 0.77 - ETA: 0s - loss: 0.6335 - accuracy: 0.78 - ETA: 0s - loss: 0.6087 - accuracy: 0.80 - ETA: 0s - loss: 0.6182 - accuracy: 0.79 - ETA: 0s - loss: 0.5882 - accuracy: 0.80 - ETA: 0s - loss: 0.6593 - accuracy: 0.79 - ETA: 0s - loss: 0.6515 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6427 - accuracy: 0.7977 - val_loss: 0.6968 - val_accuracy: 0.7567\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9737 - accuracy: 0.65 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.5852 - accuracy: 0.79 - ETA: 0s - loss: 0.6187 - accuracy: 0.78 - ETA: 0s - loss: 0.6610 - accuracy: 0.77 - ETA: 0s - loss: 0.6948 - accuracy: 0.77 - ETA: 0s - loss: 0.6721 - accuracy: 0.78 - ETA: 0s - loss: 0.6700 - accuracy: 0.78 - ETA: 0s - loss: 0.6575 - accuracy: 0.78 - 0s 6ms/step - loss: 0.6575 - accuracy: 0.7872 - val_loss: 1.0815 - val_accuracy: 0.6791\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5169 - accuracy: 0.71 - ETA: 0s - loss: 0.8972 - accuracy: 0.80 - ETA: 0s - loss: 0.8061 - accuracy: 0.78 - ETA: 0s - loss: 0.8343 - accuracy: 0.80 - ETA: 0s - loss: 0.7938 - accuracy: 0.78 - ETA: 0s - loss: 0.7604 - accuracy: 0.78 - ETA: 0s - loss: 0.7424 - accuracy: 0.78 - ETA: 0s - loss: 0.8308 - accuracy: 0.77 - ETA: 0s - loss: 0.8105 - accuracy: 0.76 - 0s 6ms/step - loss: 0.8105 - accuracy: 0.7682 - val_loss: 0.9655 - val_accuracy: 0.7313\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.81 - ETA: 0s - loss: 0.6075 - accuracy: 0.79 - ETA: 0s - loss: 0.6458 - accuracy: 0.79 - ETA: 0s - loss: 0.6297 - accuracy: 0.78 - ETA: 0s - loss: 0.6216 - accuracy: 0.78 - ETA: 0s - loss: 0.6099 - accuracy: 0.78 - ETA: 0s - loss: 0.6156 - accuracy: 0.78 - ETA: 0s - loss: 0.6189 - accuracy: 0.77 - ETA: 0s - loss: 0.7224 - accuracy: 0.77 - 0s 6ms/step - loss: 0.7170 - accuracy: 0.7749 - val_loss: 0.7294 - val_accuracy: 0.6970\n",
      "Epoch 14/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.8216 - accuracy: 0.59 - ETA: 0s - loss: 0.7557 - accuracy: 0.74 - ETA: 0s - loss: 0.6935 - accuracy: 0.75 - ETA: 0s - loss: 0.6968 - accuracy: 0.75 - ETA: 0s - loss: 0.6953 - accuracy: 0.75 - ETA: 0s - loss: 0.6800 - accuracy: 0.75 - ETA: 0s - loss: 0.7079 - accuracy: 0.74 - ETA: 0s - loss: 0.6958 - accuracy: 0.75 - ETA: 0s - loss: 0.6865 - accuracy: 0.7534Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.6882 - accuracy: 0.7525 - val_loss: 0.6193 - val_accuracy: 0.7463\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7984 - accuracy: 0.71 - ETA: 0s - loss: 5.9801 - accuracy: 0.58 - ETA: 0s - loss: 3.5114 - accuracy: 0.60 - ETA: 0s - loss: 2.5145 - accuracy: 0.63 - ETA: 0s - loss: 2.0179 - accuracy: 0.66 - ETA: 0s - loss: 1.7689 - accuracy: 0.64 - ETA: 0s - loss: 1.5818 - accuracy: 0.64 - ETA: 0s - loss: 1.4440 - accuracy: 0.66 - 1s 6ms/step - loss: 1.4211 - accuracy: 0.6629 - val_loss: 0.5685 - val_accuracy: 0.7403\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.78 - ETA: 0s - loss: 0.6614 - accuracy: 0.73 - ETA: 0s - loss: 0.6237 - accuracy: 0.71 - ETA: 0s - loss: 0.6181 - accuracy: 0.72 - ETA: 0s - loss: 0.6302 - accuracy: 0.72 - ETA: 0s - loss: 0.6340 - accuracy: 0.72 - ETA: 0s - loss: 0.6433 - accuracy: 0.71 - ETA: 0s - loss: 0.6365 - accuracy: 0.71 - 0s 5ms/step - loss: 0.6370 - accuracy: 0.7189 - val_loss: 0.6064 - val_accuracy: 0.7388\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.75 - ETA: 0s - loss: 0.5576 - accuracy: 0.77 - ETA: 0s - loss: 0.5563 - accuracy: 0.78 - ETA: 0s - loss: 0.5971 - accuracy: 0.78 - ETA: 0s - loss: 0.6035 - accuracy: 0.77 - ETA: 0s - loss: 0.5908 - accuracy: 0.77 - ETA: 0s - loss: 0.5828 - accuracy: 0.77 - ETA: 0s - loss: 0.5777 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5786 - accuracy: 0.7786 - val_loss: 0.6169 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.84 - ETA: 0s - loss: 0.5177 - accuracy: 0.81 - ETA: 0s - loss: 0.5282 - accuracy: 0.81 - ETA: 0s - loss: 0.5226 - accuracy: 0.81 - ETA: 0s - loss: 0.5246 - accuracy: 0.81 - ETA: 0s - loss: 0.5335 - accuracy: 0.80 - ETA: 0s - loss: 0.5484 - accuracy: 0.79 - ETA: 0s - loss: 0.5459 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5421 - accuracy: 0.7992 - val_loss: 0.7518 - val_accuracy: 0.7090\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.81 - ETA: 0s - loss: 0.4978 - accuracy: 0.80 - ETA: 0s - loss: 0.4786 - accuracy: 0.82 - ETA: 0s - loss: 0.4948 - accuracy: 0.81 - ETA: 0s - loss: 0.5104 - accuracy: 0.81 - ETA: 0s - loss: 0.4950 - accuracy: 0.82 - ETA: 0s - loss: 0.4993 - accuracy: 0.82 - ETA: 0s - loss: 0.5012 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4951 - accuracy: 0.8227 - val_loss: 0.6450 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.84 - ETA: 0s - loss: 0.4075 - accuracy: 0.85 - ETA: 0s - loss: 0.4383 - accuracy: 0.84 - ETA: 0s - loss: 0.4425 - accuracy: 0.84 - ETA: 0s - loss: 0.4447 - accuracy: 0.84 - ETA: 0s - loss: 0.4468 - accuracy: 0.84 - ETA: 0s - loss: 0.4555 - accuracy: 0.83 - ETA: 0s - loss: 0.4619 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4615 - accuracy: 0.8343 - val_loss: 0.5763 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.81 - ETA: 0s - loss: 0.4516 - accuracy: 0.83 - ETA: 0s - loss: 0.4343 - accuracy: 0.83 - ETA: 0s - loss: 0.4644 - accuracy: 0.83 - ETA: 0s - loss: 0.4666 - accuracy: 0.83 - ETA: 0s - loss: 0.4714 - accuracy: 0.83 - ETA: 0s - loss: 0.4763 - accuracy: 0.83 - ETA: 0s - loss: 0.4843 - accuracy: 0.83 - ETA: 0s - loss: 0.5161 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5151 - accuracy: 0.8328 - val_loss: 1.0030 - val_accuracy: 0.7030\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.78 - ETA: 0s - loss: 0.4960 - accuracy: 0.82 - ETA: 0s - loss: 0.4451 - accuracy: 0.85 - ETA: 0s - loss: 0.4742 - accuracy: 0.84 - ETA: 0s - loss: 0.4791 - accuracy: 0.83 - ETA: 0s - loss: 0.4940 - accuracy: 0.83 - ETA: 0s - loss: 0.5165 - accuracy: 0.82 - ETA: 0s - loss: 0.5294 - accuracy: 0.82 - ETA: 0s - loss: 0.5321 - accuracy: 0.82 - 0s 6ms/step - loss: 0.5307 - accuracy: 0.8223 - val_loss: 0.7742 - val_accuracy: 0.7284\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.78 - ETA: 0s - loss: 0.7147 - accuracy: 0.81 - ETA: 0s - loss: 0.6101 - accuracy: 0.80 - ETA: 0s - loss: 0.5716 - accuracy: 0.81 - ETA: 0s - loss: 0.5625 - accuracy: 0.81 - ETA: 0s - loss: 0.5854 - accuracy: 0.81 - ETA: 0s - loss: 0.5805 - accuracy: 0.81 - ETA: 0s - loss: 0.6373 - accuracy: 0.81 - ETA: 0s - loss: 0.6291 - accuracy: 0.81 - ETA: 0s - loss: 0.6362 - accuracy: 0.81 - 1s 6ms/step - loss: 0.6354 - accuracy: 0.8163 - val_loss: 0.7416 - val_accuracy: 0.7209\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.87 - ETA: 0s - loss: 0.4620 - accuracy: 0.85 - ETA: 0s - loss: 0.4366 - accuracy: 0.86 - ETA: 0s - loss: 0.4520 - accuracy: 0.85 - ETA: 0s - loss: 0.4468 - accuracy: 0.85 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4976 - accuracy: 0.83 - ETA: 0s - loss: 0.5035 - accuracy: 0.83 - ETA: 0s - loss: 0.5138 - accuracy: 0.82 - 1s 6ms/step - loss: 0.5150 - accuracy: 0.8309 - val_loss: 0.7403 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.93 - ETA: 0s - loss: 0.5444 - accuracy: 0.82 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - ETA: 0s - loss: 0.5072 - accuracy: 0.83 - ETA: 0s - loss: 0.5104 - accuracy: 0.82 - ETA: 0s - loss: 0.5250 - accuracy: 0.82 - ETA: 0s - loss: 0.5262 - accuracy: 0.83 - ETA: 0s - loss: 0.5252 - accuracy: 0.83 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - 1s 6ms/step - loss: 0.5321 - accuracy: 0.8290 - val_loss: 1.3380 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.90 - ETA: 0s - loss: 0.4631 - accuracy: 0.86 - ETA: 0s - loss: 0.4833 - accuracy: 0.84 - ETA: 0s - loss: 0.4807 - accuracy: 0.84 - ETA: 0s - loss: 0.4749 - accuracy: 0.85 - ETA: 0s - loss: 0.4786 - accuracy: 0.84 - ETA: 0s - loss: 0.4747 - accuracy: 0.85 - ETA: 0s - loss: 0.4697 - accuracy: 0.85 - ETA: 0s - loss: 0.4768 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4780 - accuracy: 0.8514 - val_loss: 0.8530 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.96 - ETA: 0s - loss: 0.4412 - accuracy: 0.87 - ETA: 0s - loss: 0.4339 - accuracy: 0.87 - ETA: 0s - loss: 0.4514 - accuracy: 0.86 - ETA: 0s - loss: 0.4494 - accuracy: 0.86 - ETA: 0s - loss: 0.4639 - accuracy: 0.86 - ETA: 0s - loss: 0.4739 - accuracy: 0.85 - ETA: 0s - loss: 0.4763 - accuracy: 0.85 - ETA: 0s - loss: 0.4813 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4836 - accuracy: 0.8488 - val_loss: 1.0342 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.87 - ETA: 0s - loss: 0.5621 - accuracy: 0.84 - ETA: 0s - loss: 0.5284 - accuracy: 0.83 - ETA: 0s - loss: 0.5083 - accuracy: 0.84 - ETA: 0s - loss: 0.5107 - accuracy: 0.83 - ETA: 0s - loss: 0.4917 - accuracy: 0.84 - ETA: 0s - loss: 0.4835 - accuracy: 0.85 - ETA: 0s - loss: 0.4805 - accuracy: 0.85 - ETA: 0s - loss: 0.4872 - accuracy: 0.84 - 1s 6ms/step - loss: 0.4873 - accuracy: 0.8485 - val_loss: 0.7463 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.81 - ETA: 0s - loss: 0.4595 - accuracy: 0.85 - ETA: 0s - loss: 0.4371 - accuracy: 0.86 - ETA: 0s - loss: 0.4432 - accuracy: 0.86 - ETA: 0s - loss: 0.4320 - accuracy: 0.87 - ETA: 0s - loss: 0.4329 - accuracy: 0.87 - ETA: 0s - loss: 0.4431 - accuracy: 0.86 - ETA: 0s - loss: 0.4443 - accuracy: 0.86 - ETA: 0s - loss: 0.4467 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4496 - accuracy: 0.8649 - val_loss: 1.0546 - val_accuracy: 0.7343\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.84 - ETA: 0s - loss: 0.4899 - accuracy: 0.84 - ETA: 0s - loss: 0.4660 - accuracy: 0.85 - ETA: 0s - loss: 0.4529 - accuracy: 0.86 - ETA: 0s - loss: 0.4588 - accuracy: 0.85 - ETA: 0s - loss: 0.5054 - accuracy: 0.85 - ETA: 0s - loss: 0.4968 - accuracy: 0.86 - ETA: 0s - loss: 0.5195 - accuracy: 0.85 - ETA: 0s - loss: 0.5117 - accuracy: 0.85 - 1s 6ms/step - loss: 0.5044 - accuracy: 0.8567 - val_loss: 0.7043 - val_accuracy: 0.7463\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.87 - ETA: 0s - loss: 0.6459 - accuracy: 0.82 - ETA: 0s - loss: 0.5425 - accuracy: 0.84 - ETA: 0s - loss: 0.5535 - accuracy: 0.83 - ETA: 0s - loss: 0.5444 - accuracy: 0.83 - ETA: 0s - loss: 0.5585 - accuracy: 0.83 - ETA: 0s - loss: 0.5638 - accuracy: 0.83 - ETA: 0s - loss: 0.5625 - accuracy: 0.83 - ETA: 0s - loss: 0.5603 - accuracy: 0.83 - 1s 6ms/step - loss: 0.5589 - accuracy: 0.8309 - val_loss: 1.4780 - val_accuracy: 0.7478\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.78 - ETA: 0s - loss: 0.5409 - accuracy: 0.83 - ETA: 0s - loss: 0.5880 - accuracy: 0.80 - ETA: 0s - loss: 0.5906 - accuracy: 0.80 - ETA: 0s - loss: 0.5774 - accuracy: 0.80 - ETA: 0s - loss: 0.5686 - accuracy: 0.80 - ETA: 0s - loss: 0.5626 - accuracy: 0.80 - ETA: 0s - loss: 0.5558 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5688 - accuracy: 0.8078 - val_loss: 1.7344 - val_accuracy: 0.7313\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.87 - ETA: 0s - loss: 0.5644 - accuracy: 0.82 - ETA: 0s - loss: 0.5525 - accuracy: 0.82 - ETA: 0s - loss: 0.6389 - accuracy: 0.81 - ETA: 0s - loss: 0.6203 - accuracy: 0.80 - ETA: 0s - loss: 0.6500 - accuracy: 0.79 - ETA: 0s - loss: 0.6424 - accuracy: 0.80 - ETA: 0s - loss: 0.6449 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6487 - accuracy: 0.7973 - val_loss: 0.6862 - val_accuracy: 0.7478\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.75 - ETA: 0s - loss: 0.5664 - accuracy: 0.81 - ETA: 0s - loss: 0.5893 - accuracy: 0.80 - ETA: 0s - loss: 0.6014 - accuracy: 0.79 - ETA: 0s - loss: 0.5877 - accuracy: 0.81 - ETA: 0s - loss: 0.5760 - accuracy: 0.81 - ETA: 0s - loss: 0.5686 - accuracy: 0.81 - ETA: 0s - loss: 0.5862 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5888 - accuracy: 0.8167 - val_loss: 0.5929 - val_accuracy: 0.7284\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.87 - ETA: 0s - loss: 0.6028 - accuracy: 0.80 - ETA: 0s - loss: 0.6294 - accuracy: 0.78 - ETA: 0s - loss: 0.6339 - accuracy: 0.78 - ETA: 0s - loss: 0.6720 - accuracy: 0.78 - ETA: 0s - loss: 0.6629 - accuracy: 0.78 - ETA: 0s - loss: 0.6612 - accuracy: 0.78 - ETA: 0s - loss: 0.6698 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6708 - accuracy: 0.7876 - val_loss: 1.4172 - val_accuracy: 0.7418\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.81 - ETA: 0s - loss: 0.7857 - accuracy: 0.79 - ETA: 0s - loss: 0.7390 - accuracy: 0.80 - ETA: 0s - loss: 0.6632 - accuracy: 0.81 - ETA: 0s - loss: 0.6463 - accuracy: 0.81 - ETA: 0s - loss: 0.6643 - accuracy: 0.80 - ETA: 0s - loss: 0.6549 - accuracy: 0.80 - ETA: 0s - loss: 0.6730 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6732 - accuracy: 0.7947 - val_loss: 2.4466 - val_accuracy: 0.7358\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7753 - accuracy: 0.84 - ETA: 0s - loss: 0.5825 - accuracy: 0.81 - ETA: 0s - loss: 0.5637 - accuracy: 0.81 - ETA: 0s - loss: 0.5926 - accuracy: 0.81 - ETA: 0s - loss: 0.5850 - accuracy: 0.80 - ETA: 0s - loss: 0.5890 - accuracy: 0.80 - ETA: 0s - loss: 0.6135 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6139 - accuracy: 0.7977 - val_loss: 0.9387 - val_accuracy: 0.7403\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.65 - ETA: 0s - loss: 0.5687 - accuracy: 0.77 - ETA: 0s - loss: 0.6061 - accuracy: 0.77 - ETA: 0s - loss: 0.6068 - accuracy: 0.77 - ETA: 0s - loss: 0.5837 - accuracy: 0.79 - ETA: 0s - loss: 0.5920 - accuracy: 0.79 - ETA: 0s - loss: 0.6066 - accuracy: 0.79 - ETA: 0s - loss: 0.6036 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6043 - accuracy: 0.7925 - val_loss: 0.7467 - val_accuracy: 0.7493\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.75 - ETA: 0s - loss: 0.5411 - accuracy: 0.82 - ETA: 0s - loss: 0.5387 - accuracy: 0.82 - ETA: 0s - loss: 0.5503 - accuracy: 0.81 - ETA: 0s - loss: 0.5634 - accuracy: 0.81 - ETA: 0s - loss: 0.5550 - accuracy: 0.81 - ETA: 0s - loss: 0.5492 - accuracy: 0.81 - ETA: 0s - loss: 0.5479 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5467 - accuracy: 0.8182 - val_loss: 1.4357 - val_accuracy: 0.7493\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.84 - ETA: 0s - loss: 0.5197 - accuracy: 0.82 - ETA: 0s - loss: 0.5040 - accuracy: 0.83 - ETA: 0s - loss: 0.5689 - accuracy: 0.81 - ETA: 0s - loss: 0.5633 - accuracy: 0.81 - ETA: 0s - loss: 0.5768 - accuracy: 0.81 - ETA: 0s - loss: 0.5727 - accuracy: 0.81 - ETA: 0s - loss: 0.5760 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5777 - accuracy: 0.8096 - val_loss: 2.2189 - val_accuracy: 0.7313\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7286 - accuracy: 0.65 - ETA: 0s - loss: 0.5858 - accuracy: 0.78 - ETA: 0s - loss: 0.5952 - accuracy: 0.78 - ETA: 0s - loss: 0.5979 - accuracy: 0.78 - ETA: 0s - loss: 0.6122 - accuracy: 0.77 - ETA: 0s - loss: 0.6295 - accuracy: 0.76 - ETA: 0s - loss: 0.6522 - accuracy: 0.77 - ETA: 0s - loss: 0.6563 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6571 - accuracy: 0.7637 - val_loss: 1.0567 - val_accuracy: 0.7179\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.75 - ETA: 0s - loss: 0.6474 - accuracy: 0.72 - ETA: 0s - loss: 0.6402 - accuracy: 0.73 - ETA: 0s - loss: 0.6345 - accuracy: 0.74 - ETA: 0s - loss: 0.6538 - accuracy: 0.74 - ETA: 0s - loss: 0.6480 - accuracy: 0.74 - ETA: 0s - loss: 0.6448 - accuracy: 0.75 - ETA: 0s - loss: 0.6398 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6427 - accuracy: 0.7548 - val_loss: 0.9262 - val_accuracy: 0.7313\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 3.2902 - accuracy: 0.65 - ETA: 0s - loss: 0.8611 - accuracy: 0.75 - ETA: 0s - loss: 0.7426 - accuracy: 0.75 - ETA: 0s - loss: 0.7107 - accuracy: 0.75 - ETA: 0s - loss: 0.6906 - accuracy: 0.75 - ETA: 0s - loss: 0.6741 - accuracy: 0.75 - ETA: 0s - loss: 0.6619 - accuracy: 0.75 - ETA: 0s - loss: 0.6555 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6519 - accuracy: 0.7581 - val_loss: 0.7080 - val_accuracy: 0.7299\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.87 - ETA: 0s - loss: 0.6084 - accuracy: 0.76 - ETA: 0s - loss: 0.6285 - accuracy: 0.74 - ETA: 0s - loss: 0.6437 - accuracy: 0.73 - ETA: 0s - loss: 0.6341 - accuracy: 0.73 - ETA: 0s - loss: 0.6325 - accuracy: 0.73 - ETA: 0s - loss: 0.6304 - accuracy: 0.73 - ETA: 0s - loss: 0.6349 - accuracy: 0.73 - ETA: 0s - loss: 0.6344 - accuracy: 0.73 - 0s 6ms/step - loss: 0.6344 - accuracy: 0.7342 - val_loss: 0.7136 - val_accuracy: 0.7373\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.75 - ETA: 0s - loss: 0.6114 - accuracy: 0.75 - ETA: 0s - loss: 0.6385 - accuracy: 0.74 - ETA: 0s - loss: 0.6394 - accuracy: 0.74 - ETA: 0s - loss: 0.6438 - accuracy: 0.73 - ETA: 0s - loss: 0.6380 - accuracy: 0.74 - ETA: 0s - loss: 0.6374 - accuracy: 0.74 - ETA: 0s - loss: 0.6350 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6360 - accuracy: 0.7436 - val_loss: 0.6375 - val_accuracy: 0.7343\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7267 - accuracy: 0.65 - ETA: 0s - loss: 0.6206 - accuracy: 0.75 - ETA: 0s - loss: 0.6284 - accuracy: 0.74 - ETA: 0s - loss: 0.6265 - accuracy: 0.75 - ETA: 0s - loss: 0.6366 - accuracy: 0.74 - ETA: 0s - loss: 0.6358 - accuracy: 0.74 - ETA: 0s - loss: 0.6350 - accuracy: 0.74 - ETA: 0s - loss: 0.6322 - accuracy: 0.75 - ETA: 0s - loss: 0.6326 - accuracy: 0.74 - 0s 6ms/step - loss: 0.6332 - accuracy: 0.7492 - val_loss: 0.6329 - val_accuracy: 0.7299\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8133 - accuracy: 0.56 - ETA: 0s - loss: 0.6466 - accuracy: 0.71 - ETA: 0s - loss: 0.6387 - accuracy: 0.74 - ETA: 0s - loss: 0.6304 - accuracy: 0.75 - ETA: 0s - loss: 0.6316 - accuracy: 0.74 - ETA: 0s - loss: 0.6290 - accuracy: 0.74 - ETA: 0s - loss: 0.6258 - accuracy: 0.75 - ETA: 0s - loss: 0.6297 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6347 - accuracy: 0.7548 - val_loss: 0.6290 - val_accuracy: 0.7269\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/84 [==========================>...] - ETA: 0s - loss: 0.6583 - accuracy: 0.71 - ETA: 0s - loss: 0.6853 - accuracy: 0.70 - ETA: 0s - loss: 0.6817 - accuracy: 0.70 - ETA: 0s - loss: 0.6798 - accuracy: 0.70 - ETA: 0s - loss: 0.6586 - accuracy: 0.73 - ETA: 0s - loss: 0.6598 - accuracy: 0.72 - ETA: 0s - loss: 0.6625 - accuracy: 0.72 - ETA: 0s - loss: 0.6569 - accuracy: 0.7256Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6510 - accuracy: 0.7297 - val_loss: 0.6384 - val_accuracy: 0.7313\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5790 - accuracy: 0.81 - ETA: 0s - loss: 2.8362 - accuracy: 0.58 - ETA: 0s - loss: 1.8357 - accuracy: 0.59 - ETA: 0s - loss: 1.4308 - accuracy: 0.63 - ETA: 0s - loss: 1.2448 - accuracy: 0.65 - ETA: 0s - loss: 1.1080 - accuracy: 0.67 - ETA: 0s - loss: 1.0291 - accuracy: 0.68 - ETA: 0s - loss: 0.9731 - accuracy: 0.68 - 1s 6ms/step - loss: 0.9604 - accuracy: 0.6857 - val_loss: 0.5945 - val_accuracy: 0.7075\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6088 - accuracy: 0.68 - ETA: 0s - loss: 0.7476 - accuracy: 0.69 - ETA: 0s - loss: 0.6951 - accuracy: 0.70 - ETA: 0s - loss: 0.6930 - accuracy: 0.72 - ETA: 0s - loss: 0.6706 - accuracy: 0.72 - ETA: 0s - loss: 0.6453 - accuracy: 0.73 - ETA: 0s - loss: 0.6384 - accuracy: 0.73 - ETA: 0s - loss: 0.6220 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6207 - accuracy: 0.7480 - val_loss: 0.5899 - val_accuracy: 0.7090\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.90 - ETA: 0s - loss: 0.5349 - accuracy: 0.81 - ETA: 0s - loss: 0.5234 - accuracy: 0.81 - ETA: 0s - loss: 0.5521 - accuracy: 0.79 - ETA: 0s - loss: 0.5722 - accuracy: 0.79 - ETA: 0s - loss: 0.5757 - accuracy: 0.79 - ETA: 0s - loss: 0.5913 - accuracy: 0.77 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5869 - accuracy: 0.7839 - val_loss: 0.5600 - val_accuracy: 0.7597\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.78 - ETA: 0s - loss: 0.5484 - accuracy: 0.77 - ETA: 0s - loss: 0.5876 - accuracy: 0.76 - ETA: 0s - loss: 0.5979 - accuracy: 0.76 - ETA: 0s - loss: 0.5874 - accuracy: 0.77 - ETA: 0s - loss: 0.5790 - accuracy: 0.77 - ETA: 0s - loss: 0.5920 - accuracy: 0.77 - ETA: 0s - loss: 0.5858 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5836 - accuracy: 0.7753 - val_loss: 0.6140 - val_accuracy: 0.7493\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.75 - ETA: 0s - loss: 0.5574 - accuracy: 0.79 - ETA: 0s - loss: 0.6407 - accuracy: 0.81 - ETA: 0s - loss: 0.6137 - accuracy: 0.81 - ETA: 0s - loss: 0.6000 - accuracy: 0.80 - ETA: 0s - loss: 0.6034 - accuracy: 0.79 - ETA: 0s - loss: 0.6075 - accuracy: 0.78 - ETA: 0s - loss: 0.6061 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6013 - accuracy: 0.7876 - val_loss: 0.6047 - val_accuracy: 0.7373\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.81 - ETA: 0s - loss: 0.5880 - accuracy: 0.80 - ETA: 0s - loss: 0.5495 - accuracy: 0.80 - ETA: 0s - loss: 0.5492 - accuracy: 0.80 - ETA: 0s - loss: 0.5727 - accuracy: 0.79 - ETA: 0s - loss: 0.5692 - accuracy: 0.79 - ETA: 0s - loss: 0.5705 - accuracy: 0.79 - ETA: 0s - loss: 0.5677 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5649 - accuracy: 0.7988 - val_loss: 0.6030 - val_accuracy: 0.7657\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.93 - ETA: 0s - loss: 0.6059 - accuracy: 0.79 - ETA: 0s - loss: 0.6123 - accuracy: 0.78 - ETA: 0s - loss: 0.6128 - accuracy: 0.79 - ETA: 0s - loss: 0.5960 - accuracy: 0.80 - ETA: 0s - loss: 0.6105 - accuracy: 0.79 - ETA: 0s - loss: 0.6103 - accuracy: 0.78 - ETA: 0s - loss: 0.6059 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6122 - accuracy: 0.7910 - val_loss: 0.5831 - val_accuracy: 0.7299\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.81 - ETA: 0s - loss: 0.6252 - accuracy: 0.79 - ETA: 0s - loss: 0.6094 - accuracy: 0.78 - ETA: 0s - loss: 0.6103 - accuracy: 0.78 - ETA: 0s - loss: 0.5975 - accuracy: 0.79 - ETA: 0s - loss: 0.5976 - accuracy: 0.79 - ETA: 0s - loss: 0.5993 - accuracy: 0.79 - ETA: 0s - loss: 0.5977 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6026 - accuracy: 0.7898 - val_loss: 0.7567 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4519 - accuracy: 0.84 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.6118 - accuracy: 0.80 - ETA: 0s - loss: 0.5930 - accuracy: 0.80 - ETA: 0s - loss: 0.5970 - accuracy: 0.79 - ETA: 0s - loss: 0.6152 - accuracy: 0.79 - ETA: 0s - loss: 0.6245 - accuracy: 0.78 - ETA: 0s - loss: 0.6213 - accuracy: 0.78 - 0s 5ms/step - loss: 0.6176 - accuracy: 0.7895 - val_loss: 1.2424 - val_accuracy: 0.6746\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.5856 - accuracy: 0.65 - ETA: 0s - loss: 1.1409 - accuracy: 0.76 - ETA: 0s - loss: 0.8776 - accuracy: 0.79 - ETA: 0s - loss: 0.8376 - accuracy: 0.78 - ETA: 0s - loss: 0.7893 - accuracy: 0.78 - ETA: 0s - loss: 0.7549 - accuracy: 0.78 - ETA: 0s - loss: 0.7381 - accuracy: 0.78 - ETA: 0s - loss: 0.7257 - accuracy: 0.77 - 0s 5ms/step - loss: 0.7248 - accuracy: 0.7779 - val_loss: 0.6296 - val_accuracy: 0.7358\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.68 - ETA: 0s - loss: 0.6005 - accuracy: 0.77 - ETA: 0s - loss: 0.6123 - accuracy: 0.78 - ETA: 0s - loss: 0.6795 - accuracy: 0.78 - ETA: 0s - loss: 0.7166 - accuracy: 0.77 - ETA: 0s - loss: 0.7178 - accuracy: 0.76 - ETA: 0s - loss: 0.7180 - accuracy: 0.76 - ETA: 0s - loss: 0.7162 - accuracy: 0.75 - 0s 5ms/step - loss: 0.7127 - accuracy: 0.7514 - val_loss: 0.6435 - val_accuracy: 0.7090\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.65 - ETA: 0s - loss: 0.6692 - accuracy: 0.69 - ETA: 0s - loss: 0.6831 - accuracy: 0.65 - ETA: 0s - loss: 0.6721 - accuracy: 0.67 - ETA: 0s - loss: 0.6662 - accuracy: 0.68 - ETA: 0s - loss: 0.6750 - accuracy: 0.67 - ETA: 0s - loss: 0.6619 - accuracy: 0.67 - ETA: 0s - loss: 0.6571 - accuracy: 0.68 - 0s 5ms/step - loss: 0.6577 - accuracy: 0.6887 - val_loss: 0.7823 - val_accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8467 - accuracy: 0.78 - ETA: 0s - loss: 0.6325 - accuracy: 0.77 - ETA: 0s - loss: 0.6507 - accuracy: 0.75 - ETA: 0s - loss: 0.6518 - accuracy: 0.74 - ETA: 0s - loss: 0.6470 - accuracy: 0.74 - ETA: 0s - loss: 0.6396 - accuracy: 0.74 - ETA: 0s - loss: 0.6408 - accuracy: 0.74 - ETA: 0s - loss: 0.6357 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6374 - accuracy: 0.7536 - val_loss: 0.6311 - val_accuracy: 0.7478\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.68 - ETA: 0s - loss: 0.7270 - accuracy: 0.73 - ETA: 0s - loss: 0.6877 - accuracy: 0.73 - ETA: 0s - loss: 0.6655 - accuracy: 0.73 - ETA: 0s - loss: 0.6516 - accuracy: 0.74 - ETA: 0s - loss: 0.6451 - accuracy: 0.74 - ETA: 0s - loss: 0.6387 - accuracy: 0.74 - ETA: 0s - loss: 0.6563 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6550 - accuracy: 0.7484 - val_loss: 0.6268 - val_accuracy: 0.7358\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7849 - accuracy: 0.62 - ETA: 0s - loss: 0.6818 - accuracy: 0.70 - ETA: 0s - loss: 0.6702 - accuracy: 0.67 - ETA: 0s - loss: 0.6456 - accuracy: 0.70 - ETA: 0s - loss: 0.6925 - accuracy: 0.72 - ETA: 0s - loss: 0.7156 - accuracy: 0.72 - ETA: 0s - loss: 0.7129 - accuracy: 0.71 - ETA: 0s - loss: 0.7110 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7088 - accuracy: 0.7010 - val_loss: 1.1475 - val_accuracy: 0.7015\n",
      "Epoch 16/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6592 - accuracy: 0.68 - ETA: 0s - loss: 0.6992 - accuracy: 0.65 - ETA: 0s - loss: 0.6953 - accuracy: 0.63 - ETA: 0s - loss: 0.6859 - accuracy: 0.62 - ETA: 0s - loss: 0.6881 - accuracy: 0.64 - ETA: 0s - loss: 0.6892 - accuracy: 0.65 - ETA: 0s - loss: 0.6883 - accuracy: 0.65 - ETA: 0s - loss: 0.6873 - accuracy: 0.6524Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.6510 - val_loss: 0.7075 - val_accuracy: 0.7030\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f65e9317b55745aaa9b6f2bf18a2234e</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7616915504137675</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7464332393964828</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 135</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3653 - accuracy: 0.43 - ETA: 0s - loss: 2.4179 - accuracy: 0.66 - ETA: 0s - loss: 1.9013 - accuracy: 0.63 - ETA: 0s - loss: 1.5687 - accuracy: 0.65 - ETA: 0s - loss: 1.3478 - accuracy: 0.67 - ETA: 0s - loss: 1.2170 - accuracy: 0.67 - 0s 5ms/step - loss: 1.2021 - accuracy: 0.6820 - val_loss: 0.6056 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6387 - accuracy: 0.59 - ETA: 0s - loss: 0.7359 - accuracy: 0.71 - ETA: 0s - loss: 0.6769 - accuracy: 0.73 - ETA: 0s - loss: 0.6783 - accuracy: 0.72 - ETA: 0s - loss: 0.6598 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6597 - accuracy: 0.7271 - val_loss: 0.6122 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6501 - accuracy: 0.68 - ETA: 0s - loss: 0.5994 - accuracy: 0.76 - ETA: 0s - loss: 0.6163 - accuracy: 0.75 - ETA: 0s - loss: 0.6299 - accuracy: 0.75 - ETA: 0s - loss: 0.6265 - accuracy: 0.75 - 0s 3ms/step - loss: 0.6299 - accuracy: 0.7480 - val_loss: 0.6356 - val_accuracy: 0.7299\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.75 - ETA: 0s - loss: 0.5831 - accuracy: 0.76 - ETA: 0s - loss: 0.5887 - accuracy: 0.77 - ETA: 0s - loss: 0.6134 - accuracy: 0.76 - ETA: 0s - loss: 0.6063 - accuracy: 0.76 - 0s 3ms/step - loss: 0.6066 - accuracy: 0.7656 - val_loss: 0.6581 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1283 - accuracy: 0.75 - ETA: 0s - loss: 0.7234 - accuracy: 0.76 - ETA: 0s - loss: 0.6735 - accuracy: 0.75 - ETA: 0s - loss: 0.6543 - accuracy: 0.76 - ETA: 0s - loss: 0.6348 - accuracy: 0.76 - 0s 4ms/step - loss: 0.6307 - accuracy: 0.7667 - val_loss: 0.6753 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7206 - accuracy: 0.65 - ETA: 0s - loss: 0.5770 - accuracy: 0.78 - ETA: 0s - loss: 0.5465 - accuracy: 0.78 - ETA: 0s - loss: 0.6105 - accuracy: 0.78 - ETA: 0s - loss: 0.6262 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6176 - accuracy: 0.7928 - val_loss: 0.6671 - val_accuracy: 0.7313\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2515 - accuracy: 0.71 - ETA: 0s - loss: 0.6195 - accuracy: 0.79 - ETA: 0s - loss: 0.6143 - accuracy: 0.78 - ETA: 0s - loss: 0.5976 - accuracy: 0.79 - ETA: 0s - loss: 0.5838 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5769 - accuracy: 0.8022 - val_loss: 0.6070 - val_accuracy: 0.7284\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5472 - accuracy: 0.84 - ETA: 0s - loss: 0.5237 - accuracy: 0.82 - ETA: 0s - loss: 0.5281 - accuracy: 0.81 - ETA: 0s - loss: 0.5843 - accuracy: 0.81 - ETA: 0s - loss: 0.5675 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5670 - accuracy: 0.8205 - val_loss: 0.6482 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4737 - accuracy: 0.84 - ETA: 0s - loss: 0.5542 - accuracy: 0.80 - ETA: 0s - loss: 0.5381 - accuracy: 0.81 - ETA: 0s - loss: 0.5274 - accuracy: 0.82 - ETA: 0s - loss: 0.5319 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5416 - accuracy: 0.8134 - val_loss: 0.5803 - val_accuracy: 0.7522\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.78 - ETA: 0s - loss: 0.5715 - accuracy: 0.84 - ETA: 0s - loss: 0.5419 - accuracy: 0.83 - ETA: 0s - loss: 0.5448 - accuracy: 0.82 - ETA: 0s - loss: 0.5366 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5559 - accuracy: 0.8208 - val_loss: 0.5767 - val_accuracy: 0.7522\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.81 - ETA: 0s - loss: 0.6080 - accuracy: 0.81 - ETA: 0s - loss: 0.5890 - accuracy: 0.81 - ETA: 0s - loss: 0.6129 - accuracy: 0.80 - ETA: 0s - loss: 0.6067 - accuracy: 0.81 - 0s 3ms/step - loss: 0.6268 - accuracy: 0.8089 - val_loss: 0.7367 - val_accuracy: 0.7433\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.78 - ETA: 0s - loss: 0.5422 - accuracy: 0.81 - ETA: 0s - loss: 0.5498 - accuracy: 0.81 - ETA: 0s - loss: 0.5624 - accuracy: 0.80 - ETA: 0s - loss: 0.5630 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5978 - accuracy: 0.8025 - val_loss: 0.7204 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.68 - ETA: 0s - loss: 0.5543 - accuracy: 0.79 - ETA: 0s - loss: 0.5606 - accuracy: 0.80 - ETA: 0s - loss: 0.5517 - accuracy: 0.80 - ETA: 0s - loss: 0.5463 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5550 - accuracy: 0.8081 - val_loss: 0.8541 - val_accuracy: 0.7239\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6572 - accuracy: 0.68 - ETA: 0s - loss: 0.5400 - accuracy: 0.80 - ETA: 0s - loss: 0.5759 - accuracy: 0.81 - ETA: 0s - loss: 0.5467 - accuracy: 0.82 - ETA: 0s - loss: 0.5499 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5445 - accuracy: 0.8219 - val_loss: 0.6954 - val_accuracy: 0.7507\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.87 - ETA: 0s - loss: 0.5380 - accuracy: 0.81 - ETA: 0s - loss: 0.5040 - accuracy: 0.83 - ETA: 0s - loss: 0.5298 - accuracy: 0.82 - ETA: 0s - loss: 0.5314 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5329 - accuracy: 0.8268 - val_loss: 0.9029 - val_accuracy: 0.7209\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9640 - accuracy: 0.81 - ETA: 0s - loss: 0.5396 - accuracy: 0.83 - ETA: 0s - loss: 0.5268 - accuracy: 0.83 - ETA: 0s - loss: 0.5376 - accuracy: 0.83 - ETA: 0s - loss: 0.5417 - accuracy: 0.82 - ETA: 0s - loss: 0.5440 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5440 - accuracy: 0.8294 - val_loss: 0.7356 - val_accuracy: 0.7209\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.90 - ETA: 0s - loss: 0.6154 - accuracy: 0.80 - ETA: 0s - loss: 0.5913 - accuracy: 0.80 - ETA: 0s - loss: 0.5747 - accuracy: 0.80 - ETA: 0s - loss: 0.5629 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5464 - accuracy: 0.8246 - val_loss: 0.7813 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.75 - ETA: 0s - loss: 0.5292 - accuracy: 0.81 - ETA: 0s - loss: 0.5751 - accuracy: 0.81 - ETA: 0s - loss: 0.7887 - accuracy: 0.82 - ETA: 0s - loss: 0.7186 - accuracy: 0.82 - 0s 3ms/step - loss: 0.6951 - accuracy: 0.8227 - val_loss: 0.7245 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "68/84 [=======================>......] - ETA: 0s - loss: 0.5925 - accuracy: 0.78 - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.5702 - accuracy: 0.82 - ETA: 0s - loss: 0.5805 - accuracy: 0.81 - ETA: 0s - loss: 0.5718 - accuracy: 0.8208Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.8137 - val_loss: 0.7884 - val_accuracy: 0.7284\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.9148 - accuracy: 0.50 - ETA: 0s - loss: 2.6516 - accuracy: 0.62 - ETA: 0s - loss: 2.0763 - accuracy: 0.60 - ETA: 0s - loss: 1.6381 - accuracy: 0.61 - ETA: 0s - loss: 1.3832 - accuracy: 0.64 - ETA: 0s - loss: 1.2326 - accuracy: 0.66 - 0s 5ms/step - loss: 1.2213 - accuracy: 0.6693 - val_loss: 0.6119 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6323 - accuracy: 0.62 - ETA: 0s - loss: 0.6354 - accuracy: 0.71 - ETA: 0s - loss: 0.6435 - accuracy: 0.73 - ETA: 0s - loss: 0.6500 - accuracy: 0.73 - ETA: 0s - loss: 0.6373 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6386 - accuracy: 0.7320 - val_loss: 0.6294 - val_accuracy: 0.7149\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.71 - ETA: 0s - loss: 0.6605 - accuracy: 0.76 - ETA: 0s - loss: 0.6359 - accuracy: 0.74 - ETA: 0s - loss: 0.6281 - accuracy: 0.74 - ETA: 0s - loss: 0.6238 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6198 - accuracy: 0.7484 - val_loss: 0.6284 - val_accuracy: 0.7209\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.90 - ETA: 0s - loss: 0.5462 - accuracy: 0.79 - ETA: 0s - loss: 0.5731 - accuracy: 0.79 - ETA: 0s - loss: 0.5961 - accuracy: 0.78 - ETA: 0s - loss: 0.5996 - accuracy: 0.77 - 0s 3ms/step - loss: 0.6097 - accuracy: 0.7768 - val_loss: 0.5959 - val_accuracy: 0.7269\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5555 - accuracy: 0.81 - ETA: 0s - loss: 0.5537 - accuracy: 0.78 - ETA: 0s - loss: 0.5597 - accuracy: 0.78 - ETA: 0s - loss: 0.5537 - accuracy: 0.78 - ETA: 0s - loss: 0.5384 - accuracy: 0.79 - ETA: 0s - loss: 0.5538 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5560 - accuracy: 0.7869 - val_loss: 0.5978 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.78 - ETA: 0s - loss: 0.5692 - accuracy: 0.80 - ETA: 0s - loss: 0.6067 - accuracy: 0.77 - ETA: 0s - loss: 0.5882 - accuracy: 0.78 - ETA: 0s - loss: 0.6075 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6189 - accuracy: 0.7816 - val_loss: 0.6414 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.6912 - accuracy: 0.75 - ETA: 0s - loss: 0.5936 - accuracy: 0.83 - ETA: 0s - loss: 0.5931 - accuracy: 0.81 - ETA: 0s - loss: 0.6059 - accuracy: 0.81 - ETA: 0s - loss: 0.6683 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6631 - accuracy: 0.7947 - val_loss: 0.6419 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6063 - accuracy: 0.75 - ETA: 0s - loss: 0.6447 - accuracy: 0.80 - ETA: 0s - loss: 0.6248 - accuracy: 0.79 - ETA: 0s - loss: 0.6430 - accuracy: 0.79 - ETA: 0s - loss: 0.6365 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6282 - accuracy: 0.7928 - val_loss: 0.7691 - val_accuracy: 0.7269\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.81 - ETA: 0s - loss: 0.7341 - accuracy: 0.78 - ETA: 0s - loss: 0.6575 - accuracy: 0.78 - ETA: 0s - loss: 0.6222 - accuracy: 0.79 - ETA: 0s - loss: 0.6022 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5988 - accuracy: 0.8052 - val_loss: 0.6725 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.87 - ETA: 0s - loss: 0.5663 - accuracy: 0.81 - ETA: 0s - loss: 0.5331 - accuracy: 0.82 - ETA: 0s - loss: 0.5584 - accuracy: 0.81 - ETA: 0s - loss: 0.5558 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5601 - accuracy: 0.8119 - val_loss: 0.6953 - val_accuracy: 0.7463\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6394 - accuracy: 0.81 - ETA: 0s - loss: 0.5441 - accuracy: 0.82 - ETA: 0s - loss: 0.5211 - accuracy: 0.83 - ETA: 0s - loss: 0.5254 - accuracy: 0.82 - ETA: 0s - loss: 0.5348 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5523 - accuracy: 0.8193 - val_loss: 0.6293 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.75 - ETA: 0s - loss: 0.5489 - accuracy: 0.80 - ETA: 0s - loss: 0.5397 - accuracy: 0.81 - ETA: 0s - loss: 0.5315 - accuracy: 0.81 - ETA: 0s - loss: 0.5355 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5346 - accuracy: 0.8190 - val_loss: 0.6702 - val_accuracy: 0.7418\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.84 - ETA: 0s - loss: 0.5111 - accuracy: 0.83 - ETA: 0s - loss: 0.4706 - accuracy: 0.85 - ETA: 0s - loss: 0.4986 - accuracy: 0.84 - ETA: 0s - loss: 0.4972 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4992 - accuracy: 0.8384 - val_loss: 0.8232 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.87 - ETA: 0s - loss: 0.4970 - accuracy: 0.84 - ETA: 0s - loss: 0.4937 - accuracy: 0.84 - ETA: 0s - loss: 0.5049 - accuracy: 0.83 - ETA: 0s - loss: 0.5080 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4990 - accuracy: 0.8425 - val_loss: 0.6732 - val_accuracy: 0.7493\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.93 - ETA: 0s - loss: 0.4661 - accuracy: 0.85 - ETA: 0s - loss: 0.4803 - accuracy: 0.84 - ETA: 0s - loss: 0.4858 - accuracy: 0.84 - ETA: 0s - loss: 0.6882 - accuracy: 0.83 - 0s 3ms/step - loss: 0.6687 - accuracy: 0.8358 - val_loss: 0.6696 - val_accuracy: 0.7299\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5312 - accuracy: 0.90 - ETA: 0s - loss: 0.5086 - accuracy: 0.85 - ETA: 0s - loss: 0.4884 - accuracy: 0.85 - ETA: 0s - loss: 0.5240 - accuracy: 0.84 - ETA: 0s - loss: 0.5441 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5412 - accuracy: 0.8324 - val_loss: 0.9143 - val_accuracy: 0.7418\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.93 - ETA: 0s - loss: 0.5053 - accuracy: 0.83 - ETA: 0s - loss: 0.5149 - accuracy: 0.84 - ETA: 0s - loss: 0.5123 - accuracy: 0.84 - ETA: 0s - loss: 0.5231 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5119 - accuracy: 0.8373 - val_loss: 0.7119 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.87 - ETA: 0s - loss: 0.5028 - accuracy: 0.83 - ETA: 0s - loss: 0.5128 - accuracy: 0.83 - ETA: 0s - loss: 0.5140 - accuracy: 0.83 - ETA: 0s - loss: 0.5066 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5036 - accuracy: 0.8384 - val_loss: 1.0832 - val_accuracy: 0.7507\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.2924 - accuracy: 0.71 - ETA: 0s - loss: 0.5417 - accuracy: 0.84 - ETA: 0s - loss: 0.5090 - accuracy: 0.85 - ETA: 0s - loss: 0.5311 - accuracy: 0.84 - ETA: 0s - loss: 0.5262 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5217 - accuracy: 0.8402 - val_loss: 0.7814 - val_accuracy: 0.7358\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.87 - ETA: 0s - loss: 0.5035 - accuracy: 0.83 - ETA: 0s - loss: 0.5124 - accuracy: 0.84 - ETA: 0s - loss: 0.4886 - accuracy: 0.85 - ETA: 0s - loss: 0.5095 - accuracy: 0.85 - 0s 3ms/step - loss: 0.5195 - accuracy: 0.8507 - val_loss: 2.3692 - val_accuracy: 0.7239\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.90 - ETA: 0s - loss: 0.6489 - accuracy: 0.83 - ETA: 0s - loss: 0.6429 - accuracy: 0.82 - ETA: 0s - loss: 0.6542 - accuracy: 0.81 - ETA: 0s - loss: 0.6830 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6674 - accuracy: 0.7925 - val_loss: 0.8838 - val_accuracy: 0.7119\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0796 - accuracy: 0.71 - ETA: 0s - loss: 0.7319 - accuracy: 0.75 - ETA: 0s - loss: 0.7247 - accuracy: 0.75 - ETA: 0s - loss: 0.7050 - accuracy: 0.75 - ETA: 0s - loss: 0.7030 - accuracy: 0.74 - 0s 3ms/step - loss: 0.7741 - accuracy: 0.7451 - val_loss: 0.6496 - val_accuracy: 0.7060\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.75 - ETA: 0s - loss: 0.8572 - accuracy: 0.71 - ETA: 0s - loss: 1.2618 - accuracy: 0.71 - ETA: 0s - loss: 1.0838 - accuracy: 0.71 - ETA: 0s - loss: 0.9892 - accuracy: 0.71 - 0s 4ms/step - loss: 0.9568 - accuracy: 0.7219 - val_loss: 0.6571 - val_accuracy: 0.7209\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.68 - ETA: 0s - loss: 0.6714 - accuracy: 0.76 - ETA: 0s - loss: 0.9709 - accuracy: 0.75 - ETA: 0s - loss: 0.8766 - accuracy: 0.73 - ETA: 0s - loss: 0.8284 - accuracy: 0.73 - 0s 3ms/step - loss: 0.8413 - accuracy: 0.7342 - val_loss: 0.6505 - val_accuracy: 0.7209\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.84 - ETA: 0s - loss: 0.7773 - accuracy: 0.72 - ETA: 0s - loss: 0.8129 - accuracy: 0.73 - ETA: 0s - loss: 0.7828 - accuracy: 0.73 - ETA: 0s - loss: 0.7932 - accuracy: 0.73 - 0s 3ms/step - loss: 0.7777 - accuracy: 0.7365 - val_loss: 0.6723 - val_accuracy: 0.7119\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7288 - accuracy: 0.62 - ETA: 0s - loss: 0.8299 - accuracy: 0.73 - ETA: 0s - loss: 0.7560 - accuracy: 0.75 - ETA: 0s - loss: 0.7117 - accuracy: 0.75 - ETA: 0s - loss: 0.7225 - accuracy: 0.75 - 0s 3ms/step - loss: 0.7140 - accuracy: 0.7559 - val_loss: 0.6961 - val_accuracy: 0.7164\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6688 - accuracy: 0.71 - ETA: 0s - loss: 0.7328 - accuracy: 0.72 - ETA: 0s - loss: 0.7139 - accuracy: 0.73 - ETA: 0s - loss: 0.7213 - accuracy: 0.74 - ETA: 0s - loss: 0.7014 - accuracy: 0.74 - 0s 3ms/step - loss: 0.6885 - accuracy: 0.7499 - val_loss: 0.7202 - val_accuracy: 0.7269\n",
      "Epoch 28/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.6005 - accuracy: 0.71 - ETA: 0s - loss: 0.6216 - accuracy: 0.76 - ETA: 0s - loss: 0.6020 - accuracy: 0.78 - ETA: 0s - loss: 0.6156 - accuracy: 0.77 - ETA: 0s - loss: 0.6190 - accuracy: 0.7695Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.7686 - val_loss: 0.8696 - val_accuracy: 0.7224\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9363 - accuracy: 0.50 - ETA: 0s - loss: 2.6739 - accuracy: 0.60 - ETA: 0s - loss: 1.8219 - accuracy: 0.62 - ETA: 0s - loss: 1.5545 - accuracy: 0.62 - ETA: 0s - loss: 1.3483 - accuracy: 0.64 - ETA: 0s - loss: 1.2078 - accuracy: 0.65 - 0s 5ms/step - loss: 1.2019 - accuracy: 0.6532 - val_loss: 0.6510 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.87 - ETA: 0s - loss: 0.6386 - accuracy: 0.73 - ETA: 0s - loss: 0.6276 - accuracy: 0.74 - ETA: 0s - loss: 0.6431 - accuracy: 0.73 - ETA: 0s - loss: 0.6379 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6378 - accuracy: 0.7327 - val_loss: 0.6181 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.68 - ETA: 0s - loss: 0.5919 - accuracy: 0.75 - ETA: 0s - loss: 0.5998 - accuracy: 0.74 - ETA: 0s - loss: 0.5869 - accuracy: 0.75 - ETA: 0s - loss: 0.5914 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5977 - accuracy: 0.7544 - val_loss: 0.6059 - val_accuracy: 0.7313\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.81 - ETA: 0s - loss: 0.6435 - accuracy: 0.76 - ETA: 0s - loss: 0.6158 - accuracy: 0.76 - ETA: 0s - loss: 0.6056 - accuracy: 0.76 - ETA: 0s - loss: 0.5968 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5910 - accuracy: 0.7678 - val_loss: 0.5827 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.93 - ETA: 0s - loss: 0.5177 - accuracy: 0.81 - ETA: 0s - loss: 0.5959 - accuracy: 0.79 - ETA: 0s - loss: 0.5990 - accuracy: 0.78 - ETA: 0s - loss: 0.6029 - accuracy: 0.78 - 0s 3ms/step - loss: 0.6060 - accuracy: 0.7772 - val_loss: 0.6440 - val_accuracy: 0.7343\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.84 - ETA: 0s - loss: 0.5594 - accuracy: 0.80 - ETA: 0s - loss: 0.5661 - accuracy: 0.79 - ETA: 0s - loss: 0.5609 - accuracy: 0.80 - ETA: 0s - loss: 0.5608 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5622 - accuracy: 0.7962 - val_loss: 0.6021 - val_accuracy: 0.7179\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.93 - ETA: 0s - loss: 0.5250 - accuracy: 0.81 - ETA: 0s - loss: 0.5213 - accuracy: 0.81 - ETA: 0s - loss: 0.5257 - accuracy: 0.81 - ETA: 0s - loss: 0.5271 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5434 - accuracy: 0.8048 - val_loss: 0.6708 - val_accuracy: 0.7239\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.81 - ETA: 0s - loss: 0.4867 - accuracy: 0.82 - ETA: 0s - loss: 0.5103 - accuracy: 0.81 - ETA: 0s - loss: 0.5127 - accuracy: 0.82 - ETA: 0s - loss: 0.5278 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5378 - accuracy: 0.8081 - val_loss: 0.5799 - val_accuracy: 0.7478\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.84 - ETA: 0s - loss: 0.5720 - accuracy: 0.81 - ETA: 0s - loss: 0.5701 - accuracy: 0.81 - ETA: 0s - loss: 0.5528 - accuracy: 0.81 - ETA: 0s - loss: 0.5516 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5595 - accuracy: 0.8152 - val_loss: 0.5728 - val_accuracy: 0.7493\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.78 - ETA: 0s - loss: 0.5789 - accuracy: 0.80 - ETA: 0s - loss: 0.5508 - accuracy: 0.81 - ETA: 0s - loss: 0.5506 - accuracy: 0.81 - ETA: 0s - loss: 0.5419 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5415 - accuracy: 0.8152 - val_loss: 0.7915 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.87 - ETA: 0s - loss: 0.5152 - accuracy: 0.82 - ETA: 0s - loss: 0.5012 - accuracy: 0.82 - ETA: 0s - loss: 0.5038 - accuracy: 0.82 - ETA: 0s - loss: 0.4999 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4931 - accuracy: 0.8331 - val_loss: 0.6844 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8548 - accuracy: 0.78 - ETA: 0s - loss: 0.4903 - accuracy: 0.84 - ETA: 0s - loss: 0.4822 - accuracy: 0.84 - ETA: 0s - loss: 0.4812 - accuracy: 0.84 - ETA: 0s - loss: 0.5186 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5180 - accuracy: 0.8313 - val_loss: 0.6957 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.84 - ETA: 0s - loss: 0.4831 - accuracy: 0.86 - ETA: 0s - loss: 0.5012 - accuracy: 0.85 - ETA: 0s - loss: 0.5372 - accuracy: 0.83 - ETA: 0s - loss: 0.5454 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5650 - accuracy: 0.8264 - val_loss: 0.6907 - val_accuracy: 0.7343\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.87 - ETA: 0s - loss: 0.5530 - accuracy: 0.83 - ETA: 0s - loss: 0.5693 - accuracy: 0.82 - ETA: 0s - loss: 0.5583 - accuracy: 0.82 - ETA: 0s - loss: 0.5502 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5475 - accuracy: 0.8253 - val_loss: 0.7529 - val_accuracy: 0.7463\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.84 - ETA: 0s - loss: 0.6036 - accuracy: 0.83 - ETA: 0s - loss: 0.5783 - accuracy: 0.83 - ETA: 0s - loss: 0.5588 - accuracy: 0.83 - ETA: 0s - loss: 0.5608 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5649 - accuracy: 0.8298 - val_loss: 0.7439 - val_accuracy: 0.7224\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.75 - ETA: 0s - loss: 0.5909 - accuracy: 0.78 - ETA: 0s - loss: 0.6152 - accuracy: 0.78 - ETA: 0s - loss: 0.5832 - accuracy: 0.80 - ETA: 0s - loss: 0.5672 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5643 - accuracy: 0.8074 - val_loss: 0.6584 - val_accuracy: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.71 - ETA: 0s - loss: 0.6486 - accuracy: 0.79 - ETA: 0s - loss: 0.5641 - accuracy: 0.82 - ETA: 0s - loss: 0.5500 - accuracy: 0.82 - ETA: 0s - loss: 0.5390 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5285 - accuracy: 0.8328 - val_loss: 0.8812 - val_accuracy: 0.7388\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.78 - ETA: 0s - loss: 0.4967 - accuracy: 0.83 - ETA: 0s - loss: 0.4749 - accuracy: 0.85 - ETA: 0s - loss: 0.5041 - accuracy: 0.83 - ETA: 0s - loss: 0.5028 - accuracy: 0.84 - 0s 3ms/step - loss: 0.5039 - accuracy: 0.8380 - val_loss: 1.0798 - val_accuracy: 0.7463\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 0.81 - ETA: 0s - loss: 0.4896 - accuracy: 0.85 - ETA: 0s - loss: 0.4917 - accuracy: 0.85 - ETA: 0s - loss: 0.5082 - accuracy: 0.84 - ETA: 0s - loss: 0.5489 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5640 - accuracy: 0.8305 - val_loss: 0.6128 - val_accuracy: 0.7537\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8022 - accuracy: 0.65 - ETA: 0s - loss: 0.6112 - accuracy: 0.77 - ETA: 0s - loss: 0.5844 - accuracy: 0.79 - ETA: 0s - loss: 0.5694 - accuracy: 0.80 - ETA: 0s - loss: 0.5518 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5540 - accuracy: 0.8134 - val_loss: 0.7436 - val_accuracy: 0.7567\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.84 - ETA: 0s - loss: 0.5368 - accuracy: 0.83 - ETA: 0s - loss: 0.5180 - accuracy: 0.84 - ETA: 0s - loss: 0.5495 - accuracy: 0.82 - ETA: 0s - loss: 0.5386 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5351 - accuracy: 0.8309 - val_loss: 0.8865 - val_accuracy: 0.7343\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.78 - ETA: 0s - loss: 0.4944 - accuracy: 0.83 - ETA: 0s - loss: 0.5144 - accuracy: 0.83 - ETA: 0s - loss: 0.5146 - accuracy: 0.83 - ETA: 0s - loss: 0.5310 - accuracy: 0.83 - 0s 3ms/step - loss: 0.5446 - accuracy: 0.8264 - val_loss: 0.6128 - val_accuracy: 0.7552\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.84 - ETA: 0s - loss: 0.6345 - accuracy: 0.81 - ETA: 0s - loss: 0.6162 - accuracy: 0.81 - ETA: 0s - loss: 0.6934 - accuracy: 0.81 - ETA: 0s - loss: 0.6945 - accuracy: 0.81 - 0s 3ms/step - loss: 0.6853 - accuracy: 0.8104 - val_loss: 0.6876 - val_accuracy: 0.7313\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.68 - ETA: 0s - loss: 0.7413 - accuracy: 0.76 - ETA: 0s - loss: 2.0508 - accuracy: 0.76 - ETA: 0s - loss: 1.5928 - accuracy: 0.75 - ETA: 0s - loss: 1.3509 - accuracy: 0.76 - 0s 3ms/step - loss: 1.2810 - accuracy: 0.7675 - val_loss: 0.7775 - val_accuracy: 0.7164\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.93 - ETA: 0s - loss: 0.7685 - accuracy: 0.74 - ETA: 0s - loss: 0.7906 - accuracy: 0.73 - ETA: 0s - loss: 0.7754 - accuracy: 0.72 - ETA: 0s - loss: 0.7356 - accuracy: 0.74 - 0s 3ms/step - loss: 0.7177 - accuracy: 0.7469 - val_loss: 0.7216 - val_accuracy: 0.7328\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.81 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.5927 - accuracy: 0.79 - ETA: 0s - loss: 0.5937 - accuracy: 0.78 - ETA: 0s - loss: 0.5927 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5904 - accuracy: 0.7880 - val_loss: 0.8135 - val_accuracy: 0.7403\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4738 - accuracy: 0.84 - ETA: 0s - loss: 0.5511 - accuracy: 0.84 - ETA: 0s - loss: 0.6124 - accuracy: 0.81 - ETA: 0s - loss: 0.6125 - accuracy: 0.80 - ETA: 0s - loss: 0.6119 - accuracy: 0.79 - 0s 3ms/step - loss: 0.6110 - accuracy: 0.7947 - val_loss: 0.6935 - val_accuracy: 0.7537\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.84 - ETA: 0s - loss: 0.5727 - accuracy: 0.81 - ETA: 0s - loss: 0.5692 - accuracy: 0.80 - ETA: 0s - loss: 0.5570 - accuracy: 0.80 - ETA: 0s - loss: 0.5644 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5625 - accuracy: 0.8074 - val_loss: 0.9016 - val_accuracy: 0.7552\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5457 - accuracy: 0.84 - ETA: 0s - loss: 0.6036 - accuracy: 0.79 - ETA: 0s - loss: 0.5972 - accuracy: 0.79 - ETA: 0s - loss: 0.5842 - accuracy: 0.80 - ETA: 0s - loss: 0.5897 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7895 - val_loss: 0.7186 - val_accuracy: 0.7388\n",
      "Epoch 30/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.81 - ETA: 0s - loss: 0.6006 - accuracy: 0.78 - ETA: 0s - loss: 0.5761 - accuracy: 0.79 - ETA: 0s - loss: 0.5728 - accuracy: 0.79 - ETA: 0s - loss: 0.5750 - accuracy: 0.79 - ETA: 0s - loss: 0.5745 - accuracy: 0.7956Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5756 - accuracy: 0.7947 - val_loss: 0.9642 - val_accuracy: 0.7478\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 5f75254bda604b794a6f29acf6af81ec</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7532338301340739</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8036457044607256</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 120</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 35</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9619 - accuracy: 0.46 - ETA: 0s - loss: 2.0113 - accuracy: 0.55 - ETA: 0s - loss: 1.4592 - accuracy: 0.61 - ETA: 0s - loss: 1.1971 - accuracy: 0.65 - ETA: 0s - loss: 1.0776 - accuracy: 0.66 - ETA: 0s - loss: 0.9630 - accuracy: 0.68 - ETA: 0s - loss: 0.8991 - accuracy: 0.69 - ETA: 0s - loss: 0.8524 - accuracy: 0.69 - 1s 7ms/step - loss: 0.8357 - accuracy: 0.6954 - val_loss: 0.5710 - val_accuracy: 0.6881\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.71 - ETA: 0s - loss: 0.4985 - accuracy: 0.72 - ETA: 0s - loss: 0.4868 - accuracy: 0.74 - ETA: 0s - loss: 0.5023 - accuracy: 0.75 - ETA: 0s - loss: 0.5204 - accuracy: 0.76 - ETA: 0s - loss: 0.5303 - accuracy: 0.75 - ETA: 0s - loss: 0.5366 - accuracy: 0.75 - ETA: 0s - loss: 0.5363 - accuracy: 0.76 - ETA: 0s - loss: 0.5363 - accuracy: 0.76 - 1s 6ms/step - loss: 0.5375 - accuracy: 0.7607 - val_loss: 0.5916 - val_accuracy: 0.7015\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.81 - ETA: 0s - loss: 0.4338 - accuracy: 0.81 - ETA: 0s - loss: 0.4468 - accuracy: 0.80 - ETA: 0s - loss: 0.4481 - accuracy: 0.81 - ETA: 0s - loss: 0.4409 - accuracy: 0.81 - ETA: 0s - loss: 0.4449 - accuracy: 0.81 - ETA: 0s - loss: 0.4468 - accuracy: 0.81 - ETA: 0s - loss: 0.4511 - accuracy: 0.81 - 0s 6ms/step - loss: 0.4589 - accuracy: 0.8134 - val_loss: 0.5831 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.87 - ETA: 0s - loss: 0.4170 - accuracy: 0.81 - ETA: 0s - loss: 0.4530 - accuracy: 0.80 - ETA: 0s - loss: 0.4341 - accuracy: 0.82 - ETA: 0s - loss: 0.4260 - accuracy: 0.82 - ETA: 0s - loss: 0.4216 - accuracy: 0.82 - ETA: 0s - loss: 0.4235 - accuracy: 0.82 - ETA: 0s - loss: 0.4169 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4169 - accuracy: 0.8238 - val_loss: 0.6476 - val_accuracy: 0.7045\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.78 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3905 - accuracy: 0.83 - ETA: 0s - loss: 0.3887 - accuracy: 0.83 - ETA: 0s - loss: 0.3952 - accuracy: 0.83 - ETA: 0s - loss: 0.3985 - accuracy: 0.83 - ETA: 0s - loss: 0.4028 - accuracy: 0.83 - ETA: 0s - loss: 0.4051 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8361 - val_loss: 0.6462 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.84 - ETA: 0s - loss: 0.3065 - accuracy: 0.86 - ETA: 0s - loss: 0.2955 - accuracy: 0.87 - ETA: 0s - loss: 0.3265 - accuracy: 0.86 - ETA: 0s - loss: 0.3447 - accuracy: 0.85 - ETA: 0s - loss: 0.3428 - accuracy: 0.85 - ETA: 0s - loss: 0.3637 - accuracy: 0.85 - ETA: 0s - loss: 0.3844 - accuracy: 0.84 - 0s 6ms/step - loss: 0.3842 - accuracy: 0.8458 - val_loss: 0.6981 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.87 - ETA: 0s - loss: 0.3435 - accuracy: 0.87 - ETA: 0s - loss: 0.3334 - accuracy: 0.86 - ETA: 0s - loss: 0.3740 - accuracy: 0.85 - ETA: 0s - loss: 0.3688 - accuracy: 0.84 - ETA: 0s - loss: 0.3715 - accuracy: 0.84 - ETA: 0s - loss: 0.3743 - accuracy: 0.83 - ETA: 0s - loss: 0.3714 - accuracy: 0.83 - 0s 5ms/step - loss: 0.3706 - accuracy: 0.8391 - val_loss: 1.1974 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.81 - ETA: 0s - loss: 0.3124 - accuracy: 0.83 - ETA: 0s - loss: 0.3198 - accuracy: 0.84 - ETA: 0s - loss: 0.3363 - accuracy: 0.84 - ETA: 0s - loss: 0.3430 - accuracy: 0.85 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - ETA: 0s - loss: 0.3458 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3461 - accuracy: 0.8529 - val_loss: 0.8864 - val_accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.90 - ETA: 0s - loss: 0.3279 - accuracy: 0.86 - ETA: 0s - loss: 0.3091 - accuracy: 0.86 - ETA: 0s - loss: 0.3471 - accuracy: 0.86 - ETA: 0s - loss: 0.3402 - accuracy: 0.86 - ETA: 0s - loss: 0.3542 - accuracy: 0.86 - ETA: 0s - loss: 0.3625 - accuracy: 0.85 - 0s 5ms/step - loss: 0.3564 - accuracy: 0.8537 - val_loss: 1.6078 - val_accuracy: 0.6925\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.84 - ETA: 0s - loss: 0.4280 - accuracy: 0.83 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.84 - ETA: 0s - loss: 0.3750 - accuracy: 0.83 - ETA: 0s - loss: 0.4360 - accuracy: 0.83 - ETA: 0s - loss: 0.4372 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4320 - accuracy: 0.8324 - val_loss: 0.9121 - val_accuracy: 0.7119\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.84 - ETA: 0s - loss: 0.3032 - accuracy: 0.87 - ETA: 0s - loss: 0.3435 - accuracy: 0.85 - ETA: 0s - loss: 0.3747 - accuracy: 0.83 - ETA: 0s - loss: 0.3732 - accuracy: 0.83 - ETA: 0s - loss: 0.3815 - accuracy: 0.84 - ETA: 0s - loss: 0.3937 - accuracy: 0.84 - 0s 5ms/step - loss: 0.3972 - accuracy: 0.8436 - val_loss: 1.0902 - val_accuracy: 0.7493\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.81 - ETA: 0s - loss: 0.5099 - accuracy: 0.83 - ETA: 0s - loss: 0.4397 - accuracy: 0.83 - ETA: 0s - loss: 0.4234 - accuracy: 0.84 - ETA: 0s - loss: 0.4115 - accuracy: 0.84 - ETA: 0s - loss: 0.4231 - accuracy: 0.84 - ETA: 0s - loss: 0.4370 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4501 - accuracy: 0.8290 - val_loss: 0.8771 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.84 - ETA: 0s - loss: 0.4212 - accuracy: 0.81 - ETA: 0s - loss: 0.4337 - accuracy: 0.83 - ETA: 0s - loss: 0.4052 - accuracy: 0.84 - ETA: 0s - loss: 0.4007 - accuracy: 0.84 - ETA: 0s - loss: 0.3916 - accuracy: 0.84 - ETA: 0s - loss: 0.4092 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4091 - accuracy: 0.8402 - val_loss: 1.8426 - val_accuracy: 0.7388\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.96 - ETA: 0s - loss: 0.4014 - accuracy: 0.83 - ETA: 0s - loss: 0.4200 - accuracy: 0.80 - ETA: 0s - loss: 0.3920 - accuracy: 0.81 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.83 - ETA: 0s - loss: 0.3936 - accuracy: 0.83 - 0s 5ms/step - loss: 0.3909 - accuracy: 0.8339 - val_loss: 1.0239 - val_accuracy: 0.7478\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3558 - accuracy: 0.93 - ETA: 0s - loss: 0.3416 - accuracy: 0.87 - ETA: 0s - loss: 0.3416 - accuracy: 0.87 - ETA: 0s - loss: 0.3283 - accuracy: 0.87 - ETA: 0s - loss: 0.3409 - accuracy: 0.86 - ETA: 0s - loss: 0.3436 - accuracy: 0.86 - ETA: 0s - loss: 0.3385 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3372 - accuracy: 0.8652 - val_loss: 1.2695 - val_accuracy: 0.7433\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.93 - ETA: 0s - loss: 0.3313 - accuracy: 0.87 - ETA: 0s - loss: 0.3353 - accuracy: 0.86 - ETA: 0s - loss: 0.3381 - accuracy: 0.86 - ETA: 0s - loss: 0.3297 - accuracy: 0.86 - ETA: 0s - loss: 0.3313 - accuracy: 0.86 - ETA: 0s - loss: 0.3289 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8626 - val_loss: 1.1518 - val_accuracy: 0.7433\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.87 - ETA: 0s - loss: 0.2993 - accuracy: 0.88 - ETA: 0s - loss: 0.3213 - accuracy: 0.87 - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.3758 - accuracy: 0.87 - ETA: 0s - loss: 0.3770 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3820 - accuracy: 0.8634 - val_loss: 1.8324 - val_accuracy: 0.7642\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.93 - ETA: 0s - loss: 0.3594 - accuracy: 0.86 - ETA: 0s - loss: 0.4868 - accuracy: 0.86 - ETA: 0s - loss: 0.4608 - accuracy: 0.86 - ETA: 0s - loss: 0.4951 - accuracy: 0.85 - ETA: 0s - loss: 0.5058 - accuracy: 0.84 - ETA: 0s - loss: 0.5017 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5013 - accuracy: 0.8406 - val_loss: 1.1549 - val_accuracy: 0.7597\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.75 - ETA: 0s - loss: 0.4005 - accuracy: 0.82 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4754 - accuracy: 0.76 - ETA: 0s - loss: 0.5052 - accuracy: 0.74 - ETA: 0s - loss: 0.4839 - accuracy: 0.72 - ETA: 0s - loss: 0.4818 - accuracy: 0.71 - 0s 4ms/step - loss: 0.4832 - accuracy: 0.7100 - val_loss: 2.4052 - val_accuracy: 0.5358\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.71 - ETA: 0s - loss: 0.4318 - accuracy: 0.65 - ETA: 0s - loss: 0.5260 - accuracy: 0.68 - ETA: 0s - loss: 0.5448 - accuracy: 0.62 - ETA: 0s - loss: 0.5597 - accuracy: 0.54 - ETA: 0s - loss: 0.5795 - accuracy: 0.57 - ETA: 0s - loss: 0.5921 - accuracy: 0.60 - 0s 4ms/step - loss: 0.5890 - accuracy: 0.6181 - val_loss: 0.7632 - val_accuracy: 0.7567\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5177 - accuracy: 0.81 - ETA: 0s - loss: 1.0813 - accuracy: 0.77 - ETA: 0s - loss: 0.8442 - accuracy: 0.78 - ETA: 0s - loss: 0.7853 - accuracy: 0.77 - ETA: 0s - loss: 0.7428 - accuracy: 0.77 - ETA: 0s - loss: 0.7078 - accuracy: 0.77 - ETA: 0s - loss: 0.6888 - accuracy: 0.78 - 0s 4ms/step - loss: 0.6890 - accuracy: 0.7831 - val_loss: 0.9152 - val_accuracy: 0.7403\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.90 - ETA: 0s - loss: 0.5341 - accuracy: 0.83 - ETA: 0s - loss: 0.6425 - accuracy: 0.80 - ETA: 0s - loss: 0.6500 - accuracy: 0.79 - ETA: 0s - loss: 0.6913 - accuracy: 0.79 - ETA: 0s - loss: 0.6757 - accuracy: 0.78 - ETA: 0s - loss: 0.6761 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6731 - accuracy: 0.7727 - val_loss: 0.6478 - val_accuracy: 0.7313\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.68 - ETA: 0s - loss: 0.6274 - accuracy: 0.75 - ETA: 0s - loss: 0.7883 - accuracy: 0.74 - ETA: 0s - loss: 0.7352 - accuracy: 0.75 - ETA: 0s - loss: 0.7186 - accuracy: 0.74 - ETA: 0s - loss: 0.7047 - accuracy: 0.74 - ETA: 0s - loss: 0.6931 - accuracy: 0.75 - ETA: 0s - loss: 0.6838 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6835 - accuracy: 0.7540 - val_loss: 0.6945 - val_accuracy: 0.7209\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5475 - accuracy: 0.87 - ETA: 0s - loss: 0.6248 - accuracy: 0.76 - ETA: 0s - loss: 0.6288 - accuracy: 0.75 - ETA: 0s - loss: 0.6299 - accuracy: 0.76 - ETA: 0s - loss: 0.6283 - accuracy: 0.76 - ETA: 0s - loss: 0.6235 - accuracy: 0.76 - ETA: 0s - loss: 0.6215 - accuracy: 0.76 - ETA: 0s - loss: 0.6192 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6223 - accuracy: 0.7671 - val_loss: 0.9476 - val_accuracy: 0.7299\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.84 - ETA: 0s - loss: 0.6097 - accuracy: 0.76 - ETA: 0s - loss: 0.6058 - accuracy: 0.77 - ETA: 0s - loss: 0.6435 - accuracy: 0.77 - ETA: 0s - loss: 0.6423 - accuracy: 0.77 - ETA: 0s - loss: 0.6356 - accuracy: 0.77 - ETA: 0s - loss: 0.6346 - accuracy: 0.76 - ETA: 0s - loss: 0.6293 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6275 - accuracy: 0.7719 - val_loss: 0.6512 - val_accuracy: 0.7403\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.68 - ETA: 0s - loss: 0.6285 - accuracy: 0.74 - ETA: 0s - loss: 0.6157 - accuracy: 0.76 - ETA: 0s - loss: 0.5941 - accuracy: 0.78 - ETA: 0s - loss: 0.5898 - accuracy: 0.77 - ETA: 0s - loss: 0.5868 - accuracy: 0.78 - ETA: 0s - loss: 0.5871 - accuracy: 0.78 - ETA: 0s - loss: 0.5838 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5837 - accuracy: 0.7831 - val_loss: 0.7350 - val_accuracy: 0.7284\n",
      "Epoch 27/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6631 - accuracy: 0.75 - ETA: 0s - loss: 0.5409 - accuracy: 0.80 - ETA: 0s - loss: 0.5672 - accuracy: 0.79 - ETA: 0s - loss: 0.5779 - accuracy: 0.79 - ETA: 0s - loss: 0.5919 - accuracy: 0.78 - ETA: 0s - loss: 0.5788 - accuracy: 0.79 - ETA: 0s - loss: 0.5728 - accuracy: 0.79 - ETA: 0s - loss: 0.5759 - accuracy: 0.7882Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5821 - accuracy: 0.7842 - val_loss: 0.6314 - val_accuracy: 0.7149\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.71 - ETA: 0s - loss: 1.7203 - accuracy: 0.62 - ETA: 0s - loss: 1.1672 - accuracy: 0.67 - ETA: 0s - loss: 0.9805 - accuracy: 0.67 - ETA: 0s - loss: 0.8895 - accuracy: 0.68 - ETA: 0s - loss: 0.8283 - accuracy: 0.69 - ETA: 0s - loss: 0.7940 - accuracy: 0.70 - ETA: 0s - loss: 0.7659 - accuracy: 0.70 - 1s 6ms/step - loss: 0.7639 - accuracy: 0.7055 - val_loss: 0.5612 - val_accuracy: 0.7015\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.84 - ETA: 0s - loss: 0.5227 - accuracy: 0.75 - ETA: 0s - loss: 0.5261 - accuracy: 0.75 - ETA: 0s - loss: 0.5289 - accuracy: 0.75 - ETA: 0s - loss: 0.5191 - accuracy: 0.76 - ETA: 0s - loss: 0.5200 - accuracy: 0.76 - ETA: 0s - loss: 0.5222 - accuracy: 0.77 - 0s 5ms/step - loss: 0.5212 - accuracy: 0.7738 - val_loss: 0.5707 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.81 - ETA: 0s - loss: 0.4714 - accuracy: 0.80 - ETA: 0s - loss: 0.4883 - accuracy: 0.80 - ETA: 0s - loss: 0.4785 - accuracy: 0.81 - ETA: 0s - loss: 0.4949 - accuracy: 0.79 - ETA: 0s - loss: 0.4884 - accuracy: 0.79 - ETA: 0s - loss: 0.4775 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4870 - accuracy: 0.7984 - val_loss: 0.7160 - val_accuracy: 0.6836\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.93 - ETA: 0s - loss: 0.3538 - accuracy: 0.84 - ETA: 0s - loss: 0.3588 - accuracy: 0.84 - ETA: 0s - loss: 0.3786 - accuracy: 0.84 - ETA: 0s - loss: 0.3989 - accuracy: 0.84 - ETA: 0s - loss: 0.4334 - accuracy: 0.82 - ETA: 0s - loss: 0.4363 - accuracy: 0.82 - ETA: 0s - loss: 0.4604 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4631 - accuracy: 0.8205 - val_loss: 0.5781 - val_accuracy: 0.7164\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.81 - ETA: 0s - loss: 0.4606 - accuracy: 0.79 - ETA: 0s - loss: 0.4252 - accuracy: 0.83 - ETA: 0s - loss: 0.4014 - accuracy: 0.83 - ETA: 0s - loss: 0.4774 - accuracy: 0.83 - ETA: 0s - loss: 0.4722 - accuracy: 0.82 - ETA: 0s - loss: 0.4715 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4705 - accuracy: 0.8246 - val_loss: 0.9610 - val_accuracy: 0.6746\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.87 - ETA: 0s - loss: 0.4167 - accuracy: 0.85 - ETA: 0s - loss: 0.4098 - accuracy: 0.86 - ETA: 0s - loss: 0.4160 - accuracy: 0.86 - ETA: 0s - loss: 0.4433 - accuracy: 0.85 - ETA: 0s - loss: 0.4525 - accuracy: 0.85 - ETA: 0s - loss: 0.5033 - accuracy: 0.84 - ETA: 0s - loss: 0.5440 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5409 - accuracy: 0.8249 - val_loss: 0.5495 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.71 - ETA: 0s - loss: 0.4642 - accuracy: 0.79 - ETA: 0s - loss: 0.4297 - accuracy: 0.82 - ETA: 0s - loss: 0.4260 - accuracy: 0.82 - ETA: 0s - loss: 0.4516 - accuracy: 0.82 - ETA: 0s - loss: 0.4462 - accuracy: 0.82 - ETA: 0s - loss: 0.4456 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4424 - accuracy: 0.8272 - val_loss: 0.6810 - val_accuracy: 0.7224\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.90 - ETA: 0s - loss: 0.3988 - accuracy: 0.85 - ETA: 0s - loss: 0.3776 - accuracy: 0.86 - ETA: 0s - loss: 0.3738 - accuracy: 0.86 - ETA: 0s - loss: 0.3643 - accuracy: 0.86 - ETA: 0s - loss: 0.3679 - accuracy: 0.86 - ETA: 0s - loss: 0.3781 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3842 - accuracy: 0.8626 - val_loss: 0.9605 - val_accuracy: 0.6836\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.93 - ETA: 0s - loss: 0.3320 - accuracy: 0.89 - ETA: 0s - loss: 0.3196 - accuracy: 0.89 - ETA: 0s - loss: 0.3336 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - ETA: 0s - loss: 0.3430 - accuracy: 0.88 - ETA: 0s - loss: 0.3474 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3439 - accuracy: 0.8802 - val_loss: 1.1504 - val_accuracy: 0.6925\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.87 - ETA: 0s - loss: 0.3466 - accuracy: 0.88 - ETA: 0s - loss: 0.3251 - accuracy: 0.89 - ETA: 0s - loss: 0.3361 - accuracy: 0.88 - ETA: 0s - loss: 0.3455 - accuracy: 0.88 - ETA: 0s - loss: 0.3388 - accuracy: 0.89 - ETA: 0s - loss: 0.3425 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3425 - accuracy: 0.8869 - val_loss: 0.8780 - val_accuracy: 0.7060\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.96 - ETA: 0s - loss: 0.3074 - accuracy: 0.90 - ETA: 0s - loss: 0.3007 - accuracy: 0.89 - ETA: 0s - loss: 0.2966 - accuracy: 0.90 - ETA: 0s - loss: 0.3409 - accuracy: 0.90 - ETA: 0s - loss: 0.3458 - accuracy: 0.89 - ETA: 0s - loss: 0.3469 - accuracy: 0.89 - ETA: 0s - loss: 0.3458 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3482 - accuracy: 0.8925 - val_loss: 0.7934 - val_accuracy: 0.7104\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.84 - ETA: 0s - loss: 0.3645 - accuracy: 0.88 - ETA: 0s - loss: 0.3264 - accuracy: 0.89 - ETA: 0s - loss: 0.3258 - accuracy: 0.89 - ETA: 0s - loss: 0.3375 - accuracy: 0.89 - ETA: 0s - loss: 0.3502 - accuracy: 0.88 - ETA: 0s - loss: 0.3695 - accuracy: 0.88 - 0s 5ms/step - loss: 0.3632 - accuracy: 0.8843 - val_loss: 0.9019 - val_accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 1.00 - ETA: 0s - loss: 0.4280 - accuracy: 0.90 - ETA: 0s - loss: 0.4305 - accuracy: 0.87 - ETA: 0s - loss: 0.4185 - accuracy: 0.86 - ETA: 0s - loss: 0.4037 - accuracy: 0.86 - ETA: 0s - loss: 0.4001 - accuracy: 0.86 - ETA: 0s - loss: 0.3846 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3820 - accuracy: 0.8667 - val_loss: 0.9751 - val_accuracy: 0.6642\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.87 - ETA: 0s - loss: 0.2682 - accuracy: 0.92 - ETA: 0s - loss: 0.2602 - accuracy: 0.91 - ETA: 0s - loss: 0.2689 - accuracy: 0.91 - ETA: 0s - loss: 0.2686 - accuracy: 0.91 - ETA: 0s - loss: 0.2651 - accuracy: 0.91 - ETA: 0s - loss: 0.2713 - accuracy: 0.91 - ETA: 0s - loss: 0.2707 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2707 - accuracy: 0.9130 - val_loss: 1.2634 - val_accuracy: 0.6806\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.81 - ETA: 0s - loss: 0.2895 - accuracy: 0.92 - ETA: 0s - loss: 0.2685 - accuracy: 0.91 - ETA: 0s - loss: 0.2589 - accuracy: 0.91 - ETA: 0s - loss: 0.2515 - accuracy: 0.92 - ETA: 0s - loss: 0.2745 - accuracy: 0.91 - ETA: 0s - loss: 0.2766 - accuracy: 0.91 - ETA: 0s - loss: 0.2723 - accuracy: 0.91 - 0s 5ms/step - loss: 0.2737 - accuracy: 0.9175 - val_loss: 1.0213 - val_accuracy: 0.7313\n",
      "Epoch 16/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.2928 - accuracy: 0.93 - ETA: 0s - loss: 0.2791 - accuracy: 0.92 - ETA: 0s - loss: 0.2415 - accuracy: 0.93 - ETA: 0s - loss: 0.2869 - accuracy: 0.91 - ETA: 0s - loss: 0.2967 - accuracy: 0.90 - ETA: 0s - loss: 0.2984 - accuracy: 0.89 - ETA: 0s - loss: 0.3737 - accuracy: 0.8942Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3776 - accuracy: 0.8858 - val_loss: 0.7993 - val_accuracy: 0.6970\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9133 - accuracy: 0.68 - ETA: 0s - loss: 1.9164 - accuracy: 0.57 - ETA: 0s - loss: 1.3165 - accuracy: 0.61 - ETA: 0s - loss: 1.0624 - accuracy: 0.65 - ETA: 0s - loss: 0.9391 - accuracy: 0.67 - ETA: 0s - loss: 0.8760 - accuracy: 0.67 - ETA: 0s - loss: 0.8349 - accuracy: 0.68 - 1s 6ms/step - loss: 0.8097 - accuracy: 0.6932 - val_loss: 0.5469 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.75 - ETA: 0s - loss: 0.5422 - accuracy: 0.72 - ETA: 0s - loss: 0.5472 - accuracy: 0.73 - ETA: 0s - loss: 0.5277 - accuracy: 0.74 - ETA: 0s - loss: 0.5325 - accuracy: 0.75 - ETA: 0s - loss: 0.5294 - accuracy: 0.75 - ETA: 0s - loss: 0.5240 - accuracy: 0.74 - 0s 5ms/step - loss: 0.5240 - accuracy: 0.7514 - val_loss: 0.6359 - val_accuracy: 0.7015\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.81 - ETA: 0s - loss: 0.4347 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.83 - ETA: 0s - loss: 0.4312 - accuracy: 0.82 - ETA: 0s - loss: 0.4442 - accuracy: 0.81 - ETA: 0s - loss: 0.4523 - accuracy: 0.81 - ETA: 0s - loss: 0.4593 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4632 - accuracy: 0.8126 - val_loss: 0.5628 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.93 - ETA: 0s - loss: 0.3990 - accuracy: 0.83 - ETA: 0s - loss: 0.3803 - accuracy: 0.83 - ETA: 0s - loss: 0.3782 - accuracy: 0.83 - ETA: 0s - loss: 0.3809 - accuracy: 0.83 - ETA: 0s - loss: 0.3926 - accuracy: 0.83 - ETA: 0s - loss: 0.3846 - accuracy: 0.83 - 0s 5ms/step - loss: 0.3918 - accuracy: 0.8302 - val_loss: 0.8709 - val_accuracy: 0.6791\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4341 - accuracy: 0.81 - ETA: 0s - loss: 0.4207 - accuracy: 0.83 - ETA: 0s - loss: 0.4116 - accuracy: 0.81 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.82 - ETA: 0s - loss: 0.4147 - accuracy: 0.81 - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4231 - accuracy: 0.8130 - val_loss: 0.6548 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.93 - ETA: 0s - loss: 0.3323 - accuracy: 0.86 - ETA: 0s - loss: 0.3422 - accuracy: 0.86 - ETA: 0s - loss: 0.3619 - accuracy: 0.85 - ETA: 0s - loss: 0.3822 - accuracy: 0.84 - ETA: 0s - loss: 0.4104 - accuracy: 0.83 - ETA: 0s - loss: 0.4077 - accuracy: 0.82 - 0s 5ms/step - loss: 0.4039 - accuracy: 0.8261 - val_loss: 0.9647 - val_accuracy: 0.6866\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.81 - ETA: 0s - loss: 0.3536 - accuracy: 0.85 - ETA: 0s - loss: 0.5117 - accuracy: 0.81 - ETA: 0s - loss: 0.4938 - accuracy: 0.82 - ETA: 0s - loss: 0.4572 - accuracy: 0.83 - ETA: 0s - loss: 0.4607 - accuracy: 0.82 - ETA: 0s - loss: 0.4824 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4839 - accuracy: 0.8145 - val_loss: 0.7592 - val_accuracy: 0.6821\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.81 - ETA: 0s - loss: 0.4293 - accuracy: 0.78 - ETA: 0s - loss: 0.4690 - accuracy: 0.79 - ETA: 0s - loss: 0.4567 - accuracy: 0.80 - ETA: 0s - loss: 0.4575 - accuracy: 0.81 - ETA: 0s - loss: 0.4484 - accuracy: 0.81 - ETA: 0s - loss: 0.4466 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4419 - accuracy: 0.8208 - val_loss: 0.6778 - val_accuracy: 0.7015\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.87 - ETA: 0s - loss: 0.5716 - accuracy: 0.85 - ETA: 0s - loss: 0.5295 - accuracy: 0.83 - ETA: 0s - loss: 0.5112 - accuracy: 0.82 - ETA: 0s - loss: 0.4984 - accuracy: 0.82 - ETA: 0s - loss: 0.4862 - accuracy: 0.83 - ETA: 0s - loss: 0.4801 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4729 - accuracy: 0.8346 - val_loss: 0.8949 - val_accuracy: 0.6851\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.81 - ETA: 0s - loss: 0.4104 - accuracy: 0.83 - ETA: 0s - loss: 0.3817 - accuracy: 0.85 - ETA: 0s - loss: 0.4076 - accuracy: 0.85 - ETA: 0s - loss: 0.4383 - accuracy: 0.84 - ETA: 0s - loss: 0.4587 - accuracy: 0.83 - ETA: 0s - loss: 0.4530 - accuracy: 0.83 - ETA: 0s - loss: 0.4760 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4731 - accuracy: 0.8358 - val_loss: 0.6110 - val_accuracy: 0.7448\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.96 - ETA: 0s - loss: 0.4570 - accuracy: 0.84 - ETA: 0s - loss: 0.4425 - accuracy: 0.83 - ETA: 0s - loss: 0.4380 - accuracy: 0.83 - ETA: 0s - loss: 0.4239 - accuracy: 0.84 - ETA: 0s - loss: 0.4217 - accuracy: 0.84 - ETA: 0s - loss: 0.4167 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8522 - val_loss: 0.6251 - val_accuracy: 0.7284\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.84 - ETA: 0s - loss: 0.4160 - accuracy: 0.83 - ETA: 0s - loss: 0.3950 - accuracy: 0.85 - ETA: 0s - loss: 0.4094 - accuracy: 0.84 - ETA: 0s - loss: 0.4185 - accuracy: 0.84 - ETA: 0s - loss: 0.4807 - accuracy: 0.83 - ETA: 0s - loss: 0.5194 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5665 - accuracy: 0.8122 - val_loss: 0.7649 - val_accuracy: 0.6955\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8126 - accuracy: 0.71 - ETA: 0s - loss: 0.9039 - accuracy: 0.82 - ETA: 0s - loss: 0.7030 - accuracy: 0.82 - ETA: 0s - loss: 0.6917 - accuracy: 0.81 - ETA: 0s - loss: 0.6918 - accuracy: 0.81 - ETA: 0s - loss: 0.6506 - accuracy: 0.81 - ETA: 0s - loss: 0.6588 - accuracy: 0.81 - ETA: 0s - loss: 0.6359 - accuracy: 0.81 - 0s 5ms/step - loss: 0.6389 - accuracy: 0.8141 - val_loss: 1.0199 - val_accuracy: 0.6701\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.84 - ETA: 0s - loss: 0.4254 - accuracy: 0.86 - ETA: 0s - loss: 0.4700 - accuracy: 0.83 - ETA: 0s - loss: 0.4732 - accuracy: 0.84 - ETA: 0s - loss: 0.4954 - accuracy: 0.83 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4971 - accuracy: 0.83 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4944 - accuracy: 0.8373 - val_loss: 0.7078 - val_accuracy: 0.7328\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.90 - ETA: 0s - loss: 0.4880 - accuracy: 0.83 - ETA: 0s - loss: 0.4501 - accuracy: 0.84 - ETA: 0s - loss: 0.4322 - accuracy: 0.85 - ETA: 0s - loss: 0.4390 - accuracy: 0.85 - ETA: 0s - loss: 0.4367 - accuracy: 0.85 - ETA: 0s - loss: 0.4239 - accuracy: 0.86 - ETA: 0s - loss: 0.4173 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4173 - accuracy: 0.8623 - val_loss: 0.8002 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.90 - ETA: 0s - loss: 0.8316 - accuracy: 0.88 - ETA: 0s - loss: 0.6460 - accuracy: 0.86 - ETA: 0s - loss: 0.5941 - accuracy: 0.86 - ETA: 0s - loss: 0.5564 - accuracy: 0.86 - ETA: 0s - loss: 0.5410 - accuracy: 0.85 - ETA: 0s - loss: 0.5266 - accuracy: 0.86 - ETA: 0s - loss: 0.5273 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5265 - accuracy: 0.8567 - val_loss: 0.9347 - val_accuracy: 0.7090\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4054 - accuracy: 0.87 - ETA: 0s - loss: 0.4424 - accuracy: 0.86 - ETA: 0s - loss: 0.4290 - accuracy: 0.87 - ETA: 0s - loss: 0.4441 - accuracy: 0.87 - ETA: 0s - loss: 0.4667 - accuracy: 0.85 - ETA: 0s - loss: 0.4937 - accuracy: 0.85 - ETA: 0s - loss: 0.4925 - accuracy: 0.85 - ETA: 0s - loss: 0.5104 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5104 - accuracy: 0.8511 - val_loss: 0.5654 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9388 - accuracy: 0.62 - ETA: 0s - loss: 0.6690 - accuracy: 0.83 - ETA: 0s - loss: 0.7221 - accuracy: 0.77 - ETA: 0s - loss: 0.7413 - accuracy: 0.74 - ETA: 0s - loss: 0.7443 - accuracy: 0.73 - ETA: 0s - loss: 0.7301 - accuracy: 0.73 - ETA: 0s - loss: 0.7248 - accuracy: 0.73 - ETA: 0s - loss: 0.7246 - accuracy: 0.72 - ETA: 0s - loss: 0.7212 - accuracy: 0.71 - 0s 6ms/step - loss: 0.7210 - accuracy: 0.7163 - val_loss: 0.6711 - val_accuracy: 0.6955\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.75 - ETA: 0s - loss: 0.6605 - accuracy: 0.74 - ETA: 0s - loss: 0.6757 - accuracy: 0.72 - ETA: 0s - loss: 0.6936 - accuracy: 0.70 - ETA: 0s - loss: 0.6952 - accuracy: 0.69 - ETA: 0s - loss: 0.6973 - accuracy: 0.66 - ETA: 0s - loss: 0.7037 - accuracy: 0.60 - 0s 5ms/step - loss: 0.7012 - accuracy: 0.5629 - val_loss: 0.6960 - val_accuracy: 0.3045\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.21 - ETA: 0s - loss: 0.6957 - accuracy: 0.30 - ETA: 0s - loss: 0.7004 - accuracy: 0.31 - ETA: 0s - loss: 0.6967 - accuracy: 0.30 - ETA: 0s - loss: 0.6965 - accuracy: 0.30 - ETA: 0s - loss: 0.6920 - accuracy: 0.30 - ETA: 0s - loss: 0.6803 - accuracy: 0.34 - ETA: 0s - loss: 0.6719 - accuracy: 0.40 - 0s 5ms/step - loss: 0.6811 - accuracy: 0.4255 - val_loss: 0.6484 - val_accuracy: 0.7537\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.71 - ETA: 0s - loss: 0.6432 - accuracy: 0.74 - ETA: 0s - loss: 0.6431 - accuracy: 0.76 - ETA: 0s - loss: 0.6492 - accuracy: 0.75 - ETA: 0s - loss: 0.6515 - accuracy: 0.75 - ETA: 0s - loss: 0.6442 - accuracy: 0.75 - ETA: 0s - loss: 0.6433 - accuracy: 0.75 - ETA: 0s - loss: 0.6343 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6891 - accuracy: 0.7600 - val_loss: 6.5404 - val_accuracy: 0.7328\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8516 - accuracy: 0.68 - ETA: 0s - loss: 1.0993 - accuracy: 0.74 - ETA: 0s - loss: 0.9082 - accuracy: 0.72 - ETA: 0s - loss: 0.8394 - accuracy: 0.71 - ETA: 0s - loss: 0.8056 - accuracy: 0.71 - ETA: 0s - loss: 0.7892 - accuracy: 0.70 - ETA: 0s - loss: 0.7779 - accuracy: 0.70 - ETA: 0s - loss: 0.7651 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7632 - accuracy: 0.7014 - val_loss: 0.6903 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7221 - accuracy: 0.65 - ETA: 0s - loss: 0.6802 - accuracy: 0.71 - ETA: 0s - loss: 0.6638 - accuracy: 0.74 - ETA: 0s - loss: 0.6755 - accuracy: 0.72 - ETA: 0s - loss: 0.6781 - accuracy: 0.72 - ETA: 0s - loss: 0.6861 - accuracy: 0.71 - ETA: 0s - loss: 0.6881 - accuracy: 0.70 - ETA: 0s - loss: 0.6921 - accuracy: 0.67 - 0s 5ms/step - loss: 0.6933 - accuracy: 0.6361 - val_loss: 0.7009 - val_accuracy: 0.3045\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.34 - ETA: 0s - loss: 0.6899 - accuracy: 0.29 - ETA: 0s - loss: 0.6850 - accuracy: 0.28 - ETA: 0s - loss: 0.6972 - accuracy: 0.30 - ETA: 0s - loss: 0.6933 - accuracy: 0.30 - ETA: 0s - loss: 0.6944 - accuracy: 0.30 - ETA: 0s - loss: 0.6945 - accuracy: 0.30 - ETA: 0s - loss: 0.6919 - accuracy: 0.29 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.3124 - val_loss: 0.6927 - val_accuracy: 0.6955\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.65 - ETA: 0s - loss: 0.6839 - accuracy: 0.71 - ETA: 0s - loss: 0.6906 - accuracy: 0.70 - ETA: 0s - loss: 0.6862 - accuracy: 0.70 - ETA: 0s - loss: 0.6926 - accuracy: 0.70 - ETA: 0s - loss: 0.6945 - accuracy: 0.69 - ETA: 0s - loss: 0.6914 - accuracy: 0.70 - ETA: 0s - loss: 0.6898 - accuracy: 0.70 - ETA: 0s - loss: 0.6904 - accuracy: 0.70 - ETA: 0s - loss: 0.6894 - accuracy: 0.70 - ETA: 0s - loss: 0.6934 - accuracy: 0.69 - 1s 7ms/step - loss: 0.6934 - accuracy: 0.6991 - val_loss: 0.6921 - val_accuracy: 0.6955\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.75 - ETA: 0s - loss: 0.6905 - accuracy: 0.45 - ETA: 0s - loss: 0.7040 - accuracy: 0.40 - ETA: 0s - loss: 0.6984 - accuracy: 0.36 - ETA: 0s - loss: 0.6909 - accuracy: 0.33 - ETA: 0s - loss: 0.6932 - accuracy: 0.37 - ETA: 0s - loss: 0.6914 - accuracy: 0.42 - ETA: 0s - loss: 0.6911 - accuracy: 0.46 - 0s 5ms/step - loss: 0.6935 - accuracy: 0.4797 - val_loss: 0.6902 - val_accuracy: 0.6955\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.71 - ETA: 0s - loss: 0.6785 - accuracy: 0.72 - ETA: 0s - loss: 0.6910 - accuracy: 0.60 - ETA: 0s - loss: 0.6888 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.48 - ETA: 0s - loss: 0.6931 - accuracy: 0.45 - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4375 - val_loss: 0.6955 - val_accuracy: 0.3045\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7809 - accuracy: 0.43 - ETA: 0s - loss: 0.7064 - accuracy: 0.32 - ETA: 0s - loss: 0.6925 - accuracy: 0.29 - ETA: 0s - loss: 0.6853 - accuracy: 0.34 - ETA: 0s - loss: 0.6848 - accuracy: 0.44 - ETA: 0s - loss: 0.6858 - accuracy: 0.50 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5159 - val_loss: 0.6968 - val_accuracy: 0.3045\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.28 - ETA: 0s - loss: 0.6960 - accuracy: 0.30 - ETA: 0s - loss: 0.6908 - accuracy: 0.29 - ETA: 0s - loss: 0.6869 - accuracy: 0.29 - ETA: 0s - loss: 0.6886 - accuracy: 0.29 - ETA: 0s - loss: 0.6879 - accuracy: 0.31 - ETA: 0s - loss: 0.6910 - accuracy: 0.34 - ETA: 0s - loss: 0.6896 - accuracy: 0.39 - ETA: 0s - loss: 0.6919 - accuracy: 0.44 - ETA: 0s - loss: 0.6894 - accuracy: 0.47 - ETA: 0s - loss: 0.6933 - accuracy: 0.49 - ETA: 0s - loss: 0.6922 - accuracy: 0.48 - 1s 8ms/step - loss: 0.6933 - accuracy: 0.4681 - val_loss: 0.6958 - val_accuracy: 0.3045\n",
      "Epoch 30/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.6606 - accuracy: 0.25 - ETA: 0s - loss: 0.6851 - accuracy: 0.28 - ETA: 0s - loss: 0.6985 - accuracy: 0.30 - ETA: 0s - loss: 0.6976 - accuracy: 0.30 - ETA: 0s - loss: 0.6948 - accuracy: 0.30 - ETA: 0s - loss: 0.6930 - accuracy: 0.30 - ETA: 0s - loss: 0.6920 - accuracy: 0.31 - ETA: 0s - loss: 0.6914 - accuracy: 0.37 - ETA: 0s - loss: 0.6908 - accuracy: 0.41 - ETA: 0s - loss: 0.6903 - accuracy: 0.45 - ETA: 0s - loss: 0.6917 - accuracy: 0.4850Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.6934 - accuracy: 0.4998 - val_loss: 0.6921 - val_accuracy: 0.6955\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b819d779cc717d3df09d8891dd1ce56f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7522388100624084</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.21364941980821572</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 105</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0536 - accuracy: 0.65 - ETA: 0s - loss: 3.2564 - accuracy: 0.60 - ETA: 0s - loss: 2.4243 - accuracy: 0.56 - ETA: 0s - loss: 1.9477 - accuracy: 0.60 - ETA: 0s - loss: 1.5273 - accuracy: 0.65 - ETA: 0s - loss: 1.3026 - accuracy: 0.66 - ETA: 0s - loss: 1.1949 - accuracy: 0.67 - ETA: 0s - loss: 1.0949 - accuracy: 0.67 - ETA: 0s - loss: 1.0233 - accuracy: 0.67 - 1s 9ms/step - loss: 0.9828 - accuracy: 0.6790 - val_loss: 0.5869 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.87 - ETA: 0s - loss: 0.5248 - accuracy: 0.77 - ETA: 0s - loss: 0.5262 - accuracy: 0.78 - ETA: 0s - loss: 0.5593 - accuracy: 0.74 - ETA: 0s - loss: 0.5650 - accuracy: 0.73 - ETA: 0s - loss: 0.6382 - accuracy: 0.72 - ETA: 0s - loss: 0.6112 - accuracy: 0.72 - ETA: 0s - loss: 0.5937 - accuracy: 0.73 - ETA: 0s - loss: 0.5949 - accuracy: 0.73 - ETA: 0s - loss: 0.5848 - accuracy: 0.74 - 1s 7ms/step - loss: 0.5842 - accuracy: 0.7432 - val_loss: 0.6618 - val_accuracy: 0.6791\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.78 - ETA: 0s - loss: 0.5862 - accuracy: 0.79 - ETA: 0s - loss: 0.5973 - accuracy: 0.76 - ETA: 0s - loss: 0.5642 - accuracy: 0.78 - ETA: 0s - loss: 0.5612 - accuracy: 0.77 - ETA: 0s - loss: 0.5595 - accuracy: 0.78 - ETA: 0s - loss: 0.5499 - accuracy: 0.78 - ETA: 0s - loss: 0.5490 - accuracy: 0.78 - ETA: 0s - loss: 0.5300 - accuracy: 0.78 - ETA: 0s - loss: 0.5241 - accuracy: 0.79 - ETA: 0s - loss: 0.5307 - accuracy: 0.78 - 1s 7ms/step - loss: 0.5393 - accuracy: 0.7861 - val_loss: 0.6165 - val_accuracy: 0.6881\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.84 - ETA: 0s - loss: 0.4791 - accuracy: 0.81 - ETA: 0s - loss: 0.4945 - accuracy: 0.81 - ETA: 0s - loss: 0.4761 - accuracy: 0.82 - ETA: 0s - loss: 0.4812 - accuracy: 0.81 - ETA: 0s - loss: 0.4837 - accuracy: 0.80 - ETA: 0s - loss: 0.4726 - accuracy: 0.81 - ETA: 0s - loss: 0.5022 - accuracy: 0.80 - ETA: 0s - loss: 0.4981 - accuracy: 0.80 - ETA: 0s - loss: 0.5317 - accuracy: 0.79 - ETA: 0s - loss: 0.5451 - accuracy: 0.78 - 1s 8ms/step - loss: 0.5489 - accuracy: 0.7816 - val_loss: 0.5678 - val_accuracy: 0.7582\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.87 - ETA: 0s - loss: 0.6169 - accuracy: 0.78 - ETA: 0s - loss: 0.6096 - accuracy: 0.77 - ETA: 0s - loss: 0.5828 - accuracy: 0.78 - ETA: 0s - loss: 0.6102 - accuracy: 0.77 - ETA: 0s - loss: 0.6102 - accuracy: 0.77 - ETA: 0s - loss: 0.6148 - accuracy: 0.77 - 0s 5ms/step - loss: 0.6048 - accuracy: 0.7749 - val_loss: 1.1074 - val_accuracy: 0.7119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.84 - ETA: 0s - loss: 0.5369 - accuracy: 0.79 - ETA: 0s - loss: 0.5486 - accuracy: 0.78 - ETA: 0s - loss: 0.5567 - accuracy: 0.78 - ETA: 0s - loss: 0.5466 - accuracy: 0.79 - ETA: 0s - loss: 0.5479 - accuracy: 0.79 - ETA: 0s - loss: 0.5539 - accuracy: 0.78 - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7876 - val_loss: 1.0073 - val_accuracy: 0.6463\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.75 - ETA: 0s - loss: 0.4728 - accuracy: 0.82 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - ETA: 0s - loss: 0.4706 - accuracy: 0.81 - ETA: 0s - loss: 0.4861 - accuracy: 0.80 - ETA: 0s - loss: 0.4849 - accuracy: 0.81 - ETA: 0s - loss: 0.4824 - accuracy: 0.81 - 0s 5ms/step - loss: 0.4877 - accuracy: 0.8122 - val_loss: 0.7168 - val_accuracy: 0.7045\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.78 - ETA: 0s - loss: 0.4092 - accuracy: 0.83 - ETA: 0s - loss: 0.4581 - accuracy: 0.84 - ETA: 0s - loss: 0.4434 - accuracy: 0.84 - ETA: 0s - loss: 0.4862 - accuracy: 0.83 - ETA: 0s - loss: 0.5050 - accuracy: 0.82 - ETA: 0s - loss: 0.5254 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5261 - accuracy: 0.8201 - val_loss: 0.6953 - val_accuracy: 0.7075\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.81 - ETA: 0s - loss: 0.4306 - accuracy: 0.84 - ETA: 0s - loss: 0.4538 - accuracy: 0.84 - ETA: 0s - loss: 0.4608 - accuracy: 0.83 - ETA: 0s - loss: 0.4791 - accuracy: 0.83 - ETA: 0s - loss: 0.4675 - accuracy: 0.83 - ETA: 0s - loss: 0.4673 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4716 - accuracy: 0.8328 - val_loss: 0.7217 - val_accuracy: 0.7164\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.90 - ETA: 0s - loss: 0.3776 - accuracy: 0.88 - ETA: 0s - loss: 0.4267 - accuracy: 0.86 - ETA: 0s - loss: 0.4256 - accuracy: 0.85 - ETA: 0s - loss: 0.4471 - accuracy: 0.84 - ETA: 0s - loss: 0.4494 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4622 - accuracy: 0.8417 - val_loss: 0.5976 - val_accuracy: 0.7343\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.84 - ETA: 0s - loss: 0.5214 - accuracy: 0.85 - ETA: 0s - loss: 0.5701 - accuracy: 0.84 - ETA: 0s - loss: 0.6046 - accuracy: 0.83 - ETA: 0s - loss: 0.5790 - accuracy: 0.83 - ETA: 0s - loss: 0.5590 - accuracy: 0.83 - ETA: 0s - loss: 0.5560 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5567 - accuracy: 0.8317 - val_loss: 0.6757 - val_accuracy: 0.7149\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4925 - accuracy: 0.75 - ETA: 0s - loss: 0.5623 - accuracy: 0.81 - ETA: 0s - loss: 0.5038 - accuracy: 0.82 - ETA: 0s - loss: 0.4922 - accuracy: 0.83 - ETA: 0s - loss: 0.5084 - accuracy: 0.83 - ETA: 0s - loss: 0.5309 - accuracy: 0.83 - 0s 4ms/step - loss: 0.5385 - accuracy: 0.8331 - val_loss: 0.7450 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.81 - ETA: 0s - loss: 0.4257 - accuracy: 0.84 - ETA: 0s - loss: 0.4359 - accuracy: 0.84 - ETA: 0s - loss: 0.4328 - accuracy: 0.84 - ETA: 0s - loss: 0.4362 - accuracy: 0.84 - ETA: 0s - loss: 0.4726 - accuracy: 0.84 - ETA: 0s - loss: 0.4574 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4742 - accuracy: 0.8499 - val_loss: 0.8274 - val_accuracy: 0.6881\n",
      "Epoch 14/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.87 - ETA: 0s - loss: 0.7311 - accuracy: 0.79 - ETA: 0s - loss: 0.6239 - accuracy: 0.80 - ETA: 0s - loss: 0.5547 - accuracy: 0.82 - ETA: 0s - loss: 0.5363 - accuracy: 0.83 - ETA: 0s - loss: 0.5326 - accuracy: 0.83 - ETA: 0s - loss: 0.5273 - accuracy: 0.83 - ETA: 0s - loss: 0.5251 - accuracy: 0.8327Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5221 - accuracy: 0.8335 - val_loss: 0.7651 - val_accuracy: 0.7239\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7348 - accuracy: 0.62 - ETA: 0s - loss: 2.6980 - accuracy: 0.58 - ETA: 0s - loss: 1.6635 - accuracy: 0.65 - ETA: 0s - loss: 1.3444 - accuracy: 0.66 - ETA: 0s - loss: 1.1742 - accuracy: 0.67 - ETA: 0s - loss: 1.0661 - accuracy: 0.67 - ETA: 0s - loss: 0.9868 - accuracy: 0.68 - 0s 6ms/step - loss: 0.9632 - accuracy: 0.6909 - val_loss: 0.5849 - val_accuracy: 0.7254\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.90 - ETA: 0s - loss: 0.5989 - accuracy: 0.71 - ETA: 0s - loss: 0.5687 - accuracy: 0.73 - ETA: 0s - loss: 0.5628 - accuracy: 0.73 - ETA: 0s - loss: 0.5859 - accuracy: 0.73 - ETA: 0s - loss: 0.5878 - accuracy: 0.74 - ETA: 0s - loss: 0.5777 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5774 - accuracy: 0.7499 - val_loss: 0.5945 - val_accuracy: 0.7164\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.81 - ETA: 0s - loss: 0.5285 - accuracy: 0.75 - ETA: 0s - loss: 0.5586 - accuracy: 0.76 - ETA: 0s - loss: 0.5671 - accuracy: 0.75 - ETA: 0s - loss: 0.5786 - accuracy: 0.75 - ETA: 0s - loss: 0.5752 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5617 - accuracy: 0.7734 - val_loss: 0.7784 - val_accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.71 - ETA: 0s - loss: 0.5272 - accuracy: 0.79 - ETA: 0s - loss: 0.5821 - accuracy: 0.79 - ETA: 0s - loss: 0.5813 - accuracy: 0.79 - ETA: 0s - loss: 0.5677 - accuracy: 0.78 - ETA: 0s - loss: 0.5854 - accuracy: 0.78 - ETA: 0s - loss: 0.5830 - accuracy: 0.77 - 0s 4ms/step - loss: 0.5832 - accuracy: 0.7786 - val_loss: 0.5818 - val_accuracy: 0.7269\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4417 - accuracy: 0.90 - ETA: 0s - loss: 0.5234 - accuracy: 0.78 - ETA: 0s - loss: 0.5182 - accuracy: 0.79 - ETA: 0s - loss: 0.5096 - accuracy: 0.79 - ETA: 0s - loss: 0.5076 - accuracy: 0.79 - ETA: 0s - loss: 0.5046 - accuracy: 0.79 - ETA: 0s - loss: 0.5185 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5260 - accuracy: 0.7984 - val_loss: 0.6136 - val_accuracy: 0.6716\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.81 - ETA: 0s - loss: 0.4795 - accuracy: 0.80 - ETA: 0s - loss: 0.4630 - accuracy: 0.82 - ETA: 0s - loss: 0.4666 - accuracy: 0.82 - ETA: 0s - loss: 0.4673 - accuracy: 0.82 - ETA: 0s - loss: 0.4890 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4847 - accuracy: 0.8227 - val_loss: 0.6877 - val_accuracy: 0.6925\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.93 - ETA: 0s - loss: 0.4026 - accuracy: 0.83 - ETA: 0s - loss: 0.4364 - accuracy: 0.83 - ETA: 0s - loss: 0.4306 - accuracy: 0.83 - ETA: 0s - loss: 0.4430 - accuracy: 0.84 - ETA: 0s - loss: 0.4656 - accuracy: 0.83 - ETA: 0s - loss: 0.4699 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4708 - accuracy: 0.8346 - val_loss: 0.6048 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.81 - ETA: 0s - loss: 0.4876 - accuracy: 0.82 - ETA: 0s - loss: 0.4694 - accuracy: 0.83 - ETA: 0s - loss: 0.4604 - accuracy: 0.83 - ETA: 0s - loss: 0.4543 - accuracy: 0.84 - ETA: 0s - loss: 0.4649 - accuracy: 0.84 - ETA: 0s - loss: 0.4929 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4907 - accuracy: 0.8417 - val_loss: 0.6024 - val_accuracy: 0.7149\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.90 - ETA: 0s - loss: 0.3957 - accuracy: 0.86 - ETA: 0s - loss: 0.4312 - accuracy: 0.86 - ETA: 0s - loss: 0.4078 - accuracy: 0.87 - ETA: 0s - loss: 0.4240 - accuracy: 0.86 - ETA: 0s - loss: 0.4272 - accuracy: 0.85 - ETA: 0s - loss: 0.4270 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4306 - accuracy: 0.8511 - val_loss: 0.8269 - val_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.84 - ETA: 0s - loss: 0.3741 - accuracy: 0.85 - ETA: 0s - loss: 0.4165 - accuracy: 0.85 - ETA: 0s - loss: 0.4001 - accuracy: 0.86 - ETA: 0s - loss: 0.4080 - accuracy: 0.86 - ETA: 0s - loss: 0.4327 - accuracy: 0.86 - ETA: 0s - loss: 0.4327 - accuracy: 0.85 - ETA: 0s - loss: 0.4395 - accuracy: 0.85 - 0s 6ms/step - loss: 0.4415 - accuracy: 0.8548 - val_loss: 0.9117 - val_accuracy: 0.7284\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.81 - ETA: 0s - loss: 0.4243 - accuracy: 0.86 - ETA: 0s - loss: 0.3598 - accuracy: 0.89 - ETA: 0s - loss: 0.4289 - accuracy: 0.88 - ETA: 0s - loss: 0.4108 - accuracy: 0.87 - ETA: 0s - loss: 0.4155 - accuracy: 0.87 - ETA: 0s - loss: 0.4305 - accuracy: 0.86 - ETA: 0s - loss: 0.4325 - accuracy: 0.86 - ETA: 0s - loss: 0.4387 - accuracy: 0.86 - 0s 6ms/step - loss: 0.4370 - accuracy: 0.8630 - val_loss: 0.7918 - val_accuracy: 0.7328\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.81 - ETA: 0s - loss: 0.4388 - accuracy: 0.84 - ETA: 0s - loss: 0.3953 - accuracy: 0.86 - ETA: 0s - loss: 0.4064 - accuracy: 0.86 - ETA: 0s - loss: 0.4114 - accuracy: 0.85 - ETA: 0s - loss: 0.4242 - accuracy: 0.85 - ETA: 0s - loss: 0.4323 - accuracy: 0.85 - ETA: 0s - loss: 0.4323 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4431 - accuracy: 0.8511 - val_loss: 1.0644 - val_accuracy: 0.7179\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.87 - ETA: 0s - loss: 0.4178 - accuracy: 0.86 - ETA: 0s - loss: 0.4198 - accuracy: 0.85 - ETA: 0s - loss: 0.4291 - accuracy: 0.85 - ETA: 0s - loss: 0.4272 - accuracy: 0.86 - ETA: 0s - loss: 0.4241 - accuracy: 0.85 - ETA: 0s - loss: 0.4223 - accuracy: 0.85 - ETA: 0s - loss: 0.4090 - accuracy: 0.85 - 0s 6ms/step - loss: 0.3959 - accuracy: 0.8619 - val_loss: 0.8245 - val_accuracy: 0.7149\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.90 - ETA: 0s - loss: 0.3201 - accuracy: 0.89 - ETA: 0s - loss: 0.3463 - accuracy: 0.88 - ETA: 0s - loss: 0.3489 - accuracy: 0.89 - ETA: 0s - loss: 0.3654 - accuracy: 0.88 - ETA: 0s - loss: 0.4348 - accuracy: 0.87 - ETA: 0s - loss: 0.4498 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4431 - accuracy: 0.8671 - val_loss: 0.7531 - val_accuracy: 0.6970\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3934 - accuracy: 0.87 - ETA: 0s - loss: 0.4496 - accuracy: 0.87 - ETA: 0s - loss: 0.4169 - accuracy: 0.87 - ETA: 0s - loss: 0.4912 - accuracy: 0.86 - ETA: 0s - loss: 0.4880 - accuracy: 0.85 - ETA: 0s - loss: 0.4777 - accuracy: 0.85 - ETA: 0s - loss: 0.4839 - accuracy: 0.85 - ETA: 0s - loss: 0.5264 - accuracy: 0.85 - ETA: 0s - loss: 0.5356 - accuracy: 0.85 - 0s 6ms/step - loss: 0.5356 - accuracy: 0.8518 - val_loss: 0.6439 - val_accuracy: 0.7373\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.93 - ETA: 0s - loss: 0.3558 - accuracy: 0.91 - ETA: 0s - loss: 0.4032 - accuracy: 0.88 - ETA: 0s - loss: 0.4797 - accuracy: 0.87 - ETA: 0s - loss: 0.4790 - accuracy: 0.87 - ETA: 0s - loss: 0.5015 - accuracy: 0.87 - ETA: 0s - loss: 0.5063 - accuracy: 0.86 - ETA: 0s - loss: 0.4941 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4944 - accuracy: 0.8645 - val_loss: 0.6823 - val_accuracy: 0.7164\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.84 - ETA: 0s - loss: 0.3862 - accuracy: 0.87 - ETA: 0s - loss: 0.4388 - accuracy: 0.84 - ETA: 0s - loss: 0.4429 - accuracy: 0.85 - ETA: 0s - loss: 0.4343 - accuracy: 0.86 - ETA: 0s - loss: 0.4467 - accuracy: 0.86 - ETA: 0s - loss: 0.4648 - accuracy: 0.86 - ETA: 0s - loss: 0.4807 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4719 - accuracy: 0.8600 - val_loss: 1.2399 - val_accuracy: 0.7373\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.87 - ETA: 0s - loss: 0.4774 - accuracy: 0.85 - ETA: 0s - loss: 0.4768 - accuracy: 0.85 - ETA: 0s - loss: 0.4787 - accuracy: 0.85 - ETA: 0s - loss: 0.4636 - accuracy: 0.86 - ETA: 0s - loss: 0.4565 - accuracy: 0.86 - ETA: 0s - loss: 0.4483 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4497 - accuracy: 0.8623 - val_loss: 0.7353 - val_accuracy: 0.7388\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.84 - ETA: 0s - loss: 0.4314 - accuracy: 0.85 - ETA: 0s - loss: 0.4371 - accuracy: 0.86 - ETA: 0s - loss: 0.4267 - accuracy: 0.87 - ETA: 0s - loss: 0.4085 - accuracy: 0.88 - ETA: 0s - loss: 0.3942 - accuracy: 0.88 - ETA: 0s - loss: 0.3867 - accuracy: 0.88 - ETA: 0s - loss: 0.3858 - accuracy: 0.88 - ETA: 0s - loss: 0.3815 - accuracy: 0.88 - ETA: 0s - loss: 0.4256 - accuracy: 0.88 - 1s 6ms/step - loss: 0.4259 - accuracy: 0.8895 - val_loss: 1.2802 - val_accuracy: 0.7090\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.93 - ETA: 0s - loss: 0.3814 - accuracy: 0.89 - ETA: 0s - loss: 0.4249 - accuracy: 0.88 - ETA: 0s - loss: 0.4926 - accuracy: 0.89 - ETA: 0s - loss: 0.4747 - accuracy: 0.89 - ETA: 0s - loss: 0.4571 - accuracy: 0.89 - ETA: 0s - loss: 0.4778 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4868 - accuracy: 0.8817 - val_loss: 0.9830 - val_accuracy: 0.7299\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3717 - accuracy: 0.90 - ETA: 0s - loss: 0.5110 - accuracy: 0.84 - ETA: 0s - loss: 0.5147 - accuracy: 0.85 - ETA: 0s - loss: 0.5340 - accuracy: 0.85 - ETA: 0s - loss: 0.5192 - accuracy: 0.85 - ETA: 0s - loss: 0.4955 - accuracy: 0.85 - ETA: 0s - loss: 0.4789 - accuracy: 0.85 - ETA: 0s - loss: 0.4579 - accuracy: 0.86 - ETA: 0s - loss: 1.0652 - accuracy: 0.86 - 1s 6ms/step - loss: 1.0112 - accuracy: 0.8667 - val_loss: 1.1226 - val_accuracy: 0.7090\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3864 - accuracy: 0.87 - ETA: 0s - loss: 0.4547 - accuracy: 0.87 - ETA: 0s - loss: 0.4549 - accuracy: 0.86 - ETA: 0s - loss: 0.7424 - accuracy: 0.85 - ETA: 0s - loss: 0.8014 - accuracy: 0.85 - ETA: 0s - loss: 0.7338 - accuracy: 0.85 - ETA: 0s - loss: 0.6869 - accuracy: 0.85 - ETA: 0s - loss: 0.6525 - accuracy: 0.86 - 0s 6ms/step - loss: 0.6317 - accuracy: 0.8623 - val_loss: 3.2228 - val_accuracy: 0.7209\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.90 - ETA: 0s - loss: 0.7248 - accuracy: 0.89 - ETA: 0s - loss: 0.7479 - accuracy: 0.85 - ETA: 0s - loss: 0.7181 - accuracy: 0.83 - ETA: 0s - loss: 0.6863 - accuracy: 0.82 - ETA: 0s - loss: 0.7021 - accuracy: 0.82 - ETA: 0s - loss: 0.6685 - accuracy: 0.82 - 0s 5ms/step - loss: 0.6637 - accuracy: 0.8227 - val_loss: 0.5880 - val_accuracy: 0.7194\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.84 - ETA: 0s - loss: 1.2903 - accuracy: 0.86 - ETA: 0s - loss: 0.9974 - accuracy: 0.83 - ETA: 0s - loss: 0.8617 - accuracy: 0.83 - ETA: 0s - loss: 0.7963 - accuracy: 0.83 - ETA: 0s - loss: 0.7647 - accuracy: 0.83 - ETA: 0s - loss: 0.7310 - accuracy: 0.83 - ETA: 0s - loss: 0.7106 - accuracy: 0.82 - 0s 6ms/step - loss: 0.7127 - accuracy: 0.8257 - val_loss: 0.7081 - val_accuracy: 0.7493\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8730 - accuracy: 0.62 - ETA: 0s - loss: 0.5585 - accuracy: 0.80 - ETA: 0s - loss: 0.5565 - accuracy: 0.80 - ETA: 0s - loss: 0.5367 - accuracy: 0.81 - ETA: 0s - loss: 0.5284 - accuracy: 0.81 - ETA: 0s - loss: 0.5161 - accuracy: 0.82 - ETA: 0s - loss: 0.5201 - accuracy: 0.83 - ETA: 0s - loss: 0.5194 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5073 - accuracy: 0.8451 - val_loss: 1.6707 - val_accuracy: 0.7343\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.87 - ETA: 0s - loss: 0.4787 - accuracy: 0.85 - ETA: 0s - loss: 0.4507 - accuracy: 0.87 - ETA: 0s - loss: 0.4421 - accuracy: 0.87 - ETA: 0s - loss: 0.4429 - accuracy: 0.87 - ETA: 0s - loss: 0.4432 - accuracy: 0.86 - ETA: 0s - loss: 0.4523 - accuracy: 0.86 - ETA: 0s - loss: 0.4406 - accuracy: 0.86 - 0s 5ms/step - loss: 0.4443 - accuracy: 0.8656 - val_loss: 1.1826 - val_accuracy: 0.7478\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.93 - ETA: 0s - loss: 0.3413 - accuracy: 0.92 - ETA: 0s - loss: 0.3590 - accuracy: 0.90 - ETA: 0s - loss: 0.4129 - accuracy: 0.89 - ETA: 0s - loss: 0.9825 - accuracy: 0.89 - ETA: 0s - loss: 0.9516 - accuracy: 0.89 - ETA: 0s - loss: 0.8572 - accuracy: 0.89 - ETA: 0s - loss: 0.8017 - accuracy: 0.88 - ETA: 0s - loss: 0.7648 - accuracy: 0.87 - 0s 6ms/step - loss: 0.7510 - accuracy: 0.8731 - val_loss: 1.1916 - val_accuracy: 0.7090\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.87 - ETA: 0s - loss: 0.6928 - accuracy: 0.87 - ETA: 0s - loss: 0.6349 - accuracy: 0.86 - ETA: 0s - loss: 0.6714 - accuracy: 0.86 - ETA: 0s - loss: 0.6456 - accuracy: 0.85 - ETA: 0s - loss: 0.6094 - accuracy: 0.85 - ETA: 0s - loss: 0.5899 - accuracy: 0.85 - ETA: 0s - loss: 0.5746 - accuracy: 0.85 - 0s 5ms/step - loss: 0.5746 - accuracy: 0.8548 - val_loss: 1.0821 - val_accuracy: 0.7433\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.90 - ETA: 0s - loss: 0.4913 - accuracy: 0.83 - ETA: 0s - loss: 0.4609 - accuracy: 0.85 - ETA: 0s - loss: 0.4594 - accuracy: 0.86 - ETA: 0s - loss: 0.4729 - accuracy: 0.85 - ETA: 0s - loss: 0.4781 - accuracy: 0.85 - ETA: 0s - loss: 0.4730 - accuracy: 0.85 - ETA: 0s - loss: 0.4716 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8541 - val_loss: 1.0817 - val_accuracy: 0.7403\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.87 - ETA: 0s - loss: 0.3980 - accuracy: 0.88 - ETA: 0s - loss: 0.4123 - accuracy: 0.88 - ETA: 0s - loss: 0.4020 - accuracy: 0.88 - ETA: 0s - loss: 0.4006 - accuracy: 0.88 - ETA: 0s - loss: 0.4096 - accuracy: 0.88 - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8839 - val_loss: 1.3251 - val_accuracy: 0.7224\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.93 - ETA: 0s - loss: 0.7024 - accuracy: 0.87 - ETA: 0s - loss: 0.5527 - accuracy: 0.88 - ETA: 0s - loss: 0.5392 - accuracy: 0.87 - ETA: 0s - loss: 0.5006 - accuracy: 0.88 - ETA: 0s - loss: 0.5087 - accuracy: 0.88 - ETA: 0s - loss: 0.4963 - accuracy: 0.88 - 0s 5ms/step - loss: 0.4949 - accuracy: 0.8809 - val_loss: 0.7610 - val_accuracy: 0.7463\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.81 - ETA: 0s - loss: 0.4671 - accuracy: 0.85 - ETA: 0s - loss: 0.4703 - accuracy: 0.86 - ETA: 0s - loss: 0.4739 - accuracy: 0.86 - ETA: 0s - loss: 0.4683 - accuracy: 0.86 - ETA: 0s - loss: 0.4537 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4429 - accuracy: 0.8738 - val_loss: 1.3546 - val_accuracy: 0.7224\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.81 - ETA: 0s - loss: 0.3721 - accuracy: 0.89 - ETA: 0s - loss: 0.3565 - accuracy: 0.90 - ETA: 0s - loss: 0.3646 - accuracy: 0.90 - ETA: 0s - loss: 0.3616 - accuracy: 0.90 - ETA: 0s - loss: 0.3655 - accuracy: 0.90 - ETA: 0s - loss: 0.3759 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3746 - accuracy: 0.9003 - val_loss: 3.9323 - val_accuracy: 0.7269\n",
      "Epoch 34/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.4389 - accuracy: 0.87 - ETA: 0s - loss: 0.3941 - accuracy: 0.90 - ETA: 0s - loss: 0.4922 - accuracy: 0.89 - ETA: 0s - loss: 0.5625 - accuracy: 0.88 - ETA: 0s - loss: 0.5947 - accuracy: 0.87 - ETA: 0s - loss: 0.5996 - accuracy: 0.85 - ETA: 0s - loss: 0.6125 - accuracy: 0.8383Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6118 - accuracy: 0.8376 - val_loss: 2.2016 - val_accuracy: 0.7433\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.4096 - accuracy: 0.87 - ETA: 0s - loss: 2.9110 - accuracy: 0.66 - ETA: 0s - loss: 2.1021 - accuracy: 0.65 - ETA: 0s - loss: 1.6117 - accuracy: 0.66 - ETA: 0s - loss: 1.3418 - accuracy: 0.69 - ETA: 0s - loss: 1.2155 - accuracy: 0.70 - ETA: 0s - loss: 1.1439 - accuracy: 0.70 - ETA: 0s - loss: 1.1075 - accuracy: 0.70 - ETA: 0s - loss: 1.0621 - accuracy: 0.71 - 1s 7ms/step - loss: 1.0621 - accuracy: 0.7115 - val_loss: 0.6015 - val_accuracy: 0.7134\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7754 - accuracy: 0.68 - ETA: 0s - loss: 0.6347 - accuracy: 0.71 - ETA: 0s - loss: 0.6299 - accuracy: 0.73 - ETA: 0s - loss: 0.6207 - accuracy: 0.74 - ETA: 0s - loss: 0.6098 - accuracy: 0.74 - ETA: 0s - loss: 0.6068 - accuracy: 0.74 - ETA: 0s - loss: 0.6132 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6106 - accuracy: 0.7417 - val_loss: 0.6467 - val_accuracy: 0.7403\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.75 - ETA: 0s - loss: 0.6417 - accuracy: 0.77 - ETA: 0s - loss: 0.5932 - accuracy: 0.77 - ETA: 0s - loss: 0.5633 - accuracy: 0.78 - ETA: 0s - loss: 0.5805 - accuracy: 0.79 - ETA: 0s - loss: 0.5763 - accuracy: 0.78 - ETA: 0s - loss: 0.5743 - accuracy: 0.78 - ETA: 0s - loss: 0.5810 - accuracy: 0.78 - 0s 6ms/step - loss: 0.5759 - accuracy: 0.7846 - val_loss: 0.5947 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.81 - ETA: 0s - loss: 0.5369 - accuracy: 0.77 - ETA: 0s - loss: 0.5261 - accuracy: 0.79 - ETA: 0s - loss: 0.5248 - accuracy: 0.80 - ETA: 0s - loss: 0.5290 - accuracy: 0.80 - ETA: 0s - loss: 0.5318 - accuracy: 0.79 - ETA: 0s - loss: 0.5276 - accuracy: 0.80 - ETA: 0s - loss: 0.5269 - accuracy: 0.80 - ETA: 0s - loss: 0.5362 - accuracy: 0.79 - ETA: 0s - loss: 0.5344 - accuracy: 0.79 - ETA: 0s - loss: 0.5302 - accuracy: 0.79 - ETA: 0s - loss: 0.5336 - accuracy: 0.79 - 1s 8ms/step - loss: 0.5303 - accuracy: 0.8003 - val_loss: 0.7098 - val_accuracy: 0.7343\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.87 - ETA: 0s - loss: 0.4509 - accuracy: 0.83 - ETA: 0s - loss: 0.4865 - accuracy: 0.83 - ETA: 0s - loss: 0.4941 - accuracy: 0.82 - ETA: 0s - loss: 0.5011 - accuracy: 0.82 - ETA: 0s - loss: 0.5158 - accuracy: 0.82 - ETA: 0s - loss: 0.5156 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5230 - accuracy: 0.8175 - val_loss: 0.6871 - val_accuracy: 0.7119\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4082 - accuracy: 0.93 - ETA: 0s - loss: 0.5354 - accuracy: 0.82 - ETA: 0s - loss: 0.5268 - accuracy: 0.83 - ETA: 0s - loss: 0.5319 - accuracy: 0.83 - ETA: 0s - loss: 0.5719 - accuracy: 0.82 - ETA: 0s - loss: 0.5974 - accuracy: 0.80 - ETA: 0s - loss: 0.5806 - accuracy: 0.81 - ETA: 0s - loss: 0.5891 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5891 - accuracy: 0.8040 - val_loss: 0.6508 - val_accuracy: 0.7493\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4641 - accuracy: 0.87 - ETA: 0s - loss: 0.5505 - accuracy: 0.80 - ETA: 0s - loss: 0.5546 - accuracy: 0.80 - ETA: 0s - loss: 0.6038 - accuracy: 0.80 - ETA: 0s - loss: 0.6347 - accuracy: 0.80 - ETA: 0s - loss: 0.6234 - accuracy: 0.81 - ETA: 0s - loss: 0.6521 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6469 - accuracy: 0.8122 - val_loss: 0.7956 - val_accuracy: 0.7433\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.84 - ETA: 0s - loss: 0.6341 - accuracy: 0.82 - ETA: 0s - loss: 0.6243 - accuracy: 0.81 - ETA: 0s - loss: 0.5944 - accuracy: 0.81 - ETA: 0s - loss: 0.5770 - accuracy: 0.81 - ETA: 0s - loss: 0.5685 - accuracy: 0.81 - ETA: 0s - loss: 0.5813 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5788 - accuracy: 0.8130 - val_loss: 0.5946 - val_accuracy: 0.7507\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.87 - ETA: 0s - loss: 0.5522 - accuracy: 0.81 - ETA: 0s - loss: 0.5583 - accuracy: 0.80 - ETA: 0s - loss: 0.5650 - accuracy: 0.81 - ETA: 0s - loss: 0.5344 - accuracy: 0.82 - ETA: 0s - loss: 0.5278 - accuracy: 0.83 - ETA: 0s - loss: 0.5241 - accuracy: 0.83 - ETA: 0s - loss: 0.5271 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5255 - accuracy: 0.8268 - val_loss: 0.7572 - val_accuracy: 0.7313\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.84 - ETA: 0s - loss: 0.5443 - accuracy: 0.82 - ETA: 0s - loss: 0.5628 - accuracy: 0.82 - ETA: 0s - loss: 0.5523 - accuracy: 0.82 - ETA: 0s - loss: 0.5396 - accuracy: 0.82 - ETA: 0s - loss: 0.5381 - accuracy: 0.82 - ETA: 0s - loss: 0.5333 - accuracy: 0.82 - ETA: 0s - loss: 0.5149 - accuracy: 0.83 - ETA: 0s - loss: 0.5242 - accuracy: 0.83 - 0s 6ms/step - loss: 0.5241 - accuracy: 0.8376 - val_loss: 0.9550 - val_accuracy: 0.7239\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.87 - ETA: 0s - loss: 0.7655 - accuracy: 0.79 - ETA: 0s - loss: 0.6604 - accuracy: 0.80 - ETA: 0s - loss: 0.6503 - accuracy: 0.80 - ETA: 0s - loss: 0.6474 - accuracy: 0.80 - ETA: 0s - loss: 0.6507 - accuracy: 0.80 - ETA: 0s - loss: 0.6457 - accuracy: 0.79 - ETA: 0s - loss: 0.6364 - accuracy: 0.79 - ETA: 0s - loss: 0.6275 - accuracy: 0.79 - 0s 6ms/step - loss: 0.6269 - accuracy: 0.7988 - val_loss: 0.6339 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.75 - ETA: 0s - loss: 0.5825 - accuracy: 0.81 - ETA: 0s - loss: 0.6302 - accuracy: 0.79 - ETA: 0s - loss: 0.6231 - accuracy: 0.79 - ETA: 0s - loss: 0.5995 - accuracy: 0.80 - ETA: 0s - loss: 0.5924 - accuracy: 0.81 - ETA: 0s - loss: 0.5989 - accuracy: 0.81 - 0s 5ms/step - loss: 0.6223 - accuracy: 0.8085 - val_loss: 0.7261 - val_accuracy: 0.7343\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.81 - ETA: 0s - loss: 0.4913 - accuracy: 0.84 - ETA: 0s - loss: 0.5003 - accuracy: 0.83 - ETA: 0s - loss: 0.5285 - accuracy: 0.83 - ETA: 0s - loss: 0.5217 - accuracy: 0.83 - ETA: 0s - loss: 0.5080 - accuracy: 0.84 - ETA: 0s - loss: 0.5220 - accuracy: 0.83 - ETA: 0s - loss: 0.5619 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5598 - accuracy: 0.8294 - val_loss: 0.6637 - val_accuracy: 0.7433\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.90 - ETA: 0s - loss: 0.4674 - accuracy: 0.85 - ETA: 0s - loss: 0.5127 - accuracy: 0.84 - ETA: 0s - loss: 0.5119 - accuracy: 0.83 - ETA: 0s - loss: 0.5050 - accuracy: 0.84 - ETA: 0s - loss: 0.5481 - accuracy: 0.83 - ETA: 0s - loss: 0.5437 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5384 - accuracy: 0.8287 - val_loss: 0.6267 - val_accuracy: 0.7522\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.75 - ETA: 0s - loss: 0.4710 - accuracy: 0.83 - ETA: 0s - loss: 0.4602 - accuracy: 0.84 - ETA: 0s - loss: 0.4920 - accuracy: 0.83 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - ETA: 0s - loss: 0.4898 - accuracy: 0.83 - ETA: 0s - loss: 0.4805 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4884 - accuracy: 0.8402 - val_loss: 0.7438 - val_accuracy: 0.7373\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.93 - ETA: 0s - loss: 0.4604 - accuracy: 0.85 - ETA: 0s - loss: 0.4751 - accuracy: 0.83 - ETA: 0s - loss: 0.4551 - accuracy: 0.85 - ETA: 0s - loss: 0.4581 - accuracy: 0.84 - ETA: 0s - loss: 0.4897 - accuracy: 0.85 - ETA: 0s - loss: 0.5028 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5084 - accuracy: 0.8447 - val_loss: 0.6321 - val_accuracy: 0.7478\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4972 - accuracy: 0.87 - ETA: 0s - loss: 0.5269 - accuracy: 0.85 - ETA: 0s - loss: 0.5880 - accuracy: 0.84 - ETA: 0s - loss: 0.5400 - accuracy: 0.85 - ETA: 0s - loss: 0.5520 - accuracy: 0.84 - ETA: 0s - loss: 0.5425 - accuracy: 0.84 - ETA: 0s - loss: 0.5581 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5527 - accuracy: 0.8380 - val_loss: 0.7307 - val_accuracy: 0.7478\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.68 - ETA: 0s - loss: 0.5192 - accuracy: 0.80 - ETA: 0s - loss: 0.4888 - accuracy: 0.83 - ETA: 0s - loss: 0.4832 - accuracy: 0.83 - ETA: 0s - loss: 0.4740 - accuracy: 0.84 - ETA: 0s - loss: 0.4696 - accuracy: 0.85 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - ETA: 0s - loss: 0.4755 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4740 - accuracy: 0.8526 - val_loss: 0.9611 - val_accuracy: 0.7418\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3063 - accuracy: 0.93 - ETA: 0s - loss: 0.3934 - accuracy: 0.88 - ETA: 0s - loss: 0.4190 - accuracy: 0.87 - ETA: 0s - loss: 0.4614 - accuracy: 0.85 - ETA: 0s - loss: 0.4631 - accuracy: 0.85 - ETA: 0s - loss: 0.4775 - accuracy: 0.85 - ETA: 0s - loss: 0.4697 - accuracy: 0.85 - 0s 5ms/step - loss: 0.4693 - accuracy: 0.8541 - val_loss: 0.6975 - val_accuracy: 0.7373\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.96 - ETA: 0s - loss: 0.4711 - accuracy: 0.85 - ETA: 0s - loss: 0.5139 - accuracy: 0.83 - ETA: 0s - loss: 0.5139 - accuracy: 0.83 - ETA: 0s - loss: 0.5025 - accuracy: 0.83 - ETA: 0s - loss: 0.5219 - accuracy: 0.84 - ETA: 0s - loss: 0.5330 - accuracy: 0.85 - ETA: 0s - loss: 0.5427 - accuracy: 0.84 - 0s 5ms/step - loss: 0.5440 - accuracy: 0.8455 - val_loss: 0.6213 - val_accuracy: 0.7433\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.84 - ETA: 0s - loss: 0.5386 - accuracy: 0.81 - ETA: 0s - loss: 0.5771 - accuracy: 0.80 - ETA: 0s - loss: 0.5592 - accuracy: 0.81 - ETA: 0s - loss: 0.5549 - accuracy: 0.81 - ETA: 0s - loss: 0.5510 - accuracy: 0.81 - ETA: 0s - loss: 0.5754 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5712 - accuracy: 0.8197 - val_loss: 0.5894 - val_accuracy: 0.7567\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7520 - accuracy: 0.65 - ETA: 0s - loss: 0.5256 - accuracy: 0.81 - ETA: 0s - loss: 0.4948 - accuracy: 0.83 - ETA: 0s - loss: 0.5321 - accuracy: 0.83 - ETA: 0s - loss: 0.5525 - accuracy: 0.82 - ETA: 0s - loss: 0.5603 - accuracy: 0.81 - ETA: 0s - loss: 0.5516 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5510 - accuracy: 0.8216 - val_loss: 0.6231 - val_accuracy: 0.7582\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.75 - ETA: 0s - loss: 0.5156 - accuracy: 0.83 - ETA: 0s - loss: 0.5066 - accuracy: 0.82 - ETA: 0s - loss: 0.5161 - accuracy: 0.82 - ETA: 0s - loss: 0.5354 - accuracy: 0.82 - ETA: 0s - loss: 0.5355 - accuracy: 0.82 - ETA: 0s - loss: 0.5345 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5333 - accuracy: 0.8219 - val_loss: 0.6478 - val_accuracy: 0.7493\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.68 - ETA: 0s - loss: 0.5605 - accuracy: 0.79 - ETA: 0s - loss: 0.5407 - accuracy: 0.81 - ETA: 0s - loss: 0.5246 - accuracy: 0.81 - ETA: 0s - loss: 0.5266 - accuracy: 0.82 - ETA: 0s - loss: 0.5249 - accuracy: 0.82 - ETA: 0s - loss: 0.5186 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5205 - accuracy: 0.8246 - val_loss: 0.7497 - val_accuracy: 0.7537\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.90 - ETA: 0s - loss: 0.5064 - accuracy: 0.82 - ETA: 0s - loss: 0.4974 - accuracy: 0.83 - ETA: 0s - loss: 0.4893 - accuracy: 0.83 - ETA: 0s - loss: 0.4993 - accuracy: 0.83 - ETA: 0s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4916 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4873 - accuracy: 0.8436 - val_loss: 0.7515 - val_accuracy: 0.7507\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.93 - ETA: 0s - loss: 0.4338 - accuracy: 0.87 - ETA: 0s - loss: 0.4627 - accuracy: 0.85 - ETA: 0s - loss: 0.4587 - accuracy: 0.85 - ETA: 0s - loss: 0.4640 - accuracy: 0.85 - ETA: 0s - loss: 0.4682 - accuracy: 0.84 - ETA: 0s - loss: 0.4719 - accuracy: 0.84 - ETA: 0s - loss: 0.4653 - accuracy: 0.85 - 1s 6ms/step - loss: 0.4647 - accuracy: 0.8526 - val_loss: 0.7135 - val_accuracy: 0.7597\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.84 - ETA: 0s - loss: 0.4223 - accuracy: 0.87 - ETA: 0s - loss: 0.4494 - accuracy: 0.85 - ETA: 0s - loss: 0.4617 - accuracy: 0.85 - ETA: 0s - loss: 0.6437 - accuracy: 0.84 - ETA: 0s - loss: 0.6208 - accuracy: 0.84 - ETA: 0s - loss: 0.6061 - accuracy: 0.84 - 0s 5ms/step - loss: 0.6004 - accuracy: 0.8391 - val_loss: 0.7114 - val_accuracy: 0.7493\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.78 - ETA: 0s - loss: 0.5576 - accuracy: 0.81 - ETA: 0s - loss: 0.5846 - accuracy: 0.82 - ETA: 0s - loss: 0.5803 - accuracy: 0.82 - ETA: 0s - loss: 0.5616 - accuracy: 0.83 - ETA: 0s - loss: 0.5580 - accuracy: 0.83 - ETA: 0s - loss: 0.5583 - accuracy: 0.82 - ETA: 0s - loss: 0.5627 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5627 - accuracy: 0.8242 - val_loss: 0.6277 - val_accuracy: 0.7403\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.87 - ETA: 0s - loss: 0.5163 - accuracy: 0.82 - ETA: 0s - loss: 0.4903 - accuracy: 0.84 - ETA: 0s - loss: 0.5090 - accuracy: 0.85 - ETA: 0s - loss: 0.5224 - accuracy: 0.84 - ETA: 0s - loss: 0.5322 - accuracy: 0.83 - ETA: 0s - loss: 0.5204 - accuracy: 0.83 - 0s 5ms/step - loss: 0.5354 - accuracy: 0.8298 - val_loss: 0.7922 - val_accuracy: 0.7463\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.75 - ETA: 0s - loss: 0.5068 - accuracy: 0.83 - ETA: 0s - loss: 0.5054 - accuracy: 0.83 - ETA: 0s - loss: 0.5274 - accuracy: 0.82 - ETA: 0s - loss: 0.5318 - accuracy: 0.82 - ETA: 0s - loss: 0.5510 - accuracy: 0.81 - ETA: 0s - loss: 0.6000 - accuracy: 0.80 - 0s 5ms/step - loss: 0.6049 - accuracy: 0.7984 - val_loss: 0.6441 - val_accuracy: 0.7045\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.78 - ETA: 0s - loss: 0.6634 - accuracy: 0.75 - ETA: 0s - loss: 0.7135 - accuracy: 0.74 - ETA: 0s - loss: 0.7019 - accuracy: 0.73 - ETA: 0s - loss: 0.6912 - accuracy: 0.73 - ETA: 0s - loss: 0.6831 - accuracy: 0.73 - ETA: 0s - loss: 0.6764 - accuracy: 0.73 - 0s 5ms/step - loss: 0.6739 - accuracy: 0.7368 - val_loss: 0.6434 - val_accuracy: 0.7224\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.71 - ETA: 0s - loss: 0.6431 - accuracy: 0.75 - ETA: 0s - loss: 0.6554 - accuracy: 0.75 - ETA: 0s - loss: 0.6427 - accuracy: 0.75 - ETA: 0s - loss: 0.6341 - accuracy: 0.76 - ETA: 0s - loss: 0.6382 - accuracy: 0.75 - ETA: 0s - loss: 0.6277 - accuracy: 0.76 - 0s 5ms/step - loss: 0.6207 - accuracy: 0.7671 - val_loss: 0.9516 - val_accuracy: 0.7224\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.84 - ETA: 0s - loss: 0.6073 - accuracy: 0.81 - ETA: 0s - loss: 0.6028 - accuracy: 0.81 - ETA: 0s - loss: 0.6019 - accuracy: 0.80 - ETA: 0s - loss: 0.6035 - accuracy: 0.79 - ETA: 0s - loss: 0.5959 - accuracy: 0.79 - ETA: 0s - loss: 0.5868 - accuracy: 0.79 - ETA: 0s - loss: 0.5887 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5877 - accuracy: 0.7969 - val_loss: 0.8832 - val_accuracy: 0.7358\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6664 - accuracy: 0.68 - ETA: 0s - loss: 0.5371 - accuracy: 0.81 - ETA: 0s - loss: 0.5858 - accuracy: 0.81 - ETA: 0s - loss: 0.5963 - accuracy: 0.79 - ETA: 0s - loss: 0.6146 - accuracy: 0.79 - ETA: 0s - loss: 0.6042 - accuracy: 0.79 - ETA: 0s - loss: 0.6173 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6089 - accuracy: 0.7981 - val_loss: 1.2949 - val_accuracy: 0.7403\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.81 - ETA: 0s - loss: 0.5626 - accuracy: 0.81 - ETA: 0s - loss: 0.5809 - accuracy: 0.81 - ETA: 0s - loss: 0.5834 - accuracy: 0.80 - ETA: 0s - loss: 0.5750 - accuracy: 0.80 - ETA: 0s - loss: 0.5746 - accuracy: 0.80 - ETA: 0s - loss: 0.5797 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5773 - accuracy: 0.8003 - val_loss: 0.7670 - val_accuracy: 0.7299\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/84 [=========================>....] - ETA: 0s - loss: 0.4651 - accuracy: 0.87 - ETA: 0s - loss: 0.5328 - accuracy: 0.81 - ETA: 0s - loss: 0.5479 - accuracy: 0.80 - ETA: 0s - loss: 0.5408 - accuracy: 0.81 - ETA: 0s - loss: 0.5396 - accuracy: 0.81 - ETA: 0s - loss: 0.5382 - accuracy: 0.8138Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.8167 - val_loss: 0.8228 - val_accuracy: 0.7448\n",
      "Epoch 00036: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d24603a7c6688b2f39dbf6af54d80012</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.755721390247345</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.4700452932583201</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 350</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 175</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.78 - ETA: 0s - loss: 1.0398 - accuracy: 0.67 - ETA: 0s - loss: 0.8462 - accuracy: 0.68 - ETA: 0s - loss: 0.7823 - accuracy: 0.69 - ETA: 0s - loss: 0.7387 - accuracy: 0.70 - ETA: 0s - loss: 0.7425 - accuracy: 0.70 - 0s 5ms/step - loss: 0.7425 - accuracy: 0.7051 - val_loss: 0.5892 - val_accuracy: 0.7343\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.78 - ETA: 0s - loss: 0.5665 - accuracy: 0.75 - ETA: 0s - loss: 0.5646 - accuracy: 0.75 - ETA: 0s - loss: 0.5603 - accuracy: 0.75 - ETA: 0s - loss: 0.5491 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5506 - accuracy: 0.7630 - val_loss: 0.6722 - val_accuracy: 0.7119\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.78 - ETA: 0s - loss: 0.5192 - accuracy: 0.79 - ETA: 0s - loss: 0.5018 - accuracy: 0.80 - ETA: 0s - loss: 0.4912 - accuracy: 0.81 - ETA: 0s - loss: 0.5120 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5160 - accuracy: 0.7962 - val_loss: 0.6069 - val_accuracy: 0.7060\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.84 - ETA: 0s - loss: 0.4536 - accuracy: 0.82 - ETA: 0s - loss: 0.4932 - accuracy: 0.81 - ETA: 0s - loss: 0.4903 - accuracy: 0.82 - ETA: 0s - loss: 0.4838 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4774 - accuracy: 0.8234 - val_loss: 0.8195 - val_accuracy: 0.6955\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.90 - ETA: 0s - loss: 0.4225 - accuracy: 0.85 - ETA: 0s - loss: 0.4231 - accuracy: 0.86 - ETA: 0s - loss: 0.4896 - accuracy: 0.82 - ETA: 0s - loss: 0.5013 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5022 - accuracy: 0.8089 - val_loss: 0.7000 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.81 - ETA: 0s - loss: 0.4228 - accuracy: 0.84 - ETA: 0s - loss: 0.4661 - accuracy: 0.83 - ETA: 0s - loss: 0.4512 - accuracy: 0.83 - ETA: 0s - loss: 0.4525 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4573 - accuracy: 0.8376 - val_loss: 0.6122 - val_accuracy: 0.7507\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.71 - ETA: 0s - loss: 0.4394 - accuracy: 0.86 - ETA: 0s - loss: 0.4593 - accuracy: 0.85 - ETA: 0s - loss: 0.4596 - accuracy: 0.85 - ETA: 0s - loss: 0.4531 - accuracy: 0.85 - ETA: 0s - loss: 0.4482 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4498 - accuracy: 0.8473 - val_loss: 0.7566 - val_accuracy: 0.7119\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.90 - ETA: 0s - loss: 0.4197 - accuracy: 0.87 - ETA: 0s - loss: 0.4381 - accuracy: 0.86 - ETA: 0s - loss: 0.4531 - accuracy: 0.86 - ETA: 0s - loss: 0.4601 - accuracy: 0.86 - ETA: 0s - loss: 0.4425 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4425 - accuracy: 0.8634 - val_loss: 0.6650 - val_accuracy: 0.7030\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.84 - ETA: 0s - loss: 0.4705 - accuracy: 0.81 - ETA: 0s - loss: 0.5045 - accuracy: 0.82 - ETA: 0s - loss: 0.4870 - accuracy: 0.83 - ETA: 0s - loss: 0.4754 - accuracy: 0.83 - ETA: 0s - loss: 0.4717 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4717 - accuracy: 0.8380 - val_loss: 0.6820 - val_accuracy: 0.7030\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.81 - ETA: 0s - loss: 0.4837 - accuracy: 0.84 - ETA: 0s - loss: 0.4286 - accuracy: 0.85 - ETA: 0s - loss: 0.4294 - accuracy: 0.85 - ETA: 0s - loss: 0.4295 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8600 - val_loss: 0.7707 - val_accuracy: 0.7149\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.84 - ETA: 0s - loss: 0.3878 - accuracy: 0.86 - ETA: 0s - loss: 0.3726 - accuracy: 0.88 - ETA: 0s - loss: 0.4272 - accuracy: 0.87 - ETA: 0s - loss: 0.4220 - accuracy: 0.87 - ETA: 0s - loss: 0.4210 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8708 - val_loss: 0.7241 - val_accuracy: 0.7134\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.84 - ETA: 0s - loss: 0.3639 - accuracy: 0.87 - ETA: 0s - loss: 0.3916 - accuracy: 0.86 - ETA: 0s - loss: 0.3942 - accuracy: 0.86 - ETA: 0s - loss: 0.3726 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8809 - val_loss: 0.9051 - val_accuracy: 0.7119\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.87 - ETA: 0s - loss: 0.3605 - accuracy: 0.88 - ETA: 0s - loss: 0.3897 - accuracy: 0.87 - ETA: 0s - loss: 0.3870 - accuracy: 0.88 - ETA: 0s - loss: 0.3892 - accuracy: 0.87 - ETA: 0s - loss: 0.3791 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8742 - val_loss: 0.6394 - val_accuracy: 0.7254\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.90 - ETA: 0s - loss: 0.4757 - accuracy: 0.86 - ETA: 0s - loss: 0.5019 - accuracy: 0.84 - ETA: 0s - loss: 0.4728 - accuracy: 0.85 - ETA: 0s - loss: 0.4452 - accuracy: 0.85 - ETA: 0s - loss: 0.4373 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4265 - accuracy: 0.8645 - val_loss: 0.9964 - val_accuracy: 0.6955\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.90 - ETA: 0s - loss: 0.4001 - accuracy: 0.89 - ETA: 0s - loss: 0.3507 - accuracy: 0.90 - ETA: 0s - loss: 0.3428 - accuracy: 0.90 - ETA: 0s - loss: 0.3615 - accuracy: 0.89 - ETA: 0s - loss: 0.3931 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3973 - accuracy: 0.8854 - val_loss: 0.6624 - val_accuracy: 0.7149\n",
      "Epoch 16/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.87 - ETA: 0s - loss: 0.3361 - accuracy: 0.89 - ETA: 0s - loss: 0.3472 - accuracy: 0.89 - ETA: 0s - loss: 0.3629 - accuracy: 0.90 - ETA: 0s - loss: 0.3555 - accuracy: 0.90 - ETA: 0s - loss: 0.3675 - accuracy: 0.8980Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8977 - val_loss: 0.6221 - val_accuracy: 0.7299\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8641 - accuracy: 0.62 - ETA: 0s - loss: 1.3519 - accuracy: 0.59 - ETA: 0s - loss: 1.0588 - accuracy: 0.63 - ETA: 0s - loss: 0.9021 - accuracy: 0.66 - ETA: 0s - loss: 0.8366 - accuracy: 0.66 - ETA: 0s - loss: 0.7884 - accuracy: 0.68 - 0s 5ms/step - loss: 0.7860 - accuracy: 0.6805 - val_loss: 0.5663 - val_accuracy: 0.7284\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.71 - ETA: 0s - loss: 0.5097 - accuracy: 0.78 - ETA: 0s - loss: 0.5547 - accuracy: 0.75 - ETA: 0s - loss: 0.5640 - accuracy: 0.74 - ETA: 0s - loss: 0.5542 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7499 - val_loss: 0.5902 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.81 - ETA: 0s - loss: 0.5108 - accuracy: 0.75 - ETA: 0s - loss: 0.4924 - accuracy: 0.77 - ETA: 0s - loss: 0.5056 - accuracy: 0.78 - ETA: 0s - loss: 0.5185 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5177 - accuracy: 0.7786 - val_loss: 0.5595 - val_accuracy: 0.7418\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.87 - ETA: 0s - loss: 0.4786 - accuracy: 0.82 - ETA: 0s - loss: 0.4574 - accuracy: 0.81 - ETA: 0s - loss: 0.4607 - accuracy: 0.81 - ETA: 0s - loss: 0.4805 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4780 - accuracy: 0.8078 - val_loss: 0.5919 - val_accuracy: 0.6881\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.84 - ETA: 0s - loss: 0.5560 - accuracy: 0.82 - ETA: 0s - loss: 0.5528 - accuracy: 0.80 - ETA: 0s - loss: 0.5198 - accuracy: 0.81 - ETA: 0s - loss: 0.5140 - accuracy: 0.80 - ETA: 0s - loss: 0.5095 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5095 - accuracy: 0.8033 - val_loss: 0.6714 - val_accuracy: 0.6910\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.83 - ETA: 0s - loss: 0.4095 - accuracy: 0.84 - ETA: 0s - loss: 0.4804 - accuracy: 0.82 - ETA: 0s - loss: 0.4934 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4946 - accuracy: 0.8152 - val_loss: 0.5830 - val_accuracy: 0.7284\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.87 - ETA: 0s - loss: 0.4376 - accuracy: 0.82 - ETA: 0s - loss: 0.4087 - accuracy: 0.83 - ETA: 0s - loss: 0.4177 - accuracy: 0.83 - ETA: 0s - loss: 0.4424 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4638 - accuracy: 0.8272 - val_loss: 0.7453 - val_accuracy: 0.6881\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.4724 - accuracy: 0.82 - ETA: 0s - loss: 0.6083 - accuracy: 0.81 - ETA: 0s - loss: 0.5821 - accuracy: 0.81 - ETA: 0s - loss: 0.5604 - accuracy: 0.81 - 0s 3ms/step - loss: 0.5386 - accuracy: 0.8208 - val_loss: 0.9237 - val_accuracy: 0.7194\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.90 - ETA: 0s - loss: 0.4168 - accuracy: 0.87 - ETA: 0s - loss: 0.4024 - accuracy: 0.87 - ETA: 0s - loss: 0.3961 - accuracy: 0.87 - ETA: 0s - loss: 0.3944 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8582 - val_loss: 0.6636 - val_accuracy: 0.7149\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.87 - ETA: 0s - loss: 0.5256 - accuracy: 0.85 - ETA: 0s - loss: 0.5019 - accuracy: 0.83 - ETA: 0s - loss: 0.4861 - accuracy: 0.82 - ETA: 0s - loss: 0.4817 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4728 - accuracy: 0.8283 - val_loss: 0.6437 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.96 - ETA: 0s - loss: 0.3910 - accuracy: 0.88 - ETA: 0s - loss: 0.3821 - accuracy: 0.88 - ETA: 0s - loss: 0.3998 - accuracy: 0.87 - ETA: 0s - loss: 0.4144 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8615 - val_loss: 0.7262 - val_accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.84 - ETA: 0s - loss: 0.4220 - accuracy: 0.86 - ETA: 0s - loss: 0.3852 - accuracy: 0.87 - ETA: 0s - loss: 0.3834 - accuracy: 0.86 - ETA: 0s - loss: 0.3866 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3984 - accuracy: 0.8694 - val_loss: 0.7781 - val_accuracy: 0.7567\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3881 - accuracy: 0.84 - ETA: 0s - loss: 0.4024 - accuracy: 0.87 - ETA: 0s - loss: 0.3913 - accuracy: 0.86 - ETA: 0s - loss: 0.3838 - accuracy: 0.86 - ETA: 0s - loss: 0.3743 - accuracy: 0.87 - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8764 - val_loss: 1.0573 - val_accuracy: 0.7299\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.96 - ETA: 0s - loss: 0.2902 - accuracy: 0.92 - ETA: 0s - loss: 0.3238 - accuracy: 0.90 - ETA: 0s - loss: 0.3298 - accuracy: 0.90 - ETA: 0s - loss: 0.3470 - accuracy: 0.89 - ETA: 0s - loss: 0.3603 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3609 - accuracy: 0.8925 - val_loss: 0.7795 - val_accuracy: 0.7343\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.87 - ETA: 0s - loss: 0.3915 - accuracy: 0.87 - ETA: 0s - loss: 0.3873 - accuracy: 0.88 - ETA: 0s - loss: 0.3681 - accuracy: 0.89 - ETA: 0s - loss: 0.3773 - accuracy: 0.88 - ETA: 0s - loss: 0.3712 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8884 - val_loss: 0.8317 - val_accuracy: 0.7239\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.93 - ETA: 0s - loss: 0.3775 - accuracy: 0.88 - ETA: 0s - loss: 0.3435 - accuracy: 0.90 - ETA: 0s - loss: 0.3470 - accuracy: 0.90 - ETA: 0s - loss: 0.3551 - accuracy: 0.89 - ETA: 0s - loss: 0.3611 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3611 - accuracy: 0.8936 - val_loss: 0.7903 - val_accuracy: 0.7403\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.84 - ETA: 0s - loss: 0.3335 - accuracy: 0.90 - ETA: 0s - loss: 0.3673 - accuracy: 0.89 - ETA: 0s - loss: 0.3879 - accuracy: 0.88 - ETA: 0s - loss: 0.3829 - accuracy: 0.87 - ETA: 0s - loss: 0.3853 - accuracy: 0.87 - 0s 4ms/step - loss: 0.3853 - accuracy: 0.8791 - val_loss: 0.8420 - val_accuracy: 0.7209\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3992 - accuracy: 0.87 - ETA: 0s - loss: 0.2822 - accuracy: 0.91 - ETA: 0s - loss: 0.3082 - accuracy: 0.90 - ETA: 0s - loss: 0.3206 - accuracy: 0.90 - ETA: 0s - loss: 0.3307 - accuracy: 0.89 - ETA: 0s - loss: 0.3818 - accuracy: 0.88 - 0s 4ms/step - loss: 0.3892 - accuracy: 0.8865 - val_loss: 0.7571 - val_accuracy: 0.7149\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.84 - ETA: 0s - loss: 0.3963 - accuracy: 0.88 - ETA: 0s - loss: 0.4346 - accuracy: 0.88 - ETA: 0s - loss: 0.4300 - accuracy: 0.87 - ETA: 0s - loss: 0.4347 - accuracy: 0.86 - ETA: 0s - loss: 0.4245 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4252 - accuracy: 0.8686 - val_loss: 1.0410 - val_accuracy: 0.7075\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.96 - ETA: 0s - loss: 0.3133 - accuracy: 0.91 - ETA: 0s - loss: 0.3607 - accuracy: 0.89 - ETA: 0s - loss: 0.3853 - accuracy: 0.88 - ETA: 0s - loss: 0.4149 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4157 - accuracy: 0.8708 - val_loss: 1.1595 - val_accuracy: 0.7239\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.96 - ETA: 0s - loss: 0.3486 - accuracy: 0.90 - ETA: 0s - loss: 0.4531 - accuracy: 0.87 - ETA: 0s - loss: 0.4354 - accuracy: 0.87 - ETA: 0s - loss: 0.4581 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4439 - accuracy: 0.8652 - val_loss: 0.9848 - val_accuracy: 0.7149\n",
      "Epoch 22/50\n",
      "70/84 [========================>.....] - ETA: 0s - loss: 0.2174 - accuracy: 0.90 - ETA: 0s - loss: 0.3246 - accuracy: 0.88 - ETA: 0s - loss: 0.3482 - accuracy: 0.87 - ETA: 0s - loss: 0.3566 - accuracy: 0.87 - ETA: 0s - loss: 0.3553 - accuracy: 0.8839Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.8876 - val_loss: 1.0398 - val_accuracy: 0.7194\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.59 - ETA: 0s - loss: 1.1905 - accuracy: 0.63 - ETA: 0s - loss: 0.9517 - accuracy: 0.63 - ETA: 0s - loss: 0.8426 - accuracy: 0.66 - ETA: 0s - loss: 0.8026 - accuracy: 0.67 - ETA: 0s - loss: 0.7731 - accuracy: 0.67 - 0s 5ms/step - loss: 0.7684 - accuracy: 0.6790 - val_loss: 0.6217 - val_accuracy: 0.7194\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.71 - ETA: 0s - loss: 0.5203 - accuracy: 0.79 - ETA: 0s - loss: 0.5437 - accuracy: 0.77 - ETA: 0s - loss: 0.5779 - accuracy: 0.75 - ETA: 0s - loss: 0.5757 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5653 - accuracy: 0.7563 - val_loss: 0.5721 - val_accuracy: 0.7537\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.84 - ETA: 0s - loss: 0.5281 - accuracy: 0.79 - ETA: 0s - loss: 0.5183 - accuracy: 0.79 - ETA: 0s - loss: 0.5077 - accuracy: 0.81 - ETA: 0s - loss: 0.5097 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5202 - accuracy: 0.8048 - val_loss: 0.6761 - val_accuracy: 0.7149\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.90 - ETA: 0s - loss: 0.4786 - accuracy: 0.80 - ETA: 0s - loss: 0.5440 - accuracy: 0.78 - ETA: 0s - loss: 0.5521 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7906 - val_loss: 0.5556 - val_accuracy: 0.7463\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.81 - ETA: 0s - loss: 0.5276 - accuracy: 0.80 - ETA: 0s - loss: 0.4928 - accuracy: 0.82 - ETA: 0s - loss: 0.4948 - accuracy: 0.82 - ETA: 0s - loss: 0.4989 - accuracy: 0.82 - 0s 3ms/step - loss: 0.5010 - accuracy: 0.8231 - val_loss: 0.7931 - val_accuracy: 0.6881\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.84 - ETA: 0s - loss: 0.4528 - accuracy: 0.83 - ETA: 0s - loss: 0.4671 - accuracy: 0.82 - ETA: 0s - loss: 0.4690 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4742 - accuracy: 0.8231 - val_loss: 0.5698 - val_accuracy: 0.7194\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.84 - ETA: 0s - loss: 0.4625 - accuracy: 0.81 - ETA: 0s - loss: 0.4402 - accuracy: 0.83 - ETA: 0s - loss: 0.4335 - accuracy: 0.83 - ETA: 0s - loss: 0.4363 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4296 - accuracy: 0.8436 - val_loss: 0.6293 - val_accuracy: 0.7194\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4953 - accuracy: 0.78 - ETA: 0s - loss: 0.3913 - accuracy: 0.88 - ETA: 0s - loss: 0.4173 - accuracy: 0.86 - ETA: 0s - loss: 0.4051 - accuracy: 0.87 - ETA: 0s - loss: 0.4238 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4380 - accuracy: 0.8548 - val_loss: 0.6841 - val_accuracy: 0.6955\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.84 - ETA: 0s - loss: 0.4156 - accuracy: 0.84 - ETA: 0s - loss: 0.4269 - accuracy: 0.85 - ETA: 0s - loss: 0.4158 - accuracy: 0.86 - ETA: 0s - loss: 0.4194 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4216 - accuracy: 0.8582 - val_loss: 0.5895 - val_accuracy: 0.7284\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.87 - ETA: 0s - loss: 0.3466 - accuracy: 0.88 - ETA: 0s - loss: 0.4000 - accuracy: 0.86 - ETA: 0s - loss: 0.4036 - accuracy: 0.86 - ETA: 0s - loss: 0.3928 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4115 - accuracy: 0.8649 - val_loss: 0.8170 - val_accuracy: 0.7179\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.84 - ETA: 0s - loss: 0.3935 - accuracy: 0.87 - ETA: 0s - loss: 0.4091 - accuracy: 0.86 - ETA: 0s - loss: 0.4157 - accuracy: 0.86 - ETA: 0s - loss: 0.4092 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8735 - val_loss: 0.7047 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "70/84 [========================>.....] - ETA: 0s - loss: 0.3981 - accuracy: 0.90 - ETA: 0s - loss: 0.3616 - accuracy: 0.89 - ETA: 0s - loss: 0.3557 - accuracy: 0.89 - ETA: 0s - loss: 0.3491 - accuracy: 0.90 - ETA: 0s - loss: 0.3882 - accuracy: 0.8879Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8847 - val_loss: 0.5598 - val_accuracy: 0.7418\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 7fb878b3c166e5d675d6f1317ba00824</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.753731350104014</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.46177298000692524</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 165</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9979 - accuracy: 0.34 - ETA: 0s - loss: 2.3946 - accuracy: 0.58 - ETA: 0s - loss: 1.8042 - accuracy: 0.60 - 0s 3ms/step - loss: 1.6438 - accuracy: 0.6178 - val_loss: 0.5814 - val_accuracy: 0.7269\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.68 - ETA: 0s - loss: 0.7117 - accuracy: 0.65 - ETA: 0s - loss: 0.7272 - accuracy: 0.66 - 0s 2ms/step - loss: 0.7190 - accuracy: 0.6697 - val_loss: 0.5911 - val_accuracy: 0.7104\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6457 - accuracy: 0.68 - ETA: 0s - loss: 0.6572 - accuracy: 0.69 - ETA: 0s - loss: 0.6427 - accuracy: 0.69 - 0s 2ms/step - loss: 0.6366 - accuracy: 0.7021 - val_loss: 0.5663 - val_accuracy: 0.7478\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9035 - accuracy: 0.65 - ETA: 0s - loss: 0.6128 - accuracy: 0.74 - ETA: 0s - loss: 0.6168 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6279 - accuracy: 0.7275 - val_loss: 0.5812 - val_accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.78 - ETA: 0s - loss: 0.5861 - accuracy: 0.75 - ETA: 0s - loss: 0.5725 - accuracy: 0.76 - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7660 - val_loss: 0.5745 - val_accuracy: 0.7418\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.78 - ETA: 0s - loss: 0.5731 - accuracy: 0.78 - ETA: 0s - loss: 0.5669 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7708 - val_loss: 0.5776 - val_accuracy: 0.7269\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6197 - accuracy: 0.75 - ETA: 0s - loss: 0.5646 - accuracy: 0.78 - ETA: 0s - loss: 0.5511 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5439 - accuracy: 0.7969 - val_loss: 0.5564 - val_accuracy: 0.7478\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.87 - ETA: 0s - loss: 0.5255 - accuracy: 0.81 - ETA: 0s - loss: 0.5233 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5243 - accuracy: 0.8145 - val_loss: 0.5465 - val_accuracy: 0.7701\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.71 - ETA: 0s - loss: 0.5226 - accuracy: 0.81 - ETA: 0s - loss: 0.5226 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5260 - accuracy: 0.8115 - val_loss: 0.5626 - val_accuracy: 0.7299\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.78 - ETA: 0s - loss: 0.5181 - accuracy: 0.81 - ETA: 0s - loss: 0.5277 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5295 - accuracy: 0.8190 - val_loss: 0.9890 - val_accuracy: 0.7313\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.81 - ETA: 0s - loss: 0.7707 - accuracy: 0.78 - ETA: 0s - loss: 0.7726 - accuracy: 0.78 - 0s 2ms/step - loss: 0.7276 - accuracy: 0.7895 - val_loss: 0.6596 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9793 - accuracy: 0.75 - ETA: 0s - loss: 0.6481 - accuracy: 0.79 - ETA: 0s - loss: 0.6705 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6712 - accuracy: 0.7906 - val_loss: 0.6886 - val_accuracy: 0.7090\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.87 - ETA: 0s - loss: 0.6259 - accuracy: 0.79 - ETA: 0s - loss: 0.6073 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6032 - accuracy: 0.8063 - val_loss: 0.8922 - val_accuracy: 0.6642\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.62 - ETA: 0s - loss: 0.5698 - accuracy: 0.80 - ETA: 0s - loss: 0.5713 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5782 - accuracy: 0.8100 - val_loss: 0.9640 - val_accuracy: 0.5821\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.84 - ETA: 0s - loss: 0.5503 - accuracy: 0.82 - ETA: 0s - loss: 0.5413 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5530 - accuracy: 0.8182 - val_loss: 0.6383 - val_accuracy: 0.7194\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.84 - ETA: 0s - loss: 0.7254 - accuracy: 0.80 - ETA: 0s - loss: 0.6479 - accuracy: 0.81 - 0s 2ms/step - loss: 0.6635 - accuracy: 0.8111 - val_loss: 0.5732 - val_accuracy: 0.7299\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.81 - ETA: 0s - loss: 0.6152 - accuracy: 0.78 - ETA: 0s - loss: 0.6453 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6491 - accuracy: 0.7996 - val_loss: 0.6754 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "69/84 [=======================>......] - ETA: 0s - loss: 0.6855 - accuracy: 0.75 - ETA: 0s - loss: 0.6379 - accuracy: 0.80 - ETA: 0s - loss: 0.7512 - accuracy: 0.7894Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.7467 - accuracy: 0.7872 - val_loss: 0.6449 - val_accuracy: 0.7313\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0356 - accuracy: 0.59 - ETA: 0s - loss: 2.4482 - accuracy: 0.62 - ETA: 0s - loss: 1.8394 - accuracy: 0.63 - 0s 3ms/step - loss: 1.7722 - accuracy: 0.6301 - val_loss: 0.7322 - val_accuracy: 0.5925\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6107 - accuracy: 0.68 - ETA: 0s - loss: 0.7873 - accuracy: 0.64 - ETA: 0s - loss: 0.7152 - accuracy: 0.68 - 0s 2ms/step - loss: 0.7217 - accuracy: 0.6868 - val_loss: 0.6145 - val_accuracy: 0.6970\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.65 - ETA: 0s - loss: 0.6277 - accuracy: 0.69 - ETA: 0s - loss: 0.6255 - accuracy: 0.71 - 0s 2ms/step - loss: 0.6269 - accuracy: 0.7126 - val_loss: 0.5987 - val_accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.78 - ETA: 0s - loss: 0.6299 - accuracy: 0.73 - ETA: 0s - loss: 0.6111 - accuracy: 0.74 - 0s 2ms/step - loss: 0.6084 - accuracy: 0.7439 - val_loss: 0.5899 - val_accuracy: 0.7478\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7075 - accuracy: 0.68 - ETA: 0s - loss: 0.5812 - accuracy: 0.76 - ETA: 0s - loss: 0.5668 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7648 - val_loss: 0.5812 - val_accuracy: 0.7418\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.75 - ETA: 0s - loss: 0.5337 - accuracy: 0.79 - ETA: 0s - loss: 0.5648 - accuracy: 0.77 - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7768 - val_loss: 0.5668 - val_accuracy: 0.7388\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.84 - ETA: 0s - loss: 0.5268 - accuracy: 0.81 - ETA: 0s - loss: 0.6877 - accuracy: 0.78 - 0s 2ms/step - loss: 0.6681 - accuracy: 0.7876 - val_loss: 0.6012 - val_accuracy: 0.7194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3947 - accuracy: 0.87 - ETA: 0s - loss: 0.6041 - accuracy: 0.80 - ETA: 0s - loss: 0.5698 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7981 - val_loss: 0.5551 - val_accuracy: 0.7418\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.84 - ETA: 0s - loss: 0.5062 - accuracy: 0.82 - ETA: 0s - loss: 0.5135 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5200 - accuracy: 0.8190 - val_loss: 0.5948 - val_accuracy: 0.7075\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.65 - ETA: 0s - loss: 0.5220 - accuracy: 0.81 - ETA: 0s - loss: 0.4989 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5117 - accuracy: 0.8223 - val_loss: 0.5468 - val_accuracy: 0.7388\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.93 - ETA: 0s - loss: 0.4807 - accuracy: 0.84 - ETA: 0s - loss: 0.5069 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5266 - accuracy: 0.8328 - val_loss: 0.6202 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.93 - ETA: 0s - loss: 0.5563 - accuracy: 0.82 - ETA: 0s - loss: 0.6677 - accuracy: 0.80 - 0s 2ms/step - loss: 0.6562 - accuracy: 0.8074 - val_loss: 0.5853 - val_accuracy: 0.7254\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7149 - accuracy: 0.75 - ETA: 0s - loss: 0.5463 - accuracy: 0.83 - ETA: 0s - loss: 0.5612 - accuracy: 0.81 - 0s 2ms/step - loss: 0.6007 - accuracy: 0.8111 - val_loss: 0.5668 - val_accuracy: 0.7373\n",
      "Epoch 14/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.4771 - accuracy: 0.87 - ETA: 0s - loss: 0.6143 - accuracy: 0.81 - ETA: 0s - loss: 0.8626 - accuracy: 0.8092Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.8680 - accuracy: 0.8048 - val_loss: 0.6549 - val_accuracy: 0.7313\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1654 - accuracy: 0.65 - ETA: 0s - loss: 2.5728 - accuracy: 0.59 - ETA: 0s - loss: 1.8835 - accuracy: 0.61 - 0s 3ms/step - loss: 1.7732 - accuracy: 0.6200 - val_loss: 0.6037 - val_accuracy: 0.7209\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8441 - accuracy: 0.59 - ETA: 0s - loss: 0.8249 - accuracy: 0.62 - ETA: 0s - loss: 0.7592 - accuracy: 0.64 - 0s 2ms/step - loss: 0.7460 - accuracy: 0.6480 - val_loss: 0.6496 - val_accuracy: 0.7045\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0599 - accuracy: 0.59 - ETA: 0s - loss: 0.6855 - accuracy: 0.67 - ETA: 0s - loss: 0.6798 - accuracy: 0.67 - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6838 - val_loss: 0.5719 - val_accuracy: 0.7284\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.87 - ETA: 0s - loss: 0.6543 - accuracy: 0.71 - ETA: 0s - loss: 0.6682 - accuracy: 0.70 - 0s 2ms/step - loss: 0.6651 - accuracy: 0.7122 - val_loss: 0.5686 - val_accuracy: 0.7030\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7261 - accuracy: 0.65 - ETA: 0s - loss: 0.6634 - accuracy: 0.74 - ETA: 0s - loss: 0.6623 - accuracy: 0.73 - 0s 2ms/step - loss: 0.6643 - accuracy: 0.7342 - val_loss: 0.5631 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.87 - ETA: 0s - loss: 0.5872 - accuracy: 0.75 - ETA: 0s - loss: 0.6329 - accuracy: 0.75 - 0s 2ms/step - loss: 0.6258 - accuracy: 0.7503 - val_loss: 0.5706 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.71 - ETA: 0s - loss: 0.5690 - accuracy: 0.77 - ETA: 0s - loss: 0.5479 - accuracy: 0.78 - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7872 - val_loss: 0.5900 - val_accuracy: 0.7328\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6688 - accuracy: 0.78 - ETA: 0s - loss: 0.5301 - accuracy: 0.81 - ETA: 0s - loss: 0.5332 - accuracy: 0.80 - 0s 2ms/step - loss: 0.5605 - accuracy: 0.8074 - val_loss: 0.5696 - val_accuracy: 0.7448\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.87 - ETA: 0s - loss: 0.5491 - accuracy: 0.80 - ETA: 0s - loss: 0.5670 - accuracy: 0.80 - ETA: 0s - loss: 0.5769 - accuracy: 0.79 - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7973 - val_loss: 0.5412 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.90 - ETA: 0s - loss: 0.6253 - accuracy: 0.78 - ETA: 0s - loss: 0.6068 - accuracy: 0.79 - 0s 2ms/step - loss: 0.6028 - accuracy: 0.7940 - val_loss: 0.5731 - val_accuracy: 0.7418\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3838 - accuracy: 0.90 - ETA: 0s - loss: 1.0354 - accuracy: 0.76 - ETA: 0s - loss: 0.8165 - accuracy: 0.77 - 0s 2ms/step - loss: 0.7690 - accuracy: 0.7749 - val_loss: 0.6604 - val_accuracy: 0.6672\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6246 - accuracy: 0.81 - ETA: 0s - loss: 0.6177 - accuracy: 0.81 - ETA: 0s - loss: 0.5743 - accuracy: 0.81 - 0s 2ms/step - loss: 0.5684 - accuracy: 0.8134 - val_loss: 0.5495 - val_accuracy: 0.7463\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3321 - accuracy: 0.93 - ETA: 0s - loss: 0.5024 - accuracy: 0.83 - ETA: 0s - loss: 0.5145 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5251 - accuracy: 0.8163 - val_loss: 0.5840 - val_accuracy: 0.7522\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.84 - ETA: 0s - loss: 0.5056 - accuracy: 0.82 - ETA: 0s - loss: 0.5102 - accuracy: 0.82 - 0s 2ms/step - loss: 0.5113 - accuracy: 0.8268 - val_loss: 0.5646 - val_accuracy: 0.7567\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4325 - accuracy: 0.87 - ETA: 0s - loss: 0.4975 - accuracy: 0.83 - ETA: 0s - loss: 0.4805 - accuracy: 0.84 - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8406 - val_loss: 0.5671 - val_accuracy: 0.7209\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.87 - ETA: 0s - loss: 0.4674 - accuracy: 0.85 - ETA: 0s - loss: 0.4756 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8518 - val_loss: 0.5865 - val_accuracy: 0.7328\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.93 - ETA: 0s - loss: 0.4473 - accuracy: 0.85 - ETA: 0s - loss: 0.4538 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4593 - accuracy: 0.8541 - val_loss: 0.6254 - val_accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.84 - ETA: 0s - loss: 0.4456 - accuracy: 0.85 - ETA: 0s - loss: 0.4461 - accuracy: 0.86 - 0s 2ms/step - loss: 0.4587 - accuracy: 0.8526 - val_loss: 0.7337 - val_accuracy: 0.7030\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.84 - ETA: 0s - loss: 0.4893 - accuracy: 0.84 - ETA: 0s - loss: 0.4710 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8563 - val_loss: 0.6026 - val_accuracy: 0.7403\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.87 - ETA: 0s - loss: 0.4510 - accuracy: 0.85 - ETA: 0s - loss: 0.4571 - accuracy: 0.85 - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8593 - val_loss: 0.7370 - val_accuracy: 0.6776\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.81 - ETA: 0s - loss: 0.5175 - accuracy: 0.83 - ETA: 0s - loss: 0.5179 - accuracy: 0.83 - ETA: 0s - loss: 0.5151 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5145 - accuracy: 0.8384 - val_loss: 0.5772 - val_accuracy: 0.7403\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.78 - ETA: 0s - loss: 0.4723 - accuracy: 0.85 - ETA: 0s - loss: 0.5106 - accuracy: 0.83 - 0s 2ms/step - loss: 0.5103 - accuracy: 0.8391 - val_loss: 0.8275 - val_accuracy: 0.7254\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.93 - ETA: 0s - loss: 0.4071 - accuracy: 0.89 - ETA: 0s - loss: 0.4339 - accuracy: 0.87 - ETA: 0s - loss: 0.4719 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4742 - accuracy: 0.8570 - val_loss: 1.0014 - val_accuracy: 0.7149\n",
      "Epoch 24/50\n",
      "61/84 [====================>.........] - ETA: 0s - loss: 0.5809 - accuracy: 0.81 - ETA: 0s - loss: 0.5545 - accuracy: 0.85 - ETA: 0s - loss: 0.5240 - accuracy: 0.8545Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.8529 - val_loss: 0.5999 - val_accuracy: 0.7358\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cd6fd92933a1ef580e9e5f58fe91d667</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7582089503606161</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.891123320268961</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 100</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 70</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.9250 - accuracy: 0.65 - ETA: 0s - loss: 1.6139 - accuracy: 0.59 - ETA: 0s - loss: 1.1866 - accuracy: 0.62 - ETA: 0s - loss: 0.9783 - accuracy: 0.67 - ETA: 0s - loss: 0.9033 - accuracy: 0.67 - ETA: 0s - loss: 0.8387 - accuracy: 0.68 - ETA: 0s - loss: 0.8013 - accuracy: 0.69 - 1s 7ms/step - loss: 0.7973 - accuracy: 0.6891 - val_loss: 0.6561 - val_accuracy: 0.6582\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.5431 - accuracy: 0.75 - ETA: 0s - loss: 0.5197 - accuracy: 0.75 - ETA: 0s - loss: 0.5278 - accuracy: 0.76 - ETA: 0s - loss: 0.5345 - accuracy: 0.75 - ETA: 0s - loss: 0.5305 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5349 - accuracy: 0.7652 - val_loss: 0.6250 - val_accuracy: 0.6448\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.87 - ETA: 0s - loss: 0.4334 - accuracy: 0.80 - ETA: 0s - loss: 0.4484 - accuracy: 0.81 - ETA: 0s - loss: 0.4531 - accuracy: 0.80 - ETA: 0s - loss: 0.4503 - accuracy: 0.81 - ETA: 0s - loss: 0.4593 - accuracy: 0.80 - ETA: 0s - loss: 0.4612 - accuracy: 0.80 - 0s 5ms/step - loss: 0.4727 - accuracy: 0.7895 - val_loss: 0.5764 - val_accuracy: 0.6836\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.81 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4208 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4151 - accuracy: 0.8134 - val_loss: 0.9335 - val_accuracy: 0.6761\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.84 - ETA: 0s - loss: 0.3725 - accuracy: 0.84 - ETA: 0s - loss: 0.3779 - accuracy: 0.83 - ETA: 0s - loss: 0.3843 - accuracy: 0.84 - ETA: 0s - loss: 0.3822 - accuracy: 0.83 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - 0s 5ms/step - loss: 0.3945 - accuracy: 0.8279 - val_loss: 0.6650 - val_accuracy: 0.6955\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.87 - ETA: 0s - loss: 0.3434 - accuracy: 0.87 - ETA: 0s - loss: 0.3453 - accuracy: 0.87 - ETA: 0s - loss: 0.3698 - accuracy: 0.85 - ETA: 0s - loss: 0.3798 - accuracy: 0.84 - ETA: 0s - loss: 0.3921 - accuracy: 0.84 - ETA: 0s - loss: 0.4044 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4049 - accuracy: 0.8320 - val_loss: 0.6879 - val_accuracy: 0.6940\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.81 - ETA: 0s - loss: 0.3294 - accuracy: 0.86 - ETA: 0s - loss: 0.3876 - accuracy: 0.85 - ETA: 0s - loss: 0.3817 - accuracy: 0.85 - ETA: 0s - loss: 0.3850 - accuracy: 0.85 - ETA: 0s - loss: 0.3751 - accuracy: 0.85 - ETA: 0s - loss: 0.3715 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3715 - accuracy: 0.8514 - val_loss: 1.0645 - val_accuracy: 0.6866\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 1.00 - ETA: 0s - loss: 0.4353 - accuracy: 0.83 - ETA: 0s - loss: 0.4159 - accuracy: 0.84 - ETA: 0s - loss: 0.3814 - accuracy: 0.84 - ETA: 0s - loss: 0.4313 - accuracy: 0.84 - ETA: 0s - loss: 0.4468 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4433 - accuracy: 0.8309 - val_loss: 0.9073 - val_accuracy: 0.7060\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.93 - ETA: 0s - loss: 0.2915 - accuracy: 0.88 - ETA: 0s - loss: 0.3144 - accuracy: 0.88 - ETA: 0s - loss: 0.3344 - accuracy: 0.88 - ETA: 0s - loss: 0.3544 - accuracy: 0.87 - ETA: 0s - loss: 0.3553 - accuracy: 0.86 - 0s 5ms/step - loss: 0.3614 - accuracy: 0.8656 - val_loss: 0.8120 - val_accuracy: 0.7104\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.84 - ETA: 0s - loss: 0.6544 - accuracy: 0.84 - ETA: 0s - loss: 0.6711 - accuracy: 0.79 - ETA: 0s - loss: 0.6309 - accuracy: 0.79 - ETA: 0s - loss: 0.6402 - accuracy: 0.79 - ETA: 0s - loss: 0.6152 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5903 - accuracy: 0.7992 - val_loss: 1.0591 - val_accuracy: 0.6925\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.90 - ETA: 0s - loss: 0.3356 - accuracy: 0.90 - ETA: 0s - loss: 0.3459 - accuracy: 0.88 - ETA: 0s - loss: 0.3481 - accuracy: 0.87 - ETA: 0s - loss: 0.3779 - accuracy: 0.85 - ETA: 0s - loss: 0.3733 - accuracy: 0.85 - 0s 4ms/step - loss: 0.3700 - accuracy: 0.8529 - val_loss: 1.1421 - val_accuracy: 0.6866\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.84 - ETA: 0s - loss: 0.2986 - accuracy: 0.89 - ETA: 0s - loss: 0.3262 - accuracy: 0.88 - ETA: 0s - loss: 0.3682 - accuracy: 0.86 - ETA: 0s - loss: 0.3726 - accuracy: 0.86 - ETA: 0s - loss: 0.3910 - accuracy: 0.86 - 0s 4ms/step - loss: 0.3922 - accuracy: 0.8611 - val_loss: 1.6486 - val_accuracy: 0.7075\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.87 - ETA: 0s - loss: 0.3050 - accuracy: 0.87 - ETA: 0s - loss: 0.3193 - accuracy: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.87 - ETA: 0s - loss: 0.3160 - accuracy: 0.87 - ETA: 0s - loss: 0.3196 - accuracy: 0.87 - ETA: 0s - loss: 0.3177 - accuracy: 0.87 - 0s 5ms/step - loss: 0.3196 - accuracy: 0.8764 - val_loss: 0.8381 - val_accuracy: 0.7134\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 1.00 - ETA: 0s - loss: 0.3352 - accuracy: 0.88 - ETA: 0s - loss: 0.3096 - accuracy: 0.88 - ETA: 0s - loss: 0.3137 - accuracy: 0.88 - ETA: 0s - loss: 0.3175 - accuracy: 0.89 - ETA: 0s - loss: 0.3091 - accuracy: 0.89 - ETA: 0s - loss: 0.3058 - accuracy: 0.89 - 0s 5ms/step - loss: 0.3078 - accuracy: 0.8929 - val_loss: 1.0755 - val_accuracy: 0.7179\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.90 - ETA: 0s - loss: 0.2866 - accuracy: 0.90 - ETA: 0s - loss: 0.3517 - accuracy: 0.89 - ETA: 0s - loss: 0.3279 - accuracy: 0.90 - ETA: 0s - loss: 0.3329 - accuracy: 0.89 - ETA: 0s - loss: 0.3324 - accuracy: 0.89 - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8951 - val_loss: 1.7200 - val_accuracy: 0.6806\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.96 - ETA: 0s - loss: 0.2643 - accuracy: 0.90 - ETA: 0s - loss: 0.5856 - accuracy: 0.90 - ETA: 0s - loss: 0.6089 - accuracy: 0.89 - ETA: 0s - loss: 0.6442 - accuracy: 0.87 - ETA: 0s - loss: 0.6761 - accuracy: 0.84 - ETA: 0s - loss: 0.7072 - accuracy: 0.81 - 0s 4ms/step - loss: 0.7072 - accuracy: 0.8178 - val_loss: 2.0601 - val_accuracy: 0.6866\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.71 - ETA: 0s - loss: 0.8174 - accuracy: 0.70 - ETA: 0s - loss: 0.7143 - accuracy: 0.73 - ETA: 0s - loss: 0.6867 - accuracy: 0.73 - ETA: 0s - loss: 0.6628 - accuracy: 0.74 - ETA: 0s - loss: 0.6759 - accuracy: 0.74 - ETA: 0s - loss: 0.6965 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6963 - accuracy: 0.7510 - val_loss: 11.4111 - val_accuracy: 0.7164\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.75 - ETA: 0s - loss: 0.9300 - accuracy: 0.74 - ETA: 0s - loss: 0.7818 - accuracy: 0.73 - ETA: 0s - loss: 0.7563 - accuracy: 0.74 - ETA: 0s - loss: 0.7208 - accuracy: 0.72 - ETA: 0s - loss: 0.7553 - accuracy: 0.71 - ETA: 0s - loss: 0.7276 - accuracy: 0.69 - 0s 4ms/step - loss: 0.7245 - accuracy: 0.7010 - val_loss: 3.9872 - val_accuracy: 0.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.65 - ETA: 0s - loss: 0.6775 - accuracy: 0.66 - ETA: 0s - loss: 0.6561 - accuracy: 0.69 - ETA: 0s - loss: 0.6296 - accuracy: 0.70 - ETA: 0s - loss: 0.6285 - accuracy: 0.70 - ETA: 0s - loss: 0.6189 - accuracy: 0.70 - ETA: 0s - loss: 0.6174 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6169 - accuracy: 0.7014 - val_loss: 1.9970 - val_accuracy: 0.5881\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.71 - ETA: 0s - loss: 0.5880 - accuracy: 0.67 - ETA: 0s - loss: 0.6264 - accuracy: 0.67 - ETA: 0s - loss: 0.6093 - accuracy: 0.68 - ETA: 0s - loss: 0.6059 - accuracy: 0.67 - ETA: 0s - loss: 0.5991 - accuracy: 0.68 - ETA: 0s - loss: 0.6016 - accuracy: 0.68 - 0s 4ms/step - loss: 0.6020 - accuracy: 0.6838 - val_loss: 0.8058 - val_accuracy: 0.5642\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.68 - ETA: 0s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.59 - ETA: 0s - loss: 0.6457 - accuracy: 0.60 - ETA: 0s - loss: 0.6546 - accuracy: 0.60 - ETA: 0s - loss: 0.6448 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6407 - accuracy: 0.6103 - val_loss: 1.1549 - val_accuracy: 0.5269\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.65 - ETA: 0s - loss: 0.6113 - accuracy: 0.62 - ETA: 0s - loss: 0.6068 - accuracy: 0.63 - ETA: 0s - loss: 0.6292 - accuracy: 0.62 - ETA: 0s - loss: 0.6440 - accuracy: 0.62 - ETA: 0s - loss: 0.6417 - accuracy: 0.62 - 0s 4ms/step - loss: 0.6425 - accuracy: 0.6178 - val_loss: 0.6698 - val_accuracy: 0.5104\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6664 - accuracy: 0.54 - ETA: 0s - loss: 0.6738 - accuracy: 0.54 - ETA: 0s - loss: 0.6634 - accuracy: 0.54 - ETA: 0s - loss: 0.6533 - accuracy: 0.54 - ETA: 0s - loss: 0.6494 - accuracy: 0.53 - ETA: 0s - loss: 0.6538 - accuracy: 0.54 - 0s 4ms/step - loss: 0.6531 - accuracy: 0.5409 - val_loss: 0.6886 - val_accuracy: 0.5015\n",
      "Epoch 24/50\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7230 - accuracy: 0.53 - ETA: 0s - loss: 0.6211 - accuracy: 0.54 - ETA: 0s - loss: 0.6627 - accuracy: 0.53 - ETA: 0s - loss: 0.6654 - accuracy: 0.54 - ETA: 0s - loss: 0.6629 - accuracy: 0.54 - ETA: 0s - loss: 0.6633 - accuracy: 0.54 - ETA: 0s - loss: 0.6657 - accuracy: 0.5409Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5435 - val_loss: 0.6996 - val_accuracy: 0.4985\n",
      "Epoch 00024: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.71 - ETA: 0s - loss: 2.3228 - accuracy: 0.60 - ETA: 0s - loss: 1.4859 - accuracy: 0.63 - ETA: 0s - loss: 1.1625 - accuracy: 0.67 - ETA: 0s - loss: 1.0198 - accuracy: 0.69 - ETA: 0s - loss: 0.9454 - accuracy: 0.70 - ETA: 0s - loss: 0.8944 - accuracy: 0.70 - 1s 6ms/step - loss: 0.8696 - accuracy: 0.7077 - val_loss: 0.5357 - val_accuracy: 0.7493\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7693 - accuracy: 0.71 - ETA: 0s - loss: 0.5620 - accuracy: 0.71 - ETA: 0s - loss: 0.5824 - accuracy: 0.72 - ETA: 0s - loss: 0.5610 - accuracy: 0.75 - ETA: 0s - loss: 0.5699 - accuracy: 0.75 - ETA: 0s - loss: 0.5745 - accuracy: 0.74 - ETA: 0s - loss: 0.5738 - accuracy: 0.74 - 0s 4ms/step - loss: 0.5752 - accuracy: 0.7447 - val_loss: 0.6086 - val_accuracy: 0.6657\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.59 - ETA: 0s - loss: 0.5095 - accuracy: 0.77 - ETA: 0s - loss: 0.5216 - accuracy: 0.78 - ETA: 0s - loss: 0.5113 - accuracy: 0.78 - ETA: 0s - loss: 0.5458 - accuracy: 0.77 - ETA: 0s - loss: 0.5394 - accuracy: 0.78 - ETA: 0s - loss: 0.5327 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7857 - val_loss: 0.6663 - val_accuracy: 0.7060\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.81 - ETA: 0s - loss: 0.4906 - accuracy: 0.81 - ETA: 0s - loss: 0.4731 - accuracy: 0.81 - ETA: 0s - loss: 0.4675 - accuracy: 0.82 - ETA: 0s - loss: 0.4755 - accuracy: 0.82 - ETA: 0s - loss: 0.4718 - accuracy: 0.82 - ETA: 0s - loss: 0.4781 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4788 - accuracy: 0.8231 - val_loss: 0.6204 - val_accuracy: 0.7269\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.90 - ETA: 0s - loss: 0.3943 - accuracy: 0.86 - ETA: 0s - loss: 0.4594 - accuracy: 0.85 - ETA: 0s - loss: 0.4900 - accuracy: 0.83 - ETA: 0s - loss: 0.4869 - accuracy: 0.82 - ETA: 0s - loss: 0.4704 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4691 - accuracy: 0.8309 - val_loss: 0.7021 - val_accuracy: 0.6896\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.84 - ETA: 0s - loss: 0.4108 - accuracy: 0.86 - ETA: 0s - loss: 0.4985 - accuracy: 0.85 - ETA: 0s - loss: 0.4992 - accuracy: 0.84 - ETA: 0s - loss: 0.4955 - accuracy: 0.83 - ETA: 0s - loss: 0.4843 - accuracy: 0.83 - 0s 4ms/step - loss: 0.4828 - accuracy: 0.8350 - val_loss: 0.6889 - val_accuracy: 0.6866\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.84 - ETA: 0s - loss: 0.5018 - accuracy: 0.82 - ETA: 0s - loss: 0.6731 - accuracy: 0.82 - ETA: 0s - loss: 0.6292 - accuracy: 0.81 - ETA: 0s - loss: 0.6081 - accuracy: 0.81 - ETA: 0s - loss: 0.5944 - accuracy: 0.81 - 0s 4ms/step - loss: 0.6013 - accuracy: 0.8171 - val_loss: 0.7826 - val_accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.75 - ETA: 0s - loss: 0.5591 - accuracy: 0.81 - ETA: 0s - loss: 0.5491 - accuracy: 0.81 - ETA: 0s - loss: 0.5480 - accuracy: 0.81 - ETA: 0s - loss: 0.5418 - accuracy: 0.81 - ETA: 0s - loss: 0.5285 - accuracy: 0.81 - ETA: 0s - loss: 0.5304 - accuracy: 0.81 - 0s 5ms/step - loss: 0.5370 - accuracy: 0.8152 - val_loss: 0.6487 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.81 - ETA: 0s - loss: 0.5358 - accuracy: 0.82 - ETA: 0s - loss: 0.5485 - accuracy: 0.81 - ETA: 0s - loss: 0.5323 - accuracy: 0.81 - ETA: 0s - loss: 0.5251 - accuracy: 0.81 - ETA: 0s - loss: 0.5215 - accuracy: 0.82 - ETA: 0s - loss: 0.5291 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5299 - accuracy: 0.8216 - val_loss: 0.7074 - val_accuracy: 0.7388\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.87 - ETA: 0s - loss: 0.5049 - accuracy: 0.85 - ETA: 0s - loss: 0.5378 - accuracy: 0.84 - ETA: 0s - loss: 0.5449 - accuracy: 0.82 - ETA: 0s - loss: 0.5422 - accuracy: 0.82 - ETA: 0s - loss: 0.5658 - accuracy: 0.80 - ETA: 0s - loss: 0.5616 - accuracy: 0.80 - 0s 5ms/step - loss: 0.5558 - accuracy: 0.8108 - val_loss: 0.7143 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.68 - ETA: 0s - loss: 0.5577 - accuracy: 0.80 - ETA: 0s - loss: 0.5298 - accuracy: 0.80 - ETA: 0s - loss: 0.5117 - accuracy: 0.82 - ETA: 0s - loss: 0.5157 - accuracy: 0.81 - ETA: 0s - loss: 0.5103 - accuracy: 0.82 - ETA: 0s - loss: 0.5086 - accuracy: 0.82 - ETA: 0s - loss: 0.5155 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5155 - accuracy: 0.8205 - val_loss: 0.6414 - val_accuracy: 0.7522\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.81 - ETA: 0s - loss: 0.5277 - accuracy: 0.80 - ETA: 0s - loss: 0.5430 - accuracy: 0.80 - ETA: 0s - loss: 0.5265 - accuracy: 0.81 - ETA: 0s - loss: 0.5194 - accuracy: 0.81 - ETA: 0s - loss: 0.5195 - accuracy: 0.82 - ETA: 0s - loss: 0.5076 - accuracy: 0.82 - 0s 5ms/step - loss: 0.5074 - accuracy: 0.8279 - val_loss: 0.7612 - val_accuracy: 0.7433\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.90 - ETA: 0s - loss: 0.5287 - accuracy: 0.84 - ETA: 0s - loss: 0.5243 - accuracy: 0.83 - ETA: 0s - loss: 0.5237 - accuracy: 0.82 - ETA: 0s - loss: 0.5177 - accuracy: 0.82 - ETA: 0s - loss: 0.5198 - accuracy: 0.82 - ETA: 0s - loss: 0.5150 - accuracy: 0.82 - 0s 4ms/step - loss: 0.5141 - accuracy: 0.8283 - val_loss: 0.7281 - val_accuracy: 0.7328\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7410 - accuracy: 0.68 - ETA: 0s - loss: 0.4492 - accuracy: 0.85 - ETA: 0s - loss: 0.4542 - accuracy: 0.85 - ETA: 0s - loss: 0.4571 - accuracy: 0.85 - ETA: 0s - loss: 0.4664 - accuracy: 0.84 - ETA: 0s - loss: 0.4580 - accuracy: 0.85 - ETA: 0s - loss: 0.4618 - accuracy: 0.84 - 0s 5ms/step - loss: 0.4645 - accuracy: 0.8496 - val_loss: 0.6588 - val_accuracy: 0.7552\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4999 - accuracy: 0.84 - ETA: 0s - loss: 0.4948 - accuracy: 0.84 - ETA: 0s - loss: 0.4849 - accuracy: 0.84 - ETA: 0s - loss: 0.4830 - accuracy: 0.85 - ETA: 0s - loss: 0.4872 - accuracy: 0.85 - ETA: 0s - loss: 0.5007 - accuracy: 0.85 - ETA: 0s - loss: 0.5024 - accuracy: 0.85 - 0s 4ms/step - loss: 0.5036 - accuracy: 0.8514 - val_loss: 1.1594 - val_accuracy: 0.7313\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.75 - ETA: 0s - loss: 0.5457 - accuracy: 0.81 - ETA: 0s - loss: 0.5203 - accuracy: 0.83 - ETA: 0s - loss: 0.4929 - accuracy: 0.84 - ETA: 0s - loss: 0.4803 - accuracy: 0.84 - ETA: 0s - loss: 0.4719 - accuracy: 0.84 - ETA: 0s - loss: 0.4722 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4723 - accuracy: 0.8492 - val_loss: 0.8235 - val_accuracy: 0.7433\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.81 - ETA: 0s - loss: 0.4963 - accuracy: 0.85 - ETA: 0s - loss: 0.4702 - accuracy: 0.85 - ETA: 0s - loss: 0.4813 - accuracy: 0.84 - ETA: 0s - loss: 0.4770 - accuracy: 0.84 - ETA: 0s - loss: 0.4769 - accuracy: 0.84 - ETA: 0s - loss: 0.4869 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4869 - accuracy: 0.8421 - val_loss: 0.8933 - val_accuracy: 0.7224\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.93 - ETA: 0s - loss: 0.4266 - accuracy: 0.87 - ETA: 0s - loss: 0.4606 - accuracy: 0.86 - ETA: 0s - loss: 0.4641 - accuracy: 0.86 - ETA: 0s - loss: 0.4658 - accuracy: 0.85 - ETA: 0s - loss: 0.4568 - accuracy: 0.85 - ETA: 0s - loss: 0.4494 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4494 - accuracy: 0.8578 - val_loss: 0.9329 - val_accuracy: 0.7239\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.81 - ETA: 0s - loss: 0.3677 - accuracy: 0.89 - ETA: 0s - loss: 0.4737 - accuracy: 0.87 - ETA: 0s - loss: 0.4884 - accuracy: 0.85 - ETA: 0s - loss: 0.4977 - accuracy: 0.85 - ETA: 0s - loss: 0.4791 - accuracy: 0.85 - ETA: 0s - loss: 0.4759 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4761 - accuracy: 0.8552 - val_loss: 0.8673 - val_accuracy: 0.7254\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6332 - accuracy: 0.78 - ETA: 0s - loss: 0.4548 - accuracy: 0.86 - ETA: 0s - loss: 0.4997 - accuracy: 0.86 - ETA: 0s - loss: 0.5162 - accuracy: 0.85 - ETA: 0s - loss: 0.5279 - accuracy: 0.84 - ETA: 0s - loss: 0.5123 - accuracy: 0.84 - 0s 4ms/step - loss: 0.5164 - accuracy: 0.8429 - val_loss: 0.7104 - val_accuracy: 0.7448\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.78 - ETA: 0s - loss: 0.4697 - accuracy: 0.84 - ETA: 0s - loss: 0.4502 - accuracy: 0.85 - ETA: 0s - loss: 0.4665 - accuracy: 0.85 - ETA: 0s - loss: 0.4527 - accuracy: 0.86 - ETA: 0s - loss: 0.4490 - accuracy: 0.86 - ETA: 0s - loss: 0.4537 - accuracy: 0.86 - 0s 4ms/step - loss: 0.4544 - accuracy: 0.8634 - val_loss: 0.9300 - val_accuracy: 0.7358\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.90 - ETA: 0s - loss: 0.4261 - accuracy: 0.87 - ETA: 0s - loss: 0.4287 - accuracy: 0.87 - ETA: 0s - loss: 0.4194 - accuracy: 0.88 - ETA: 0s - loss: 0.4254 - accuracy: 0.87 - ETA: 0s - loss: 0.4267 - accuracy: 0.87 - ETA: 0s - loss: 0.4318 - accuracy: 0.87 - 0s 4ms/step - loss: 0.4318 - accuracy: 0.8750 - val_loss: 0.9023 - val_accuracy: 0.7507\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.78 - ETA: 0s - loss: 0.4350 - accuracy: 0.86 - ETA: 0s - loss: 0.4234 - accuracy: 0.87 - ETA: 0s - loss: 0.4529 - accuracy: 0.86 - ETA: 0s - loss: 0.4722 - accuracy: 0.85 - ETA: 0s - loss: 0.4839 - accuracy: 0.85 - ETA: 0s - loss: 0.4734 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4671 - accuracy: 0.8608 - val_loss: 0.9887 - val_accuracy: 0.7418\n",
      "Epoch 24/50\n",
      "76/84 [==========================>...] - ETA: 0s - loss: 0.4477 - accuracy: 0.87 - ETA: 0s - loss: 0.4177 - accuracy: 0.88 - ETA: 0s - loss: 0.4158 - accuracy: 0.88 - ETA: 0s - loss: 0.4269 - accuracy: 0.87 - ETA: 0s - loss: 0.4197 - accuracy: 0.87 - ETA: 0s - loss: 0.4153 - accuracy: 0.87 - ETA: 0s - loss: 0.4475 - accuracy: 0.87 - ETA: 0s - loss: 0.4991 - accuracy: 0.8590Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5256 - accuracy: 0.8458 - val_loss: 0.6160 - val_accuracy: 0.6955\n",
      "Epoch 00024: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1062 - accuracy: 0.50 - ETA: 0s - loss: 2.7145 - accuracy: 0.59 - ETA: 0s - loss: 1.6723 - accuracy: 0.63 - ETA: 0s - loss: 1.2881 - accuracy: 0.66 - ETA: 0s - loss: 1.1136 - accuracy: 0.67 - ETA: 0s - loss: 1.0116 - accuracy: 0.68 - ETA: 0s - loss: 0.9382 - accuracy: 0.69 - 0s 6ms/step - loss: 0.9212 - accuracy: 0.6973 - val_loss: 0.6028 - val_accuracy: 0.7149\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - ETA: 0s - loss: 0.5679 - accuracy: 0.73 - ETA: 0s - loss: 0.5367 - accuracy: 0.75 - ETA: 0s - loss: 0.5509 - accuracy: 0.74 - ETA: 0s - loss: 0.5379 - accuracy: 0.75 - ETA: 0s - loss: 0.5605 - accuracy: 0.75 - ETA: 0s - loss: 0.5604 - accuracy: 0.75 - 0s 4ms/step - loss: 0.5594 - accuracy: 0.7600 - val_loss: 0.7364 - val_accuracy: 0.6373\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5182 - accuracy: 0.75 - ETA: 0s - loss: 0.5164 - accuracy: 0.78 - ETA: 0s - loss: 0.5236 - accuracy: 0.79 - ETA: 0s - loss: 0.5071 - accuracy: 0.80 - ETA: 0s - loss: 0.5076 - accuracy: 0.80 - ETA: 0s - loss: 0.5250 - accuracy: 0.79 - ETA: 0s - loss: 0.5170 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5164 - accuracy: 0.7977 - val_loss: 0.5345 - val_accuracy: 0.7567\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.84 - ETA: 0s - loss: 0.4868 - accuracy: 0.82 - ETA: 0s - loss: 0.5136 - accuracy: 0.81 - ETA: 0s - loss: 0.5354 - accuracy: 0.80 - ETA: 0s - loss: 0.5375 - accuracy: 0.80 - ETA: 0s - loss: 0.5377 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5341 - accuracy: 0.7962 - val_loss: 0.5854 - val_accuracy: 0.6985\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.84 - ETA: 0s - loss: 0.4297 - accuracy: 0.83 - ETA: 0s - loss: 0.5318 - accuracy: 0.81 - ETA: 0s - loss: 0.5343 - accuracy: 0.80 - ETA: 0s - loss: 0.5092 - accuracy: 0.81 - ETA: 0s - loss: 0.5020 - accuracy: 0.81 - ETA: 0s - loss: 0.5145 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5145 - accuracy: 0.8070 - val_loss: 0.6267 - val_accuracy: 0.7313\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.90 - ETA: 0s - loss: 0.4768 - accuracy: 0.84 - ETA: 0s - loss: 0.4985 - accuracy: 0.83 - ETA: 0s - loss: 0.4700 - accuracy: 0.84 - ETA: 0s - loss: 0.4828 - accuracy: 0.83 - ETA: 0s - loss: 0.4727 - accuracy: 0.83 - ETA: 0s - loss: 0.4884 - accuracy: 0.82 - 0s 4ms/step - loss: 0.4886 - accuracy: 0.8294 - val_loss: 0.9340 - val_accuracy: 0.6597\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.71 - ETA: 0s - loss: 1.0052 - accuracy: 0.83 - ETA: 0s - loss: 0.7419 - accuracy: 0.82 - ETA: 0s - loss: 0.6845 - accuracy: 0.80 - ETA: 0s - loss: 0.6182 - accuracy: 0.80 - ETA: 0s - loss: 0.6023 - accuracy: 0.81 - ETA: 0s - loss: 0.6029 - accuracy: 0.80 - 0s 4ms/step - loss: 0.6058 - accuracy: 0.8052 - val_loss: 0.7292 - val_accuracy: 0.7313\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.81 - ETA: 0s - loss: 0.5230 - accuracy: 0.83 - ETA: 0s - loss: 0.5270 - accuracy: 0.82 - ETA: 0s - loss: 0.5255 - accuracy: 0.82 - ETA: 0s - loss: 0.5291 - accuracy: 0.82 - ETA: 0s - loss: 0.5344 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5317 - accuracy: 0.8223 - val_loss: 1.0380 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.87 - ETA: 0s - loss: 0.4878 - accuracy: 0.85 - ETA: 0s - loss: 0.4611 - accuracy: 0.85 - ETA: 0s - loss: 0.4627 - accuracy: 0.85 - ETA: 0s - loss: 0.4656 - accuracy: 0.85 - ETA: 0s - loss: 0.4648 - accuracy: 0.85 - 0s 4ms/step - loss: 0.4670 - accuracy: 0.8541 - val_loss: 0.6035 - val_accuracy: 0.7448\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.90 - ETA: 0s - loss: 0.4888 - accuracy: 0.84 - ETA: 0s - loss: 0.5076 - accuracy: 0.84 - ETA: 0s - loss: 0.4886 - accuracy: 0.84 - ETA: 0s - loss: 0.4864 - accuracy: 0.84 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.4962 - accuracy: 0.83 - ETA: 0s - loss: 0.4867 - accuracy: 0.83 - 0s 5ms/step - loss: 0.4871 - accuracy: 0.8328 - val_loss: 0.6791 - val_accuracy: 0.7418\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6493 - accuracy: 0.81 - ETA: 0s - loss: 0.4179 - accuracy: 0.87 - ETA: 0s - loss: 0.4194 - accuracy: 0.87 - ETA: 0s - loss: 0.4659 - accuracy: 0.86 - ETA: 0s - loss: 0.4876 - accuracy: 0.85 - ETA: 0s - loss: 0.4813 - accuracy: 0.84 - ETA: 0s - loss: 0.4705 - accuracy: 0.84 - 0s 4ms/step - loss: 0.4693 - accuracy: 0.8503 - val_loss: 0.7140 - val_accuracy: 0.7388\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.87 - ETA: 0s - loss: 0.4163 - accuracy: 0.87 - ETA: 0s - loss: 0.4165 - accuracy: 0.87 - ETA: 0s - loss: 0.4114 - accuracy: 0.87 - ETA: 0s - loss: 0.3960 - accuracy: 0.88 - ETA: 0s - loss: 0.4141 - accuracy: 0.88 - ETA: 0s - loss: 0.4306 - accuracy: 0.87 - 0s 5ms/step - loss: 0.4281 - accuracy: 0.8708 - val_loss: 0.8326 - val_accuracy: 0.7358\n",
      "Epoch 13/50\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.3782 - accuracy: 0.90 - ETA: 0s - loss: 0.3853 - accuracy: 0.88 - ETA: 0s - loss: 0.3970 - accuracy: 0.88 - ETA: 0s - loss: 0.4098 - accuracy: 0.87 - ETA: 0s - loss: 0.4177 - accuracy: 0.87 - ETA: 0s - loss: 0.4115 - accuracy: 0.87 - ETA: 0s - loss: 0.4069 - accuracy: 0.8766Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4123 - accuracy: 0.8753 - val_loss: 0.8687 - val_accuracy: 0.7403\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 6ff9f1c05410da4541b4e8c6e0fd04d2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7432835896809896</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.27405196661302966</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 290</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0370 - accuracy: 0.46 - ETA: 0s - loss: 1.9120 - accuracy: 0.58 - ETA: 0s - loss: 1.5211 - accuracy: 0.59 - ETA: 0s - loss: 1.2847 - accuracy: 0.61 - ETA: 0s - loss: 1.1333 - accuracy: 0.63 - 0s 5ms/step - loss: 1.1219 - accuracy: 0.6402 - val_loss: 0.6129 - val_accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8109 - accuracy: 0.59 - ETA: 0s - loss: 0.6213 - accuracy: 0.73 - ETA: 0s - loss: 0.6017 - accuracy: 0.73 - ETA: 0s - loss: 0.6007 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6119 - accuracy: 0.7234 - val_loss: 0.5890 - val_accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3858 - accuracy: 0.93 - ETA: 0s - loss: 0.5892 - accuracy: 0.73 - ETA: 0s - loss: 0.5510 - accuracy: 0.75 - ETA: 0s - loss: 0.5572 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5588 - accuracy: 0.7488 - val_loss: 0.6010 - val_accuracy: 0.7134\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.87 - ETA: 0s - loss: 0.5110 - accuracy: 0.77 - ETA: 0s - loss: 0.4958 - accuracy: 0.79 - ETA: 0s - loss: 0.5263 - accuracy: 0.78 - ETA: 0s - loss: 0.5304 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7872 - val_loss: 0.6362 - val_accuracy: 0.6910\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4086 - accuracy: 0.78 - ETA: 0s - loss: 0.4879 - accuracy: 0.78 - ETA: 0s - loss: 0.4904 - accuracy: 0.79 - ETA: 0s - loss: 0.4808 - accuracy: 0.79 - ETA: 0s - loss: 0.4860 - accuracy: 0.80 - 0s 3ms/step - loss: 0.4949 - accuracy: 0.7999 - val_loss: 0.7130 - val_accuracy: 0.7030\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.75 - ETA: 0s - loss: 0.4635 - accuracy: 0.81 - ETA: 0s - loss: 0.4806 - accuracy: 0.80 - ETA: 0s - loss: 0.4465 - accuracy: 0.81 - ETA: 0s - loss: 0.4558 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8145 - val_loss: 0.7610 - val_accuracy: 0.6970\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.81 - ETA: 0s - loss: 0.5002 - accuracy: 0.81 - ETA: 0s - loss: 0.4796 - accuracy: 0.81 - ETA: 0s - loss: 0.4535 - accuracy: 0.81 - ETA: 0s - loss: 0.4430 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4508 - accuracy: 0.8175 - val_loss: 0.7170 - val_accuracy: 0.7000\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.84 - ETA: 0s - loss: 0.3778 - accuracy: 0.83 - ETA: 0s - loss: 0.3700 - accuracy: 0.83 - ETA: 0s - loss: 0.3764 - accuracy: 0.83 - ETA: 0s - loss: 0.3971 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8380 - val_loss: 0.7089 - val_accuracy: 0.7090\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.87 - ETA: 0s - loss: 0.3787 - accuracy: 0.87 - ETA: 0s - loss: 0.3787 - accuracy: 0.85 - ETA: 0s - loss: 0.3729 - accuracy: 0.84 - ETA: 0s - loss: 0.3902 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8447 - val_loss: 0.6680 - val_accuracy: 0.7045\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.78 - ETA: 0s - loss: 0.3620 - accuracy: 0.83 - ETA: 0s - loss: 0.3302 - accuracy: 0.85 - ETA: 0s - loss: 0.3231 - accuracy: 0.86 - ETA: 0s - loss: 0.3495 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3482 - accuracy: 0.8533 - val_loss: 0.8534 - val_accuracy: 0.7090\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4382 - accuracy: 0.81 - ETA: 0s - loss: 0.3326 - accuracy: 0.85 - ETA: 0s - loss: 0.3489 - accuracy: 0.85 - ETA: 0s - loss: 0.3328 - accuracy: 0.86 - ETA: 0s - loss: 0.3481 - accuracy: 0.85 - 0s 3ms/step - loss: 0.3506 - accuracy: 0.8544 - val_loss: 0.9001 - val_accuracy: 0.7015\n",
      "Epoch 12/50\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.1800 - accuracy: 1.00 - ETA: 0s - loss: 0.2845 - accuracy: 0.85 - ETA: 0s - loss: 0.3229 - accuracy: 0.86 - ETA: 0s - loss: 0.3453 - accuracy: 0.86 - ETA: 0s - loss: 0.3459 - accuracy: 0.8523Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8522 - val_loss: 1.3355 - val_accuracy: 0.6821\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.65 - ETA: 0s - loss: 1.4992 - accuracy: 0.60 - ETA: 0s - loss: 1.3024 - accuracy: 0.60 - ETA: 0s - loss: 1.1317 - accuracy: 0.61 - 0s 4ms/step - loss: 1.0262 - accuracy: 0.6357 - val_loss: 0.5811 - val_accuracy: 0.7224\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4132 - accuracy: 0.87 - ETA: 0s - loss: 0.5361 - accuracy: 0.78 - ETA: 0s - loss: 0.5746 - accuracy: 0.76 - ETA: 0s - loss: 0.5974 - accuracy: 0.74 - ETA: 0s - loss: 0.6001 - accuracy: 0.73 - 0s 3ms/step - loss: 0.5996 - accuracy: 0.7312 - val_loss: 0.6489 - val_accuracy: 0.7090\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.81 - ETA: 0s - loss: 0.5895 - accuracy: 0.76 - ETA: 0s - loss: 0.5715 - accuracy: 0.76 - ETA: 0s - loss: 0.5684 - accuracy: 0.76 - ETA: 0s - loss: 0.5648 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5561 - accuracy: 0.7712 - val_loss: 0.5732 - val_accuracy: 0.7373\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.84 - ETA: 0s - loss: 0.4916 - accuracy: 0.80 - ETA: 0s - loss: 0.4940 - accuracy: 0.80 - ETA: 0s - loss: 0.5057 - accuracy: 0.80 - ETA: 0s - loss: 0.5174 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5189 - accuracy: 0.7984 - val_loss: 0.6170 - val_accuracy: 0.7224\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.78 - ETA: 0s - loss: 0.5379 - accuracy: 0.80 - ETA: 0s - loss: 0.5190 - accuracy: 0.80 - ETA: 0s - loss: 0.5005 - accuracy: 0.81 - ETA: 0s - loss: 0.4981 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4964 - accuracy: 0.8111 - val_loss: 0.6449 - val_accuracy: 0.7134\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.84 - ETA: 0s - loss: 0.5168 - accuracy: 0.84 - ETA: 0s - loss: 0.5122 - accuracy: 0.82 - ETA: 0s - loss: 0.4967 - accuracy: 0.82 - ETA: 0s - loss: 0.4944 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4969 - accuracy: 0.8253 - val_loss: 0.5975 - val_accuracy: 0.7149\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.75 - ETA: 0s - loss: 0.4508 - accuracy: 0.82 - ETA: 0s - loss: 0.4729 - accuracy: 0.82 - ETA: 0s - loss: 0.4583 - accuracy: 0.83 - ETA: 0s - loss: 0.4505 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8354 - val_loss: 0.7459 - val_accuracy: 0.6821\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.87 - ETA: 0s - loss: 0.3707 - accuracy: 0.85 - ETA: 0s - loss: 0.3844 - accuracy: 0.86 - ETA: 0s - loss: 0.4026 - accuracy: 0.85 - ETA: 0s - loss: 0.4047 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4077 - accuracy: 0.8518 - val_loss: 0.7830 - val_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4040 - accuracy: 0.87 - ETA: 0s - loss: 0.3852 - accuracy: 0.88 - ETA: 0s - loss: 0.3918 - accuracy: 0.87 - ETA: 0s - loss: 0.4052 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4215 - accuracy: 0.8626 - val_loss: 0.8537 - val_accuracy: 0.7134\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.84 - ETA: 0s - loss: 0.3714 - accuracy: 0.87 - ETA: 0s - loss: 0.3820 - accuracy: 0.86 - ETA: 0s - loss: 0.4048 - accuracy: 0.86 - ETA: 0s - loss: 0.4044 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4052 - accuracy: 0.8596 - val_loss: 0.9126 - val_accuracy: 0.7194\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.90 - ETA: 0s - loss: 0.3509 - accuracy: 0.89 - ETA: 0s - loss: 0.4134 - accuracy: 0.87 - ETA: 0s - loss: 0.4463 - accuracy: 0.86 - ETA: 0s - loss: 0.4702 - accuracy: 0.85 - 0s 3ms/step - loss: 0.4668 - accuracy: 0.8552 - val_loss: 0.9912 - val_accuracy: 0.7030\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.87 - ETA: 0s - loss: 0.4041 - accuracy: 0.86 - ETA: 0s - loss: 0.4358 - accuracy: 0.85 - ETA: 0s - loss: 0.4078 - accuracy: 0.86 - ETA: 0s - loss: 0.4229 - accuracy: 0.86 - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8619 - val_loss: 0.7441 - val_accuracy: 0.7284\n",
      "Epoch 13/50\n",
      "75/84 [=========================>....] - ETA: 0s - loss: 0.3596 - accuracy: 0.87 - ETA: 0s - loss: 0.3975 - accuracy: 0.87 - ETA: 0s - loss: 0.4140 - accuracy: 0.87 - ETA: 0s - loss: 0.4109 - accuracy: 0.86 - ETA: 0s - loss: 0.4085 - accuracy: 0.8654Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8667 - val_loss: 0.9980 - val_accuracy: 0.6970\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.71 - ETA: 0s - loss: 1.8981 - accuracy: 0.59 - ETA: 0s - loss: 1.3660 - accuracy: 0.62 - ETA: 0s - loss: 1.1460 - accuracy: 0.64 - 0s 4ms/step - loss: 1.0374 - accuracy: 0.6682 - val_loss: 0.5817 - val_accuracy: 0.7119\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.71 - ETA: 0s - loss: 0.5934 - accuracy: 0.75 - ETA: 0s - loss: 0.6078 - accuracy: 0.73 - ETA: 0s - loss: 0.6131 - accuracy: 0.73 - 0s 3ms/step - loss: 0.6125 - accuracy: 0.7331 - val_loss: 0.5479 - val_accuracy: 0.7358\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.87 - ETA: 0s - loss: 0.5433 - accuracy: 0.77 - ETA: 0s - loss: 0.5624 - accuracy: 0.77 - ETA: 0s - loss: 0.5615 - accuracy: 0.77 - ETA: 0s - loss: 0.5540 - accuracy: 0.77 - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7704 - val_loss: 0.5774 - val_accuracy: 0.7493\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.75 - ETA: 0s - loss: 0.5212 - accuracy: 0.78 - ETA: 0s - loss: 0.5347 - accuracy: 0.78 - ETA: 0s - loss: 0.5331 - accuracy: 0.79 - ETA: 0s - loss: 0.5170 - accuracy: 0.79 - 0s 3ms/step - loss: 0.5253 - accuracy: 0.7898 - val_loss: 0.5822 - val_accuracy: 0.7328\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.71 - ETA: 0s - loss: 0.4471 - accuracy: 0.80 - ETA: 0s - loss: 0.4816 - accuracy: 0.79 - ETA: 0s - loss: 0.4885 - accuracy: 0.79 - ETA: 0s - loss: 0.4997 - accuracy: 0.79 - 0s 3ms/step - loss: 0.4997 - accuracy: 0.7917 - val_loss: 0.5746 - val_accuracy: 0.7403\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.87 - ETA: 0s - loss: 0.4739 - accuracy: 0.79 - ETA: 0s - loss: 0.4964 - accuracy: 0.80 - ETA: 0s - loss: 0.5056 - accuracy: 0.80 - ETA: 0s - loss: 0.5037 - accuracy: 0.80 - 0s 3ms/step - loss: 0.5040 - accuracy: 0.8018 - val_loss: 0.8180 - val_accuracy: 0.6806\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.81 - ETA: 0s - loss: 0.4133 - accuracy: 0.81 - ETA: 0s - loss: 0.4561 - accuracy: 0.82 - ETA: 0s - loss: 0.4465 - accuracy: 0.83 - ETA: 0s - loss: 0.4521 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4530 - accuracy: 0.8290 - val_loss: 0.6305 - val_accuracy: 0.7343\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.71 - ETA: 0s - loss: 0.4750 - accuracy: 0.82 - ETA: 0s - loss: 0.4542 - accuracy: 0.82 - ETA: 0s - loss: 0.4509 - accuracy: 0.81 - ETA: 0s - loss: 0.4360 - accuracy: 0.82 - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8249 - val_loss: 0.7804 - val_accuracy: 0.7134\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.96 - ETA: 0s - loss: 0.4087 - accuracy: 0.84 - ETA: 0s - loss: 0.4193 - accuracy: 0.83 - ETA: 0s - loss: 0.4065 - accuracy: 0.83 - ETA: 0s - loss: 0.4048 - accuracy: 0.83 - 0s 3ms/step - loss: 0.4066 - accuracy: 0.8339 - val_loss: 0.6943 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.93 - ETA: 0s - loss: 0.3401 - accuracy: 0.86 - ETA: 0s - loss: 0.3638 - accuracy: 0.86 - ETA: 0s - loss: 0.3868 - accuracy: 0.85 - ETA: 0s - loss: 0.3980 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3884 - accuracy: 0.8485 - val_loss: 1.1804 - val_accuracy: 0.7045\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7324 - accuracy: 0.75 - ETA: 0s - loss: 0.4095 - accuracy: 0.83 - ETA: 0s - loss: 0.4004 - accuracy: 0.84 - ETA: 0s - loss: 0.4047 - accuracy: 0.84 - ETA: 0s - loss: 0.3940 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8470 - val_loss: 0.8167 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.84 - ETA: 0s - loss: 0.3013 - accuracy: 0.87 - ETA: 0s - loss: 0.3121 - accuracy: 0.87 - ETA: 0s - loss: 0.3201 - accuracy: 0.87 - ETA: 0s - loss: 0.3237 - accuracy: 0.88 - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8802 - val_loss: 0.8238 - val_accuracy: 0.6806\n",
      "Epoch 13/50\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.84 - ETA: 0s - loss: 0.2780 - accuracy: 0.88 - ETA: 0s - loss: 0.2984 - accuracy: 0.88 - ETA: 0s - loss: 0.3216 - accuracy: 0.87 - ETA: 0s - loss: 0.3446 - accuracy: 0.8670Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8660 - val_loss: 0.7797 - val_accuracy: 0.7388\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 6a73b357c4b884ffae052f8ca4a9cdfa</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7388059894243876</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.6335213907286303</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 180</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 35</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.68 - ETA: 0s - loss: 1.9978 - accuracy: 0.59 - ETA: 0s - loss: 1.3521 - accuracy: 0.63 - ETA: 0s - loss: 1.1380 - accuracy: 0.62 - ETA: 0s - loss: 1.0368 - accuracy: 0.63 - ETA: 0s - loss: 0.9597 - accuracy: 0.65 - 1s 6ms/step - loss: 0.9134 - accuracy: 0.6611 - val_loss: 0.6114 - val_accuracy: 0.6821\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.78 - ETA: 0s - loss: 0.6497 - accuracy: 0.71 - ETA: 0s - loss: 0.6364 - accuracy: 0.73 - ETA: 0s - loss: 0.6424 - accuracy: 0.73 - ETA: 0s - loss: 0.6366 - accuracy: 0.73 - ETA: 0s - loss: 0.6384 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6528 - accuracy: 0.7088 - val_loss: 0.6508 - val_accuracy: 0.6791\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.75 - ETA: 0s - loss: 0.6235 - accuracy: 0.76 - ETA: 0s - loss: 0.6250 - accuracy: 0.75 - ETA: 0s - loss: 0.6212 - accuracy: 0.75 - ETA: 0s - loss: 0.6328 - accuracy: 0.75 - ETA: 0s - loss: 0.6561 - accuracy: 0.74 - 0s 5ms/step - loss: 0.6548 - accuracy: 0.7443 - val_loss: 0.6258 - val_accuracy: 0.7433\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7395 - accuracy: 0.65 - ETA: 0s - loss: 0.6465 - accuracy: 0.74 - ETA: 0s - loss: 0.6327 - accuracy: 0.76 - ETA: 0s - loss: 0.6612 - accuracy: 0.75 - ETA: 0s - loss: 0.6575 - accuracy: 0.75 - ETA: 0s - loss: 0.6561 - accuracy: 0.75 - ETA: 0s - loss: 0.6536 - accuracy: 0.75 - 0s 5ms/step - loss: 0.6527 - accuracy: 0.7540 - val_loss: 0.8109 - val_accuracy: 0.7448\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.87 - ETA: 0s - loss: 0.6162 - accuracy: 0.79 - ETA: 0s - loss: 0.6762 - accuracy: 0.74 - ETA: 0s - loss: 0.6582 - accuracy: 0.74 - ETA: 0s - loss: 0.6578 - accuracy: 0.75 - ETA: 0s - loss: 0.6530 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6540 - accuracy: 0.7518 - val_loss: 0.6197 - val_accuracy: 0.7164\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.68 - ETA: 0s - loss: 0.6331 - accuracy: 0.75 - ETA: 0s - loss: 0.6394 - accuracy: 0.76 - ETA: 0s - loss: 0.6419 - accuracy: 0.76 - ETA: 0s - loss: 0.6531 - accuracy: 0.76 - ETA: 0s - loss: 0.6514 - accuracy: 0.76 - ETA: 0s - loss: 0.6439 - accuracy: 0.76 - ETA: 0s - loss: 0.6475 - accuracy: 0.75 - 0s 6ms/step - loss: 0.6433 - accuracy: 0.7596 - val_loss: 0.6386 - val_accuracy: 0.7373\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7083 - accuracy: 0.68 - ETA: 0s - loss: 0.6488 - accuracy: 0.73 - ETA: 0s - loss: 0.6319 - accuracy: 0.76 - ETA: 0s - loss: 0.6331 - accuracy: 0.76 - ETA: 0s - loss: 0.6381 - accuracy: 0.75 - ETA: 0s - loss: 0.6542 - accuracy: 0.75 - ETA: 0s - loss: 0.6589 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6583 - accuracy: 0.7514 - val_loss: 0.6522 - val_accuracy: 0.7418\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.78 - ETA: 0s - loss: 0.5705 - accuracy: 0.80 - ETA: 0s - loss: 0.6127 - accuracy: 0.77 - ETA: 0s - loss: 0.6325 - accuracy: 0.75 - ETA: 0s - loss: 0.6382 - accuracy: 0.74 - ETA: 0s - loss: 0.6422 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6427 - accuracy: 0.7473 - val_loss: 0.6452 - val_accuracy: 0.7299\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.90 - ETA: 0s - loss: 0.5787 - accuracy: 0.79 - ETA: 0s - loss: 0.6092 - accuracy: 0.77 - ETA: 0s - loss: 0.6261 - accuracy: 0.76 - ETA: 0s - loss: 0.6362 - accuracy: 0.75 - ETA: 0s - loss: 0.6362 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6380 - accuracy: 0.7484 - val_loss: 0.6638 - val_accuracy: 0.7328\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.68 - ETA: 0s - loss: 0.6399 - accuracy: 0.74 - ETA: 0s - loss: 0.6257 - accuracy: 0.75 - ETA: 0s - loss: 0.6302 - accuracy: 0.75 - ETA: 0s - loss: 0.6300 - accuracy: 0.75 - ETA: 0s - loss: 0.6182 - accuracy: 0.76 - ETA: 0s - loss: 0.6214 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6281 - accuracy: 0.7585 - val_loss: 0.6327 - val_accuracy: 0.7433\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.84 - ETA: 0s - loss: 0.6365 - accuracy: 0.77 - ETA: 0s - loss: 0.6471 - accuracy: 0.75 - ETA: 0s - loss: 0.6467 - accuracy: 0.74 - ETA: 0s - loss: 0.6439 - accuracy: 0.74 - ETA: 0s - loss: 0.6436 - accuracy: 0.74 - ETA: 0s - loss: 0.6429 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6466 - accuracy: 0.7417 - val_loss: 0.6427 - val_accuracy: 0.7299\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.65 - ETA: 0s - loss: 0.6513 - accuracy: 0.73 - ETA: 0s - loss: 0.6438 - accuracy: 0.73 - ETA: 0s - loss: 0.6423 - accuracy: 0.74 - ETA: 0s - loss: 0.6394 - accuracy: 0.74 - ETA: 0s - loss: 0.6361 - accuracy: 0.74 - ETA: 0s - loss: 0.6351 - accuracy: 0.74 - 0s 4ms/step - loss: 0.6351 - accuracy: 0.7473 - val_loss: 0.6378 - val_accuracy: 0.7239\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.71 - ETA: 0s - loss: 0.6638 - accuracy: 0.71 - ETA: 0s - loss: 0.6461 - accuracy: 0.73 - ETA: 0s - loss: 0.6358 - accuracy: 0.74 - ETA: 0s - loss: 0.6629 - accuracy: 0.74 - ETA: 0s - loss: 0.6615 - accuracy: 0.74 - ETA: 0s - loss: 0.6639 - accuracy: 0.73 - 0s 4ms/step - loss: 0.6650 - accuracy: 0.7357 - val_loss: 0.6802 - val_accuracy: 0.6955\n",
      "Epoch 14/50\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.6561 - accuracy: 0.75 - ETA: 0s - loss: 0.7346 - accuracy: 0.58 - ETA: 0s - loss: 0.7123 - accuracy: 0.43 - ETA: 0s - loss: 0.7034 - accuracy: 0.46 - ETA: 0s - loss: 0.6980 - accuracy: 0.52 - ETA: 0s - loss: 0.6948 - accuracy: 0.56 - ETA: 0s - loss: 0.6976 - accuracy: 0.5817Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.5786 - val_loss: 0.7156 - val_accuracy: 0.3015\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8325 - accuracy: 0.46 - ETA: 0s - loss: 1.5729 - accuracy: 0.58 - ETA: 0s - loss: 1.1674 - accuracy: 0.61 - ETA: 0s - loss: 1.0451 - accuracy: 0.62 - ETA: 0s - loss: 0.9803 - accuracy: 0.61 - ETA: 0s - loss: 0.9315 - accuracy: 0.61 - ETA: 0s - loss: 0.8939 - accuracy: 0.63 - 0s 6ms/step - loss: 0.8833 - accuracy: 0.6331 - val_loss: 0.6236 - val_accuracy: 0.7090\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5983 - accuracy: 0.68 - ETA: 0s - loss: 0.6208 - accuracy: 0.67 - ETA: 0s - loss: 0.6850 - accuracy: 0.67 - ETA: 0s - loss: 0.6458 - accuracy: 0.71 - ETA: 0s - loss: 0.6518 - accuracy: 0.71 - ETA: 0s - loss: 0.6532 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6518 - accuracy: 0.7100 - val_loss: 0.6051 - val_accuracy: 0.7433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.84 - ETA: 0s - loss: 0.6277 - accuracy: 0.75 - ETA: 0s - loss: 0.6109 - accuracy: 0.75 - ETA: 0s - loss: 0.6273 - accuracy: 0.75 - ETA: 0s - loss: 0.6283 - accuracy: 0.75 - ETA: 0s - loss: 0.6275 - accuracy: 0.75 - 0s 4ms/step - loss: 0.6295 - accuracy: 0.7518 - val_loss: 0.5848 - val_accuracy: 0.7343\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.75 - ETA: 0s - loss: 0.5904 - accuracy: 0.77 - ETA: 0s - loss: 0.5902 - accuracy: 0.76 - ETA: 0s - loss: 0.5991 - accuracy: 0.77 - ETA: 0s - loss: 0.5980 - accuracy: 0.76 - ETA: 0s - loss: 0.6000 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5982 - accuracy: 0.7701 - val_loss: 0.5890 - val_accuracy: 0.7299\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.75 - ETA: 0s - loss: 0.5559 - accuracy: 0.80 - ETA: 0s - loss: 0.5576 - accuracy: 0.80 - ETA: 0s - loss: 0.5608 - accuracy: 0.80 - ETA: 0s - loss: 0.5638 - accuracy: 0.80 - ETA: 0s - loss: 0.5701 - accuracy: 0.79 - ETA: 0s - loss: 0.5761 - accuracy: 0.79 - 0s 5ms/step - loss: 0.5737 - accuracy: 0.7951 - val_loss: 0.5842 - val_accuracy: 0.7522\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6113 - accuracy: 0.78 - ETA: 0s - loss: 0.5861 - accuracy: 0.77 - ETA: 0s - loss: 0.6248 - accuracy: 0.77 - ETA: 0s - loss: 0.6059 - accuracy: 0.78 - ETA: 0s - loss: 0.6005 - accuracy: 0.78 - ETA: 0s - loss: 0.5802 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5940 - accuracy: 0.7921 - val_loss: 0.5737 - val_accuracy: 0.7433\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.84 - ETA: 0s - loss: 0.5598 - accuracy: 0.78 - ETA: 0s - loss: 0.5460 - accuracy: 0.79 - ETA: 0s - loss: 0.5559 - accuracy: 0.78 - ETA: 0s - loss: 0.5553 - accuracy: 0.79 - ETA: 0s - loss: 0.5633 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5663 - accuracy: 0.7969 - val_loss: 0.6144 - val_accuracy: 0.7269\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.84 - ETA: 0s - loss: 0.5063 - accuracy: 0.84 - ETA: 0s - loss: 0.5222 - accuracy: 0.83 - ETA: 0s - loss: 0.5656 - accuracy: 0.81 - ETA: 0s - loss: 0.5625 - accuracy: 0.81 - ETA: 0s - loss: 0.5780 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5713 - accuracy: 0.8134 - val_loss: 0.6116 - val_accuracy: 0.7224\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.87 - ETA: 0s - loss: 0.6087 - accuracy: 0.78 - ETA: 0s - loss: 0.5757 - accuracy: 0.80 - ETA: 0s - loss: 0.6053 - accuracy: 0.79 - ETA: 0s - loss: 0.6222 - accuracy: 0.79 - ETA: 0s - loss: 0.6235 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6336 - accuracy: 0.7921 - val_loss: 0.5891 - val_accuracy: 0.7254\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.84 - ETA: 0s - loss: 0.6481 - accuracy: 0.78 - ETA: 0s - loss: 0.5997 - accuracy: 0.81 - ETA: 0s - loss: 0.5940 - accuracy: 0.80 - ETA: 0s - loss: 0.6053 - accuracy: 0.80 - ETA: 0s - loss: 0.6045 - accuracy: 0.80 - ETA: 0s - loss: 0.5933 - accuracy: 0.80 - 0s 4ms/step - loss: 0.5956 - accuracy: 0.8029 - val_loss: 0.6435 - val_accuracy: 0.7224\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4677 - accuracy: 0.87 - ETA: 0s - loss: 0.6491 - accuracy: 0.82 - ETA: 0s - loss: 0.6170 - accuracy: 0.80 - ETA: 0s - loss: 0.6732 - accuracy: 0.79 - ETA: 0s - loss: 0.6522 - accuracy: 0.79 - ETA: 0s - loss: 0.7717 - accuracy: 0.78 - ETA: 0s - loss: 0.7832 - accuracy: 0.78 - 0s 4ms/step - loss: 0.7776 - accuracy: 0.7824 - val_loss: 1.1496 - val_accuracy: 0.7313\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 2.1552 - accuracy: 0.81 - ETA: 0s - loss: 0.8068 - accuracy: 0.79 - ETA: 0s - loss: 0.8486 - accuracy: 0.76 - ETA: 0s - loss: 0.8210 - accuracy: 0.71 - ETA: 0s - loss: 0.7853 - accuracy: 0.66 - ETA: 0s - loss: 0.7708 - accuracy: 0.63 - ETA: 0s - loss: 0.7551 - accuracy: 0.60 - 0s 4ms/step - loss: 0.7550 - accuracy: 0.6040 - val_loss: 0.7471 - val_accuracy: 0.6045\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6444 - accuracy: 0.53 - ETA: 0s - loss: 0.6521 - accuracy: 0.46 - ETA: 0s - loss: 0.6620 - accuracy: 0.48 - ETA: 0s - loss: 0.6749 - accuracy: 0.48 - ETA: 0s - loss: 0.6747 - accuracy: 0.48 - ETA: 0s - loss: 0.6746 - accuracy: 0.48 - 0s 4ms/step - loss: 0.6766 - accuracy: 0.4841 - val_loss: 0.6684 - val_accuracy: 0.6060\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.37 - ETA: 0s - loss: 0.6830 - accuracy: 0.51 - ETA: 0s - loss: 0.6696 - accuracy: 0.54 - ETA: 0s - loss: 0.6661 - accuracy: 0.53 - ETA: 0s - loss: 0.6706 - accuracy: 0.52 - ETA: 0s - loss: 0.6727 - accuracy: 0.53 - 0s 4ms/step - loss: 0.6756 - accuracy: 0.5398 - val_loss: 0.6778 - val_accuracy: 0.6119\n",
      "Epoch 15/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.7120 - accuracy: 0.59 - ETA: 0s - loss: 0.6797 - accuracy: 0.52 - ETA: 0s - loss: 0.6768 - accuracy: 0.50 - ETA: 0s - loss: 0.6804 - accuracy: 0.50 - ETA: 0s - loss: 0.6766 - accuracy: 0.51 - ETA: 0s - loss: 0.6769 - accuracy: 0.5190Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5147 - val_loss: 0.6796 - val_accuracy: 0.5433\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7386 - accuracy: 0.65 - ETA: 0s - loss: 1.8188 - accuracy: 0.54 - ETA: 0s - loss: 1.3594 - accuracy: 0.57 - ETA: 0s - loss: 1.1414 - accuracy: 0.58 - ETA: 0s - loss: 1.0495 - accuracy: 0.60 - ETA: 0s - loss: 0.9869 - accuracy: 0.60 - ETA: 0s - loss: 0.9339 - accuracy: 0.61 - 0s 6ms/step - loss: 0.9337 - accuracy: 0.6122 - val_loss: 0.6105 - val_accuracy: 0.6955\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.78 - ETA: 0s - loss: 0.6689 - accuracy: 0.67 - ETA: 0s - loss: 0.6876 - accuracy: 0.68 - ETA: 0s - loss: 0.6791 - accuracy: 0.68 - ETA: 0s - loss: 0.6619 - accuracy: 0.67 - ETA: 0s - loss: 0.6557 - accuracy: 0.69 - ETA: 0s - loss: 0.6526 - accuracy: 0.70 - 0s 4ms/step - loss: 0.6518 - accuracy: 0.7032 - val_loss: 0.5846 - val_accuracy: 0.7060\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.59 - ETA: 0s - loss: 0.5998 - accuracy: 0.70 - ETA: 0s - loss: 0.6530 - accuracy: 0.70 - ETA: 0s - loss: 0.6451 - accuracy: 0.70 - ETA: 0s - loss: 0.6568 - accuracy: 0.70 - ETA: 0s - loss: 0.6471 - accuracy: 0.71 - 0s 4ms/step - loss: 0.6544 - accuracy: 0.7156 - val_loss: 0.5898 - val_accuracy: 0.7045\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.87 - ETA: 0s - loss: 0.5923 - accuracy: 0.76 - ETA: 0s - loss: 0.6084 - accuracy: 0.76 - ETA: 0s - loss: 0.5827 - accuracy: 0.77 - ETA: 0s - loss: 0.5947 - accuracy: 0.76 - ETA: 0s - loss: 0.5938 - accuracy: 0.76 - 0s 4ms/step - loss: 0.5956 - accuracy: 0.7712 - val_loss: 0.6017 - val_accuracy: 0.7254\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.87 - ETA: 0s - loss: 0.5456 - accuracy: 0.80 - ETA: 0s - loss: 0.5661 - accuracy: 0.79 - ETA: 0s - loss: 0.5607 - accuracy: 0.80 - ETA: 0s - loss: 0.5872 - accuracy: 0.78 - ETA: 0s - loss: 0.5835 - accuracy: 0.79 - 0s 4ms/step - loss: 0.5836 - accuracy: 0.7917 - val_loss: 0.5994 - val_accuracy: 0.7269\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.71 - ETA: 0s - loss: 0.5925 - accuracy: 0.77 - ETA: 0s - loss: 0.5677 - accuracy: 0.78 - ETA: 0s - loss: 0.5922 - accuracy: 0.78 - ETA: 0s - loss: 0.5833 - accuracy: 0.78 - ETA: 0s - loss: 0.5821 - accuracy: 0.78 - 0s 4ms/step - loss: 0.5823 - accuracy: 0.7850 - val_loss: 0.5726 - val_accuracy: 0.7358\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.96 - ETA: 0s - loss: 0.5762 - accuracy: 0.80 - ETA: 0s - loss: 0.6154 - accuracy: 0.78 - ETA: 0s - loss: 0.5936 - accuracy: 0.79 - ETA: 0s - loss: 0.5852 - accuracy: 0.80 - ETA: 0s - loss: 0.6057 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6070 - accuracy: 0.7928 - val_loss: 0.6112 - val_accuracy: 0.6985\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.78 - ETA: 0s - loss: 0.6725 - accuracy: 0.73 - ETA: 0s - loss: 0.6439 - accuracy: 0.76 - ETA: 0s - loss: 0.6428 - accuracy: 0.76 - ETA: 0s - loss: 0.6313 - accuracy: 0.76 - ETA: 0s - loss: 0.6210 - accuracy: 0.77 - 0s 4ms/step - loss: 0.6106 - accuracy: 0.7805 - val_loss: 0.5994 - val_accuracy: 0.7433\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5104 - accuracy: 0.84 - ETA: 0s - loss: 0.5397 - accuracy: 0.82 - ETA: 0s - loss: 0.6773 - accuracy: 0.80 - ETA: 0s - loss: 0.6739 - accuracy: 0.79 - ETA: 0s - loss: 0.6615 - accuracy: 0.79 - ETA: 0s - loss: 0.6498 - accuracy: 0.79 - ETA: 0s - loss: 0.6385 - accuracy: 0.79 - 0s 4ms/step - loss: 0.6365 - accuracy: 0.7925 - val_loss: 0.6349 - val_accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.90 - ETA: 0s - loss: 0.5796 - accuracy: 0.81 - ETA: 0s - loss: 0.6579 - accuracy: 0.80 - ETA: 0s - loss: 0.6363 - accuracy: 0.80 - ETA: 0s - loss: 0.6390 - accuracy: 0.79 - ETA: 0s - loss: 0.6596 - accuracy: 0.78 - ETA: 0s - loss: 0.6417 - accuracy: 0.79 - 0s 5ms/step - loss: 0.6403 - accuracy: 0.7884 - val_loss: 0.6174 - val_accuracy: 0.7373\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.81 - ETA: 0s - loss: 0.5766 - accuracy: 0.78 - ETA: 0s - loss: 0.5765 - accuracy: 0.79 - ETA: 0s - loss: 0.9036 - accuracy: 0.78 - ETA: 0s - loss: 0.9532 - accuracy: 0.76 - ETA: 0s - loss: 0.8972 - accuracy: 0.75 - ETA: 0s - loss: 0.8699 - accuracy: 0.72 - 0s 4ms/step - loss: 0.8699 - accuracy: 0.7245 - val_loss: 0.6658 - val_accuracy: 0.7254\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.75 - ETA: 0s - loss: 0.6684 - accuracy: 0.72 - ETA: 0s - loss: 0.6698 - accuracy: 0.72 - ETA: 0s - loss: 0.6786 - accuracy: 0.71 - ETA: 0s - loss: 0.6852 - accuracy: 0.67 - ETA: 0s - loss: 0.6801 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6834 - accuracy: 0.6125 - val_loss: 0.6728 - val_accuracy: 0.7015\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.71 - ETA: 0s - loss: 0.6845 - accuracy: 0.70 - ETA: 0s - loss: 0.6822 - accuracy: 0.71 - ETA: 0s - loss: 0.6864 - accuracy: 0.70 - ETA: 0s - loss: 0.6894 - accuracy: 0.68 - ETA: 0s - loss: 0.6884 - accuracy: 0.60 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5823 - val_loss: 2.0619 - val_accuracy: 0.7119\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.8136 - accuracy: 0.68 - ETA: 0s - loss: 0.7711 - accuracy: 0.70 - ETA: 0s - loss: 0.7260 - accuracy: 0.70 - ETA: 0s - loss: 0.7081 - accuracy: 0.71 - ETA: 0s - loss: 0.7137 - accuracy: 0.66 - ETA: 0s - loss: 0.7097 - accuracy: 0.59 - ETA: 0s - loss: 0.7018 - accuracy: 0.56 - 0s 4ms/step - loss: 0.7018 - accuracy: 0.5655 - val_loss: 1.0199 - val_accuracy: 0.6896\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.62 - ETA: 0s - loss: 0.8120 - accuracy: 0.70 - ETA: 0s - loss: 0.7561 - accuracy: 0.69 - ETA: 0s - loss: 0.7326 - accuracy: 0.70 - ETA: 0s - loss: 0.7241 - accuracy: 0.69 - ETA: 0s - loss: 0.7162 - accuracy: 0.68 - 0s 4ms/step - loss: 0.7136 - accuracy: 0.6846 - val_loss: 0.7006 - val_accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.68 - ETA: 0s - loss: 0.6780 - accuracy: 0.45 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6920 - accuracy: 0.61 - ETA: 0s - loss: 0.6881 - accuracy: 0.58 - ETA: 0s - loss: 0.6877 - accuracy: 0.60 - 0s 4ms/step - loss: 0.6922 - accuracy: 0.6092 - val_loss: 0.6938 - val_accuracy: 0.3045\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.31 - ETA: 0s - loss: 0.6760 - accuracy: 0.27 - ETA: 0s - loss: 0.6939 - accuracy: 0.35 - ETA: 0s - loss: 0.6978 - accuracy: 0.34 - ETA: 0s - loss: 0.7026 - accuracy: 0.34 - ETA: 0s - loss: 0.6966 - accuracy: 0.32 - ETA: 0s - loss: 0.6943 - accuracy: 0.34 - 0s 4ms/step - loss: 0.6940 - accuracy: 0.3580 - val_loss: 0.6834 - val_accuracy: 0.6970\n",
      "Epoch 18/50\n",
      "74/84 [=========================>....] - ETA: 0s - loss: 0.7241 - accuracy: 0.65 - ETA: 0s - loss: 0.7154 - accuracy: 0.66 - ETA: 0s - loss: 0.7062 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.47 - ETA: 0s - loss: 0.6956 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.5625Restoring model weights from the end of the best epoch.\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5767 - val_loss: 0.6880 - val_accuracy: 0.6970\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 294d4d67c233e81ff13c1dfa03ba24fe</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7467661698659261</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7838882821336427</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 85</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in ./ale_snp_scaled</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective(name='val_accuracy', direction='max')</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: d74032051361f6c133412f6be1395099</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7641791105270386</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7483283416535789</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 170</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 420</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: cc31ce1c31e660768b7ec9d3570fb875</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7631840904553732</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8001698624704281</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 270</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 175</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: b69a375adbf352c6790b20ff2128d808</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7621890505154928</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.550773413066413</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 330</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 65</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 7821731c10bca5148d0a653205a79799</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7616915504137675</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8244673724051693</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 190</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 150</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f65e9317b55745aaa9b6f2bf18a2234e</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7616915504137675</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7464332393964828</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 390</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 135</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f9afa533d51204f9aa52d0fed8a0b7dd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7597015102704366</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.312365988850128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 440</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 450</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 110</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 260</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 40</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 18efa8d9c189a9bf687ada8ec573c897</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7597015102704366</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8237770244597469</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 300</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 230</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 380</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 370</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 85</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 1b7baa97414c204a3f386bd76880e3f6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064703305563</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.8838076771990834</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 280</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 210</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 490</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 130</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 519c339adbed828597a07be2163b5f58</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064703305563</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.7955469839600595</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 340</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 400</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 460</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 410</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: bfede7fdb4a63b5944940f6dff23f1a2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7587064504623413</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-num_layers_big: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.9206970548274799</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 140</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 250</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_small: 45</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
